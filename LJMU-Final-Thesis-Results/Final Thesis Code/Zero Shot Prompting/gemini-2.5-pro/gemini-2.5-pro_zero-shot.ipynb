{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMOVpY/lwR440by4yKjlrDd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"78edafe28098478aafc8ef34fc13a851":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e7b10ff64744c96b11e065642b3a24a","IPY_MODEL_889e4dd284be4dde8886b580f7939973","IPY_MODEL_d95316828deb4e18a5843d712d900e6a"],"layout":"IPY_MODEL_a6684269945a436abaa452193e9b2d11"}},"9e7b10ff64744c96b11e065642b3a24a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42a5f394f9d446a49f73b89b0967f706","placeholder":"​","style":"IPY_MODEL_d83799cc4f6f4d55a67f2a30defd6d68","value":"tokenizer_config.json: 100%"}},"889e4dd284be4dde8886b580f7939973":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b75040e6b84f4b3eb958f0bd86cecb35","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb02230b88b64ce4814c96662eafa8e4","value":25}},"d95316828deb4e18a5843d712d900e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f0c15ecb0d1400597a9418472d2acd2","placeholder":"​","style":"IPY_MODEL_3951138979c3441aaaadff6517faa877","value":" 25.0/25.0 [00:00&lt;00:00, 2.46kB/s]"}},"a6684269945a436abaa452193e9b2d11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42a5f394f9d446a49f73b89b0967f706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d83799cc4f6f4d55a67f2a30defd6d68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b75040e6b84f4b3eb958f0bd86cecb35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb02230b88b64ce4814c96662eafa8e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f0c15ecb0d1400597a9418472d2acd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3951138979c3441aaaadff6517faa877":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5206b7cd3c8e410a8d725b80039bb932":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0b34d6e150144d3b5d51ad9d2297483","IPY_MODEL_b96106c198e142939b9568af681a89ee","IPY_MODEL_b2a327c416f34ee7807b5aaf4b894881"],"layout":"IPY_MODEL_d48c5127662c46aa94e5cfb5901028f4"}},"e0b34d6e150144d3b5d51ad9d2297483":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39ab8223574243c8a7d80e3b254366f7","placeholder":"​","style":"IPY_MODEL_9de2a287a3d94f3ebe85a0e4ba5023ac","value":"config.json: 100%"}},"b96106c198e142939b9568af681a89ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09970ff4c30f41b4a0f4db2f866512d0","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfebec6dfb504fb891fdd621bf9f6bc4","value":482}},"b2a327c416f34ee7807b5aaf4b894881":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b7cca9cde634e22bc490e8224b9610f","placeholder":"​","style":"IPY_MODEL_e2f249ce0c2b49a8a50caf030cfa7270","value":" 482/482 [00:00&lt;00:00, 37.0kB/s]"}},"d48c5127662c46aa94e5cfb5901028f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39ab8223574243c8a7d80e3b254366f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9de2a287a3d94f3ebe85a0e4ba5023ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09970ff4c30f41b4a0f4db2f866512d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfebec6dfb504fb891fdd621bf9f6bc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b7cca9cde634e22bc490e8224b9610f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2f249ce0c2b49a8a50caf030cfa7270":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0bada71a22947f086578455c9b41686":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49f121b47cc14409a69c29d3b965a8cb","IPY_MODEL_ec28feb9a6584aefac95e4b6dd231e97","IPY_MODEL_ce90a96701f3460ab70509e6548e5b60"],"layout":"IPY_MODEL_4bc94db12ce34976a870d86bfc05323f"}},"49f121b47cc14409a69c29d3b965a8cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85966a6721164bd08a68f0c94aa09086","placeholder":"​","style":"IPY_MODEL_e0e297bd000245a087e0b3f152012e0c","value":"vocab.json: 100%"}},"ec28feb9a6584aefac95e4b6dd231e97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd7912f6c49c4375b955b1b536b659cf","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0e52ce31ed04a3f994e8915e04b9602","value":898823}},"ce90a96701f3460ab70509e6548e5b60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_969ba69bdff74d59b7fc36d26c945614","placeholder":"​","style":"IPY_MODEL_a0f7d40f5e3e455186accb5c32119c1c","value":" 899k/899k [00:00&lt;00:00, 14.6MB/s]"}},"4bc94db12ce34976a870d86bfc05323f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85966a6721164bd08a68f0c94aa09086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0e297bd000245a087e0b3f152012e0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd7912f6c49c4375b955b1b536b659cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0e52ce31ed04a3f994e8915e04b9602":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"969ba69bdff74d59b7fc36d26c945614":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f7d40f5e3e455186accb5c32119c1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0480596a228419baaf9ff879bb05b14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd16f540384b4c71a8cbf6cb3a482b6a","IPY_MODEL_928ee214deee4025a5ecbd764d45d1b7","IPY_MODEL_ef5b36595f8647268c805aabb3e15902"],"layout":"IPY_MODEL_c0469c082f2a4b2486350e40ea1f3935"}},"cd16f540384b4c71a8cbf6cb3a482b6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_054c802ca3d44fcfa6872513f4200d9b","placeholder":"​","style":"IPY_MODEL_545242f5ac9841668bf5831ca3cd9d92","value":"merges.txt: 100%"}},"928ee214deee4025a5ecbd764d45d1b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5706092ce20640ba83f0aeb0d17a4b8f","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fb8585d02a249e0947f13e57737e241","value":456318}},"ef5b36595f8647268c805aabb3e15902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4abce1f7317b4d118f45ac5b37f94ca9","placeholder":"​","style":"IPY_MODEL_f2d4c5e9e4d44ab0bc15345db0cf4c62","value":" 456k/456k [00:00&lt;00:00, 35.4MB/s]"}},"c0469c082f2a4b2486350e40ea1f3935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"054c802ca3d44fcfa6872513f4200d9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"545242f5ac9841668bf5831ca3cd9d92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5706092ce20640ba83f0aeb0d17a4b8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fb8585d02a249e0947f13e57737e241":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4abce1f7317b4d118f45ac5b37f94ca9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2d4c5e9e4d44ab0bc15345db0cf4c62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14fc9cdfa39b4eeda87d575a563c0153":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_733a2b7148e346bcabb6834827a04d0d","IPY_MODEL_c28fbfe177c64b15970d77f0041a12ab","IPY_MODEL_8ab0d19a8b874474a91132734a9e8349"],"layout":"IPY_MODEL_35375efc4faf493088d44dae396540ff"}},"733a2b7148e346bcabb6834827a04d0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_775a4a59085f472fb308ae319416dc86","placeholder":"​","style":"IPY_MODEL_e9fac8773e9c46ccb533b5398a26bd58","value":"tokenizer.json: 100%"}},"c28fbfe177c64b15970d77f0041a12ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b72c6288e8b4a538488717ef829e683","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6169c2e5a334ecb8bb45c6e35c2b7a1","value":1355863}},"8ab0d19a8b874474a91132734a9e8349":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33be5b214df84fb188ae156404f2bc86","placeholder":"​","style":"IPY_MODEL_0eccd088afa144a09f052e3c887a0caf","value":" 1.36M/1.36M [00:00&lt;00:00, 80.5MB/s]"}},"35375efc4faf493088d44dae396540ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"775a4a59085f472fb308ae319416dc86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9fac8773e9c46ccb533b5398a26bd58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b72c6288e8b4a538488717ef829e683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6169c2e5a334ecb8bb45c6e35c2b7a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33be5b214df84fb188ae156404f2bc86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eccd088afa144a09f052e3c887a0caf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2745938dfb2b4dfda29dcdb382bb5b99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e527324dd8ec45018e9daf2882ab0ac6","IPY_MODEL_43297b4e6e524d11bf2454cf197c61ca","IPY_MODEL_2ec95dc1731c4cb5b783dbd76c2de831"],"layout":"IPY_MODEL_baac219482704db69bf75c35c776b3c4"}},"e527324dd8ec45018e9daf2882ab0ac6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc3ac1dc8101427ca360efc834e698b9","placeholder":"​","style":"IPY_MODEL_687fc8936e5b48ad83aa3beeb2530505","value":"model.safetensors: 100%"}},"43297b4e6e524d11bf2454cf197c61ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ac7bd7d094a435ca8483987d4ac81c8","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87a2d4679b3245b38c2d6dc1410d3396","value":1421700479}},"2ec95dc1731c4cb5b783dbd76c2de831":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd47ccf879364d068ba1f1ecc70ac320","placeholder":"​","style":"IPY_MODEL_337fe228d4774ef99cff6dd26c4d624b","value":" 1.42G/1.42G [00:13&lt;00:00, 126MB/s]"}},"baac219482704db69bf75c35c776b3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc3ac1dc8101427ca360efc834e698b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"687fc8936e5b48ad83aa3beeb2530505":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ac7bd7d094a435ca8483987d4ac81c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87a2d4679b3245b38c2d6dc1410d3396":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd47ccf879364d068ba1f1ecc70ac320":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"337fe228d4774ef99cff6dd26c4d624b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ypXj3_kG48vq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763900971171,"user_tz":-330,"elapsed":31015,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"5895c7a6-2a60-4206-b81c-b40add0f92be"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=46d080f8998963c1e3755f787b7889751a611337322c2bcdfffae2a8ee1a6e25\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"u4jcltRaL7Wq","executionInfo":{"status":"ok","timestamp":1763900971481,"user_tz":-330,"elapsed":308,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"533593f9-5639-4f46-91fd-93c1f0ff778a"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","import google.generativeai as genai\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-pro_zero_shot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key9.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","print(\"Input:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Gemini API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"gemini-2.5-pro\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 70\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","\n","    # acronym expansion for stability\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","\n","    if cur:\n","        chunks.append(cur)\n","\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","\n","    candidate = text[start:end+1]\n","\n","    try:\n","        return json.loads(candidate)\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GEMINI CALL — GLOBAL WAIT + RETRIES\n","#####################################################################\n","def gemini_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s to respect global wait\")\n","        time.sleep(wait)\n","\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    cfg = {\"temperature\": temperature}\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = model.generate_content(prompt, generation_config=cfg)\n","            LAST_TS = time.time()\n","            return (getattr(resp, \"text\", \"\") or \"\").strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Gemini call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Gemini failed after retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ZERO-SHOT PROMPTING (ALL TASKS IN ONE CALL)\n","#####################################################################\n","def generate_zero_shot(transcript: str) -> Dict[str, Any]:\n","    topics_short = \", \".join(VALID_TOPICS)\n","\n","    prompt = (\n","        \"You are an expert NLP assistant.\\n\"\n","        \"Perform ALL tasks concisely using ONLY the transcript.\\n\\n\"\n","        \"Respond in a JSON-like object with EXACTLY these keys:\\n\"\n","        \"generated_summary, predicted_topics, generated_questions, key_concepts.\\n\\n\"\n","        \"Requirements:\\n\"\n","        \"- generated_summary: 3–5 sentence abstractive summary.\\n\"\n","        f\"- predicted_topics: 1–3 most relevant from [{topics_short}]. Use exact labels.\\n\"\n","        \"- generated_questions: 3 short Q&A pairs (objects with keys q and a).\\n\"\n","        \"- key_concepts: 6–10 short noun phrases, no duplicates.\\n\"\n","        \"- Do NOT add explanations outside the JSON-like object.\\n\\n\"\n","        f\"Transcript:\\n\\\"\\\"\\\"{transcript}\\\"\\\"\\\"\"\n","    )\n","\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    # Summary\n","    summary = j.get(\"generated_summary\", \"\").strip()\n","\n","    # Topics\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","    topics = [t for t in topics if t in VALID_TOPICS] or [\"Other\"]\n","\n","    # Q&A\n","    qas = j.get(\"generated_questions\", [])\n","    qa_lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = qa.get(\"q\", \"\").strip()\n","            a = qa.get(\"a\", \"\").strip()\n","            if q: qa_lines.append(f\"Q: {q}\")\n","            if a: qa_lines.append(f\"A: {a}\")\n","\n","    # Concepts\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    concepts_text = \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","    return {\n","        \"summary\": summary,\n","        \"topics\": topics,\n","        \"qa_text\": \"\\n\".join(qa_lines),\n","        \"concepts\": concepts_text\n","    }\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — ZERO-SHOT (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    # RESUME LOGIC\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming — {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            res = generate_zero_shot(transcript)\n","            summary = res[\"summary\"]\n","            topics = res[\"topics\"]\n","            qa_text = res[\"qa_text\"]\n","            concepts = res[\"concepts\"]\n","\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        # Auto-save\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN ZERO-SHOT GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nZero-Shot pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kJmfdaUOL7dF","outputId":"9337bdc5-4345-4a0f-8fb4-08408cea8baf","executionInfo":{"status":"ok","timestamp":1763903411017,"user_tz":-330,"elapsed":2439534,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro\n","Gemini API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement Learning with Human Feedback (RLHF) is a framework that integrates human guidance into an agent's training process to accelerate learning and align outcomes with human preferences. This method is demonstrated with an agent named Frank navigating a grid world, where human nudges help him find the optimal path faster. In a practical application like ChatGPT, RLHF involves training a reward model on human-ranked responses to act as a proxy for a human advisor. This reward model is then used with an algorithm like Proximal Policy Optimization (PPO) to fine-tune the language model, enhancing its ability to generate high-quality, human-favored answers.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: How does human feedback contribute to the reinforcement learning process?\n","A: Human feedback accelerates the learning process and allows the algorithm to make more informed decisions that are more human-favored.\n","Q: What are the two main parts of applying RLHF to fine-tune ChatGPT?\n","A: The process involves first training a reward model to act as a human advisor, and second, using this model with the Proximal Policy Optimization algorithm to fine-tune ChatGPT.\n","Q: What is the primary purpose of the rewards model in the context of ChatGPT?\n","A: The rewards model assesses and scores the quality of answers generated by ChatGPT, taking a question and answer as input and outputting a numerical score.\n","\n","KEY CONCEPTS:\n"," reinforcement learning with human feedback, grid world, reward model, proximal policy optimization, human feedback, fine-tune chat GPT, learning process, GPT architecture, back propagation\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial provides a code walkthrough demonstrating the impact of kernels and soft margins on Support Vector Machines using the CVXopt library. The speaker clarifies that CVXopt is used for educational purposes to visualize how kernels modify the SVM formulation, noting that libraries like libsvm are more practical for real-world applications. The example code processes linearly separable, non-linearly separable, and overlapping data to illustrate hard-margin, kernel-based, and soft-margin classification. The video also points to external resources, including a visualization that shows how kernels project data into higher dimensions to achieve separation. The next tutorial will cover scikit-learn's SVM implementation and multi-class classification.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the primary purpose of using CVXopt in this tutorial?\n","A: It is used for educational purposes to see directly the impact of a kernel and where it is being injected and modifying the initial formal support vector machine.\n","Q: What type of data requires a soft margin SVM?\n","A: Linearly separable but overlapping data requires a soft margin.\n","Q: Besides CVXopt, what library would one almost certainly use to write their own SVM?\n","A: You would almost certainly be using libsvm.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, CVXopt, Kernels, Soft margin, Quadratic programming solver, Linearly separable data, Non-linearly separable data, Support vectors, Penalty parameter\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," This section introduces prompts as the foundational inputs for prompt engineering models. It explains that the quality and structure of a prompt, which provides context and constraints to large language models like ChatGPT, directly impact the generated output. The speaker covers different types of prompts, their key features such as length and specific language, and the importance of defining expectations and constraints. The process of deconstructing prompts to identify their core requirements and constraints is also discussed, using examples like generating code or writing a word-limited essay.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are the two main components that define a prompt?\n","A: The two main components are defining what you expect and how you want it to be done, which includes any constraints.\n","Q: What is the purpose of deconstructing a prompt?\n","A: Deconstructing a prompt is the process of breaking it down into individual components to better understand its key features, requirements, and constraints.\n","Q: According to the transcript, what are prompts?\n","A: Prompts are the inputs given to prompt engineering models that provide the context and constraints for large language models to generate the required output.\n","\n","KEY CONCEPTS:\n"," prompt engineering, large language models, text outputs, prompt constraints, key features of prompts, deconstructing a prompt, desired output, one word answer\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents are autonomous problem-solvers that can think and make their own decisions, unlike chains that follow predefined instructions. They are equipped with tools, which are specific functions like search engines or calculators, to help them complete tasks. A popular framework for creating these agents is the ReAct (Reasoning + Acting) pattern, which mimics human cognition. This iterative process involves the agent thinking, choosing an action, observing the outcome, and repeating the cycle until the problem is solved, with the LLM acting as the agent's brain.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Langraph']\n","\n","Q&A:\n"," Q: What is the primary difference between AI agents and chains?\n","A: Agents can make autonomous decisions and decide their own steps, whereas chains and routers follow specific, pre-defined instructions.\n","Q: What does the ReAct pattern stand for and what does it mimic?\n","A: ReAct stands for Reasoning + Acting and it is a concept that mimics the human thought process of thinking, acting, and observing to solve a problem.\n","Q: What are tools in the context of AI agents?\n","A: Tools are specific functions or special abilities given to an AI agent, such as a calculator, search engine, or calendar, which it can use to complete tasks.\n","\n","KEY CONCEPTS:\n"," AI agents, Autonomous decisions, Agent tools, ReAct agent pattern, Reasoning and Acting, Think-Action-Observation loop, LLM reasoning ability, LangChain execution\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The speaker demonstrates how to trace a reflection agent system using LangSmith to understand its inner workings. By setting up environment variables and an API key, the system's operations are streamed to a LangSmith project for easy visualization. The process involves a 'generate' agent creating a tweet and a 'reflect' agent providing critiques for improvement. This iterative cycle of generation and reflection is repeated multiple times, allowing for deep thinking and refinement, ultimately producing a high-quality, viral tweet.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI', 'Langraph']\n","\n","Q&A:\n"," Q: What tool is used to trace the reflection agent system?\n","A: LangSmith is used to trace the system by setting up environment variables and an API key, which streams operations to a project for monitoring.\n","Q: What are the two primary components of the reflection agent system described?\n","A: The system consists of a 'generate' agent (or workflow) that creates an initial tweet and a 'reflect' agent that critiques it to suggest improvements.\n","Q: How does the system refine the tweet over time?\n","A: It uses an iterative process where the generate agent creates a tweet, the reflect agent critiques it, and this cycle repeats multiple times to incorporate feedback and progressively improve the final output.\n","\n","KEY CONCEPTS:\n"," reflection agent system, LangSmith tracing, API key, environment variables, generate agent, reflect agent, iterative refinement, viral tweet, generation node, reflect node\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial demonstrates how to use LangChain's chat models with the OpenAI API. It begins by installing the `langchain-openai` package and importing the `ChatOpenAI` class to initialize a model like GPT-4o. The user learns to resolve a common API key error by creating a `.env` file, installing `python-dotenv`, and loading the environment variables. The core interaction with the language model is performed using the `.invoke()` method. Finally, the process of parsing the complex response object to extract only the desired text content is explained.\n","\n","TOPICS:\n"," ['LangChain', 'Python Programming', 'Generative AI']\n","\n","Q&A:\n"," Q: What method is used to make an API call to the LLM in LangChain?\n","A: The `.invoke()` method is used to make calls to the LLM.\n","Q: How was the missing API key error resolved?\n","A: The error was fixed by creating a `.env` file for the `OPENAI_API_KEY`, installing the `python-dotenv` package, and calling the `load_dotenv()` method.\n","Q: How do you access just the text content from the model's full response object?\n","A: You can access the `.content` property of the result object (e.g., `result.content`).\n","\n","KEY CONCEPTS:\n"," LangChain chat models, ChatOpenAI class, OpenAI API key, .invoke() method, python-dotenv package, .env file, API response parsing, GPT-4o model\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," This transcript explains how Python's `sort()` method operates on lists containing strings and mixed data types. When sorting a list of strings, the method is case-sensitive, placing words starting with an uppercase letter before those starting with a lowercase letter, sorting each group alphabetically. For lists containing both strings and numbers, the `sort()` method places numbers at the beginning of the list, followed by the sorted strings. Reversing the sort order maintains these grouping principles but reverses the alphabetical or numerical order within each group.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: How does Python's sort method handle a list of strings with mixed capitalization?\n","A: It sorts words starting with an uppercase letter first, then sorts words starting with a lowercase letter, with each group being sorted alphabetically.\n","Q: What is the result of sorting a Python list that contains both strings and numbers?\n","A: The sort method places the numbers at the beginning of the list, followed by the sorted strings.\n","Q: What is a potential solution if you want to sort a list of strings purely alphabetically, ignoring case?\n","A: You would need to convert all the strings to a uniform case, either all lowercase or all uppercase, before sorting.\n","\n","KEY CONCEPTS:\n"," Python lists, sort method, string sorting, case-sensitive sorting, uppercase letters, lowercase letters, mixed data types, reverse alphabetical order, strings and numbers\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explores the optimal decision-maker between a human and an AI by analyzing their respective performance curves against a confidence score. While AI excels at tasks where it is highly confident, humans perform better in ambiguous situations by incorporating external context. The most effective approach is often augmented intelligence, which combines human and AI capabilities. However, to maximize its effectiveness, this collaboration must account for human cognitive biases, such as automation bias, by carefully considering how AI recommendations are presented.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: According to the transcript, when does an AI typically outperform a human?\n","A: An AI statistically performs better when it has a very high or very low confidence score in its prediction.\n","Q: What is automation bias?\n","A: It's the propensity for humans to favor suggestions from automated systems and ignore contradictory information, trusting the AI over their own judgment.\n","Q: How does an 'optional display' of AI recommendations mitigate bias?\n","A: It allows a person to consider the case and form their own opinion before seeing the AI's recommendation, thus reducing the AI's overwhelming influence.\n","\n","KEY CONCEPTS:\n"," fraud detection system, confidence score, performance curve, augmented intelligence, human cognitive bias, automation bias, optional display, forced display\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Dimitrius Kales, a Product Manager at Google Cloud AI, discusses the launch of six new Vertex AI APIs designed to help developers build generative AI applications more effectively. These APIs address common technical challenges, particularly the concept of grounding models in enterprise data to ensure accurate and consistent responses. The new services include APIs for document understanding, embedding, vector search with hybrid capabilities, ranking, grounded generation, and fact-checking. These tools are designed to be high-quality, standalone primitives that embed Google's unique know-how from its planet-scale applications.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary challenge these new Vertex AI APIs aim to solve for enterprise generative applications?\n","A: The primary challenge is grounding, which involves making applications reliably access the correct enterprise data to produce accurate and consistent responses.\n","Q: What are two standout features of the new APIs?\n","A: The two main features are their high quality and the fact that they embed Google's unique know-how, using technology from planet-scale applications like Google Search and YouTube.\n","Q: How are developers enabled to integrate these new APIs into their workflows?\n","A: The APIs are designed as simple, standalone primitives and are also being integrated into popular frameworks like LangChain and LlamaIndex for easier prototyping and combination with other tools.\n","\n","KEY CONCEPTS:\n"," Vertex AI APIs, Generative applications, Enterprise data grounding, Document understanding, Embedding API, Vector search, Hybrid search, Ranking API, Grounded generation, Fact checking\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains the properties of unitary matrices (U and V) within the Singular Value Decomposition (SVD). Unitary transformations are described as rotations that preserve the angles and lengths of vectors, similar to how constellations maintain their shape in the night sky. A key geometric interpretation is presented: a matrix X transforms a sphere of unit vectors into an ellipsoid. The principal axes of this ellipsoid are determined by the singular values, and its orientation is defined by the singular vectors. The discussion also briefly touches upon the economy SVD and the use of the complex conjugate transpose for complex-valued data.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What fundamental properties do unitary transformations preserve?\n","A: Unitary transformations preserve the angles between vectors and the lengths of vectors.\n","Q: What is the geometric result of mapping a sphere of unit vectors through a matrix X?\n","A: The sphere is mapped into an ellipsoid, where the lengths of its principal axes are given by the singular values of X and its orientation is given by the left singular vectors.\n","Q: When is a complex conjugate transpose used instead of a regular transpose?\n","A: The complex conjugate transpose is used when dealing with complex-valued data, where in addition to transposing, the complex conjugate of every element is taken.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Unitary matrices, Geometric interpretation, Unitary transformations, Economy SVD, Complex conjugate transpose, Singular values, Left singular vectors, Inner product, Fourier transform\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," The speaker introduces Google Gemini Pro 1.5, a powerful multimodal model for building Generative AI applications. A key highlighted feature is its massive 1 million token context window, a significant increase over previous models. The tutorial demonstrates how to obtain an API key and configure it within a notebook, emphasizing that a single model now handles both text and image inputs, simplifying development. Code examples showcase generating content from text, streaming responses, and analyzing images with accompanying prompts. The new model streamlines the creation of complex applications by unifying text and vision capabilities.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What is a major difference between Gemini 1.0 Pro and Gemini 1.5 Pro mentioned in the video?\n","A: A major difference is the context length; Gemini 1.5 Pro supports up to 1 million tokens, whereas Gemini 1.0 Pro had a context length of 32k tokens.\n","Q: How does Gemini 1.5 Pro handle multimodal inputs compared to its predecessors?\n","A: Gemini 1.5 Pro uses a single model for both text and image inputs, unlike the previous version which required separate models like Gemini Pro for text and Gemini Pro Vision for images.\n","Q: Where can a user create an API key for using the Gemini model?\n","A: A user can create an API key by going to ai.google.com/app and clicking on 'Get API key'.\n","\n","KEY CONCEPTS:\n"," Google Gemini Pro 1.5, Generative AI application, Multimodal model, Long context understanding, 1 million tokens, API key, Google AI Studio, Foundation models, Text and image inputs\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," This section focuses on evaluating, testing, and debugging prompt engineering models. Key performance metrics such as perplexity, accuracy, and human evaluation are introduced to measure a model's ability to generate meaningful responses. The process involves analyzing generated outputs to identify errors, fine-tuning the model, and testing on diverse datasets to ensure generalization. This evaluation is presented as an ongoing process crucial for maintaining high performance as the model continues to be used.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What are some common metrics used to evaluate prompt engineering models?\n","A: Commonly used metrics include perplexity, accuracy, and human evaluation.\n","Q: What does the perplexity metric measure?\n","A: Perplexity measures how well a language model predicts a sequence of words, where a lower score indicates better performance.\n","Q: Why is testing models on different datasets important?\n","A: Testing on different datasets is important to determine the model's ability to generalize to new or unseen data.\n","\n","KEY CONCEPTS:\n"," prompt engineering models, evaluation matrices, perplexity, accuracy, human evaluation, large language model, cross validation, generated responses, model evaluation, fine-tune the models\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains the progression from Generative AI to AI Agents and Agentic AI. Generative AI, powered by a Large Language Model, creates new content but is limited by a knowledge cutoff date. An AI Agent enhances this by using tools and memory to autonomously perform specific, narrow tasks, such as booking a flight. Agentic AI represents a more advanced system, often with multiple agents, that handles complex, multi-step goals requiring planning and coordination, like managing an entire trip by interacting with different services.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary limitation of a simple Generative AI with only an LLM?\n","A: It has a knowledge cutoff date, so it cannot answer questions about real-time information, such as the price of a flight ticket for tomorrow.\n","Q: How does an AI Agent differ from a basic Generative AI?\n","A: An AI Agent can use tools, memory, and knowledge to perform actions and complete tasks, such as booking a flight, rather than just answering questions.\n","Q: What capability distinguishes Agentic AI systems?\n","A: Agentic AI systems can handle complex, multi-step goals autonomously, often using multiple agents, planning, and coordination to reach a goal.\n","\n","KEY CONCEPTS:\n"," Generative AI, Large Language Model, Knowledge cutoff, AI Agent, Autonomous decision making, Agentic AI, Multi-step reasoning, Tool usage\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," This transcript explains covariance as a statistical measure for quantifying the relationship between two random variables. The speaker presents the mathematical formula for covariance, highlighting its similarity to the formula for variance. It is demonstrated that a positive covariance indicates a direct relationship where both variables increase together, while a negative covariance signifies an inverse relationship. The primary limitation discussed is that covariance reveals the direction of the relationship but not its magnitude, setting the stage for a future discussion on the Pearson correlation coefficient.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What does covariance help to quantify?\n","A: Covariance helps to quantify the relationship between two random variables, such as whether one increases or decreases in relation to the other.\n","Q: What does a positive covariance value indicate?\n","A: A positive covariance indicates that as one variable (X) increases, the other variable (Y) also tends to increase.\n","Q: What is the main disadvantage of using covariance?\n","A: The main disadvantage is that while it indicates the direction of a relationship (positive or negative), it does not specify the magnitude or strength of that relationship.\n","\n","KEY CONCEPTS:\n"," covariance, random variables, variance, quantify a relationship, data analysis, positive covariance, negative covariance, Pearson correlation coefficient\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," This video explains how to define the objective in a reinforcement learning (RL) problem, which is to learn an optimal policy that maximizes a cumulative numerical reward over time. The agent learns through trial and error, interacting with an environment and receiving feedback via a reward signal. The concept is illustrated with two types of tasks: an episodic task like Tic-Tac-Toe, where rewards are assigned for winning, losing, or drawing, and a continuous task like stock trading, where a reward function defines objectives such as profit or risk-adjusted returns. Ultimately, the goal is to parameterize the agent's objective through a carefully designed reward structure.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary objective of a reinforcement learning agent?\n","A: The agent's objective is to learn an optimal policy that maximizes the cumulative numerical reward signal over time.\n","Q: How are rewards structured in the Tic-Tac-Toe game example?\n","A: The agent receives a positive reward (+1) for winning, a negative reward (-1) for losing, and no reward for a draw.\n","Q: What is a reward function in the context of a continuous task like stock trading?\n","A: It is a mathematical expression that assigns a numerical value to each state or action to reflect trading goals, such as profit, loss, or risk-adjusted measures.\n","\n","KEY CONCEPTS:\n"," reinforcement learning, optimal policy, cumulative reward, reward signal, episodic task, continuous task, reward function, trial and error learning\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial introduces the Python dictionary, a data structure composed of key-value pairs called items, which are enclosed in curly brackets. It explains that keys must be immutable types like strings or numbers, not lists. The transcript demonstrates how to create dictionaries, including a method using the `zip()` and `dict()` functions to combine two lists. It also covers fundamental operations such as accessing items, keys, and values, as well as modifying, deleting, and checking the length of dictionary entries.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is the basic structure of a Python dictionary?\n","A: A dictionary consists of key-value pairs (items) enclosed in curly brackets. The key and value are separated by a colon, and the items are separated by commas.\n","Q: What is a requirement for a dictionary key?\n","A: A dictionary key must be immutable, meaning it cannot change. Examples include strings, numbers, or tuples.\n","Q: How can you create a dictionary from two lists?\n","A: You can use the `zip()` function to pair the corresponding elements of two lists, and then use the `dict()` function to convert these pairs into a dictionary.\n","\n","KEY CONCEPTS:\n"," key-value pairs, immutable key, curly brackets, dictionary items, zip function, dict function, accessing values, deleting entries\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," AI and machine learning are crucial for enhancing security by enabling faster detection and containment of insider threats, which can reduce breach identification time by an average of 108 days. User Behavior Analytics (UBA), when integrated into a Security Information and Event Management (SIM) solution like IBM's QRadar, analyzes user activity to identify anomalies against a learned baseline of normal behavior. This technology automates investigations, prioritizes alerts, and visualizes threat data, significantly reducing the time analysts spend on manual tasks. By providing actionable insights and mapping threats to frameworks like MITRE ATT&CK, these AI-powered tools allow security teams to shift from a reactive to a proactive defense posture.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is User Behavior Analytics (UBA)?\n","A: It is a system that uses machine learning to analyze user behavior, establish a baseline of normal activity, and identify anomalies that could indicate potential insider threats.\n","Q: According to the IBM report, what was the impact of using AI and automation on data breach containment?\n","A: Organizations that extensively used AI and automation took 108 fewer days on average to identify and contain a data breach compared to those that did not.\n","Q: How does the QRadar SIM solution assist security analysts?\n","A: It helps analysts quickly identify and contain insider threats by using a UBA app to prioritize risky employees, automate investigations, visualize threat data, and map threats to the MITRE ATT&CK framework.\n","\n","KEY CONCEPTS:\n"," User Behavior Analytics (UBA), Insider threats, Data breach containment, SIM solution, Indicators of compromise (IOCs), MITRE ATT&CK mappings, QRadar SIM, Human reinforcement learning, Security operations\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta has released Llama 3, a new open-source large language model available in 8 billion and 70 billion parameter versions. This model demonstrates state-of-the-art performance, showing competitive benchmark results against models like Google's Gemini Pro 1.5 and Claude 3 Sonet. Trained on a dataset seven times larger than its predecessor, Llama 3 features an 8K context length, doubling the capacity of Llama 2. The model excels at tasks like code generation, reasoning, and contextual understanding. Access to Llama 3 is available through Meta's website, Hugging Face, and Kaggle, with detailed instructions provided in its GitHub repository.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What are the two parameter sizes available for Llama 3?\n","A: Llama 3 is available with 8 billion and 70 billion parameters.\n","Q: How does Llama 3's training dataset size compare to Llama 2's?\n","A: Llama 3 was trained on a dataset 7 times larger than that of Llama 2, using over 50 trillion tokens of data.\n","Q: What is the context length supported by Llama 3?\n","A: Llama 3 supports an 8K context length, which doubles the capacity of Llama 2.\n","\n","KEY CONCEPTS:\n"," Llama 3, open source llm, performance metrics, 8 billion parameters, 70 billion parameters, 50 trillion tokens, 8K context length, instruction tuned version, Hugging Face access, GitHub repository\n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The speaker introduces a step-by-step walkthrough of Python code used to create a decision boundary. They demonstrate how to use Google to find documentation for the scikit-learn (sklearn) library. The specific machine learning algorithm implemented is Naive Bayes. The goal is to enable the audience to replicate the code by following the process, which focuses on the Gaussian Naive Bayes variant.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is the name of the Python library frequently used in the lesson?\n","A: The library is scikit-learn, often abbreviated as sk-learn.\n","Q: What algorithm was used to create the decision boundary?\n","A: The Naive Bayes algorithm was used.\n","Q: Which specific implementation of Naive Bayes was chosen?\n","A: The Gaussian Naive Bayes variant was used.\n","\n","KEY CONCEPTS:\n"," decision boundary, Python code, scikit-learn, sk-learn, Naive Bayes, Gaussian Naive Bayes, library documentation, Google search\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion contrasts Gaussian (normal) distribution with log-normal distribution, defining the latter as a variable whose logarithm is normally distributed. It provides real-world examples, such as height following a Gaussian distribution and income or product review length following a log-normal distribution. The primary application discussed is in machine learning, where identifying a feature's distribution allows for appropriate transformations, like log normalization. This process scales data to a standard normal distribution, which can significantly improve a model's accuracy.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: How is a log-normal distribution defined?\n","A: A random variable belongs to a log-normal distribution if the logarithm of that variable is normally distributed, following a Gaussian distribution.\n","Q: What is the empirical formula for a Gaussian distribution?\n","A: The empirical formula indicates that approximately 68% of the data falls within one standard deviation, 95% within two standard deviations, and 99.7% within three standard deviations of the mean.\n","Q: Why is it important to identify data distributions in machine learning?\n","A: Identifying a feature's distribution allows for proper scaling, such as converting a log-normal distribution to a standard normal distribution. This ensures features are on the same scale, which can increase model accuracy.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, Log-normal distribution, Bell curve, Empirical formula, Standard normal distribution, Log normalization, Standard scaling, Model accuracy, Random variable\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This project outlines an end-to-end deep learning solution for the agricultural domain, specifically to help farmers identify potato plant diseases like early and late blight. The process involves building a convolutional neural network for image classification, which will be trained on images of potato leaves. The complete system architecture includes a backend using Fast API and TF Serving, deployment on Google Cloud Platform (GCP), and a user-facing mobile application built with React Native. The goal is to provide farmers with a tool to quickly diagnose plant health from a photo, thereby preventing economic losses.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Mlops']\n","\n","Q&A:\n"," Q: What problem does this end-to-end project aim to solve for farmers?\n","A: It aims to help farmers detect common potato plant diseases, specifically early blight and late blight, from a picture of a plant leaf to prevent economic losses.\n","Q: What is the core model building technology used for image classification?\n","A: A Convolutional Neural Network (CNN) built with TensorFlow is used for the image classification task.\n","Q: Which technologies are used for the backend server and MLOps?\n","A: The project uses TF Serving for model serving and a backend server built with Fast API.\n","\n","KEY CONCEPTS:\n"," end-to-end deep learning project, potato plant diseases, convolutional neural network, ML Ops, TF Serving, Fast API, Google Cloud deployment, React Native mobile app, data augmentation, TF Lite models\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The transcript outlines the evolution of autonomy in LLM applications, progressing from deterministic code to complex, agentic systems. It begins with simple applications like single LLM calls and progresses to chains, which use multiple specialized LLMs in a fixed sequence. The next level, routers, introduces decision-making by allowing the AI to select the appropriate path or tool. The most advanced level discussed is the state machine, or agent, which uses tools like LangGraph to enable loops, memory, and iterative refinement, allowing for complex, multi-step tasks with human-in-the-loop capabilities.\n","\n","TOPICS:\n"," ['Agentic AI', 'Langraph', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary disadvantage of using chains in LLM applications?\n","A: The main downside is that chains are like a rigid assembly line; they follow a fixed sequence of steps defined by a human and lack the autonomy to make intelligent decisions.\n","Q: How does a router architecture improve upon a simple chain?\n","A: A router acts as a smart traffic cop, allowing the AI itself to decide which chain or tool to use next based on the user's input, rather than following a single, predefined path.\n","Q: What key capability makes a state machine an 'agent' while a router is not?\n","A: A state machine is considered an agent because the control flow is managed by an LLM and it can have cycles or loops, allowing it to go back, refine its work, and learn, which a one-directional router cannot do.\n","\n","KEY CONCEPTS:\n"," Levels of autonomy, Single LLM call, LLM chains, Router LLM, State machine, Agent-executed systems, Human in the loop, Multi-agent systems, Control flow, Cognitive architecture\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This section explores advanced prompt engineering, moving beyond basics to cover complex techniques. It details how to handle various prompt types, including text, image, and audio, using pre-trained models for tasks like dog breed identification. The discussion covers advanced fine-tuning methods such as multitask learning and distillation, alongside best practices for data preprocessing like tokenization and normalization. Finally, it addresses the practical aspects of deploying models using frameworks like Flask and the critical ethical considerations of bias and fairness in AI.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What are two advanced fine-tuning techniques mentioned for large language models?\n","A: The two techniques are multitask learning, which involves training a model on multiple tasks simultaneously, and distillation, which trains a smaller model to mimic a larger one for efficiency.\n","Q: What are some methods discussed for deploying prompt engineering models in production?\n","A: The transcript suggests using TensorFlow Serving, a framework for serving machine learning models, or Flask, a web framework for building APIs to serve the models.\n","Q: What ethical considerations are important in prompt engineering?\n","A: Ethical considerations include addressing bias, fairness, and privacy, ensuring the training data is diverse and representative to avoid unfair or discriminatory outcomes.\n","\n","KEY CONCEPTS:\n"," advanced prompt engineering, image-based prompts, fine-tuning pre-trained models, multitask learning, distillation, data pre-processing, tokenization, model deployment, ethical considerations\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," This lecture demonstrates using Singular Value Decomposition (SVD) to create \"eigenfaces\" for image classification. The speaker processes and aligns images of celebrities like Arnold Schwarzenegger, Sylvester Stallone, and Taylor Swift, then computes an average face and uses SVD to find the principal components, or eigenfaces. By projecting individual images into this lower-dimensional eigenface space, the speaker shows how different individuals form distinct clusters. The analysis reveals that visual similarities, such as skin and hair color, can cause unexpected overlaps, as seen between Schwarzenegger and Swift, highlighting a limitation of this correlation-based technique.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence', 'Data Science']\n","\n","Q&A:\n"," Q: What is the purpose of subtracting the average face from each image?\n","A: The average face is subtracted from every image to perform principal component analysis, which finds the principal components of variation among the action heroes.\n","Q: How are the images classified after being processed?\n","A: Each image is projected into the coordinates of the first three principal components (eigenfaces), and these coordinates are plotted to see if they fall into distinct clusters for each person.\n","Q: Why did the images of Arnold Schwarzenegger and Taylor Swift overlap more than those of Arnold and Stallone?\n","A: They likely overlapped more due to shared superficial features like fair skin and blonde hair, which the algorithm correlated strongly, demonstrating a potential shallowness in this classification method.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Image Classification, Average Face, Eigenface Space, Test Image, Feature Space\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," Large Language Models (LLMs) are powerful reasoning engines but are limited because they cannot interact with the real world to perform actions like making bookings. LangChain is a popular framework that solves this problem by acting as a bridge between LLMs and external systems. It enables applications to connect the AI's 'brain' to real-world tools like APIs, databases, and websites. This allows the AI to perform practical tasks such as booking flights, querying private data, and sending emails, effectively giving it the ability to act.\n","\n","TOPICS:\n"," ['LangChain', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is one of the biggest limitations of Large Language Models?\n","A: They are smart and can reason with data they were trained on, but they cannot interact with the real world to perform actions like making bookings.\n","Q: What role does LangChain play in AI applications?\n","A: LangChain acts as a bridge or framework that connects LLMs to the real world, enabling them to communicate with APIs, databases, and other external systems.\n","Q: What is a key benefit of building applications with LangChain?\n","A: It allows for easily switching out different LLM models, such as replacing GPT-4 with a free Hugging Face model, without changing the application code.\n","\n","KEY CONCEPTS:\n"," Large Language Models, LangChain framework, Real-world interaction, LLM limitations, Application building, API access, Private company databases, Reasoning ability\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," This video explains how to use residual analysis to diagnose and improve time series forecasting models. Residuals are defined as the difference between a model's fitted values and the actual values from the training data. An effective model should have residuals with a mean of zero and no autocorrelation, indicating an unbiased fit that has captured all informational patterns. The tutorial demonstrates how to perform this analysis in Python using autocorrelation plots, histograms, and the Ljung-Box test on a Holt-Winters model.\n","\n","TOPICS:\n"," ['Time Series', 'Data Science', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in the context of time series analysis?\n","A: Residuals are the difference between the fitted value (y-hat) and the actual value (y) of the time series for the data the model was trained on.\n","Q: What two key properties should residuals have for a good model fit?\n","A: The residuals should have no autocorrelation, and their mean should be zero to ensure the forecast is not biased.\n","Q: What statistical test can be used to check if residuals are independently distributed?\n","A: The Ljung-Box test is used to test the null hypothesis that the residuals are independently distributed, meaning they have no serial correlation.\n","\n","KEY CONCEPTS:\n"," residual analysis, time series, forecasting methods, fitted values, autocorrelation, Ljung-Box test, biased forecast, Holt-Winters model, exponential smoothing\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This transcript outlines the process of building a Text2SQL AI agent capable of querying a database using natural language. The architecture involves a Next.js frontend, an in-memory SQLite database, and models running on watsonx.ai. A ReAct agent is constructed with LangGraph, which leverages LangChain functionalities. The agent is equipped with a custom tool that allows it to execute SQL queries, and its system prompt is engineered with the database schema to ensure it can generate accurate queries based on user input.\n","\n","TOPICS:\n"," ['Agentic AI', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What technology stack was used to create the application?\n","A: The application was built with LangGraph for the ReAct agent, Next.js for the frontend, watsonx.ai for the models, and an in-memory SQLite database.\n","Q: How does the AI agent know the structure of the database?\n","A: The database schema, defined in SQL create queries, is passed into the tool definition, making the large language model aware of the available tables and fields.\n","Q: What is the purpose of the system prompt in this application?\n","A: The system prompt instructs the large language model on its role as a Text2SQL agent, guiding it to generate SQLite queries and use the provided `GetFromDB` tool.\n","\n","KEY CONCEPTS:\n"," Text2SQL agent, ReAct agent, LangGraph, watsonx.ai models, in-memory SQLite database, Next.js frontend, tool definition, database schema, system prompt\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," This course introduces prompt engineering, a specialized field within Natural Language Processing that leverages pre-trained large language models to generate high-quality text. It aims to create more accurate, coherent, and contextually appropriate outputs for applications like chatbots and translation, surpassing traditional rule-based methods. The curriculum will cover the fundamentals of prompt analysis, the benefits and limitations of the field, and advanced techniques such as fine-tuning models. By the end of the initial section, students will have a solid foundational understanding of prompt engineering and its importance.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: It is a specialized field within natural language processing that focuses on building models that can generate high-quality text outputs in response to prompts.\n","Q: What are some examples of the large language models mentioned?\n","A: The transcript mentions OpenAI's GPT, Google's BERT, and Hugging Face Transformers.\n","Q: What is a key benefit of prompt engineering over traditional approaches?\n","A: It allows the generation of text outputs that are more accurate, coherent, and contextually appropriate than traditional rule-based or keyword-based approaches.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Large Language Models, High-Quality Text Outputs, Fine-Tuning Models, Prompt Analysis, User Experience, Model Architecture\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is a value-based reinforcement learning algorithm designed to find an optimal policy for an agent. It operates by learning a state-action value function, represented as a Q-table, which estimates the total future reward for taking a specific action in a given state. The algorithm iteratively updates the Q-values by having an agent explore an environment, observe rewards, and calculate the temporal difference error between the expected and observed outcomes. This process is repeated over many episodes until the Q-table converges, at which point it can be used to dictate the optimal actions that maximize cumulative reward. Because the exploration policy can be different from the final optimal policy, Q-learning is considered an off-policy method.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary goal of Q-learning?\n","A: The goal of Q-learning is to learn the Q-values in a Q-table such that the total reward is maximized.\n","Q: Why is Q-learning considered an off-policy algorithm?\n","A: It is an off-policy algorithm because the behavior policy used for exploring the environment and collecting data can be different from the target policy, which is derived from the optimal Q-table to maximize rewards.\n","Q: What is the temporal difference error in the context of Q-learning?\n","A: The temporal difference error is the difference between the observed Q-value (calculated using the Bellman equation after an action) and the expected Q-value (the current value stored in the Q-table).\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement Learning, Value-based methods, State-action value function, Q-table, Bellman equation, Temporal difference error, Off-policy algorithm, Behavior policy, Target policy\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains the logistic classifier, a linear model that generates prediction scores by applying a linear function to input data. This function involves multiplying an input vector (X) by a weights matrix (W) and adding a bias term (b). The primary goal of training is to find the optimal values for these weights and biases. To perform classification for single-label tasks, the resulting scores, also called logits, are converted into probabilities using a softmax function. This ensures the probabilities for all classes sum to one, with the correct class having a high probability.\n","\n","TOPICS:\n"," ['Machine Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier described as?\n","A: It is described as a linear classifier that applies a linear function, specifically a matrix multiplication, to inputs to generate predictions.\n","Q: What is the goal of training the model?\n","A: The goal is to find the optimal values for the weights (W) and bias (b) that are good at performing predictions.\n","Q: How are scores converted into probabilities for classification?\n","A: A softmax function is used to turn scores, also known as logits, into proper probabilities that sum to 1.\n","\n","KEY CONCEPTS:\n"," logistic classifier, linear classifier, linear function, matrix multiply, weights and bias, softmax function, proper probabilities, logits, output class\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\n","\n","Zero-Shot pipeline completed successfully.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/gemini-2.5-pro_zero_shot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-pro/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"id":"MBPEEoCAQRpD","colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["78edafe28098478aafc8ef34fc13a851","9e7b10ff64744c96b11e065642b3a24a","889e4dd284be4dde8886b580f7939973","d95316828deb4e18a5843d712d900e6a","a6684269945a436abaa452193e9b2d11","42a5f394f9d446a49f73b89b0967f706","d83799cc4f6f4d55a67f2a30defd6d68","b75040e6b84f4b3eb958f0bd86cecb35","eb02230b88b64ce4814c96662eafa8e4","4f0c15ecb0d1400597a9418472d2acd2","3951138979c3441aaaadff6517faa877","5206b7cd3c8e410a8d725b80039bb932","e0b34d6e150144d3b5d51ad9d2297483","b96106c198e142939b9568af681a89ee","b2a327c416f34ee7807b5aaf4b894881","d48c5127662c46aa94e5cfb5901028f4","39ab8223574243c8a7d80e3b254366f7","9de2a287a3d94f3ebe85a0e4ba5023ac","09970ff4c30f41b4a0f4db2f866512d0","dfebec6dfb504fb891fdd621bf9f6bc4","4b7cca9cde634e22bc490e8224b9610f","e2f249ce0c2b49a8a50caf030cfa7270","c0bada71a22947f086578455c9b41686","49f121b47cc14409a69c29d3b965a8cb","ec28feb9a6584aefac95e4b6dd231e97","ce90a96701f3460ab70509e6548e5b60","4bc94db12ce34976a870d86bfc05323f","85966a6721164bd08a68f0c94aa09086","e0e297bd000245a087e0b3f152012e0c","fd7912f6c49c4375b955b1b536b659cf","b0e52ce31ed04a3f994e8915e04b9602","969ba69bdff74d59b7fc36d26c945614","a0f7d40f5e3e455186accb5c32119c1c","b0480596a228419baaf9ff879bb05b14","cd16f540384b4c71a8cbf6cb3a482b6a","928ee214deee4025a5ecbd764d45d1b7","ef5b36595f8647268c805aabb3e15902","c0469c082f2a4b2486350e40ea1f3935","054c802ca3d44fcfa6872513f4200d9b","545242f5ac9841668bf5831ca3cd9d92","5706092ce20640ba83f0aeb0d17a4b8f","2fb8585d02a249e0947f13e57737e241","4abce1f7317b4d118f45ac5b37f94ca9","f2d4c5e9e4d44ab0bc15345db0cf4c62","14fc9cdfa39b4eeda87d575a563c0153","733a2b7148e346bcabb6834827a04d0d","c28fbfe177c64b15970d77f0041a12ab","8ab0d19a8b874474a91132734a9e8349","35375efc4faf493088d44dae396540ff","775a4a59085f472fb308ae319416dc86","e9fac8773e9c46ccb533b5398a26bd58","1b72c6288e8b4a538488717ef829e683","b6169c2e5a334ecb8bb45c6e35c2b7a1","33be5b214df84fb188ae156404f2bc86","0eccd088afa144a09f052e3c887a0caf","2745938dfb2b4dfda29dcdb382bb5b99","e527324dd8ec45018e9daf2882ab0ac6","43297b4e6e524d11bf2454cf197c61ca","2ec95dc1731c4cb5b783dbd76c2de831","baac219482704db69bf75c35c776b3c4","fc3ac1dc8101427ca360efc834e698b9","687fc8936e5b48ad83aa3beeb2530505","8ac7bd7d094a435ca8483987d4ac81c8","87a2d4679b3245b38c2d6dc1410d3396","bd47ccf879364d068ba1f1ecc70ac320","337fe228d4774ef99cff6dd26c4d624b"]},"executionInfo":{"status":"ok","timestamp":1763475690183,"user_tz":-330,"elapsed":156883,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"393832b5-41a7-4bb5-ea9f-fe30b9174eff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78edafe28098478aafc8ef34fc13a851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5206b7cd3c8e410a8d725b80039bb932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0bada71a22947f086578455c9b41686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0480596a228419baaf9ff879bb05b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fc9cdfa39b4eeda87d575a563c0153"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2745938dfb2b4dfda29dcdb382bb5b99"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3969\n","  - BLEU: 0.1228\n","  - BERTScore F1: 0.9047\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.3913\n","  - Micro F1: 0.5232\n","  - Macro F1: 0.4714\n","  - Weighted F1: 0.4799\n","\n","Q&A Generation:\n","  - BLEU: 0.0309\n","  - Diversity: 0.8120\n","  - Answerability: 0.7778\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5933\n","  - Recall@10: 0.2373\n","  - F1@10: 0.3390\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/evaluation_final.json\n"]}]}]}