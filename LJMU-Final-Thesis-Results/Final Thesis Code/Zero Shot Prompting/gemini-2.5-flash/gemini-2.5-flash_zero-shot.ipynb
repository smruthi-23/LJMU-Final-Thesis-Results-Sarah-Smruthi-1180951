{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNLSSLnqyvLiSW1qSjvO1xW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"78edafe28098478aafc8ef34fc13a851":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e7b10ff64744c96b11e065642b3a24a","IPY_MODEL_889e4dd284be4dde8886b580f7939973","IPY_MODEL_d95316828deb4e18a5843d712d900e6a"],"layout":"IPY_MODEL_a6684269945a436abaa452193e9b2d11"}},"9e7b10ff64744c96b11e065642b3a24a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42a5f394f9d446a49f73b89b0967f706","placeholder":"​","style":"IPY_MODEL_d83799cc4f6f4d55a67f2a30defd6d68","value":"tokenizer_config.json: 100%"}},"889e4dd284be4dde8886b580f7939973":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b75040e6b84f4b3eb958f0bd86cecb35","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb02230b88b64ce4814c96662eafa8e4","value":25}},"d95316828deb4e18a5843d712d900e6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f0c15ecb0d1400597a9418472d2acd2","placeholder":"​","style":"IPY_MODEL_3951138979c3441aaaadff6517faa877","value":" 25.0/25.0 [00:00&lt;00:00, 2.46kB/s]"}},"a6684269945a436abaa452193e9b2d11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42a5f394f9d446a49f73b89b0967f706":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d83799cc4f6f4d55a67f2a30defd6d68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b75040e6b84f4b3eb958f0bd86cecb35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb02230b88b64ce4814c96662eafa8e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f0c15ecb0d1400597a9418472d2acd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3951138979c3441aaaadff6517faa877":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5206b7cd3c8e410a8d725b80039bb932":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0b34d6e150144d3b5d51ad9d2297483","IPY_MODEL_b96106c198e142939b9568af681a89ee","IPY_MODEL_b2a327c416f34ee7807b5aaf4b894881"],"layout":"IPY_MODEL_d48c5127662c46aa94e5cfb5901028f4"}},"e0b34d6e150144d3b5d51ad9d2297483":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39ab8223574243c8a7d80e3b254366f7","placeholder":"​","style":"IPY_MODEL_9de2a287a3d94f3ebe85a0e4ba5023ac","value":"config.json: 100%"}},"b96106c198e142939b9568af681a89ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_09970ff4c30f41b4a0f4db2f866512d0","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfebec6dfb504fb891fdd621bf9f6bc4","value":482}},"b2a327c416f34ee7807b5aaf4b894881":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b7cca9cde634e22bc490e8224b9610f","placeholder":"​","style":"IPY_MODEL_e2f249ce0c2b49a8a50caf030cfa7270","value":" 482/482 [00:00&lt;00:00, 37.0kB/s]"}},"d48c5127662c46aa94e5cfb5901028f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39ab8223574243c8a7d80e3b254366f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9de2a287a3d94f3ebe85a0e4ba5023ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09970ff4c30f41b4a0f4db2f866512d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfebec6dfb504fb891fdd621bf9f6bc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b7cca9cde634e22bc490e8224b9610f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2f249ce0c2b49a8a50caf030cfa7270":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0bada71a22947f086578455c9b41686":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49f121b47cc14409a69c29d3b965a8cb","IPY_MODEL_ec28feb9a6584aefac95e4b6dd231e97","IPY_MODEL_ce90a96701f3460ab70509e6548e5b60"],"layout":"IPY_MODEL_4bc94db12ce34976a870d86bfc05323f"}},"49f121b47cc14409a69c29d3b965a8cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85966a6721164bd08a68f0c94aa09086","placeholder":"​","style":"IPY_MODEL_e0e297bd000245a087e0b3f152012e0c","value":"vocab.json: 100%"}},"ec28feb9a6584aefac95e4b6dd231e97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd7912f6c49c4375b955b1b536b659cf","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0e52ce31ed04a3f994e8915e04b9602","value":898823}},"ce90a96701f3460ab70509e6548e5b60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_969ba69bdff74d59b7fc36d26c945614","placeholder":"​","style":"IPY_MODEL_a0f7d40f5e3e455186accb5c32119c1c","value":" 899k/899k [00:00&lt;00:00, 14.6MB/s]"}},"4bc94db12ce34976a870d86bfc05323f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"85966a6721164bd08a68f0c94aa09086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0e297bd000245a087e0b3f152012e0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd7912f6c49c4375b955b1b536b659cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0e52ce31ed04a3f994e8915e04b9602":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"969ba69bdff74d59b7fc36d26c945614":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f7d40f5e3e455186accb5c32119c1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0480596a228419baaf9ff879bb05b14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd16f540384b4c71a8cbf6cb3a482b6a","IPY_MODEL_928ee214deee4025a5ecbd764d45d1b7","IPY_MODEL_ef5b36595f8647268c805aabb3e15902"],"layout":"IPY_MODEL_c0469c082f2a4b2486350e40ea1f3935"}},"cd16f540384b4c71a8cbf6cb3a482b6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_054c802ca3d44fcfa6872513f4200d9b","placeholder":"​","style":"IPY_MODEL_545242f5ac9841668bf5831ca3cd9d92","value":"merges.txt: 100%"}},"928ee214deee4025a5ecbd764d45d1b7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5706092ce20640ba83f0aeb0d17a4b8f","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fb8585d02a249e0947f13e57737e241","value":456318}},"ef5b36595f8647268c805aabb3e15902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4abce1f7317b4d118f45ac5b37f94ca9","placeholder":"​","style":"IPY_MODEL_f2d4c5e9e4d44ab0bc15345db0cf4c62","value":" 456k/456k [00:00&lt;00:00, 35.4MB/s]"}},"c0469c082f2a4b2486350e40ea1f3935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"054c802ca3d44fcfa6872513f4200d9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"545242f5ac9841668bf5831ca3cd9d92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5706092ce20640ba83f0aeb0d17a4b8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fb8585d02a249e0947f13e57737e241":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4abce1f7317b4d118f45ac5b37f94ca9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2d4c5e9e4d44ab0bc15345db0cf4c62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14fc9cdfa39b4eeda87d575a563c0153":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_733a2b7148e346bcabb6834827a04d0d","IPY_MODEL_c28fbfe177c64b15970d77f0041a12ab","IPY_MODEL_8ab0d19a8b874474a91132734a9e8349"],"layout":"IPY_MODEL_35375efc4faf493088d44dae396540ff"}},"733a2b7148e346bcabb6834827a04d0d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_775a4a59085f472fb308ae319416dc86","placeholder":"​","style":"IPY_MODEL_e9fac8773e9c46ccb533b5398a26bd58","value":"tokenizer.json: 100%"}},"c28fbfe177c64b15970d77f0041a12ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b72c6288e8b4a538488717ef829e683","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6169c2e5a334ecb8bb45c6e35c2b7a1","value":1355863}},"8ab0d19a8b874474a91132734a9e8349":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33be5b214df84fb188ae156404f2bc86","placeholder":"​","style":"IPY_MODEL_0eccd088afa144a09f052e3c887a0caf","value":" 1.36M/1.36M [00:00&lt;00:00, 80.5MB/s]"}},"35375efc4faf493088d44dae396540ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"775a4a59085f472fb308ae319416dc86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9fac8773e9c46ccb533b5398a26bd58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b72c6288e8b4a538488717ef829e683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6169c2e5a334ecb8bb45c6e35c2b7a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33be5b214df84fb188ae156404f2bc86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eccd088afa144a09f052e3c887a0caf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2745938dfb2b4dfda29dcdb382bb5b99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e527324dd8ec45018e9daf2882ab0ac6","IPY_MODEL_43297b4e6e524d11bf2454cf197c61ca","IPY_MODEL_2ec95dc1731c4cb5b783dbd76c2de831"],"layout":"IPY_MODEL_baac219482704db69bf75c35c776b3c4"}},"e527324dd8ec45018e9daf2882ab0ac6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc3ac1dc8101427ca360efc834e698b9","placeholder":"​","style":"IPY_MODEL_687fc8936e5b48ad83aa3beeb2530505","value":"model.safetensors: 100%"}},"43297b4e6e524d11bf2454cf197c61ca":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ac7bd7d094a435ca8483987d4ac81c8","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_87a2d4679b3245b38c2d6dc1410d3396","value":1421700479}},"2ec95dc1731c4cb5b783dbd76c2de831":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd47ccf879364d068ba1f1ecc70ac320","placeholder":"​","style":"IPY_MODEL_337fe228d4774ef99cff6dd26c4d624b","value":" 1.42G/1.42G [00:13&lt;00:00, 126MB/s]"}},"baac219482704db69bf75c35c776b3c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc3ac1dc8101427ca360efc834e698b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"687fc8936e5b48ad83aa3beeb2530505":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ac7bd7d094a435ca8483987d4ac81c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87a2d4679b3245b38c2d6dc1410d3396":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd47ccf879364d068ba1f1ecc70ac320":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"337fe228d4774ef99cff6dd26c4d624b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"ypXj3_kG48vq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763472973220,"user_tz":-330,"elapsed":17744,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"f7694c95-32a9-4a34-8484-d8237f35d681"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.10.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=160cef9e8f0ff0f4f6cd37e26aae523ace1523062716a23dd82d7e4e147cf66e\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"u4jcltRaL7Wq","executionInfo":{"status":"ok","timestamp":1763472973279,"user_tz":-330,"elapsed":34,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"9b2ca990-334c-446a-f7b3-e40e402e26d0"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","import google.generativeai as genai\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-flash_zero_shot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key7.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","print(\"Input:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Gemini API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"gemini-2.5-flash\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 70\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","\n","    # acronym expansion for stability\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","\n","    if cur:\n","        chunks.append(cur)\n","\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","\n","    candidate = text[start:end+1]\n","\n","    try:\n","        return json.loads(candidate)\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GEMINI CALL — GLOBAL WAIT + RETRIES\n","#####################################################################\n","def gemini_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s to respect global wait\")\n","        time.sleep(wait)\n","\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    cfg = {\"temperature\": temperature}\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = model.generate_content(prompt, generation_config=cfg)\n","            LAST_TS = time.time()\n","            return (getattr(resp, \"text\", \"\") or \"\").strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Gemini call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Gemini failed after retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ZERO-SHOT PROMPTING (ALL TASKS IN ONE CALL)\n","#####################################################################\n","def generate_zero_shot(transcript: str) -> Dict[str, Any]:\n","    topics_short = \", \".join(VALID_TOPICS)\n","\n","    prompt = (\n","        \"You are an expert NLP assistant.\\n\"\n","        \"Perform ALL tasks concisely using ONLY the transcript.\\n\\n\"\n","        \"Respond in a JSON-like object with EXACTLY these keys:\\n\"\n","        \"generated_summary, predicted_topics, generated_questions, key_concepts.\\n\\n\"\n","        \"Requirements:\\n\"\n","        \"- generated_summary: 3–5 sentence abstractive summary.\\n\"\n","        f\"- predicted_topics: 1–3 most relevant from [{topics_short}]. Use exact labels.\\n\"\n","        \"- generated_questions: 3 short Q&A pairs (objects with keys q and a).\\n\"\n","        \"- key_concepts: 6–10 short noun phrases, no duplicates.\\n\"\n","        \"- Do NOT add explanations outside the JSON-like object.\\n\\n\"\n","        f\"Transcript:\\n\\\"\\\"\\\"{transcript}\\\"\\\"\\\"\"\n","    )\n","\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    # Summary\n","    summary = j.get(\"generated_summary\", \"\").strip()\n","\n","    # Topics\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","    topics = [t for t in topics if t in VALID_TOPICS] or [\"Other\"]\n","\n","    # Q&A\n","    qas = j.get(\"generated_questions\", [])\n","    qa_lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = qa.get(\"q\", \"\").strip()\n","            a = qa.get(\"a\", \"\").strip()\n","            if q: qa_lines.append(f\"Q: {q}\")\n","            if a: qa_lines.append(f\"A: {a}\")\n","\n","    # Concepts\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    concepts_text = \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","    return {\n","        \"summary\": summary,\n","        \"topics\": topics,\n","        \"qa_text\": \"\\n\".join(qa_lines),\n","        \"concepts\": concepts_text\n","    }\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — ZERO-SHOT (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    # RESUME LOGIC\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming — {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            res = generate_zero_shot(transcript)\n","            summary = res[\"summary\"]\n","            topics = res[\"topics\"]\n","            qa_text = res[\"qa_text\"]\n","            concepts = res[\"concepts\"]\n","\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        # Auto-save\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN ZERO-SHOT GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nZero-Shot pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kJmfdaUOL7dF","outputId":"e9ee78d3-42ab-4145-976d-fbd7a1717613","executionInfo":{"status":"ok","timestamp":1763475323911,"user_tz":-330,"elapsed":1714305,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}}},"execution_count":3,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash\n","Gemini API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," The video introduces Reinforcement Learning with Human Feedback (RLHF), explaining how human input can significantly accelerate the learning process of reinforcement learning algorithms and guide them towards more human-favored outcomes. It illustrates this concept using a 'grid world' example where an agent named Frank learns to navigate with and without human guidance. A practical application of RLHF is then detailed through the fine-tuning of ChatGPT, which involves training a reward model based on human rankings and subsequently using this model with Proximal Policy Optimization (PPO) to enhance ChatGPT's response generation. This iterative process allows models like ChatGPT to produce high-quality, human-aligned responses.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Deep Learning']\n","\n","Q&A:\n"," Q: How does human feedback influence reinforcement learning?\n","A: Human feedback accelerates the learning process and helps the algorithm generate more human-favored responses.\n","Q: What are the two main steps in applying RLHF to fine-tune ChatGPT?\n","A: First, a reward model is trained using human rankings, and then this model is used with Proximal Policy Optimization (PPO) to fine-tune ChatGPT.\n","Q: What is the primary role of the reward model in ChatGPT's RLHF process?\n","A: The reward model assesses and scores the quality of answers generated by ChatGPT, acting as a human advisor.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning with Human Feedback, Grid world environment, Human feedback, Learning acceleration, Proximal Policy Optimization, ChatGPT fine-tuning, Reward model, Human-favored responses, Loss function, Backpropagation\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial, part 32 of a machine learning series, focuses on using CVXopt and kernels with Support Vector Machines (SVMs) for educational purposes, emphasizing visualization of their impact. It demonstrates how kernels, specifically Gaussian, polynomial, and linear, affect SVMs in handling linearly separable, nonlinearly separable, and overlapping data. The speaker highlights CVXopt's utility for observing kernel injection and modification within the SVM's formal structure, despite its limited practical use compared to libraries like libsvm. The session also covers the underlying quadratic programming solver and references external resources for deeper understanding.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the primary educational purpose of using CVXopt in this tutorial?\n","A: CVXopt is used to directly visualize the impact of kernels and where they are injected and modify the formal Support Vector Machine.\n","Q: Which types of kernels are discussed and demonstrated in the example code?\n","A: The tutorial discusses and demonstrates Gaussian, polynomial, and linear kernels.\n","Q: What kind of data necessitates the use of a soft margin Support Vector Machine?\n","A: Linearly separable but overlapping data requires a soft margin Support Vector Machine.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, Kernels, CVXopt, Quadratic Programming, Gaussian Kernel, Polynomial Kernel, Linear Kernel, Soft Margin, Hard Margin, Support Vectors\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1899.02ms\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","SUMMARY:\n"," Prompts serve as the foundational inputs for prompt engineering models, providing essential context and constraints for large language models like ChatGPT to generate desired text outputs. Understanding various prompt types and their key features, such as length, specific language, and defined constraints, is crucial for achieving accurate and efficient results. The effectiveness of a prompt hinges on clearly defining expectations and how the output should be generated. Additionally, deconstructing prompts helps in breaking them down to better understand their individual components, requirements, and limitations.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What are prompts in the context of prompt engineering?\n","A: Prompts are the inputs given to prompt engineering models, providing context and constraints for large language models to generate text outputs.\n","Q: Why is it important to understand the different types and features of prompts?\n","A: Understanding prompts helps in choosing the right prompt for desired output, impacting its complexity and quality, and providing more context for accurate results.\n","Q: What is the purpose of deconstructing a prompt?\n","A: Deconstructing a prompt involves breaking it down into individual components to better understand its key features, specific language, requirements, and constraints.\n","\n","KEY CONCEPTS:\n"," Prompts, Prompt engineering models, Large language models, Text outputs, Context, Constraints, Prompt types, Key features, Prompt deconstruction, Desired output\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2023.35ms\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","SUMMARY:\n"," AI agents are described as autonomous problem-solvers in the AI world, capable of making their own decisions, unlike chains and routers that follow specific instructions. Agents utilize 'tools,' which are specific functions like calculators or search engines, to complete tasks. A popular method for building these agents is the ReAct (Reasoning + Acting) pattern, which mimics human thought through a continuous loop of 'Think, Action, Action Input, Observe.' This pattern involves an LLM thinking about a problem, deciding on an action or tool, providing necessary inputs, and then observing the tool's output to determine the next step or if the problem is solved. The process, often facilitated by systems like LangChain, continues until a final answer is achieved, combining the LLM's reasoning with external capabilities to form an agent.\n","\n","TOPICS:\n"," ['Agentic AI', 'Prompt Engineering', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the primary characteristic that distinguishes AI agents from chains and routers?\n","A: AI agents can make autonomous decisions and decide for themselves what steps to take, whereas chains and routers follow specific instructions.\n","Q: What are 'tools' in the context of AI agents?\n","A: Tools are specific functions or special abilities (like a calculator or search engine) that agents can use to complete tasks.\n","Q: Describe the core loop of the ReAct agent pattern.\n","A: The ReAct pattern involves a continuous loop of 'Think' (LLM considers the problem), 'Action' (LLM decides what to do or which tool to use), 'Action Input' (LLM provides arguments for the tool), and 'Observe' (LLM evaluates the tool's output).\n","\n","KEY CONCEPTS:\n"," AI agents, Autonomous decisions, Agent tools, ReAct agent pattern, Reasoning plus acting, Think-Action-Observation loop, LLM reasoning ability, Action input, Control flow, LangChain system\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," This section details tracing a reflection agent system designed to create viral tweets, utilizing Lsmith for visualization and debugging. The system employs an iterative process involving a \"generation agent\" that drafts tweets and a \"reflect agent\" that critiques them, offering feedback for refinement. This cycle of generation and reflection, occurring over multiple exchanges, allows for deep thinking and continuous improvement of the tweet. The process culminates in a highly refined, thought-provoking, and viral tweet, demonstrating the power of reflection agents for complex tasks.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the primary purpose of the reflection agent system discussed?\n","A: The system's primary purpose is to iteratively refine and generate a viral tweet through multiple exchanges between generation and reflection agents.\n","Q: How does Lsmith aid in understanding the agent system's operations?\n","A: Lsmith captures and streams operations, allowing users to trace each step and component's contribution within the system.\n","Q: What are the two main types of agents involved in the tweet generation process?\n","A: The two main types are the generation agent, which drafts the tweet, and the reflect agent, which critiques it and provides feedback.\n","\n","KEY CONCEPTS:\n"," Reflection agent system, Viral tweet generation, Lsmith tracing, LangChain integration, Generation agent, Reflect agent, Iterative refinement, Environment variables, Critiquing agent, Complex tasks\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," This section details the process of integrating LangChain's chat models with OpenAI APIs, beginning with the installation of the `langchain-openai` package. It demonstrates how to initialize a `ChatOpenAI` model, such as `gpt-4o`, and make API calls using the `llm.invoke()` method. A key focus is on securely managing the `OPENAI_API_KEY` through an `.env` file and the `python-dotenv` library to resolve common API key errors. The tutorial also illustrates how to extract only the relevant textual content from the LLM's response using `result.content`. Finally, it briefly touches upon OpenAI billing and hints at sending conversation history in subsequent lessons.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What LangChain package is required to work with OpenAI chat models?\n","A: The `langchain-openai` package is required.\n","Q: Which method is used to make API calls to an initialized LangChain LLM?\n","A: The `invoke()` method is used to make API calls.\n","Q: How can the OpenAI API key be securely provided to the application?\n","A: The OpenAI API key can be provided using an `.env` file and loaded with the `python-dotenv` library.\n","\n","KEY CONCEPTS:\n"," LangChain chat models, OpenAI APIs, `langchain-openai` package, `llm.invoke()` method, OpenAI API key, `.env` file, `python-dotenv` library, `load_dotenv()` method, `result.content` property\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1467.63ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," The video demonstrates Python's `sort()` method, explaining its behavior when applied to lists containing strings and mixed data types. When sorting lists of strings, the method prioritizes words starting with uppercase letters, sorting them alphabetically, before sorting words starting with lowercase letters alphabetically. The `reverse` option inverts this order. For lists containing both strings and numbers, `sort()` places all numbers at the beginning of the list, followed by the sorted strings, with the `reverse` option similarly altering this sequence. Understanding these specific sorting rules is essential for effective list manipulation in Python.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: How does Python's `sort()` method handle strings with mixed uppercase and lowercase letters?\n","A: It sorts words starting with uppercase letters first alphabetically, followed by words starting with lowercase letters alphabetically.\n","Q: What is the behavior of `sort()` when a Python list contains both strings and numbers?\n","A: The `sort()` method places all numbers at the beginning of the list, followed by the sorted strings.\n","Q: How does applying `reverse` affect a list sorted by Python's `sort()` method?\n","A: It reverses the established sort order, for example, placing lowercase words (in reverse alphabetical order) before uppercase words (in reverse alphabetical order) for strings, or numbers at the end for mixed lists.\n","\n","KEY CONCEPTS:\n"," Python lists, Sort method, String sorting, Uppercase letters, Lowercase letters, Alphabetical order, Mixed data types, Number sorting, Reverse order, List manipulation\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explores the optimal allocation of decision-making between humans and artificial intelligence, or a combination of both. It highlights that AI excels at high and low confidence predictions, while humans perform better when AI is uncertain, especially for complex or rare cases. Augmented intelligence, which combines human judgment with AI assistance, often yields the highest success rates across various confidence levels. However, for augmented intelligence to be effective, it's crucial to account for human cognitive biases like automation bias, which can be mitigated by presenting AI recommendations optionally rather than forcing their display. Ultimately, understanding when and how to integrate human and AI capabilities can significantly improve decision-making outcomes.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: When do humans typically outperform AI in decision-making?\n","A: Humans often outperform AI when the AI is unsure (around 50% confidence) or for complex/statistically rare cases, by bringing in additional information and context.\n","Q: What is augmented intelligence?\n","A: Augmented intelligence combines human decision-making aided by AI, aiming for the highest success rate across various confidence levels.\n","Q: How can automation bias be mitigated in augmented intelligence systems?\n","A: Automation bias can be mitigated by using an optional display for AI recommendations, allowing humans to consider the case independently before consulting the AI.\n","\n","KEY CONCEPTS:\n"," Decision-making, Artificial intelligence, Human performance curves, AI performance curves, Confidence scores, Fraud detection system, Augmented intelligence, Human cognitive bias, Automation bias, Optional display\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Demitrius, a product manager at Google Cloud AI, discusses building generative applications faster and better using Vertex AI. The talk introduces six new APIs and improvements to existing services designed to address common technical challenges developers face, particularly in grounding applications with reliable enterprise data for accurate and consistent responses. These APIs include document understanding, enhanced embedding and vector search, a ranking API, a grounded generation API, and a check grounding API for fact-checking. Leveraging Google's extensive know-how and technology, these solutions aim to improve the quality and efficiency of generative AI applications. Developers can seamlessly integrate these standalone APIs into their workflows, including popular frameworks like LangChain.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is Demitrius's role at Google?\n","A: He is a product manager within Cloud AI, focusing mostly on search and document AI.\n","Q: What is a key challenge developers face when building generative applications for enterprises?\n","A: Grounding applications to reliably access the right Enterprise data for accurate and consistent responses.\n","Q: Name three new Vertex AI APIs discussed.\n","A: Document understanding API, improved Embedding API, Vector search, Ranking API, Grounded generation API, and Check grounding API.\n","\n","KEY CONCEPTS:\n"," Generative applications, Vertex AI APIs, Document understanding, Embedding API improvements, Vector search enhancements, Ranking API, Grounded generation, Fact-checking statements, Enterprise data grounding, Google know-how\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The discussion focuses on the Singular Value Decomposition (SVD) of a matrix X, particularly highlighting the properties and geometric interpretation of unitary matrices U and V. Unitary matrices are crucial because they preserve the angles and lengths of vectors, essentially acting as rotations within a vector space, similar to the Fourier transform. Geometrically, a matrix X transforms a sphere of unit vectors into an ellipsoid, where the singular values dictate the lengths of the principal axes (stretching or compression) and the singular vectors determine the ellipsoid's orientation. The lecture also briefly touches on the use of complex conjugate transpose for complex-valued data, contrasting it with the standard transpose for real-valued data.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the primary characteristic of unitary matrices?\n","A: Unitary matrices preserve the angles and lengths of vectors, acting as rotations in a vector space.\n","Q: How does a matrix X geometrically transform a sphere of unit vectors?\n","A: A matrix X transforms a sphere of unit vectors into an ellipsoid.\n","Q: What do the singular values of X represent in the geometric transformation?\n","A: The singular values represent the lengths of the principal axes of the resulting ellipsoid, indicating how directions are stretched or compressed.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Unitary matrices, Economy size SVD, Fourier transform, Complex conjugate transpose, Vector space, Geometric interpretation, Ellipsoid, Singular values, Singular vectors\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," This video introduces Google Gemini Pro 1.5, highlighting its advanced multimodal capabilities for processing both text and images, and its significantly expanded context window of up to 1 million tokens. It demonstrates how to obtain and configure an API key for the model, showcasing its use in practical applications. Examples include analyzing a 402-page Apollo 11 transcript for specific information and generating descriptive text or blog posts from combined image and text prompts. The speaker emphasizes that Gemini Pro 1.5 unifies previous separate models into a single, powerful tool for building sophisticated generative AI applications.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is a key advantage of Google Gemini Pro 1.5 over previous models?\n","A: It features a significantly larger context window of up to 1 million tokens and is a unified multimodal model for both text and images.\n","Q: How can users get started with Google Gemini Pro 1.5?\n","A: Users can generate an API key from ai.google.com/app/API key and configure it in their Python environment using the `google.generativeai` library.\n","Q: What types of inputs can Gemini Pro 1.5 process?\n","A: It can process both text and image inputs, allowing for multimodal prompts and generating responses based on combined information.\n","\n","KEY CONCEPTS:\n"," Google Gemini Pro 1.5, Generative AI applications, Multimodal model, Context window, API key generation, Apollo 11 transcript, Image and text inputs, Prompt engineering, Foundation models, generate_content method\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," This section focuses on evaluating and testing prompt engineering models, emphasizing the importance of understanding various matrices to measure performance. Key evaluation metrics discussed include perplexity, accuracy, and human evaluation, which assess a model's ability to generate accurate and meaningful responses. The process also involves debugging models by analyzing generated responses for common errors and fine-tuning them for improvement. Furthermore, testing models on different datasets and tasks is crucial to ensure their generalization ability on new or unseen data, making evaluation an ongoing process.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are some commonly used matrices for evaluating prompt engineering models?\n","A: Commonly used matrices include perplexity, accuracy, and human evaluation.\n","Q: What does perplexity measure in a language model?\n","A: Perplexity measures how well a language model predicts a sequence of words; a lower perplexity indicates a better model.\n","Q: Why is testing prompt engineering models on different datasets important?\n","A: Testing on different datasets helps determine the model's ability to generalize on new or unseen data.\n","\n","KEY CONCEPTS:\n"," Prompt engineering models, Evaluation matrices, Perplexity, Accuracy, Human evaluation, Debugging techniques, Testing techniques, Large language models, Data sets, Model generalization\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains the evolution from basic Generative AI to more sophisticated AI Agents and ultimately Agentic AI. Generative AI, exemplified by Large Language Models (LLMs), primarily generates new content but is limited by knowledge cutoffs and simple Q&A. AI Agents extend this capability by integrating tools and memory, allowing them to perform narrow tasks and take actions, such as searching the web or booking flights. Agentic AI represents the highest level of complexity, where one or more AI agents autonomously tackle multi-step, complex goals, making decisions, utilizing various tools, and even coordinating with other agents for tasks like comprehensive travel planning including visa checks. This evolution signifies an increase in task complexity, autonomy, and sophisticated problem-solving capabilities.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary function of Generative AI?\n","A: Generative AI creates new content like text, images, or video based on patterns learned from existing data.\n","Q: How do AI Agents enhance the capabilities of Large Language Models (LLMs)?\n","A: AI Agents enhance LLMs by providing access to tools (like APIs) and memory, enabling them to perform actions and complete narrow tasks beyond simple Q&A.\n","Q: What distinguishes Agentic AI from a simple AI Agent?\n","A: Agentic AI systems can involve one or more AI agents working autonomously on complex, multi-step tasks, making decisions, and coordinating with other agents to achieve a goal, offering higher complexity and autonomy.\n","\n","KEY CONCEPTS:\n"," Generative AI, Large Language Models (LLMs), Knowledge cutoff, AI Agents, Tools (for AI), Memory (for agents), Autonomous decision making, Agentic AI, Multi-step reasoning, Complex tasks\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The video introduces covariance as a crucial statistical concept for data preprocessing and analysis, explaining its role in quantifying the relationship between two random variables. It details the covariance formula and demonstrates how a positive value indicates both variables increase together, while a negative value signifies an inverse relationship. The speaker uses examples like house size and price to illustrate these concepts graphically. Finally, the video highlights a key limitation of covariance: its inability to quantify the *strength* of the relationship, setting the stage for a discussion on Pearson correlation coefficient.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary purpose of covariance?\n","A: Covariance quantifies the relationship between two random variables.\n","Q: What does a positive covariance value signify?\n","A: A positive covariance indicates that as one variable increases, the other also increases.\n","Q: What is a limitation of covariance discussed in the video?\n","A: Covariance indicates the direction of a relationship but not its strength or magnitude.\n","\n","KEY CONCEPTS:\n"," Covariance, Random variables, Data preprocessing, Data analysis, Relationship quantification, Covariance formula, Positive covariance, Negative covariance, Mean of random variable, Pearson correlation coefficient\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2073.53ms\n","ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2379.90ms\n","ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1518.19ms\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," This video explains how to define objectives in Reinforcement Learning (RL) problems, emphasizing the goal of learning an optimal policy to maximize a numerical reward signal. An RL agent interacts with its environment, making decisions based on observations and learning through trial and error to maximize cumulative reward over time. The process involves defining a reward function that provides feedback on action quality, allowing the agent to update its policy. Examples include episodic tasks like Tic-Tac-Toe, where rewards are given for winning or losing, and continuous tasks such as stock market trading, where profit or risk-adjusted measures serve as rewards. Ultimately, the objective is parameterized by a carefully defined reward function reflecting the desired goal.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary objective of reinforcement learning?\n","A: The objective is to learn an optimal policy that maximizes a numerical reward signal over time.\n","Q: How does an RL agent learn and update its policy?\n","A: An agent learns through trial and error by exploring the environment, taking actions, observing resulting states and rewards, and updating its policy accordingly.\n","Q: What are examples of tasks used to illustrate RL objectives?\n","A: Episodic tasks like Tic-Tac-Toe and continuous tasks such as stock market trading are used as examples.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Objective definition, Optimal policy, Numerical reward signal, Agent-environment interaction, Trial and error learning, Cumulative reward, Reward function, Episodic tasks, Continuous tasks\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," Python dictionaries are essential data structures composed of key-value pairs, declared within curly brackets. Keys must be immutable, while values can be any data type, making them highly useful for mapping one item to another, such as stock prices. The transcript demonstrates how to create dictionaries, including from two lists using `dict()` and `zip()`, and how to manipulate them using methods like `items()`, `keys()`, and `values()`. It also covers accessing, modifying, and deleting key-value pairs. Understanding dictionaries is crucial for data science applications, especially when working with libraries like pandas.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is the fundamental structure of a Python dictionary?\n","A: A Python dictionary consists of key-value pairs, with each pair referred to as an item.\n","Q: What is a key requirement for dictionary keys?\n","A: Dictionary keys must be immutable, such as strings or numbers, and cannot be mutable types like lists.\n","Q: How can you create a dictionary from two separate lists?\n","A: You can create a dictionary from two lists by using the `dict()` function combined with the `zip()` function to pair corresponding elements.\n","\n","KEY CONCEPTS:\n"," Python dictionary, Key-value pairs, Immutable keys, Curly brackets, `dict()` function, `zip()` function, `items()` method, `keys()` method, `values()` method, Data science applications\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," Security professionals are increasingly leveraging AI and automation to combat emerging threats and improve organizational security posture, particularly in identifying and containing data breaches faster. User Behavior Analytics (UBA), powered by AI and machine learning, is crucial for detecting and responding to costly insider threats by analyzing user behavior for anomalies. IBM's QRadar SIEM integrates UBA to provide a comprehensive solution, enabling security analysts to quickly review alerts, prioritize risks, and accelerate investigations from days to minutes. This system utilizes MITRE ATT&CK mappings, natural language insights, and human feedback to enhance analysis and shift security teams towards more proactive defense strategies.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is User Behavior Analytics (UBA)?\n","A: UBA uses machine learning to analyze user behavior, identify anomalies, and detect potential threats, especially insider threats.\n","Q: According to the IBM report, what is the average cost of an insider threat?\n","A: The average cost of an insider threat for an organization was $4.9 Million USD.\n","Q: How does QRadar help security analysts accelerate investigations?\n","A: QRadar uses AI and automation to provide MITRE ATT&CK mappings, natural language insights, and allows human feedback, significantly reducing investigation times from hours or days to minutes.\n","\n","KEY CONCEPTS:\n"," Security Professionals, Emerging Threats, User Behavior Analytics (UBA), Insider Threats, Machine Learning, AI and Automation, SIEM Solution, QRadar, MITRE ATT&CK Mappings, Natural Language Insights\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta has announced the release of Llama 3, an open-source large language model (LLM) that significantly outperforms its predecessor, Llama 2, and competes with leading proprietary models. Available in 8 billion and 70 billion parameter versions, Llama 3 excels in language nuances, contextual understanding, and complex tasks like code generation and instruction following. It was trained on an extensive dataset of over 50 trillion tokens, seven times larger than Llama 2, and supports an 8K context length. Llama 3 is integrated into Meta AI and accessible via platforms like Hugging Face and Kaggle, with instructions provided for local inference and weight downloads.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are the two main parameter variants available for Llama 3?\n","A: Llama 3 is available with 8 billion and 70 billion pre-trained and instruction-tuned versions.\n","Q: How much training data was used for Llama 3 compared to Llama 2?\n","A: Llama 3 was trained on over 50 trillion tokens of data, a training dataset 7x larger than that of Llama 2.\n","Q: What is the context length supported by Llama 3?\n","A: Llama 3 supports an 8K context length, doubling the capacity of Llama 2.\n","\n","KEY CONCEPTS:\n"," Llama 3, Open-source LLM, Performance metrics, 8 billion parameters, 70 billion parameters, Meta AI, Context length, Training data, Code generation, Hugging Face\n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The speaker is explaining the process of writing Python code to create a decision boundary, specifically using the scikit-learn (sklearn) library. The lesson involves using Google to navigate the library's documentation to understand its functions. The core algorithm being implemented is Naive Bayes, with a particular focus on Gaussian Naive Bayes, which was used for a previously demonstrated classifier. The goal is to enable viewers to write similar code themselves by walking through the steps slowly.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What Python library is central to this lesson?\n","A: The scikit-learn library, often abbreviated as sklearn.\n","Q: Which specific algorithm is being implemented for the decision boundary?\n","A: Naive Bayes, particularly Gaussian Naive Bayes.\n","Q: How does the speaker suggest finding information about the library's functions?\n","A: By using Google to search the library's documentation.\n","\n","KEY CONCEPTS:\n"," Python code, decision boundary, Python library, scikit-learn, sklearn, Naive Bayes, Gaussian Naive Bayes, algorithm, documentation, use cases\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion elaborates on Gaussian and log-normal distributions, highlighting their characteristics and practical applications. Gaussian distribution is described by a symmetrical bell curve, with specific percentages of data falling within standard deviations. Log-normal distribution is defined by a random variable whose logarithm is normally distributed, often exhibiting a right-skewed, fatter tail compared to the Gaussian curve. Examples like human height follow Gaussian distribution, while income and product review lengths often follow log-normal distribution. Understanding these distributions is crucial for data preprocessing in machine learning, enabling the transformation of data into a standard normal distribution for improved model accuracy through techniques like log normalization.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the key characteristic of a Gaussian distribution curve?\n","A: It typically follows a symmetrical bell curve, with the middle point representing the mean and 50% of the data on each side.\n","Q: How is a log-normal distribution defined mathematically?\n","A: A random variable belongs to a log-normal distribution if the logarithm of that variable is normally distributed.\n","Q: Why is it important to convert different data distributions to a standard normal distribution in machine learning?\n","A: Converting data to a standard normal distribution ensures all values are on the same scale, which significantly increases model accuracy for various algorithms.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, Normal distribution, Log-normal distribution, Bell curve, Standard deviation, Mean, Empirical formula, Standard normal distribution, Data scaling, Model accuracy, Law of normalization, Random variable\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This video introduces an end-to-end deep learning project series focused on solving potato plant diseases in agriculture. The project aims to build a mobile application that allows farmers to detect early and late blight by taking a picture of the plant. It covers data collection, pre-processing using TF data set and augmentation, model building with Convolutional Neural Networks (CNNs) in TensorFlow, and ML Ops with TF serving and FastAPI. The application will be deployed to Google Cloud Platform, featuring a React Native mobile app and a React.js web app, demonstrating a complete AI solution.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary problem this deep learning project addresses?\n","A: The project addresses economic losses faced by potato farmers due to diseases like early and late blight.\n","Q: Which deep learning model is used for image classification in this project?\n","A: Convolutional Neural Networks (CNNs) are used for image classification.\n","Q: What technologies are used for the mobile and web application frontends?\n","A: React Native is used for the mobile app, and React.js is used for the web app.\n","\n","KEY CONCEPTS:\n"," Deep learning project series, Potato disease detection, Mobile application development, Convolutional Neural Networks, ML Ops, Google Cloud deployment, Data augmentation, TensorFlow Lite quantization, FastAPI backend, Image classification\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explores six levels of autonomy in LLM applications, starting from zero autonomy with hard-coded solutions and progressing to fully autonomous agents. It details the evolution from single LLM calls, which handle one task, to 'chains' that break down problems into sequential specialist steps. 'Routers' introduce decision-making by allowing the AI to choose the next step based on input, though they lack memory or learning capabilities. The highest level discussed is 'State Machines' or 'Agents,' which incorporate loops, memory, human-in-the-loop capabilities, and adaptive learning, enabling iterative refinement and complex, intelligent control flow, often implemented using tools like Langraph.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Agentic AI', 'LangChain']\n","\n","Q&A:\n"," Q: What is the main disadvantage of a single LLM call?\n","A: A single LLM call often leads to confused or mixed-up responses when trying to accomplish multiple things in one prompt.\n","Q: How do 'chains' improve upon single LLM calls?\n","A: Chains break down complex tasks into multiple steps, with each step handled by a specialist AI, creating a smarter system than a single LLM call.\n","Q: What key features define an 'agent' (state machine) compared to a router or chain?\n","A: Agents feature LLM-controlled flow, loops, advanced memory management, human-in-the-loop capabilities, and adaptive learning, allowing for iterative refinement and dynamic decision-making.\n","\n","KEY CONCEPTS:\n"," Levels of autonomy, LLM applications, Code, LLM call, Chains, Routers, State machine, Agents, Langraph, Human in loop, Adaptive learning, Control flow\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This transcript explores advanced prompt engineering, covering techniques for handling diverse prompt types including text, image, and audio. It delves into advanced fine-tuning methods for pre-trained large language models, such as multitask learning and distillation, to enhance model performance. The discussion also highlights best practices for data preprocessing and cleaning, including tokenization and normalization, crucial for model success. Furthermore, it addresses strategies for deploying prompt engineering models in production using frameworks like TensorFlow Serving and Flask, alongside essential ethical considerations like bias, fairness, and privacy.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What advanced techniques are discussed for fine-tuning pre-trained models?\n","A: Multitask learning and distillation are discussed.\n","Q: What are key steps in data preprocessing for prompt engineering models?\n","A: Tokenization and normalization are key steps.\n","Q: What ethical considerations are important in prompt engineering?\n","A: Bias, fairness, and privacy are important ethical considerations.\n","\n","KEY CONCEPTS:\n"," Prompt engineering, Text-based prompts, Image-based prompts, Pre-trained large language models, Multitask learning, Model distillation, Data preprocessing, Tokenization, Model deployment, Ethical considerations\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," This lecture demonstrates the application of Singular Value Decomposition (SVD) to create \"eigenfaces\" for image classification, using action heroes Arnold Schwarzenegger and Sylvester Stallone as a primary example. Images of these individuals are cropped, aligned, and then processed to compute an average face and principal components, allowing their projection into a lower-dimensional eigenface space for clustering. The technique successfully separates Arnold and Stallone, and is further tested with test images like \"Harry Potter Stallone\" and \"TV Terminator.\" A surprising finding emerged when comparing Taylor Swift and Arnold Schwarzenegger, who showed more overlap in eigenface space than Arnold and Stallone, potentially due to shared features like skin and hair tone, illustrating some limitations of naive correlation-based classification.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What mathematical technique is primarily used to find the 'eigenfaces'?\n","A: The Singular Value Decomposition (SVD) is used to compute the eigenfaces from the image matrix.\n","Q: Which two action heroes were initially used to demonstrate image classification using eigenfaces?\n","A: Arnold Schwarzenegger and Sylvester Stallone were initially used for the eigen-heroes example.\n","Q: What surprising result was observed when comparing Taylor Swift and Arnold Schwarzenegger in eigenface space?\n","A: Taylor Swift and Arnold Schwarzenegger showed more overlapping distributions in eigenface space than Arnold and Stallone, possibly due to similar skin tone and hair color.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Eigenfaces, Image classification, Principal Component Analysis (PCA), Average face, Feature space, Dimensionality reduction, Test images, Facial recognition\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," Large Language Models (LLMs) possess strong reasoning abilities but are inherently limited in their capacity to interact with the real world, such as making bookings or accessing external systems. LangChain addresses this limitation by serving as a popular framework that bridges LLMs with real-world APIs and databases. It enables AI applications to perform actions like accessing flight and restaurant booking services, querying private company databases, sending emails, browsing the web, and scraping websites. This framework allows developers to build more capable LLM-powered applications that can act in the real world, while also offering flexibility to easily switch between different LLM models.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is a primary limitation of Large Language Models (LLMs) on their own?\n","A: LLMs are smart and can reason, but they cannot directly interact with the real world, such as making bookings or sending emails.\n","Q: How does LangChain address the limitations of LLMs?\n","A: LangChain acts as a bridge, enabling LLMs to communicate with real-world APIs, databases, and other external services.\n","Q: What are some examples of real-world actions LangChain allows AI to perform?\n","A: LangChain enables AI to access booking APIs, query private company databases, send emails, browse Google, and scrape websites.\n","\n","KEY CONCEPTS:\n"," LangChain framework, Large Language Models (LLMs), Real-world interaction, Application development, API access, Database communication, Reasoning ability, Flight booking, Email sending, Website scraping\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," This video introduces residuals and residual analysis as crucial tools for improving time series forecasting methods. Residuals are defined as the difference between a model's fitted values and the actual time series values, distinct from forecast errors. The analysis focuses on two key properties: residuals should ideally have no autocorrelation and a mean of zero, indicating an unbiased and well-specified model. The tutorial demonstrates how to perform this analysis in Python, using tools like autocorrelation and partial autocorrelation plots, the Young-Box test for statistical significance, and histograms to detect bias. By diagnosing issues like autocorrelation or bias, practitioners can refine and improve their forecasting models.\n","\n","TOPICS:\n"," ['Time Series', 'Python Programming', 'Machine Learning']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: Residuals are the difference between the fitted value (y-hat) and the actual value (y) of the time series.\n","Q: What two key properties should residuals ideally exhibit for a good forecasting model?\n","A: Residuals should have no autocorrelation or partial autocorrelation, and their mean should be zero.\n","Q: How can the presence of autocorrelation in residuals be statistically tested?\n","A: The Young-Box test can be used to determine if residuals are independently distributed or have serial correlation.\n","\n","KEY CONCEPTS:\n"," Residuals, Residual analysis, Forecasting methods, Fitted values, Autocorrelation, Partial autocorrelation, Young-Box test, Bias, Hul-Winters model, Time series crash course\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This video demonstrates building an AI agent capable of interacting with a database using natural language to generate SQL queries. The agent, built with LangGraph and LangChain, leverages large language models from watsonx.ai trained on SQL to connect to an in-memory SQLite database. A Next.js frontend is developed to allow users to input natural language questions, which the ReAct agent processes, generates SQL, executes against the database, and returns the results. The process involves setting up environment variables, defining database schema, creating LangChain tools, and carefully crafting system prompts to guide the LLM's behavior.\n","\n","TOPICS:\n"," ['Agentic AI', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the primary goal of the AI agent built in the video?\n","A: The primary goal is to build an AI agent that can talk to a database by generating SQL queries from natural language input.\n","Q: Which key frameworks and technologies are used for the agent and frontend?\n","A: LangGraph is used for the ReAct agent, LangChain for underlying functionalities, Next.js for the frontend application, and models from watsonx.ai.\n","Q: How is the database interaction handled by the agent?\n","A: An in-memory SQLite database is used, seeded with data, and the agent uses a custom 'GetFromDB' LangChain tool to execute generated SQL queries against it.\n","\n","KEY CONCEPTS:\n"," AI agent, Text2SQL, Large Language Models, LangGraph framework, ReAct agent, SQLite database, Database schema, LangChain tools, System prompts\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," The transcript introduces prompt engineering as a specialized field within Natural Language Processing, focusing on building models that generate high-quality text outputs from user prompts. These models leverage pre-trained large language models such as OpenAI GPT and Google Bard, which are fine-tuned for specific tasks. Prompt engineering offers significant benefits by producing more accurate, coherent, and contextually appropriate text compared to traditional rule-based methods, making it crucial for applications like chatbots and language translation. However, it faces limitations, including struggles with complex prompts and the potential for biased or inaccurate outputs. The course aims to provide a comprehensive introduction, covering fundamentals, prompt analysis, and advanced fine-tuning techniques for large language models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: Prompt engineering is a specialized field within natural language processing that focuses on building models to generate high-quality text outputs in response to prompts or inputs.\n","Q: What are the key benefits of prompt engineering?\n","A: It allows for generating text outputs that are more accurate, coherent, and contextually appropriate than traditional rule-based or keyword-based approaches.\n","Q: What are some limitations of prompt engineering models?\n","A: They may struggle with complex or ambiguous prompts and can generate biased or inaccurate outputs due to underlying data or model architecture.\n","\n","KEY CONCEPTS:\n"," Prompt engineering, Natural language processing, Large language models, Text outputs, Chatbots, Language translation, Prompt analysis, Fine-tuning, Model architecture, User experience\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is a prominent value-based reinforcement learning algorithm designed to maximize numerical reward signals by learning optimal policies. It operates by determining state-action value functions, known as Q-values, which quantify the desirability of taking a specific action in a given state. The learning process involves an agent exploring an environment, calculating observed Q-values using the Bellman equation, and updating a Q-table based on temporal difference errors and a learning rate. Through iterative episodes, the agent refines these Q-values, ultimately learning an optimal target policy. As an off-policy algorithm, Q-learning decouples the exploration (behavior) policy from the learned (target) policy.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What are the three primary machine learning paradigms?\n","A: The three primary machine learning paradigms are supervised learning, unsupervised learning, and reinforcement learning.\n","Q: What is the main objective of Q-learning?\n","A: The main objective of Q-learning is to effectively learn Q-values such that the total reward is maximized, leading to an optimal policy.\n","Q: How is the temporal difference error calculated in Q-learning?\n","A: The temporal difference error is calculated as the difference between the observed Q-value (from the Bellman equation) and the expected Q-value (from the Q-table).\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement learning, Machine learning paradigms, State-action value function, Q-table, Bellman equation, Temporal difference error, Optimal policy, Behavior policy, Target policy\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier is a linear classifier that applies a linear function, essentially a giant matrix multiply, to inputs like image pixels to generate predictions. Machine learning in this context involves training the model to find optimal values for the weights (W) and bias (b) to improve prediction accuracy. For classification, the scores generated are converted into probabilities, aiming for the correct class probability to be near one and others near zero. This conversion is achieved using a softmax function, which transforms any scores into proper probabilities that sum to one, with larger scores yielding larger probabilities.\n","\n","TOPICS:\n"," ['Machine Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is a linear classifier that applies a linear function (matrix multiply) to inputs to generate predictions.\n","Q: How does machine learning apply to a logistic classifier?\n","A: Machine learning involves training the model to find optimal values for the weights (W) and bias (b) to perform good predictions.\n","Q: What function is used to turn scores into probabilities in a logistic classifier?\n","A: The softmax function is used to convert scores into proper probabilities that sum to one.\n","\n","KEY CONCEPTS:\n"," Logistic classifier, Linear classifier, Linear function, Matrix multiply, Weights and bias, Machine learning, Softmax function, Probabilities, Scores, Logits\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n","\n","Zero-Shot pipeline completed successfully.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"id":"MBPEEoCAQRpD","colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["78edafe28098478aafc8ef34fc13a851","9e7b10ff64744c96b11e065642b3a24a","889e4dd284be4dde8886b580f7939973","d95316828deb4e18a5843d712d900e6a","a6684269945a436abaa452193e9b2d11","42a5f394f9d446a49f73b89b0967f706","d83799cc4f6f4d55a67f2a30defd6d68","b75040e6b84f4b3eb958f0bd86cecb35","eb02230b88b64ce4814c96662eafa8e4","4f0c15ecb0d1400597a9418472d2acd2","3951138979c3441aaaadff6517faa877","5206b7cd3c8e410a8d725b80039bb932","e0b34d6e150144d3b5d51ad9d2297483","b96106c198e142939b9568af681a89ee","b2a327c416f34ee7807b5aaf4b894881","d48c5127662c46aa94e5cfb5901028f4","39ab8223574243c8a7d80e3b254366f7","9de2a287a3d94f3ebe85a0e4ba5023ac","09970ff4c30f41b4a0f4db2f866512d0","dfebec6dfb504fb891fdd621bf9f6bc4","4b7cca9cde634e22bc490e8224b9610f","e2f249ce0c2b49a8a50caf030cfa7270","c0bada71a22947f086578455c9b41686","49f121b47cc14409a69c29d3b965a8cb","ec28feb9a6584aefac95e4b6dd231e97","ce90a96701f3460ab70509e6548e5b60","4bc94db12ce34976a870d86bfc05323f","85966a6721164bd08a68f0c94aa09086","e0e297bd000245a087e0b3f152012e0c","fd7912f6c49c4375b955b1b536b659cf","b0e52ce31ed04a3f994e8915e04b9602","969ba69bdff74d59b7fc36d26c945614","a0f7d40f5e3e455186accb5c32119c1c","b0480596a228419baaf9ff879bb05b14","cd16f540384b4c71a8cbf6cb3a482b6a","928ee214deee4025a5ecbd764d45d1b7","ef5b36595f8647268c805aabb3e15902","c0469c082f2a4b2486350e40ea1f3935","054c802ca3d44fcfa6872513f4200d9b","545242f5ac9841668bf5831ca3cd9d92","5706092ce20640ba83f0aeb0d17a4b8f","2fb8585d02a249e0947f13e57737e241","4abce1f7317b4d118f45ac5b37f94ca9","f2d4c5e9e4d44ab0bc15345db0cf4c62","14fc9cdfa39b4eeda87d575a563c0153","733a2b7148e346bcabb6834827a04d0d","c28fbfe177c64b15970d77f0041a12ab","8ab0d19a8b874474a91132734a9e8349","35375efc4faf493088d44dae396540ff","775a4a59085f472fb308ae319416dc86","e9fac8773e9c46ccb533b5398a26bd58","1b72c6288e8b4a538488717ef829e683","b6169c2e5a334ecb8bb45c6e35c2b7a1","33be5b214df84fb188ae156404f2bc86","0eccd088afa144a09f052e3c887a0caf","2745938dfb2b4dfda29dcdb382bb5b99","e527324dd8ec45018e9daf2882ab0ac6","43297b4e6e524d11bf2454cf197c61ca","2ec95dc1731c4cb5b783dbd76c2de831","baac219482704db69bf75c35c776b3c4","fc3ac1dc8101427ca360efc834e698b9","687fc8936e5b48ad83aa3beeb2530505","8ac7bd7d094a435ca8483987d4ac81c8","87a2d4679b3245b38c2d6dc1410d3396","bd47ccf879364d068ba1f1ecc70ac320","337fe228d4774ef99cff6dd26c4d624b"]},"executionInfo":{"status":"ok","timestamp":1763475690183,"user_tz":-330,"elapsed":156883,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"393832b5-41a7-4bb5-ea9f-fe30b9174eff"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/gemini-2.5-flash_zero_shot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78edafe28098478aafc8ef34fc13a851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5206b7cd3c8e410a8d725b80039bb932"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0bada71a22947f086578455c9b41686"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0480596a228419baaf9ff879bb05b14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14fc9cdfa39b4eeda87d575a563c0153"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2745938dfb2b4dfda29dcdb382bb5b99"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3969\n","  - BLEU: 0.1228\n","  - BERTScore F1: 0.9047\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.3913\n","  - Micro F1: 0.5232\n","  - Macro F1: 0.4714\n","  - Weighted F1: 0.4799\n","\n","Q&A Generation:\n","  - BLEU: 0.0309\n","  - Diversity: 0.8120\n","  - Answerability: 0.7778\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5933\n","  - Recall@10: 0.2373\n","  - F1@10: 0.3390\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gemini-2.5-flash/evaluation_final.json\n"]}]}]}