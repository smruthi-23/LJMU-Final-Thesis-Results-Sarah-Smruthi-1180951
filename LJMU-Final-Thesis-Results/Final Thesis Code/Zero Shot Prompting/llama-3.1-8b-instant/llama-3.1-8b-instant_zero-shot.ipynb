{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdXHbWoa99J9OjG8BA2CtW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"v7ZyIA4blt3a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763469468515,"user_tz":-330,"elapsed":20039,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"42349693-cc9a-440c-d99a-ff089a5efaf0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.34.1-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.34.1-py3-none-any.whl (136 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d9d74616997b57855cfa960007b38a7b1cad62708ed4acb7d59e67aa2af77b5b\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.34.1 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"QtdSb-YvFNqD","executionInfo":{"status":"ok","timestamp":1763454563907,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"dc4ddfb9-7fe5-4a2e-aa65-bf9bea0cee47"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.1-8b-instant/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.1-8b-instant_zero_shot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path: str):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.1-8b-instant\"\n","MAX_CHARS      = 2300\n","GLOBAL_MIN_GAP = 45   # Groq is faster, but we keep a safety gap\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\"\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t)\n","    t = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \" \", t)\n","    t = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","\n","    t = re.sub(r\"\\bNLP\\b\", \"Natural Language Processing (NLP)\", t)\n","    t = re.sub(r\"\\bML\\b\", \"Machine Learning (ML)\", t)\n","    t = re.sub(r\"\\bAI\\b\", \"Artificial Intelligence (AI)\", t)\n","    return t.strip()\n","\n","\n","def chunk_text(text: str, max_chars=MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    try:\n","        return json.loads(text[start:end+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (GLOBAL WAIT + RETRIES)\n","#####################################################################\n","def groq_call(prompt: str, temperature=0.15, retries=3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s before next request\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ZERO-SHOT PROMPT (ALL TASKS IN ONE CALL)\n","#####################################################################\n","def generate_zero_shot(transcript: str):\n","    topics_short = \", \".join(VALID_TOPICS)\n","\n","    prompt = (\n","        \"You are an expert NLP assistant.\\n\"\n","        \"Perform ALL tasks concisely using ONLY the transcript.\\n\\n\"\n","        \"Respond in a JSON-like object with EXACTLY these keys:\\n\"\n","        \"generated_summary, predicted_topics, generated_questions, key_concepts.\\n\\n\"\n","        \"Requirements:\\n\"\n","        \"- generated_summary: 3–5 sentence abstractive summary.\\n\"\n","        f\"- predicted_topics: 1–3 most relevant from [{topics_short}]. Use exact labels.\\n\"\n","        \"- generated_questions: 3 short Q&A pairs with keys q and a.\\n\"\n","        \"- key_concepts: 6–10 short noun phrases, no duplicates.\\n\"\n","        \"- No explanations outside the JSON.\\n\\n\"\n","        f\"Transcript:\\n\\\"\\\"\\\"{transcript}\\\"\\\"\\\"\"\n","    )\n","\n","    out = groq_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    summary = j.get(\"generated_summary\", \"\").strip()\n","\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","    topics = [t for t in topics if t in VALID_TOPICS] or [\"Other\"]\n","\n","    qas = j.get(\"generated_questions\", [])\n","    qa_lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = qa.get(\"q\", \"\").strip()\n","            a = qa.get(\"a\", \"\").strip()\n","            if q: qa_lines.append(f\"Q: {q}\")\n","            if a: qa_lines.append(f\"A: {a}\")\n","\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    concepts = \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","    return summary, topics, \"\\n\".join(qa_lines), concepts\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — ZERO SHOT (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming — {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary, topics, qa_text, concepts = generate_zero_shot(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error on row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"DONE. Final file:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nZero-shot Groq pipeline completed ✓\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k6aB21AbFNs9","executionInfo":{"status":"ok","timestamp":1763455983265,"user_tz":-330,"elapsed":1345584,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"8e575d05-e24b-4404-de4d-f98238494881"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.1-8b-instant\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning through human feedback is a framework that integrates human feedback into the training process of a reinforcement learning algorithm. This framework allows the algorithm to make more informed decisions and enhances its capabilities. In the context of Chat GPT, human feedback is given via the rewards model, which is a GPT architecture that assesses the quality of answers generated by Chat GPT. The iterative training process with reinforcement learning through human feedback makes Chat GPT a powerful tool for generating high-quality responses.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the primary purpose of the rewards model in Chat GPT?\n","A: To assess and score the quality of answers generated by Chat GPT\n","Q: How does human feedback contribute to reinforcement learning?\n","A: It accelerates the learning process\n","Q: What is the name of the algorithm used to fine-tune Chat GPT?\n","A: Proximal Policy Optimization\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Human Feedback, Chat GPT, Rewards Model, Proximal Policy Optimization, Grid World, Algorithms, Training Process, Decision Making, Learning Process\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial covers working with CVX opt and kernels in a Support Vector Machine (SVM). The code demonstrates how to use CVX opt to visualize the impact of kernels on the SVM. The tutorial also covers how to use different kernels, including the linear, polynomial, and radial basis function (RBF) kernels. The code is written in Python and uses the scikit-learn library.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Machine Learning', 'Deep Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is CVX opt?\n","A: CVX opt is a quadratic programming solver used in the Support Vector Machine (SVM) algorithm.\n","Q: What is the purpose of kernels in an SVM?\n","A: Kernels are used to transform the data into a higher-dimensional space to improve the accuracy of the SVM model.\n","Q: What is the difference between a hard margin and a soft margin SVM?\n","A: A hard margin SVM is a binary classifier that separates the data with a clear boundary, while a soft margin SVM allows for some overlap between the classes.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine (SVM), CVX opt, Kernels, Quadratic programming, Radial basis function (RBF) kernel, Polynomial kernel, Linear kernel, Soft margin, Hard margin, Scikit-learn library\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a crucial aspect of generating accurate text outputs from large language models. Understanding the different types of prompts, their key features, and constraints is essential for choosing the right prompt for desired outputs. Deconstructing a prompt involves breaking it down into individual components to better understand its features and constraints.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the main goal of prompt engineering?\n","A: To generate accurate text outputs from large language models.\n","Q: What are the key features of a prompt?\n","A: Length, specific language, context, and constraints.\n","Q: What is deconstruction in prompt engineering?\n","A: Breaking down a prompt into individual components to understand its features and constraints.\n","\n","KEY CONCEPTS:\n"," Prompt engineering, Large language models, Text outputs, Key features, Constraints, Deconstruction, Natural language processing, Machine learning, Prompt types, Context\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents are problem solvers that can think on their own, making autonomous decisions. They use tools, which are specific functions that enable them to complete tasks. The React Agent Pattern is a popular method for creating AI agents, mimicking human thought processes. It involves thinking, acting, observing, and repeating until a solution is found.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Prompt Engineering', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the React Agent Pattern?\n","A: It's a method for creating AI agents that mimics human thought processes, involving thinking, acting, observing, and repeating until a solution is found.\n","Q: What are tools in the context of AI agents?\n","A: Tools are specific functions that enable AI agents to complete tasks.\n","Q: How does the React Agent Pattern work?\n","A: It involves an LLM thinking, acting, observing, and repeating until a solution is found, with the LLM suggesting input arguments for tools and the system executing the tools with those arguments.\n","\n","KEY CONCEPTS:\n"," AI agents, React Agent Pattern, tools, LLM, thinking, acting, observing, Lang chain, Lang graph, prompt engineering\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The speaker demonstrates a reflection agent system that generates a viral tweet through multiple iterations of generation and critique. The system uses LangChain and LSmith to track and refine the tweet. The speaker explains how the system works, including the generation node, reflect node, and the iterative process of generating and critiquing the tweet.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Generative AI']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: LangChain is a platform that supports LSmith and other reflection agents.\n","Q: How does the reflection agent system work?\n","A: The system uses a generation node, reflect node, and iterative process to generate and refine a viral tweet.\n","Q: What is LSmith?\n","A: LSmith is a tool that tracks and records the output of the reflection agent system.\n","\n","KEY CONCEPTS:\n"," Reflection agent, LangChain, LSmith, Generation node, Reflect node, Iterative process, Viral tweet, Natural Language Processing, Prompt Engineering, Generative AI\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The speaker demonstrates how to use LangChain's chat models to communicate with OpenAI's APIs. They install the necessary package, import the chat model, and initialize it with a specific model. They then make an API call to OpenAI using the `invoke` method and store the response in a variable. However, they encounter an error due to a missing API key and resolve it by creating an ENV file and installing the `python-dotenv` package. Finally, they successfully retrieve the response from OpenAI and extract the content property.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Prompt Engineering', 'LangChain']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: LangChain is a library for building conversational AI models.\n","Q: How do you initialize a chat model in LangChain?\n","A: You import the chat model from the LangChain library and initialize it with a specific model.\n","Q: What is the purpose of the `invoke` method in LangChain?\n","A: The `invoke` method is used to make an API call to OpenAI's APIs.\n","\n","KEY CONCEPTS:\n"," LangChain chat models, OpenAI APIs, API key, ENV file, python-dotenv package, invoke method, conversational AI models, API call, response content, content property\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," The speaker demonstrates how Python's sort method handles lists containing strings and numbers. When sorting a list with both uppercase and lowercase letters, Python prioritizes uppercase letters. When inserting a number into a list of strings, Python places the number at the start of the list. The speaker also shows how sorting a list in reverse order affects the placement of numbers and strings.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Python Programming', 'Machine Learning']\n","\n","Q&A:\n"," Q: How does Python's sort method handle lists with both strings and numbers?\n","A: Python places numbers first and then strings.\n","Q: What happens when sorting a list with both uppercase and lowercase letters?\n","A: Python prioritizes uppercase letters.\n","Q: How does inserting a number into a list of strings affect the sort order?\n","A: The number is placed at the start of the list.\n","\n","KEY CONCEPTS:\n"," Python, sort method, lists, strings, numbers, uppercase letters, lowercase letters, reverse order, insertion, sorting\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," A decision-making process can be improved by combining human and artificial intelligence. An AI system can help alleviate the workload of financial analysts in fraud detection, but it's essential to determine which alerts the AI should handle and which should be processed by a human. Augmented intelligence, which combines both human and AI decision-making, can have the highest success rate for low and high confidence scores. However, human cognitive bias must be considered when presenting AI recommendations to human decision-makers.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the main advantage of using an AI system in fraud detection?\n","A: It can help alleviate the workload of financial analysts by handling false positives.\n","Q: What is the key to effective augmented intelligence?\n","A: Minimizing human cognitive bias in the decision-making process.\n","Q: Why is it essential to consider human cognitive bias when presenting AI recommendations?\n","A: Because it can lead to automation bias and influence human decision-makers to favor AI suggestions over their own judgment.\n","\n","KEY CONCEPTS:\n"," Fraud detection, Artificial intelligence, Human bias, Augmented intelligence, Automation bias, Decision-making process, Financial analysts, Confidence scores, Success rate, Cognitive bias\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Dimitrius, a product manager at Google, discussed six new Vertex AI APIs designed to help developers build generative applications for Enterprises. These APIs aim to solve technical challenges, such as document understanding, embedding, and ranking, to produce accurate and consistent responses. The APIs leverage Google's expertise and knowledge to provide high-quality solutions, including a document understanding API, improved embedding API, vector search, ranking API, grounded generation API, and fact-checking API.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Generative AI', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the main goal of the new Vertex AI APIs?\n","A: To help developers build generative applications for Enterprises and solve technical challenges.\n","Q: What is the document understanding API used for?\n","A: To process large amounts of documents and improve the quality of retrieval and answer generation components.\n","Q: What is the fact-checking API used for?\n","A: To check statements against evidence and provide information on whether the statement is supported or contradicted.\n","\n","KEY CONCEPTS:\n"," Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Fact-Checking API, Vertex AI, Generative Applications, Enterprise Solutions, Google Expertise\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The singular value decomposition (SVD) of a matrix X is discussed, focusing on the unitary matrices U and V. These matrices preserve angles and lengths of vectors, making them essential in science and engineering. The economy SVD is introduced, which is a more compact representation of the SVD. The concept of unitary transformations is explained, and its importance in preserving geometric structure is highlighted. A geometric perspective is presented, where the matrix X is seen as a transformation that maps a sphere of unit length vectors to an ellipsoid, with the singular values determining the elongation and orientation of the ellipsoid.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Deep Learning', 'Machine Learning', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the economy SVD?\n","A: A more compact representation of the SVD, where only the first M columns of U and the first M by M sub-block of Sigma are used.\n","Q: What is a unitary transformation?\n","A: A transformation that preserves angles and lengths of vectors.\n","Q: What is the geometric interpretation of the SVD?\n","A: The matrix X maps a sphere of unit length vectors to an ellipsoid, with the singular values determining the elongation and orientation of the ellipsoid.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Unitary Matrices, Economy SVD, Unitary Transformations, Geometric Interpretation, Ellipsoid, Singular Values, Column Space, Row Space, Vector Space\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," The speaker introduces Google Gemini Pro 1.5, a generative AI model that can work with both text and images. They demonstrate its capabilities by uploading a PDF of the Apollo 11 transcript and asking the model to find comedic moments, list quotes, and identify a scene in a drawing. The speaker also shows how to create an API key and use the model to generate content, including text and images. They highlight the model's context length of up to 1 million tokens and its potential applications in developing generative AI-powered applications.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Generative AI', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the main difference between Google Gemini Pro 1.5 and previous versions?\n","A: The main difference is the context length, which has increased from 32k to 1 million tokens.\n","Q: How can I create an API key for Google Gemini Pro 1.5?\n","A: You can create an API key by going to ai.google.com/apppp API key and following the instructions.\n","Q: What are the potential applications of Google Gemini Pro 1.5?\n","A: The model can be used to develop generative AI-powered applications, including text and image generation, and can be used in a variety of fields such as education, marketing, and more.\n","\n","KEY CONCEPTS:\n"," Google Gemini Pro 1.5, Generative AI, Context length, API key, Natural Language Processing, Machine Learning, Text generation, Image generation, Multimodal model, Apollo 11 transcript\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," To evaluate prompt engineering models, matrices such as perplexity, accuracy, and human evaluation are used. Perplexity measures a language model's ability to predict a sequence of words, while accuracy measures the correctness of generated responses. Human evaluation involves having humans rate the quality of responses. By analyzing generated responses and identifying common errors or patterns, models can be fine-tuned and improved.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is perplexity in the context of language models?\n","A: Perplexity measures a language model's ability to predict a sequence of words.\n","Q: What is human evaluation in prompt engineering?\n","A: Human evaluation involves having humans rate the quality of responses.\n","Q: What is the importance of testing prompt engineering models on different data sets?\n","A: Testing on different data sets helps determine a model's ability to generalize on new or unseen data.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Perplexity, Accuracy, Human Evaluation, Language Models, Model Evaluation, Model Testing, Debugging, Improving Models, Generalization\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," Generative AI is a type of AI that can create new content based on patterns learned from existing data. It can be trained on a large volume of internet data and can generate text, images, and videos. AI agents are programs that take input, think, and act to complete a task, often using tools and memory. Agentic AI is a system where one or more AI agents work autonomously to make decisions and reach a goal, often using multiple agents and tools.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Generative AI', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is the difference between Generative AI and AI Agent?\n","A: Generative AI is a type of AI that can create new content, while AI Agent is a program that takes input, thinks, and acts to complete a task.\n","Q: What is Agentic AI?\n","A: Agentic AI is a system where one or more AI agents work autonomously to make decisions and reach a goal, often using multiple agents and tools.\n","Q: What is the role of LLM in Agentic AI?\n","A: LLM is a core component of Agentic AI, providing the ability to generate text and make decisions.\n","\n","KEY CONCEPTS:\n"," Generative AI, AI Agent, Agentic AI, Large Language Model, Tools and Memory, Autonomous Decision Making, Multi-Step Reasoning, Complex Goals, N8N, Agno Framework\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The speaker discussed the concept of covariance, a measure of the relationship between two random variables. They explained how covariance can be used to quantify the relationship between variables, and how it can be positive or negative depending on the direction of the relationship. The speaker also mentioned that covariance does not provide information about the strength of the relationship, and that Pearson correlation coefficient is used to overcome this limitation.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is covariance?\n","A: Covariance is a measure of the relationship between two random variables.\n","Q: When is covariance positive?\n","A: Covariance is positive when the variables are increasing together.\n","Q: What is the limitation of covariance?\n","A: Covariance does not provide information about the strength of the relationship between variables.\n","\n","KEY CONCEPTS:\n"," Covariance, Random Variables, Relationship between Variables, Positive Covariance, Negative Covariance, Pearson Correlation Coefficient, Machine Learning, Statistics, Data Analysis, Data Pre-processing\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," In this video, we learn how to define the objective of a reinforcement learning problem. The objective is a goal given to an agent, which can be episodic or continuous. We use examples like Tic-Tac-Toe and stock market trading to understand how to define rewards and parameterize the agent's objective. The ultimate goal of reinforcement learning is to maximize the cumulative reward over time.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the ultimate goal of reinforcement learning?\n","A: To maximize the cumulative reward over time.\n","Q: How do you define rewards in a reinforcement learning problem?\n","A: Rewards are defined based on the goal and preferences of the agent.\n","Q: What is the reward function in reinforcement learning?\n","A: A mathematical expression that assigns a numerical value to each state or action of the agent.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Agent Objective, Reward Function, Episodic Tasks, Continuous Tasks, Tic-Tac-Toe, Stock Market Trading, Cumulative Reward, Expected Cumulative Reward, Sharp Ratio\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," A dictionary in Python is a data type consisting of key-value pairs, where each key is immutable and can be a string or number. It's useful for mapping one item to another, such as stock prices. Dictionaries can be created using curly brackets and can be manipulated using various functions, including items(), keys(), and values().\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is the purpose of a dictionary in Python?\n","A: A dictionary is used for mapping one item to another.\n","Q: How are key-value pairs separated in a dictionary?\n","A: Key-value pairs are separated by a colon and items are separated by a comma.\n","Q: What function is used to create a dictionary from two lists?\n","A: The dict function is used to create a dictionary from two lists.\n","\n","KEY CONCEPTS:\n"," Dictionary, Key-value pairs, Immutable keys, Curly brackets, Items() function, Keys() function, Values() function, Dict function, Zip function, Tuples\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," User Behavior Analytics (UBA) with AI and machine learning can help detect and respond to Insider threats quickly and precisely. UBA uses machine learning to analyze user behavior and identify anomalies and potential threats. By integrating UBA with a SIEM solution, security professionals can assist in detecting and responding to Insider threats more effectively.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is User Behavior Analytics (UBA)?\n","A: UBA is a technology that uses machine learning to analyze user behavior and identify anomalies and potential threats.\n","Q: How does UBA help detect Insider threats?\n","A: UBA helps detect Insider threats by analyzing user behavior and identifying anomalies and potential threats.\n","Q: What is the benefit of integrating UBA with a SIEM solution?\n","A: Integrating UBA with a SIEM solution assists security professionals in detecting and responding to Insider threats more effectively.\n","\n","KEY CONCEPTS:\n"," User Behavior Analytics, AI, Machine Learning, Insider threats, SIEM solution, Q radar, IBM Security, Curate, No text SIM, Miter attack mappings, Tactics and techniques, Threat actors, Malware families, High value assets, High value users\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Lama 3 is an open-source large language model (LLM) developed by Meta, surpassing its predecessor Lama 2 in performance metrics. With 8 billion and 70 billion pre-trained and instruction-tuned versions, Lama 3 supports a wide range of applications, exceling in language nuances, contextual understanding, and complex tasks like translation and dialog generation. The model has been trained on 50 trillion tokens of data, 7x larger than Lama 2, and is available for download on Meta, Hugging Face, and Kaggle.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Generative AI', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is Lama 3?\n","A: Lama 3 is an open-source large language model (LLM) developed by Meta.\n","Q: What are the key features of Lama 3?\n","A: Lama 3 excels in language nuances, contextual understanding, and complex tasks like translation and dialog generation.\n","Q: Where can I download Lama 3?\n","A: Lama 3 is available for download on Meta, Hugging Face, and Kaggle.\n","\n","KEY CONCEPTS:\n"," Lama 3, Meta, Large Language Model, Natural Language Processing, Artificial Intelligence, Generative AI, Machine Learning, Language Nuances, Contextual Understanding, Complex Tasks\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The speaker will walk through the steps of writing Python code using the scikit-learn library to create a decision boundary. They will use Google to find documentation on the library and its functions, specifically Naive Bayes. The speaker will explain the Naive Bayes formula and its use cases, including Gaussian Naive Bayes.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is Naive Bayes?\n","A: A type of machine learning algorithm used for classification tasks.\n","Q: What is scikit-learn?\n","A: A Python library used for machine learning tasks.\n","Q: What is Gaussian Naive Bayes?\n","A: A specific type of Naive Bayes algorithm used for Gaussian distributions.\n","\n","KEY CONCEPTS:\n"," Decision boundary, Scikit-learn, Naive Bayes, Gaussian Naive Bayes, Python code, Machine learning algorithm, Classification tasks, Google documentation, Library functions\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:__main__:Error on row 19: 'list' object has no attribute 'strip'\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:__main__:Error on row 20: 'list' object has no attribute 'strip'\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The speaker discussed the levels of autonomy in LLM applications, starting from zero autonomy to maximum autonomy. The first level is code, which has zero autonomy and is 100% deterministic. The second level is LLM call, where the app does one main thing, and the third level is chains, where multiple specialists work together. The fourth level is router, where the AI itself decides what steps to take next. The fifth level is state machine or agent, where the AI takes care of deciding the output and which steps to take, and the sixth level is completely autonomous agents. The speaker also discussed the difference between human-driven and agent-executed systems.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the difference between a chain and a router?\n","A: A chain or a router is one directional, whereas a state machine can have cycles and the flow is controlled by the LLM.\n","Q: What is the main advantage of using a state machine?\n","A: The main advantage of using a state machine is that it can have cycles and the flow is controlled by the LLM, allowing for actual intelligence to happen.\n","Q: What is the difference between human-driven and agent-executed systems?\n","A: Human-driven systems are controlled by humans, whereas agent-executed systems are controlled by AI agents.\n","\n","KEY CONCEPTS:\n"," LLM applications, Autonomy levels, Code, LLM call, Chains, Router, State machine, Agent, Human-driven systems, Agent-executed systems\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This section explores advanced topics in prompt engineering, including handling different types of prompts, fine-tuning pre-trained large language models, and deploying prompt engineering models in production. Techniques such as multitasking learning, distillation, and self-supervised learning are discussed. Best practices for data pre-processing and cleaning are also covered, including tokenization, normalization, and data augmentation. The importance of ethical considerations in prompt engineering is emphasized, including fairness and privacy.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is multitasking learning in prompt engineering?\n","A: Multitasking learning involves training a model on multiple tasks simultaneously to learn more robust representations that can generalize to different users.\n","Q: What is distillation in prompt engineering?\n","A: Distillation involves training a smaller model to mimic the behavior of a larger model, making the smaller model more efficient and faster.\n","Q: What is the importance of data pre-processing in prompt engineering?\n","A: Data pre-processing is crucial in prompt engineering as it helps the model understand the meaning of the text more accurately and avoid confusion between words with different meanings.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Multitasking Learning, Distillation, Self-Supervised Learning, Data Pre-Processing, Tokenization, Normalization, Data Augmentation, Ethical Considerations, Fairness, Privacy\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The lecturer demonstrates the use of Singular Value Decomposition (SVD) and eigenfaces to cluster images of Arnold Schwarzenegger and Sylvester Stallone. They load the images, compute the average face, subtract it from each image, and then perform SVD to find the principal components. The lecturer projects the images into the first three eigenfaces and plots the results, showing good separation between the two actors. They then repeat the process with Taylor Swift and Stallone, and surprisingly find that they are more similar in eigenface space than Arnold and Stallone. The lecturer discusses the limitations of this approach and how it can be improved by considering 3D geometry and depth information.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Deep Learning', 'Generative AI', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the purpose of using Singular Value Decomposition (SVD) in image classification?\n","A: To find the principal components of the images and reduce their dimensionality.\n","Q: How do the lecturer's results change when they use Taylor Swift instead of Arnold Schwarzenegger?\n","A: The results show that Taylor Swift and Stallone are more similar in eigenface space than Arnold and Stallone.\n","Q: What is the limitation of the eigenface approach?\n","A: It only considers 2D information and does not account for 3D geometry and depth.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Components, Image Classification, Dimensionality Reduction, 3D Geometry, Depth Information, Taylor Swift, Arnold Schwarzenegger, Sylvester Stallone\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a framework that acts as a bridge between large language models (LLMs) and the real world, enabling applications to interact with APIs, databases, and other external systems. It allows developers to build applications that can reason with LLMs while also communicating with the real world. LangChain is the most popular framework for building applications using LLMs, making it easy to switch between different LLMs without modifying the code.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'LangChain', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: LangChain is a framework that acts as a bridge between large language models and the real world.\n","Q: What are the limitations of large language models?\n","A: Large language models are smart and can reason with data, but they cannot interact with the real world.\n","Q: What can LangChain do?\n","A: LangChain can access APIs, databases, send emails, browse the web, and scrape websites, among other things.\n","\n","KEY CONCEPTS:\n"," LangChain, Large Language Models, APIs, Databases, Real World Interaction, Machine Learning, Natural Language Processing, Artificial Intelligence, LLMs, Frameworks\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:__main__:Error on row 25: 'list' object has no attribute 'strip'\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This video demonstrates how to build a Text2SQL agent using LangGraph, Next.js, and WatsonX AI. The agent connects to a database using SQL queries generated by a large language model. The agent is built using a ReAct agent and uses LangChain to connect to models on WatsonX AI. The database is an in-memory SQLite database. The agent can generate SQL queries to retrieve data from the database and can also join tables to answer complex questions.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the purpose of the Text2SQL agent?\n","A: The Text2SQL agent is used to connect to a database using SQL queries generated by a large language model.\n","Q: What is LangGraph used for in the Text2SQL agent?\n","A: LangGraph is used to build a ReAct agent and connect to models on WatsonX AI.\n","Q: What type of database is used in the Text2SQL agent?\n","A: The database used in the Text2SQL agent is an in-memory SQLite database.\n","\n","KEY CONCEPTS:\n"," Text2SQL agent, LangGraph, ReAct agent, WatsonX AI, SQL queries, Large language model, Database connection, SQLite database, SQL queries generation, Table joining\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a field within NLP that focuses on building models to generate high-quality text outputs in response to prompts. It uses pre-trained large language models fine-tuned for specific tasks and inputs. Prompt engineering allows for more accurate, coherent, and contextually appropriate outputs than traditional approaches, but it can struggle with complex prompts and generate biased outputs.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: Prompt engineering is a field within NLP that focuses on building models to generate high-quality text outputs in response to prompts.\n","Q: What are the benefits of prompt engineering?\n","A: Prompt engineering allows for more accurate, coherent, and contextually appropriate outputs than traditional approaches.\n","Q: What are the limitations of prompt engineering?\n","A: Prompt engineering can struggle with complex prompts and generate biased outputs due to underlying data or model architecture.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Pre-trained Large Language Models, Fine-tuning, Text Outputs, Chatbots, Language Translation, Content Generation\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," The speaker discusses Q-learning, a value-based reinforcement learning method. They explain how Q-learning works by learning a state-action value function, which is a table of Q-values for each state-action pair. The goal is to maximize the total reward by updating the Q-values based on the temporal difference error. The speaker also explains how Q-learning is an off-policy algorithm, meaning it can learn a target policy while using a behavior policy to explore the environment.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is Q-learning?\n","A: Q-learning is a value-based reinforcement learning method.\n","Q: What is the goal of Q-learning?\n","A: The goal of Q-learning is to maximize the total reward by updating the Q-values based on the temporal difference error.\n","Q: What type of algorithm is Q-learning?\n","A: Q-learning is an off-policy algorithm.\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement Learning, Value-Based Methods, Temporal Difference Error, Q-Table, State-Action Value Function, Behavior Policy, Target Policy, Off-Policy Algorithm, Machine Learning Paradigms\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier applies a linear function to input data to generate predictions. The linear function is a matrix multiply that takes inputs as a vector and multiplies them with a matrix to generate predictions. The model is trained by finding values for the weights and bias that perform well in predictions.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier applies a linear function to input data to generate predictions.\n","Q: How are scores turned into probabilities?\n","A: Using a softmax function.\n","Q: What is the purpose of the bias term?\n","A: To adjust the model's predictions.\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Function, Matrix Multiply, Weights, Bias, Softmax Function, Logits, Predictions, Probabilities, Machine Learning\n","Saved row 29\n","DONE. Final file: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_zero_shot_full_output.xlsx\n","\n","Zero-shot Groq pipeline completed ✓\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P8qQrRUl-Zhn","executionInfo":{"status":"ok","timestamp":1763469554676,"user_tz":-330,"elapsed":21297,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"7838278c-ee57-4f0c-e803-a2f9678b69a0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_zero_shot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.1-8b-instant/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_ht-hBF96F1","executionInfo":{"status":"ok","timestamp":1763470816158,"user_tz":-330,"elapsed":127922,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"b0fb4250-5c8b-46ef-fe49-506cb66f4025"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_zero_shot_full_output.xlsx\n","\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2934\n","  - BLEU: 0.0527\n","  - BERTScore F1: 0.8889\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3389\n","  - Micro F1: 0.4678\n","  - Macro F1: 0.4031\n","  - Weighted F1: 0.4016\n","\n","Q&A Generation:\n","  - BLEU: 0.0484\n","  - Diversity: 0.6920\n","  - Answerability: 0.7400\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5033\n","  - Recall@10: 0.2013\n","  - F1@10: 0.2876\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.1-8b-instant/evaluation_final.json\n"]}]}]}