{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOG4oqjmi8er2pbihjk5aSn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c832405e42914de0bd50e7d730420db4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_644b51a585f44b07b5bdb34317508b6c","IPY_MODEL_cbaaa86b0e244a6891425dd7449c8d6c","IPY_MODEL_eadc0ed1b9e04fe381f8ff6abe85dde6"],"layout":"IPY_MODEL_8fb833e6a6854fda9e6ebd28c8bc432b"}},"644b51a585f44b07b5bdb34317508b6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52e2bda16ff24313bca861ead060847b","placeholder":"​","style":"IPY_MODEL_5592d1fe9cfd4ba4bfa2e89df951c7bd","value":"tokenizer_config.json: 100%"}},"cbaaa86b0e244a6891425dd7449c8d6c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8d493fa753f4ef8861904d9ca1a1d26","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_670cafb9200b4886a543fa9726122ee2","value":25}},"eadc0ed1b9e04fe381f8ff6abe85dde6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71c0c050982c43dc9cc33374b2107b72","placeholder":"​","style":"IPY_MODEL_3f008387eff44057830c93b4a9504861","value":" 25.0/25.0 [00:00&lt;00:00, 1.04kB/s]"}},"8fb833e6a6854fda9e6ebd28c8bc432b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52e2bda16ff24313bca861ead060847b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5592d1fe9cfd4ba4bfa2e89df951c7bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8d493fa753f4ef8861904d9ca1a1d26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"670cafb9200b4886a543fa9726122ee2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71c0c050982c43dc9cc33374b2107b72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f008387eff44057830c93b4a9504861":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e698412af51341e0a538a9f1b32fc2b2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_739f6d306c2442c2b5f64f273847213b","IPY_MODEL_4f629bfb59b84c139bd7b4b48867f471","IPY_MODEL_7af1f852bcdb46ceb3302a040c6e9578"],"layout":"IPY_MODEL_952a285aba794b56beb9d1c0b2b59391"}},"739f6d306c2442c2b5f64f273847213b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cb1ea32bd71489a8632df40cec2904c","placeholder":"​","style":"IPY_MODEL_690828bfa57c4eb89e07dfe32e9bb050","value":"config.json: 100%"}},"4f629bfb59b84c139bd7b4b48867f471":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_405e07fd5ca44f708f131cb66b867ee0","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c792de9c95344251b5c23f20901dc57a","value":482}},"7af1f852bcdb46ceb3302a040c6e9578":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a70e2c7f87e49f2bb92cb5f71e95c60","placeholder":"​","style":"IPY_MODEL_80ec044e8c89404c8eeddfd93634d089","value":" 482/482 [00:00&lt;00:00, 7.33kB/s]"}},"952a285aba794b56beb9d1c0b2b59391":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cb1ea32bd71489a8632df40cec2904c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"690828bfa57c4eb89e07dfe32e9bb050":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"405e07fd5ca44f708f131cb66b867ee0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c792de9c95344251b5c23f20901dc57a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a70e2c7f87e49f2bb92cb5f71e95c60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80ec044e8c89404c8eeddfd93634d089":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f50851a28974437d849d7c1279272d8c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a19f854546904e11944a750491704b24","IPY_MODEL_77336ef5282648948b72e16a6e8d4fa7","IPY_MODEL_72c321a8505f42f48ac85f7c675453ae"],"layout":"IPY_MODEL_380a4e6acce4479c879797ae024b4497"}},"a19f854546904e11944a750491704b24":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d300a9889f74d5a9b579fcde1bb93ed","placeholder":"​","style":"IPY_MODEL_fe7bce62101444f49de97632f7aacd05","value":"vocab.json: 100%"}},"77336ef5282648948b72e16a6e8d4fa7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_196648811c224f3e9eb510cb17e278bc","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9194ff9531bf46fab3f8baca5e4598a3","value":898823}},"72c321a8505f42f48ac85f7c675453ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbcb88b3793e44ad9fd006414869db0a","placeholder":"​","style":"IPY_MODEL_febbf716996f428c95799f54017c5eb6","value":" 899k/899k [00:00&lt;00:00, 3.60MB/s]"}},"380a4e6acce4479c879797ae024b4497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d300a9889f74d5a9b579fcde1bb93ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe7bce62101444f49de97632f7aacd05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"196648811c224f3e9eb510cb17e278bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9194ff9531bf46fab3f8baca5e4598a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cbcb88b3793e44ad9fd006414869db0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"febbf716996f428c95799f54017c5eb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3d3805ba10f4e27936a9f77d6e8bc2f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad6b656cc01a430baa9d38c98d78a62f","IPY_MODEL_5b9c290bc7444d4c9de57e6bd27c66e5","IPY_MODEL_fbb3716d21d447828702bc31504a6acc"],"layout":"IPY_MODEL_c38c42f173f346a982a0e319b8545452"}},"ad6b656cc01a430baa9d38c98d78a62f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bf17560c0ef4683a2547c7011c0c910","placeholder":"​","style":"IPY_MODEL_b05df485280241dc87cba75ab1ab834b","value":"merges.txt: 100%"}},"5b9c290bc7444d4c9de57e6bd27c66e5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_48d6dec9439b420f8d06902eaaf7aae9","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7280f39e4d894ae5b940445f7921df1e","value":456318}},"fbb3716d21d447828702bc31504a6acc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06e8c549b421424dbee08e8322af9f73","placeholder":"​","style":"IPY_MODEL_bf5cf861f9d74e4f9ce4de89734aa8ea","value":" 456k/456k [00:00&lt;00:00, 2.76MB/s]"}},"c38c42f173f346a982a0e319b8545452":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf17560c0ef4683a2547c7011c0c910":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b05df485280241dc87cba75ab1ab834b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48d6dec9439b420f8d06902eaaf7aae9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7280f39e4d894ae5b940445f7921df1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06e8c549b421424dbee08e8322af9f73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf5cf861f9d74e4f9ce4de89734aa8ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc4996a322774cb798fabe42b319f9a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57c33c169431465ba1d0739740e401be","IPY_MODEL_87b1df774e5c4a4d9137d67f1ab1d18e","IPY_MODEL_0167aa6854b54201b7b18dd0f4b8ad78"],"layout":"IPY_MODEL_f9afaa0ab17b4684a0a6ce213ca8f8c0"}},"57c33c169431465ba1d0739740e401be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1db07e07260440698ade92de606f960c","placeholder":"​","style":"IPY_MODEL_5fb9446428b8450b8c2b23df419f151b","value":"tokenizer.json: 100%"}},"87b1df774e5c4a4d9137d67f1ab1d18e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6ad9a60fb324a6487843c87bfd7b092","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ad291e7e1cd4987bb28038ff2491b09","value":1355863}},"0167aa6854b54201b7b18dd0f4b8ad78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6e07cd2cc0d41359ffee8b18a382acb","placeholder":"​","style":"IPY_MODEL_e073437640464b71b5da21c88f1c0a0d","value":" 1.36M/1.36M [00:00&lt;00:00, 4.08MB/s]"}},"f9afaa0ab17b4684a0a6ce213ca8f8c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db07e07260440698ade92de606f960c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fb9446428b8450b8c2b23df419f151b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6ad9a60fb324a6487843c87bfd7b092":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ad291e7e1cd4987bb28038ff2491b09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6e07cd2cc0d41359ffee8b18a382acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e073437640464b71b5da21c88f1c0a0d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"085370ecbb0a4502a6118d83a776e3c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c97b24a84396422fbb12440bbaf74d46","IPY_MODEL_46581996b81149949284a462340b9f98","IPY_MODEL_bf5d5ea13f2d482c9bb9c532ca789c3d"],"layout":"IPY_MODEL_fbab987767d84adf9e1633671cdf8bad"}},"c97b24a84396422fbb12440bbaf74d46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b9b4b8043b541fd8531be29f802e768","placeholder":"​","style":"IPY_MODEL_8b11dba0903f4f069fafcb4bd9d07275","value":"model.safetensors: 100%"}},"46581996b81149949284a462340b9f98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59dec4b6e76b49198050a2332ad57bda","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1aef9ce93aff40bdb736f23fe2a6a31f","value":1421700479}},"bf5d5ea13f2d482c9bb9c532ca789c3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7753af7319234c63a0365f1ad450856d","placeholder":"​","style":"IPY_MODEL_8c4a5555220748008c069d2219000591","value":" 1.42G/1.42G [00:17&lt;00:00, 52.2MB/s]"}},"fbab987767d84adf9e1633671cdf8bad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b9b4b8043b541fd8531be29f802e768":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b11dba0903f4f069fafcb4bd9d07275":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59dec4b6e76b49198050a2332ad57bda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1aef9ce93aff40bdb736f23fe2a6a31f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7753af7319234c63a0365f1ad450856d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c4a5555220748008c069d2219000591":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"mmv3mzv8ltw9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763824052567,"user_tz":-330,"elapsed":25922,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"7ef0ec3a-2f77-478d-d36f-6bee20646d40"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=850eb4dd7ef813b62210ff468b2fcb832429e13d6301fb288932a157cdd03fc4\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"IqD8XPObGU8K","executionInfo":{"status":"ok","timestamp":1763824052606,"user_tz":-330,"elapsed":32,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"93cae146-dd45-4810-e214-9a0b1b3084a7"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-4-scout-17b-16e-instruct/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-4-scout-17b-16e-instruct_zero_shot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n","MAX_CHARS      = 2300\n","GLOBAL_MIN_GAP = 45   # Groq is faster, but we keep a safety gap\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\"\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t)\n","    t = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \" \", t)\n","    t = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","\n","    t = re.sub(r\"\\bNLP\\b\", \"Natural Language Processing (NLP)\", t)\n","    t = re.sub(r\"\\bML\\b\", \"Machine Learning (ML)\", t)\n","    t = re.sub(r\"\\bAI\\b\", \"Artificial Intelligence (AI)\", t)\n","    return t.strip()\n","\n","\n","def chunk_text(text: str, max_chars=MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    try:\n","        return json.loads(text[start:end+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (GLOBAL WAIT + RETRIES)\n","#####################################################################\n","def groq_call(prompt: str, temperature=0.15, retries=3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s before next request\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ZERO-SHOT PROMPT (ALL TASKS IN ONE CALL)\n","#####################################################################\n","def generate_zero_shot(transcript: str):\n","    topics_short = \", \".join(VALID_TOPICS)\n","\n","    prompt = (\n","        \"You are an expert NLP assistant.\\n\"\n","        \"Perform ALL tasks concisely using ONLY the transcript.\\n\\n\"\n","        \"Respond in a JSON-like object with EXACTLY these keys:\\n\"\n","        \"generated_summary, predicted_topics, generated_questions, key_concepts.\\n\\n\"\n","        \"Requirements:\\n\"\n","        \"- generated_summary: 3–5 sentence abstractive summary.\\n\"\n","        f\"- predicted_topics: 1–3 most relevant from [{topics_short}]. Use exact labels.\\n\"\n","        \"- generated_questions: 3 short Q&A pairs with keys q and a.\\n\"\n","        \"- key_concepts: 6–10 short noun phrases, no duplicates.\\n\"\n","        \"- No explanations outside the JSON.\\n\\n\"\n","        f\"Transcript:\\n\\\"\\\"\\\"{transcript}\\\"\\\"\\\"\"\n","    )\n","\n","    out = groq_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    summary = j.get(\"generated_summary\", \"\").strip()\n","\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","    topics = [t for t in topics if t in VALID_TOPICS] or [\"Other\"]\n","\n","    qas = j.get(\"generated_questions\", [])\n","    qa_lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = qa.get(\"q\", \"\").strip()\n","            a = qa.get(\"a\", \"\").strip()\n","            if q: qa_lines.append(f\"Q: {q}\")\n","            if a: qa_lines.append(f\"A: {a}\")\n","\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    concepts = \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","    return summary, topics, \"\\n\".join(qa_lines), concepts\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — ZERO SHOT (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming — {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary, topics, qa_text, concepts = generate_zero_shot(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error on row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"DONE. Final file:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nZero-shot Groq pipeline completed ✓\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHKpY0_9GVDQ","executionInfo":{"status":"ok","timestamp":1763825447004,"user_tz":-330,"elapsed":1393630,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"993b93ca-1f15-45a7-9f31-2e3f5b00d8f2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-4-scout-17b-16e-instruct\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses reinforcement learning with human feedback, using examples and ChatGPT as a practical application. It explains how human feedback accelerates the learning process and guides the algorithm to make informed decisions. The video also delves into the training process of ChatGPT using reinforcement learning and a rewards model.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the role of human feedback in reinforcement learning?\n","A: To guide and accelerate the learning process\n","Q: What is the primary purpose of the rewards model in ChatGPT?\n","A: To assess and score the quality of answers generated by ChatGPT\n","Q: How does human feedback contribute to reinforcement learning?\n","A: It accelerates the learning process\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Human Feedback, ChatGPT, Rewards Model, Proximal Policy Optimization, Grid World, Q Learning, Deep Learning\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," The tutorial covers using CVXopt with kernels in Support Vector Machines (SVMs), including code examples and visualizations for linearly separable, nonlinearly separable, and soft margin data.\n","\n","TOPICS:\n"," ['Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is CVXopt used for in the tutorial?\n","A: CVXopt is used for quadratic programming in SVMs\n","Q: What type of data requires a kernel?\n","A: Nonlinearly separable data\n","Q: What is the purpose of the soft margin in SVMs?\n","A: To handle overlapping data\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, CVXopt, Kernels, Quadratic Programming, Soft Margin, Linearly Separable Data, Nonlinearly Separable Data, Machine Learning Tutorial\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," The video explores the foundation of prompt engineering, discussing the different types of prompts, their key features, and how to deconstruct them to understand their constraints and expectations.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What are prompts in prompt engineering?\n","A: Inputs given to prompt engineering models\n","Q: How many types of prompts are there?\n","A: Seven\n","Q: What are the key features of prompts?\n","A: Length, language, context, constraints\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Large Language Models, Text Outputs, Context, Constraints, Prompt Deconstruction, Prompt Features, Language Models\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents are problem solvers that can make autonomous decisions. The ReAct agent pattern is a popular method for creating AI agents, which involves reasoning and acting. It mimics human thinking by observing the result of actions and tweaking them. The pattern is used in LangChain to create agents that can use tools to complete tasks.\n","\n","TOPICS:\n"," ['Agentic AI', 'Artificial Intelligence', 'LangChain']\n","\n","Q&A:\n"," Q: What are AI agents?\n","A: Problem solvers that can make autonomous decisions\n","Q: What is the ReAct agent pattern?\n","A: A method for creating AI agents that involves reasoning and acting\n","Q: How does the ReAct pattern work?\n","A: By observing the result of actions and tweaking them\n","\n","KEY CONCEPTS:\n"," AI agents, ReAct agent pattern, LangChain, autonomous decisions, reasoning, acting, tools, problem solvers\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains how to use LangSmith to trace the reflection agent system, which refines a viral tweet through multiple iterations. The system consists of a generation node and a reflection node that work together to produce a final tweet. The speaker demonstrates how to set up LangSmith and view the tracing results.\n","\n","TOPICS:\n"," ['LangChain', 'Prompt Engineering', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the purpose of using LangSmith?\n","A: To trace the reflection agent system\n","Q: What are the two nodes in the reflection agent system?\n","A: Generation node and reflection node\n","Q: What is the final output of the reflection agent system?\n","A: A refined viral tweet\n","\n","KEY CONCEPTS:\n"," LangSmith, Reflection Agent, Generation Node, Reflection Node, LangChain, Tracing, Viral Tweet, Generative AI, Prompt Engineering\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The video demonstrates how to use LangChain's chat models to communicate with OpenAI's API. It covers installing the necessary package, importing the ChatOpenAI class, and initializing the model. The tutorial also shows how to make API calls and handle errors, such as missing API keys.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: LangChain is a library used to interact with various AI models\n","Q: How do you initialize the ChatOpenAI model?\n","A: By importing the ChatOpenAI class and passing the desired model name\n","Q: What is the purpose of the .env file?\n","A: To store sensitive information like API keys\n","\n","KEY CONCEPTS:\n"," LangChain, ChatOpenAI, OpenAI API, API Key, Python-dotenv, Environment Variables\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," The video demonstrates how Python's sort method works on lists containing strings and a mix of strings and numbers. It shows that strings are sorted alphabetically, with uppercase letters coming before lowercase, and that numbers are sorted before strings. The video also covers sorting in reverse order.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: How does Python sort a list of strings?\n","A: Alphabetically, with uppercase before lowercase\n","Q: What happens when sorting a list with both strings and numbers?\n","A: Numbers come before strings\n","Q: How does reverse sorting work?\n","A: It reverses the sorted order\n","\n","KEY CONCEPTS:\n"," Python sort method, list sorting, string sorting, mixed data types, alphabetical order, uppercase and lowercase, reverse sorting, sorting with numbers\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The decision to use AI or human judgment can be optimized by understanding the strengths of each. AI excels when confident, while humans perform better when AI is uncertain. Augmented intelligence, combining human and AI, often yields the best results, but requires careful consideration of human cognitive bias.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: When does AI outperform humans?\n","A: When AI is confident\n","Q: What is augmented intelligence?\n","A: Combining human and AI decision making\n","Q: How can human cognitive bias be minimized?\n","A: By presenting AI recommendations in an optional display format\n","\n","KEY CONCEPTS:\n"," AI confidence score, fraud detection system, human cognitive bias, augmented intelligence, automation bias, optional display, AI recommendation, decision making process, human bias, AI performance curve\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Demitrius, a product manager at Google Cloud AI, discusses new Vertex AI APIs that help developers build generative applications for enterprises. The APIs address technical challenges such as grounding, document understanding, and retrieval. Six new APIs are introduced, including document understanding, embedding, vector search, ranking, grounded generation, and check grounding.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Generative AI', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the main focus of Demitrius' talk?\n","A: Building generative applications for enterprises with Vertex AI\n","Q: How many new Vertex AI APIs are introduced?\n","A: Six\n","Q: What is the purpose of the check grounding API?\n","A: Fact-checking statements against provided evidence\n","\n","KEY CONCEPTS:\n"," Vertex AI APIs, Generative Applications, Document Understanding, Vector Search, Grounded Generation, Check Grounding, Embedding API, Ranking API, Enterprise Data\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The lecture discusses the Singular Value Decomposition (SVD) of a matrix X, focusing on the properties of unitary matrices U and V. Unitary matrices preserve angles and lengths of vectors, and are used to rotate vectors without changing their geometry.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the property of unitary matrices?\n","A: Unitary matrices preserve angles and lengths of vectors.\n","Q: What is the SVD of a matrix X?\n","A: X = U Σ V^T, where U and V are unitary matrices.\n","Q: What is the geometric interpretation of SVD?\n","A: SVD maps a sphere of unit vectors to an ellipsoid, with principal axes given by singular values.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Unitary Matrices, Vector Space, Geometry, Fourier Transform, Coordinate Transformation, Ellipsoid, Principal Axes, Singular Values\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," The video demonstrates building a generative AI-powered application using Google Gemini Pro 1.5, showcasing its capabilities and context length of 1 million tokens. The speaker explains how to create an API key, install necessary packages, and use the Gemini Pro 1.5 model for text and image-based queries.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is Google Gemini Pro 1.5?\n","A: A multimodal model that can work with both text and images\n","Q: What is the context length of Gemini Pro 1.5?\n","A: 1 million tokens\n","Q: How to create an API key for Gemini Pro?\n","A: By visiting ai.google.com and clicking on 'Get API Key'\n","\n","KEY CONCEPTS:\n"," Google Gemini Pro 1.5, Generative AI, API Key, Multimodal Model, Context Length, Natural Language Processing, Image-based Queries, Text-based Queries\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses evaluating and testing prompt engineering models using various metrics such as perplexity, accuracy, and human evaluation. It also covers debugging and improving models by analyzing generated responses and testing on different data sets. The importance of ongoing evaluation and testing is emphasized.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What are some common metrics used to evaluate prompt engineering models?\n","A: Perplexity, accuracy, and human evaluation\n","Q: Why is testing prompt engineering models on different data sets important?\n","A: To determine the model's ability to generalize on new or unseen data\n","Q: What is the purpose of analyzing generated responses?\n","A: To identify common errors or patterns and fine-tune the models\n","\n","KEY CONCEPTS:\n"," Prompt Engineering Models, Perplexity, Accuracy, Human Evaluation, Model Debugging, Cross Validation, Large Language Models, Model Evaluation, Testing Techniques\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains the difference between Generative AI, AI Agent, and Agentic AI. Generative AI generates new content based on patterns learned from existing data, while AI Agent is a program that takes input, thinks, and acts to complete a task using tools and knowledge. Agentic AI is a system where one or more AI agents work autonomously to reach a complex goal.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Generative AI', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is Generative AI?\n","A: AI that generates new content based on patterns learned from existing data\n","Q: What is AI Agent?\n","A: A program that takes input, thinks, and acts to complete a task using tools and knowledge\n","Q: What is Agentic AI?\n","A: A system where one or more AI agents work autonomously to reach a complex goal\n","\n","KEY CONCEPTS:\n"," Generative AI, AI Agent, Agentic AI, Large Language Model, Autonomous Decision Making, Multi-Step Reasoning, Complex Task, Tools and Knowledge, N8N, Langraph Framework\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses the concept of covariance, its importance in data analysis, and how it quantifies the relationship between two random variables. Covariance measures how much two variables change together, with a positive value indicating that they increase together and a negative value indicating that one increases while the other decreases. The speaker uses examples to illustrate how to calculate covariance and its limitations, introducing the Pearson correlation coefficient as a subsequent topic.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science']\n","\n","Q&A:\n"," Q: What is covariance?\n","A: A measure of how much two variables change together\n","Q: What does a positive covariance indicate?\n","A: That the two variables increase together\n","Q: What is the limitation of covariance?\n","A: It does not indicate the strength of the relationship between the variables\n","\n","KEY CONCEPTS:\n"," Covariance, Data Analysis, Random Variables, Variance, Pearson Correlation Coefficient, Statistics, Data Preprocessing, Relationship Quantification\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses defining objectives in reinforcement learning (RL) problems. It explains how to set goals and provide feedback through rewards, using examples like Tic-Tac-Toe and stock market trading.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the objective of reinforcement learning?\n","A: To learn the optimal policy that maximizes a numerical reward signal\n","Q: How is the reward determined in Tic-Tac-Toe?\n","A: Positive for winning, negative for losing, and no reward for drawing\n","Q: What is the goal in stock market trading?\n","A: To maximize the expected cumulative reward over time\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Objective, Reward Signal, Optimal Policy, Tic-Tac-Toe, Stock Market Trading, Episodic Task, Continuous Task, Reward Function\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," The video explains the concept of dictionaries in Python, including their structure, declaration, and manipulation. Dictionaries are made up of key-value pairs, and are useful for mapping one item to another. The video covers various dictionary functions, such as items(), keys(), and values(), and demonstrates how to create, access, and modify dictionary elements.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A dictionary is a data type made up of key-value pairs\n","Q: How are dictionary elements accessed?\n","A: By using the dictionary name and the key in square brackets\n","Q: What is the purpose of the zip function?\n","A: To pair corresponding elements from two lists into tuples\n","\n","KEY CONCEPTS:\n"," Dictionaries, Key-value pairs, Python Programming, Data Science, Dictionary functions, Items, Keys, Values, Zip function, Dictionary manipulation\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," The video explores how User Behavior Analytics (UBA) with AI and machine learning can help detect and respond to Insider threats. IBM's QRadar SIEM with built-in UBA app is used to demonstrate how to identify and contain Insider threats. QRadar helps security analysts accelerate investigations and focus on proactive defense efforts.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is User Behavior Analytics?\n","A: UBA uses machine learning to analyze user behavior and identify anomalies and potential threats.\n","Q: How does QRadar help security analysts?\n","A: QRadar helps security analysts accelerate investigations and focus on proactive defense efforts.\n","Q: What is the benefit of using AI in security operations?\n","A: AI can help reduce the time to identify and contain data breaches.\n","\n","KEY CONCEPTS:\n"," User Behavior Analytics, Insider threats, QRadar SIEM, Artificial Intelligence, Machine Learning, Security Operations, Data Breach, Threat Detection, Incident Response\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses the release of Llama 3, an open-source LLM model by Meta, with 8 billion and 70 billion parameter versions. It highlights the model's performance, capabilities, and benchmarking results, comparing it to other models like Gemini Pro 1.5. The video also explains how to access and download Llama 3.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is Llama 3?\n","A: An open-source LLM model by Meta\n","Q: What are the parameter versions available for Llama 3?\n","A: 8 billion and 70 billion\n","Q: How can one access Llama 3?\n","A: By downloading from Meta's website or Hugging Face\n","\n","KEY CONCEPTS:\n"," Llama 3, Meta AI, LLM model, Open-source, Performance metrics, Benchmarking, Hugging Face, GitHub, NLP tasks\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The speaker is about to walk through the steps of writing Python code for a decision boundary using scikit-learn library, specifically the Naive Bayes algorithm. They will use Google to find the relevant documentation. The speaker searches for 'sklearn Naive Bayes' and finds a page on Gaussian Naive Bayes, which is the specific algorithm used.\n","\n","TOPICS:\n"," ['Python Programming', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What library is being used?\n","A: scikit-learn\n","Q: What algorithm is being used?\n","A: Naive Bayes\n","Q: What specific Naive Bayes algorithm is used?\n","A: Gaussian Naive Bayes\n","\n","KEY CONCEPTS:\n"," scikit-learn, Naive Bayes, Gaussian Naive Bayes, Python code, decision boundary, sklearn, algorithm, Google search, documentation\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion revolves around Gaussian and log normal distributions, their characteristics, and applications in data analysis. Gaussian distribution is symmetric and follows a bell curve, while log normal distribution is skewed and often used to model variables like income or comment lengths.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science']\n","\n","Q&A:\n"," Q: What is Gaussian distribution?\n","A: A symmetric distribution following a bell curve.\n","Q: What is log normal distribution?\n","A: A skewed distribution where the log of the variable is normally distributed.\n","Q: Why is scaling important in data analysis?\n","A: To bring different variables to the same scale, improving model accuracy.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, log normal distribution, bell curve, standard normal distribution, data scaling, variable transformation, distribution properties, statistical analysis\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," The video begins an end-to-end deep learning project series in the agriculture domain, focusing on detecting diseases in potato plants. The project involves data collection, model building using CNN, and deployment to Google Cloud. The application will be built using React Native and will utilize TF serving and Fast API.\n","\n","TOPICS:\n"," ['Deep Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the main focus of the project series?\n","A: Detecting diseases in potato plants\n","Q: What technology will be used for model building?\n","A: TensorFlow and CNN\n","Q: What is the deployment platform?\n","A: Google Cloud\n","\n","KEY CONCEPTS:\n"," Deep Learning, Potato Plant Diseases, CNN, TF Serving, Fast API, React Native, Google Cloud, Data Augmentation, Quantization\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses the levels of autonomy in LLM applications, ranging from zero autonomy in code to maximum autonomy in agents. It explains the different levels, including LLM calls, chains, routers, and state machines, highlighting their advantages and disadvantages.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'LangChain', 'Agentic AI', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the main difference between a chain and a router?\n","A: A router can make decisions on its own, whereas a chain follows a fixed sequence.\n","Q: What is the role of a state machine in LLM applications?\n","A: A state machine is a type of agent that can have loops, human review, and refinement, making it more intelligent and autonomous.\n","Q: What is the difference between human-driven and agent-executed processes?\n","A: Human-driven processes are one-directional and lack intelligence, whereas agent-executed processes are controlled by the LLM and can have cycles and loops.\n","\n","KEY CONCEPTS:\n"," LLM applications, autonomy levels, chains, routers, state machines, agents, LangGraph, control flow, human review, refinement\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This section covers advanced topics in prompt engineering, including handling different types of prompts, fine-tuning pre-trained models, data pre-processing, and deploying models in production. It also discusses ethical considerations and provides practical exercises.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What types of prompts can be handled in advanced prompt engineering?\n","A: Text-based, image-based, and audio-based prompts\n","Q: What is multitask learning in fine-tuning pre-trained models?\n","A: Training a model on multiple tasks simultaneously\n","Q: Why is data pre-processing important in prompt engineering?\n","A: To ensure the quality of the data used to train models\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Pre-trained Models, Multitask Learning, Distillation, Data Pre-processing, Tokenization, Normalization, Model Deployment, Ethical Considerations\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The lecture discusses the application of Singular Value Decomposition (SVD) to images of faces, specifically eigenfaces, and demonstrates how to cluster images of different individuals in eigenface space. The example uses images of Arnold Schwarzenegger and Sylvester Stallone, and later Taylor Swift, to illustrate the concept. The lecture highlights the potential for image classification based on eigenface coordinates.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the purpose of computing the average face in the eigenface example?\n","A: To subtract the mean from all faces and perform principal component analysis\n","Q: How are the images of Arnold Schwarzenegger and Sylvester Stallone separated in eigenface space?\n","A: By projecting their images into the first three eigenfaces and plotting them as different colored markers\n","Q: What is the significance of the eigenfaces in image classification?\n","A: Eigenfaces allow for the reduction of high-dimensional image data to a lower-dimensional representation, enabling classification based on eigenface coordinates\n","\n","KEY CONCEPTS:\n"," Eigenfaces, Singular Value Decomposition, Image Classification, Principal Component Analysis, Face Recognition, Dimensionality Reduction, Clustering, Eigenvectors\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a framework that bridges the gap between large language models (LLMs) and the real world, enabling applications to reason and interact with external APIs, databases, and services. LLMs are limited to reasoning within their training data, but LangChain allows them to access external information and perform actions. This enables AI applications to do more in the real world, such as booking flights, accessing private databases, sending emails, and scraping websites. LangChain makes it easy to switch between different LLMs without modifying the code.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: A framework that bridges LLMs and the real world\n","Q: What are the limitations of LLMs?\n","A: They can only reason within their training data\n","Q: What can LangChain enable AI applications to do?\n","A: Interact with external APIs, databases, and services\n","\n","KEY CONCEPTS:\n"," LangChain, Large Language Models, Artificial Intelligence, Real World APIs, External Databases, Application Development, LLMs Limitations, Reasoning Ability\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses residuals in time series analysis, their importance in diagnosing model performance, and how to analyze them using Python. Residuals are the difference between fitted and actual values, and their analysis can reveal autocorrelation and bias in forecasting models.\n","\n","TOPICS:\n"," ['Time Series', 'Data Science', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: The difference between fitted and actual values.\n","Q: Why is residual analysis important?\n","A: To diagnose model performance and identify autocorrelation and bias.\n","Q: How can residual analysis be performed?\n","A: Using Python, by plotting autocorrelation functions and histograms.\n","\n","KEY CONCEPTS:\n"," Residuals, Time Series Analysis, Model Diagnosis, Autocorrelation, Bias, Forecasting Models, Python, Data Science\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," The video demonstrates building a Text2SQL agent using LangGraph, Next.js, and SQLite. The agent is capable of generating SQL queries based on natural language input and executing them against a database.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the purpose of the Text2SQL agent?\n","A: To generate SQL queries based on natural language input\n","Q: What technologies are used to build the Text2SQL agent?\n","A: LangGraph, Next.js, and SQLite\n","Q: What is the role of the large language model in the Text2SQL agent?\n","A: To generate SQL queries and execute them against the database\n","\n","KEY CONCEPTS:\n"," Text2SQL agent, LangGraph, Next.js, SQLite, Natural Language Processing, SQL queries, Large language model, ReAct agent\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," This course explores the fundamentals of prompt engineering, a field within natural language processing that focuses on building models that generate high-quality text outputs in response to prompts. Prompt engineering is based on pre-trained large language models fine-tuned for specific tasks. The course covers the basics, benefits, and limitations of prompt engineering, as well as advanced techniques for fine-tuning models. Students will gain a solid understanding of prompt engineering and its applications. The course is designed to introduce students to the exciting world of prompt engineering.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A field within natural language processing that focuses on building models that generate high-quality text outputs.\n","Q: What are prompt engineering models based on?\n","A: Pre-trained large language models fine-tuned for specific tasks.\n","Q: What is the key benefit of prompt engineering?\n","A: Generating text outputs that are more accurate, coherent, and contextually appropriate.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Large Language Models, Text Outputs, Pre-trained Models, Fine-tuning, NLP Applications, Chatbots, Language Translation\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is a value-based reinforcement learning method that learns to map situations to actions to maximize a numerical reward signal. It uses a Q-table to store state-action values, updated using the Bellman equation and temporal difference error. The agent learns an optimal policy by exploring the environment and updating the Q-table.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is Q-learning?\n","A: A value-based reinforcement learning method\n","Q: What is the Q-table used for?\n","A: To store state-action values\n","Q: How is the Q-table updated?\n","A: Using the Bellman equation and temporal difference error\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement Learning, Value-based Methods, Q-table, Bellman Equation, Temporal Difference Error, Optimal Policy, Exploration Policy, State-action Values\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier is a linear classifier that uses a linear function to generate predictions from input pixels. The model is trained by finding optimal weights and bias. The scores are turned into probabilities using a softmax function.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A linear classifier\n","Q: How are scores turned into probabilities?\n","A: Using a softmax function\n","Q: What is the goal of training the model?\n","A: Finding optimal weights and bias\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Function, Softmax Function, Weights, Bias, Probabilities, Logits, Machine Learning, Classification\n","Saved row 29\n","DONE. Final file: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_zero_shot_full_output.xlsx\n","\n","Zero-shot Groq pipeline completed ✓\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_zero_shot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":715,"referenced_widgets":["c832405e42914de0bd50e7d730420db4","644b51a585f44b07b5bdb34317508b6c","cbaaa86b0e244a6891425dd7449c8d6c","eadc0ed1b9e04fe381f8ff6abe85dde6","8fb833e6a6854fda9e6ebd28c8bc432b","52e2bda16ff24313bca861ead060847b","5592d1fe9cfd4ba4bfa2e89df951c7bd","d8d493fa753f4ef8861904d9ca1a1d26","670cafb9200b4886a543fa9726122ee2","71c0c050982c43dc9cc33374b2107b72","3f008387eff44057830c93b4a9504861","e698412af51341e0a538a9f1b32fc2b2","739f6d306c2442c2b5f64f273847213b","4f629bfb59b84c139bd7b4b48867f471","7af1f852bcdb46ceb3302a040c6e9578","952a285aba794b56beb9d1c0b2b59391","5cb1ea32bd71489a8632df40cec2904c","690828bfa57c4eb89e07dfe32e9bb050","405e07fd5ca44f708f131cb66b867ee0","c792de9c95344251b5c23f20901dc57a","2a70e2c7f87e49f2bb92cb5f71e95c60","80ec044e8c89404c8eeddfd93634d089","f50851a28974437d849d7c1279272d8c","a19f854546904e11944a750491704b24","77336ef5282648948b72e16a6e8d4fa7","72c321a8505f42f48ac85f7c675453ae","380a4e6acce4479c879797ae024b4497","2d300a9889f74d5a9b579fcde1bb93ed","fe7bce62101444f49de97632f7aacd05","196648811c224f3e9eb510cb17e278bc","9194ff9531bf46fab3f8baca5e4598a3","cbcb88b3793e44ad9fd006414869db0a","febbf716996f428c95799f54017c5eb6","a3d3805ba10f4e27936a9f77d6e8bc2f","ad6b656cc01a430baa9d38c98d78a62f","5b9c290bc7444d4c9de57e6bd27c66e5","fbb3716d21d447828702bc31504a6acc","c38c42f173f346a982a0e319b8545452","6bf17560c0ef4683a2547c7011c0c910","b05df485280241dc87cba75ab1ab834b","48d6dec9439b420f8d06902eaaf7aae9","7280f39e4d894ae5b940445f7921df1e","06e8c549b421424dbee08e8322af9f73","bf5cf861f9d74e4f9ce4de89734aa8ea","bc4996a322774cb798fabe42b319f9a1","57c33c169431465ba1d0739740e401be","87b1df774e5c4a4d9137d67f1ab1d18e","0167aa6854b54201b7b18dd0f4b8ad78","f9afaa0ab17b4684a0a6ce213ca8f8c0","1db07e07260440698ade92de606f960c","5fb9446428b8450b8c2b23df419f151b","c6ad9a60fb324a6487843c87bfd7b092","8ad291e7e1cd4987bb28038ff2491b09","a6e07cd2cc0d41359ffee8b18a382acb","e073437640464b71b5da21c88f1c0a0d","085370ecbb0a4502a6118d83a776e3c4","c97b24a84396422fbb12440bbaf74d46","46581996b81149949284a462340b9f98","bf5d5ea13f2d482c9bb9c532ca789c3d","fbab987767d84adf9e1633671cdf8bad","1b9b4b8043b541fd8531be29f802e768","8b11dba0903f4f069fafcb4bd9d07275","59dec4b6e76b49198050a2332ad57bda","1aef9ce93aff40bdb736f23fe2a6a31f","7753af7319234c63a0365f1ad450856d","8c4a5555220748008c069d2219000591"]},"id":"icGLR6aPGVKg","executionInfo":{"status":"ok","timestamp":1763825658868,"user_tz":-330,"elapsed":190778,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"e98a651e-51da-4d79-c064-965029e024c0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_zero_shot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c832405e42914de0bd50e7d730420db4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e698412af51341e0a538a9f1b32fc2b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f50851a28974437d849d7c1279272d8c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3d3805ba10f4e27936a9f77d6e8bc2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc4996a322774cb798fabe42b319f9a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"085370ecbb0a4502a6118d83a776e3c4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2397\n","  - BLEU: 0.0214\n","  - BERTScore F1: 0.8811\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.4253\n","  - Micro F1: 0.5576\n","  - Macro F1: 0.5299\n","  - Weighted F1: 0.4910\n","\n","Q&A Generation:\n","  - BLEU: 0.0476\n","  - Diversity: 0.7413\n","  - Answerability: 0.8333\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5033\n","  - Recall@10: 0.2013\n","  - F1@10: 0.2876\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\n"]}]}]}