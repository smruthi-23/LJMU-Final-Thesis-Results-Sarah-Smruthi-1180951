{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCWGfommhZPHjGqNUoxl54"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"050391c6e5cd4c8ab95395f760170dd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f30ab506d77545238ce7e4c55431c8d2","IPY_MODEL_31b7345d39c542be8b8fac3650ffec6b","IPY_MODEL_f44414cbde0746c6b0d7859bae4c389f"],"layout":"IPY_MODEL_17b55af2e2b944feb24fcc8a005512d6"}},"f30ab506d77545238ce7e4c55431c8d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebaa883535774c608cd7293c2eebad1b","placeholder":"​","style":"IPY_MODEL_f3a17c0968b14936a739c1416c4ebe71","value":"tokenizer_config.json: 100%"}},"31b7345d39c542be8b8fac3650ffec6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_496c0f7e306742e78316686d795e1cc5","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7a9d1f81922243ad8bcc82ae8a89de29","value":25}},"f44414cbde0746c6b0d7859bae4c389f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5bbef69d7424fb3840e8c741e587aa5","placeholder":"​","style":"IPY_MODEL_a61cdd613e2240639d9d4ce5c5ab53b3","value":" 25.0/25.0 [00:00&lt;00:00, 2.18kB/s]"}},"17b55af2e2b944feb24fcc8a005512d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebaa883535774c608cd7293c2eebad1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3a17c0968b14936a739c1416c4ebe71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"496c0f7e306742e78316686d795e1cc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a9d1f81922243ad8bcc82ae8a89de29":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5bbef69d7424fb3840e8c741e587aa5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a61cdd613e2240639d9d4ce5c5ab53b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4bcaaba7086645dd8a49146ff3749ebf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2fa7b1fb105a4f75aeb4ca738f3a862b","IPY_MODEL_03c450eee64143609b241415f84ade28","IPY_MODEL_c631be0157a04a0ead15a817a02d4c93"],"layout":"IPY_MODEL_108192aacffd4fcba24d0f40fd9c0700"}},"2fa7b1fb105a4f75aeb4ca738f3a862b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_946afc29a2f5424f9ef26bb1ec616e7d","placeholder":"​","style":"IPY_MODEL_71adfe0dfb014bfdaf93ad0661f1303c","value":"config.json: 100%"}},"03c450eee64143609b241415f84ade28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0788c1895f184834bb9db4e3df18a20a","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_46b4abecda8044928b2c072ca6eee89e","value":482}},"c631be0157a04a0ead15a817a02d4c93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71cdfc9b2645432593c63bff2b4c872d","placeholder":"​","style":"IPY_MODEL_da128057565f4cee98b74894a8dd4520","value":" 482/482 [00:00&lt;00:00, 43.0kB/s]"}},"108192aacffd4fcba24d0f40fd9c0700":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"946afc29a2f5424f9ef26bb1ec616e7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71adfe0dfb014bfdaf93ad0661f1303c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0788c1895f184834bb9db4e3df18a20a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46b4abecda8044928b2c072ca6eee89e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71cdfc9b2645432593c63bff2b4c872d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da128057565f4cee98b74894a8dd4520":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48a49564a3ca4ade85825b320456d6f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2005a4d73c92410dae4ea908691a4943","IPY_MODEL_e7135e6c742b4cc18c71f1e400bff324","IPY_MODEL_b97f4ebb0f284e48aaac54b521a725eb"],"layout":"IPY_MODEL_a36c1881eaa2451d9e40b0cc7d1b122f"}},"2005a4d73c92410dae4ea908691a4943":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dad9eaf65004592a2be91e3b0f921ed","placeholder":"​","style":"IPY_MODEL_43c5fa0d70ce43bfb64f066961abf45a","value":"vocab.json: 100%"}},"e7135e6c742b4cc18c71f1e400bff324":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3257d54cc0024ea7804c13326ff598f8","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ec48c1ddc3a454bbdce1f5b6046a6c4","value":898823}},"b97f4ebb0f284e48aaac54b521a725eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96c74c7304514914995cdc68556a6c2d","placeholder":"​","style":"IPY_MODEL_80db653454de430bb79eb749e5d17525","value":" 899k/899k [00:00&lt;00:00, 39.5MB/s]"}},"a36c1881eaa2451d9e40b0cc7d1b122f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9dad9eaf65004592a2be91e3b0f921ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43c5fa0d70ce43bfb64f066961abf45a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3257d54cc0024ea7804c13326ff598f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ec48c1ddc3a454bbdce1f5b6046a6c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"96c74c7304514914995cdc68556a6c2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80db653454de430bb79eb749e5d17525":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"80fac788659d4cba969e3517f3cdcdf7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1310c704463f4ea7920229282d84bdde","IPY_MODEL_665877973a224e72a2d4e1aba8dfe0bd","IPY_MODEL_5036e76ea0e34de4b66d224b47627e40"],"layout":"IPY_MODEL_154f8be5a4e84dc69f8df310f63ba9ca"}},"1310c704463f4ea7920229282d84bdde":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffe23e61f9ec488db52fda19e26f765d","placeholder":"​","style":"IPY_MODEL_0a6f2a7e665d4677a80aea0e6f5401ec","value":"merges.txt: 100%"}},"665877973a224e72a2d4e1aba8dfe0bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c89de83424a4a86aecafe44c80cb981","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_106d62b4a7014017abd40cdabf083928","value":456318}},"5036e76ea0e34de4b66d224b47627e40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3ac840580b14f4e96979f593b3b6a10","placeholder":"​","style":"IPY_MODEL_2648f4d72a694d04ac10a568886a3a98","value":" 456k/456k [00:00&lt;00:00, 25.8MB/s]"}},"154f8be5a4e84dc69f8df310f63ba9ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffe23e61f9ec488db52fda19e26f765d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a6f2a7e665d4677a80aea0e6f5401ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c89de83424a4a86aecafe44c80cb981":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"106d62b4a7014017abd40cdabf083928":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d3ac840580b14f4e96979f593b3b6a10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2648f4d72a694d04ac10a568886a3a98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbb985d68cac41f0946d7732457dbc79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2065093cb91843afbd0be1c45802ebd7","IPY_MODEL_413660129ab845b4b1e1f46071f3dec6","IPY_MODEL_bb95fa5149454527aa7243297abd77f1"],"layout":"IPY_MODEL_54716ee5430340a3a119e08401aa0aa0"}},"2065093cb91843afbd0be1c45802ebd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b53efd333ce64eac9d544cbfcb97734f","placeholder":"​","style":"IPY_MODEL_7846bac9b9f245cdaee4257b9264b882","value":"tokenizer.json: 100%"}},"413660129ab845b4b1e1f46071f3dec6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_af8f066bf9c44244a9f60057f483da80","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7f46b299988b4a23a2151b8c54c9d452","value":1355863}},"bb95fa5149454527aa7243297abd77f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea5e9f78f7a946b296b67fbce3d98333","placeholder":"​","style":"IPY_MODEL_ad8168f19feb41f1a4bedd5cadb98fe1","value":" 1.36M/1.36M [00:00&lt;00:00, 35.7MB/s]"}},"54716ee5430340a3a119e08401aa0aa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b53efd333ce64eac9d544cbfcb97734f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7846bac9b9f245cdaee4257b9264b882":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af8f066bf9c44244a9f60057f483da80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f46b299988b4a23a2151b8c54c9d452":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea5e9f78f7a946b296b67fbce3d98333":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad8168f19feb41f1a4bedd5cadb98fe1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f411e638ae84463e9961f3998e7018b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abe5cdcb66af4ed292814bd6b6c7a7d7","IPY_MODEL_cb020a6301b2413d962ee4bf9e3b4158","IPY_MODEL_16cba2102f7b46f5b8fd3c1eaaf068f2"],"layout":"IPY_MODEL_b6b753d5c078486e8039650665a02dd0"}},"abe5cdcb66af4ed292814bd6b6c7a7d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39bf2d0de2384d568d3aa27f52f14f38","placeholder":"​","style":"IPY_MODEL_a45ba7821eb9421da4a325ae14a18d78","value":"model.safetensors: 100%"}},"cb020a6301b2413d962ee4bf9e3b4158":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6d8063fecf94e659c9fd4b4d9f3950b","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3c9fb7e4e46481f9980fe0d0bf68359","value":1421700479}},"16cba2102f7b46f5b8fd3c1eaaf068f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d35c623a71464ad4b072cd9050e8f313","placeholder":"​","style":"IPY_MODEL_10875899115b40b6b2b35b9059d91ff9","value":" 1.42G/1.42G [00:13&lt;00:00, 249MB/s]"}},"b6b753d5c078486e8039650665a02dd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39bf2d0de2384d568d3aa27f52f14f38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a45ba7821eb9421da4a325ae14a18d78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6d8063fecf94e659c9fd4b4d9f3950b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3c9fb7e4e46481f9980fe0d0bf68359":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d35c623a71464ad4b072cd9050e8f313":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10875899115b40b6b2b35b9059d91ff9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMGauZ30j_V8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763363183974,"user_tz":-330,"elapsed":17296,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"802cca7f-c973-48fe-eff9-0f385be3e5c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.34.1-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.34.1-py3-none-any.whl (136 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=447c26f104bc6e2a9bb7668a7c1339307ba5fd0ddafa378abf8f33ebd8003ffe\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.34.1 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"2fn_psYPojdv","executionInfo":{"status":"ok","timestamp":1763363183999,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"c2e5c3a7-0940-436e-c0eb-ae342066e937"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_zero_shot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2300\n","GLOBAL_MIN_GAP = 45   # Groq is faster, but we keep a safety gap\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\"\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t)\n","    t = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \" \", t)\n","    t = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","\n","    t = re.sub(r\"\\bNLP\\b\", \"Natural Language Processing (NLP)\", t)\n","    t = re.sub(r\"\\bML\\b\", \"Machine Learning (ML)\", t)\n","    t = re.sub(r\"\\bAI\\b\", \"Artificial Intelligence (AI)\", t)\n","    return t.strip()\n","\n","\n","def chunk_text(text: str, max_chars=MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    try:\n","        return json.loads(text[start:end+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (GLOBAL WAIT + RETRIES)\n","#####################################################################\n","def groq_call(prompt: str, temperature=0.15, retries=3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s before next request\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ZERO-SHOT PROMPT (ALL TASKS IN ONE CALL)\n","#####################################################################\n","def generate_zero_shot(transcript: str):\n","    topics_short = \", \".join(VALID_TOPICS)\n","\n","    prompt = (\n","        \"You are an expert NLP assistant.\\n\"\n","        \"Perform ALL tasks concisely using ONLY the transcript.\\n\\n\"\n","        \"Respond in a JSON-like object with EXACTLY these keys:\\n\"\n","        \"generated_summary, predicted_topics, generated_questions, key_concepts.\\n\\n\"\n","        \"Requirements:\\n\"\n","        \"- generated_summary: 3–5 sentence abstractive summary.\\n\"\n","        f\"- predicted_topics: 1–3 most relevant from [{topics_short}]. Use exact labels.\\n\"\n","        \"- generated_questions: 3 short Q&A pairs with keys q and a.\\n\"\n","        \"- key_concepts: 6–10 short noun phrases, no duplicates.\\n\"\n","        \"- No explanations outside the JSON.\\n\\n\"\n","        f\"Transcript:\\n\\\"\\\"\\\"{transcript}\\\"\\\"\\\"\"\n","    )\n","\n","    out = groq_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    summary = j.get(\"generated_summary\", \"\").strip()\n","\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","    topics = [t for t in topics if t in VALID_TOPICS] or [\"Other\"]\n","\n","    qas = j.get(\"generated_questions\", [])\n","    qa_lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = qa.get(\"q\", \"\").strip()\n","            a = qa.get(\"a\", \"\").strip()\n","            if q: qa_lines.append(f\"Q: {q}\")\n","            if a: qa_lines.append(f\"A: {a}\")\n","\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    concepts = \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","    return summary, topics, \"\\n\".join(qa_lines), concepts\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — ZERO SHOT (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming — {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary, topics, qa_text, concepts = generate_zero_shot(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error on row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"DONE. Final file:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nZero-shot Groq pipeline completed ✓\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43iNTW79ojjN","executionInfo":{"status":"ok","timestamp":1763364580878,"user_tz":-330,"elapsed":1360954,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"5d3ab66f-90b8-4d29-806d-e9b70558ea13"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.3-70b-versatile\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses reinforcement learning with human feedback, introducing the concept through a grid world example with Frank. Human feedback accelerates the learning process, allowing the algorithm to make more informed decisions. The video also explores how chat GPT uses reinforcement learning through human feedback, utilizing a rewards model and proximal policy optimization to fine-tune its responses. This process enables chat GPT to generate high-quality answers. The video concludes by highlighting the importance of reinforcement learning through human feedback in enhancing chat GPT's capabilities.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the primary purpose of human feedback in reinforcement learning?\n","A: To guide and accelerate the learning process\n","Q: How does chat GPT use reinforcement learning through human feedback?\n","A: Through a rewards model and proximal policy optimization\n","Q: What is the outcome of using reinforcement learning through human feedback in chat GPT?\n","A: Generating high-quality responses\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Human Feedback, Grid World, Chat GPT, Rewards Model, Proximal Policy Optimization, Artificial Intelligence, Natural Language Processing, Machine Learning, Algorithm\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial covers the use of CVX opt and kernels in support vector machines (SVMs), including visualization of nonlinear and soft margin cases. The code examples demonstrate the impact of different kernels on SVM performance. The tutorial also touches on the use of CVX opt for educational purposes and its limitations. The speaker discusses the solver used in the code and provides links to additional resources for further learning. The code is run through to demonstrate the effects of different kernels and parameters on SVM performance.\n","\n","TOPICS:\n"," ['Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is CVX opt used for in this tutorial?\n","A: CVX opt is used for educational purposes to demonstrate the impact of kernels on SVM performance.\n","Q: What type of kernel is used by default in the nonlinear example?\n","A: The polynomial kernel is used by default in the nonlinear example.\n","Q: How can you visualize the effect of a kernel on SVM performance?\n","A: You can visualize the effect of a kernel by using a tool like SVM Kernel visualization.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, CVX opt, Kernels, Soft Margin, Hard Margin, Linearly Separable Data, Nonlinearly Separable Data, Polynomial Kernel, Gaussian Kernel, Linear Kernel, Quadratic Programming Solver\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," This section explores the foundation of prompt engineering, which involves understanding the inputs given to large language models. Prompts can take many forms and vary in complexity and context, and it's essential to understand the different types of prompts to generate the desired output. The key features of prompts include length, language, context, and constraints, which can impact the complexity and quality of the output. Deconstructing a prompt involves breaking it down into individual components to better understand the key features and constraints. By understanding prompts and their key features, users can provide more context and information to get accurate and efficient output.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the purpose of prompt engineering?\n","A: To generate the desired output from large language models\n","Q: What are the key features of prompts?\n","A: Length, language, context, and constraints\n","Q: Why is deconstructing a prompt important?\n","A: To better understand the key features and constraints of the prompt\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Large Language Models, Context, Constraints, Key Features, Deconstructing Prompts, Output Quality, Complexity, Language Models, Input Forms\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents are problem solvers that can make autonomous decisions, and they can be created using the react agent pattern, which mimics human thinking. This pattern involves thinking, acting, and observing, and it can be used to build agents that can solve complex problems. The react agent pattern is a concept that combines reasoning and acting, and it is a key component of building AI agents. The pattern involves a cycle of thinking, acting, and observing, and it can be used to build agents that can learn and adapt to new situations. The react agent pattern is a powerful tool for building AI agents, and it has many potential applications in areas such as natural language processing and machine learning.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Natural Language Processing', 'LangChain']\n","\n","Q&A:\n"," Q: What is an AI agent?\n","A: An AI agent is a problem solver that can make autonomous decisions\n","Q: What is the react agent pattern?\n","A: The react agent pattern is a concept that combines reasoning and acting\n","Q: How is the react agent pattern used?\n","A: The react agent pattern is used to build agents that can solve complex problems\n","\n","KEY CONCEPTS:\n"," AI agents, react agent pattern, autonomous decisions, LangChain, natural language processing, machine learning, problem solvers, reasoning, acting, observing\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains how to trace a reflection agent system using LangChain and Lsmith, demonstrating how to generate an API key and set up environment variables. The system is used to create a viral tweet through multiple iterations of generation and reflection. The speaker highlights the power of reflection agents in thinking deeply about a task and generating output. The process involves a generation node and a reflect node, which critique and refine the output until a final tweet is produced.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Generative AI', 'LangChain']\n","\n","Q&A:\n"," Q: What is the purpose of the reflection agent system?\n","A: To generate a viral tweet through multiple iterations of generation and reflection\n","Q: What tools are used to trace the reflection agent system?\n","A: LangChain and Lsmith\n","Q: How many iterations are involved in generating the final tweet?\n","A: Six different exchanges between the generation and reflection nodes\n","\n","KEY CONCEPTS:\n"," Reflection Agent, LangChain, Lsmith, API Key, Environment Variables, Generation Node, Reflect Node, Viral Tweet, Natural Language Processing, Generative AI\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The speaker installs the LangChain library and imports the ChatOpenAI class to interact with OpenAI's APIs. They initialize an LLM using the gpt-4 model and make an API call to calculate the square root of 49. After resolving an API key error by creating an .env file and installing the python-dotenv package, they successfully retrieve the response from the LLM. The speaker notes that the response contains additional data, but they are only interested in the content property. They demonstrate how to access this property and plan to send an entire conversation history to the LLM in the next section.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What library does the speaker install to interact with OpenAI's APIs?\n","A: LangChain\n","Q: What model does the speaker use to initialize the LLM?\n","A: gpt-4\n","Q: What package does the speaker install to access the .env file?\n","A: python-dotenv\n","\n","KEY CONCEPTS:\n"," LangChain, OpenAI API, LLM, gpt-4 model, API key, .env file, python-dotenv package, conversation history, content property, Natural Language Processing\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," The video demonstrates how the sort method in Python works with lists containing strings and numbers. It shows that when sorting a list of strings, Python prioritizes uppercase letters over lowercase letters. The video also explores what happens when a list contains both strings and numbers, revealing that Python puts numbers first when sorting. The presenter highlights the importance of considering case sensitivity when sorting lists. The video concludes by showing the reverse sort order for lists with strings and numbers.\n","\n","TOPICS:\n"," ['Python Programming', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: How does Python sort lists with strings?\n","A: Python sorts lists with strings alphabetically, prioritizing uppercase letters over lowercase letters.\n","Q: What happens when a list contains both strings and numbers?\n","A: Python puts numbers first when sorting a list with both strings and numbers.\n","Q: Why is case sensitivity important when sorting lists?\n","A: Case sensitivity is important because Python treats uppercase and lowercase letters differently when sorting.\n","\n","KEY CONCEPTS:\n"," Python sort method, case sensitivity, uppercase letters, lowercase letters, lists with strings, lists with numbers, alphabetical order, reverse sort order, string sorting, number sorting\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The decision of who should make a decision, a human or an AI, depends on the task and the confidence level of the AI. For tasks where the AI is certain, it can outperform humans, but for tasks where the AI is unsure, humans can bring in additional information and context to make a better decision. Augmented intelligence, which combines human and AI decision-making, can be the most effective approach, but it requires careful consideration of human cognitive bias. The presentation of AI recommendations to humans can influence their decision-making, with forced display leading to automation bias and optional display allowing humans to consider the case before consulting the AI. By combining human and AI decision-making and minimizing cognitive bias, we can improve decision-making outcomes.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the role of confidence level in AI decision-making?\n","A: The confidence level of the AI determines whether it or a human should make a decision.\n","Q: What is automation bias?\n","A: Automation bias is the propensity for humans to favor suggestions from automated decision-making systems and ignore contradictory information.\n","Q: What is the benefit of augmented intelligence?\n","A: Augmented intelligence combines human and AI decision-making to produce better outcomes than either alone.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Human Bias, Augmented Intelligence, Automation Bias, Confidence Score, Decision-Making, Fraud Detection, Machine Learning, Cognitive Bias, AI Recommendations\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Dimitrius, a product manager at Google, discusses the launch of new Vertex AI APIs to help developers build generative applications for enterprises. The six new APIs include a document understanding API, an embedding API, a vector search API, a ranking API, a grounded generation API, and a fact-checking API. These APIs aim to solve technical challenges and provide high-quality results. Dimitrius highlights the unique features of each API and how they can be seamlessly integrated into developers' workflows.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: What are the six new Vertex AI APIs?\n","A: Document understanding API, embedding API, vector search API, ranking API, grounded generation API, and fact-checking API\n","Q: What is the purpose of the document understanding API?\n","A: To process large amounts of documents and improve the quality of generative applications\n","Q: How can developers integrate the new APIs into their workflow?\n","A: By using them as primitives with clear interfaces and integrating them with popular frameworks like Chain or Llama Index\n","\n","KEY CONCEPTS:\n"," Vertex AI, Generative Applications, Document Understanding, Embedding API, Vector Search, Ranking API, Grounded Generation, Fact-Checking, Google Magic, Natural Language Processing\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The lecture discusses the Singular Value Decomposition (SVD) of a matrix X, where X can be decomposed into unitary matrices U and V, and a diagonal matrix Σ. Unitary matrices preserve angles and lengths of vectors, and they play a crucial role in science and engineering. The economy SVD is also introduced, which is a reduced version of the SVD. The lecture also explores the geometric interpretation of the SVD, where the matrix X can be thought of as a transformation that maps a sphere to an ellipsoid. The orientation and elongation of the ellipsoid are determined by the singular vectors and singular values of X.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the Singular Value Decomposition (SVD) of a matrix?\n","A: The SVD of a matrix X is a decomposition of X into unitary matrices U and V, and a diagonal matrix Σ.\n","Q: What is a unitary matrix?\n","A: A unitary matrix is a matrix that preserves angles and lengths of vectors.\n","Q: What is the geometric interpretation of the SVD?\n","A: The SVD can be thought of as a transformation that maps a sphere to an ellipsoid, where the orientation and elongation of the ellipsoid are determined by the singular vectors and singular values of X.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Unitary Matrices, Economy SVD, Geometric Interpretation, Matrix Transformation, Vector Space, Linear Algebra, Principal Component Analysis, Data Transformation\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses building a generative AI-powered application using Google Gemini Pro 1.5. The speaker showcases the capabilities of the model by demonstrating its ability to process text and images, and provides a hands-on example of how to create an API key and use it to access the model. The model has a context length of up to 1 million tokens, allowing it to process large amounts of data. The speaker also highlights the differences between Gemini Pro 1.0 and 1.5, including the increased context length and the ability to work with both text and images.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is Google Gemini Pro 1.5?\n","A: A generative AI model with a context length of up to 1 million tokens\n","Q: What is the difference between Gemini Pro 1.0 and 1.5?\n","A: Gemini Pro 1.5 has a longer context length and can work with both text and images\n","Q: How can I access Google Gemini Pro 1.5?\n","A: By creating an API key on the Google AI Studio website\n","\n","KEY CONCEPTS:\n"," Google Gemini Pro 1.5, Generative AI, Context length, API key, Natural Language Processing, Artificial Intelligence, Machine Learning, Multimodal model, Text and image processing\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," This section focuses on evaluating and testing prompt engineering models, including matrices such as perplexity, accuracy, and human evaluation. The speaker demonstrates how to evaluate a large language model using a custom function, achieving 100% accuracy. Debugging and improving models involves analyzing generated responses, identifying common errors, and fine-tuning. Testing models on different data sets and tasks is crucial to determine their ability to generalize. The process of evaluating and testing prompt engineering models is ongoing, requiring continuous validation and improvement.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is perplexity in language models?\n","A: Perplexity measures how well a language model predicts a sequence of words.\n","Q: Why is human evaluation important?\n","A: Human evaluation involves having humans rate the quality of generated responses.\n","Q: What is the purpose of testing prompt engineering models?\n","A: Testing determines the model's ability to generalize on new or unseen data.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Language Models, Perplexity, Accuracy, Human Evaluation, Debugging, Model Testing, Cross Validation, Visualization Tools, Large Language Models\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains the difference between generative AI, AI agents, and agentic AI. Generative AI can create new content based on patterns learned from existing data, but has limitations such as a knowledge cutoff date. AI agents can perform tasks using tools and memory, and can take autonomous decisions. Agentic AI is a system where one or more AI agents work autonomously to reach a complex goal, using tools, knowledge, and other agents. The speaker also discusses the evolution from simple generative AI to AI agents to agentic AI, and provides examples of how to build agentic AI systems using frameworks such as N8N and Agnu.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is generative AI?\n","A: Generative AI is an AI that can create new content based on patterns learned from existing data.\n","Q: What is the main difference between AI agents and agentic AI?\n","A: AI agents can perform narrow tasks, while agentic AI can handle complex, multi-step goals with planning and coordination.\n","Q: What is the role of LLM in agentic AI systems?\n","A: LLM is a core component of agentic AI systems, providing the ability to generate text and make decisions.\n","\n","KEY CONCEPTS:\n"," Generative AI, AI Agents, Agentic AI, Large Language Model, LLM, N8N, Agnu, Autonomous Decision Making, Multi-Step Goals, Complex Tasks\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses the concept of covariance in statistics, which measures the relationship between two random variables. The speaker explains how covariance can be used to quantify the relationship between variables, such as the size of a house and its price. They provide examples of how to calculate covariance and interpret the results, including scenarios where one variable increases and the other increases or decreases. The speaker also notes the limitations of covariance and introduces the concept of Pearson correlation coefficient, which will be discussed in the next video.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is covariance?\n","A: A measure of the relationship between two random variables\n","Q: How is covariance calculated?\n","A: Using the formula: 1/n * Σ(Xi - μX)(Yi - μY)\n","Q: What is the limitation of covariance?\n","A: It does not indicate the strength of the relationship between variables\n","\n","KEY CONCEPTS:\n"," Covariance, Random Variables, Data Analysis, Statistics, Machine Learning, Pearson Correlation Coefficient, Variance, Mean, Standard Deviation\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," The video discusses the concept of objective in reinforcement learning, which is a goal that an agent strives to achieve. The objective is defined as a numerical reward signal that the agent receives for its actions, and the goal is to maximize the cumulative reward over time. The video explains how to define objectives in different scenarios, such as episodic tasks like Tic-Tac-Toe and continuous tasks like stock market trading. The agent learns to achieve its objective through trial and error, updating its policy based on the rewards it receives. The video also touches on the concept of parameterizing the agent's objective, which involves defining a reward function that reflects the agent's goals and preferences.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the ultimate goal of an agent in reinforcement learning?\n","A: To maximize the cumulative reward over time\n","Q: How does an agent learn to achieve its objective?\n","A: Through trial and error, updating its policy based on rewards\n","Q: What is the purpose of a reward function in reinforcement learning?\n","A: To assign a numerical value to each state or action of the agent\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Objective, Reward Signal, Agent, Policy, Episodic Tasks, Continuous Tasks, Tic-Tac-Toe, Stock Market Trading, Reward Function, Parameterization\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses dictionaries in Python, a data type consisting of key-value pairs. Dictionaries are declared within curly brackets and are useful for mapping one item to another. The transcript covers how to create, access, and manipulate dictionaries, including using functions such as items, keys, and values. It also demonstrates how to create a dictionary from two lists using the zip function and how to edit, change, and delete entries within a dictionary.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A dictionary is a data type consisting of key-value pairs.\n","Q: How are dictionaries declared in Python?\n","A: Dictionaries are declared within curly brackets.\n","Q: What is the purpose of the zip function in creating a dictionary?\n","A: The zip function pairs two lists together to create a dictionary.\n","\n","KEY CONCEPTS:\n"," dictionaries, key-value pairs, Python, data type, curly brackets, items, keys, values, zip function, lists, pandas, data science\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," The use of AI and machine learning in security operations can significantly improve an organization's security posture by detecting and responding to insider threats quickly and precisely. User Behavior Analytics (UBA) is a key tool in this effort, using machine learning to analyze user behavior and identify anomalies and potential threats. IBM's Q Radar SIM solution integrates UBA to help security professionals detect and respond to insider threats more effectively. The solution provides a dashboard for analysts to understand current risks, prioritize employees by risk, and review alerts and offenses. By streamlining processes and providing actionable insights, Q Radar SIM helps security analysts stay ahead of emerging threats.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is User Behavior Analytics (UBA)?\n","A: UBA uses machine learning to analyze user behavior and identify anomalies and potential threats.\n","Q: How does Q Radar SIM help security analysts?\n","A: Q Radar SIM provides a dashboard for analysts to understand current risks, prioritize employees by risk, and review alerts and offenses.\n","Q: What is the benefit of using AI and machine learning in security operations?\n","A: The use of AI and machine learning can significantly improve an organization's security posture by detecting and responding to insider threats quickly and precisely.\n","\n","KEY CONCEPTS:\n"," User Behavior Analytics, Artificial Intelligence, Machine Learning, Q Radar SIM, Security Operations, Insider Threats, Anomaly Detection, Risk Management, Security Posture\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Krishak introduces the new Lama 3 model, an open-source language model developed by Meta, and discusses its features and performance metrics. The model is available in two variants, with 8 billion and 70 billion parameters, and has been trained on a large dataset of 50 trillion tokens. Lama 3 has shown impressive results in benchmarks, outperforming other models in certain tasks. Krishak also explains how to access and use the model, including downloading it from GitHub or Hugging Face. The model has the potential to revolutionize the field of AI and is a significant improvement over its predecessor, Lama 2.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: What is Lama 3?\n","A: Lama 3 is an open-source language model developed by Meta.\n","Q: What are the key features of Lama 3?\n","A: Lama 3 has 8 billion and 70 billion parameter variants and has been trained on a large dataset of 50 trillion tokens.\n","Q: How can I access Lama 3?\n","A: Lama 3 can be accessed through GitHub or Hugging Face, and requires a download and installation process.\n","\n","KEY CONCEPTS:\n"," Lama 3, Open-source model, Language model, Meta AI, Benchmark results, Parameter variants, Training data, GitHub, Hugging Face, AI development\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The speaker is explaining how to use the scikit-learn library in Python to create a decision boundary. They start by searching Google for information on the library and the Naive Bayes algorithm. The speaker finds a page on Naive Bayes that provides a derivation of the formula and various use cases, including Gaussian Naive Bayes. They plan to use Gaussian Naive Bayes to write a classifier. By the end of the next video, viewers will be able to write this code themselves.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What library is being used?\n","A: scikit-learn\n","Q: What algorithm is being used?\n","A: Naive Bayes\n","Q: What specific type of Naive Bayes is being used?\n","A: Gaussian Naive Bayes\n","\n","KEY CONCEPTS:\n"," scikit-learn, Naive Bayes, Gaussian Naive Bayes, Python code, decision boundary, Google search, machine learning algorithm, classifier, library documentation\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion revolves around statistical distributions, specifically Gaussian and log normal distributions. Gaussian distribution is a bell-curve shaped distribution where most data points cluster around the mean, while log normal distribution is a distribution where the logarithm of the data follows a Gaussian distribution. The speaker explains how to identify and work with these distributions, including converting them to standard normal distributions for easier analysis. The importance of understanding these distributions is highlighted, particularly in machine learning and data analysis. The speaker also provides examples of real-world data that follow these distributions, such as income and product review lengths.\n","\n","TOPICS:\n"," ['Statistics', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the difference between Gaussian and log normal distributions?\n","A: Gaussian distribution is a bell-curve shaped distribution, while log normal distribution is a distribution where the logarithm of the data follows a Gaussian distribution.\n","Q: Why is it important to understand statistical distributions?\n","A: Understanding statistical distributions is important for machine learning and data analysis, as it helps to identify patterns and relationships in data.\n","Q: How can log normal distribution be converted to a standard normal distribution?\n","A: Log normal distribution can be converted to a standard normal distribution by taking the logarithm of the data and then applying a formula to scale it down to a standard normal distribution.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, log normal distribution, standard normal distribution, machine learning, data analysis, statistical distributions, bell curve, mean, standard deviation, log normalization\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This video series covers an end-to-end deep learning project in the agriculture domain, focusing on detecting diseases in potato plants using convolutional neural networks. The project involves data collection, model building, and deployment on Google Cloud, with a mobile app developed in React Native. The series will cover technical architecture, data collection, model building, and deployment, using technologies such as TensorFlow, Fast API, and TensorFlow Lite. The goal is to develop a mobile app that can detect diseases in potato plants and provide accurate predictions. The project is designed to be a comprehensive learning experience, covering various aspects of deep learning and machine learning.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the main goal of the project?\n","A: To detect diseases in potato plants using deep learning\n","Q: What technology is used for model building?\n","A: Convolutional Neural Networks (CNN)\n","Q: What framework is used for mobile app development?\n","A: React Native\n","\n","KEY CONCEPTS:\n"," Deep Learning, Convolutional Neural Networks, TensorFlow, Fast API, TensorFlow Lite, React Native, Google Cloud, Machine Learning, Data Collection, Model Deployment\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses the levels of autonomy in LLM applications, ranging from zero autonomy in code to maximum autonomy in state machines. It explains the disadvantages and advantages of each level, including code, single LLM calls, chains, routers, and state machines. The state machine level is where Landgraph comes into play, allowing for human-in-the-loop approval, multi-agent systems, and adaptive learning. The transcript also highlights the difference between human-driven and agent-executed systems, with state machines being considered agent-driven due to their ability to make decisions and refine outputs.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'LangChain']\n","\n","Q&A:\n"," Q: What is the main difference between a chain and a router in LLM applications?\n","A: A chain is a fixed sequence of steps, while a router can make decisions on its own based on user input.\n","Q: What is the role of Landgraph in state machines?\n","A: Landgraph is used in state machines to enable human-in-the-loop approval, multi-agent systems, and adaptive learning.\n","Q: What is the key characteristic of an agent-executed system?\n","A: An agent-executed system is one where the AI takes care of deciding the output, steps to take, and refining outputs, with minimal human intervention.\n","\n","KEY CONCEPTS:\n"," LLM applications, autonomy levels, state machines, Landgraph, human-in-the-loop, multi-agent systems, adaptive learning, agent-executed systems, Natural Language Processing, Artificial Intelligence\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This section covers advanced topics in prompt engineering, including handling different types of prompts such as text, image, and audio-based prompts. It also explores advanced techniques for fine-tuning pre-trained large language models, including multitask learning and distillation. Additionally, it discusses best practices for data pre-processing and cleaning, as well as deploying prompt engineering models in production. The section also touches on ethical considerations in prompt engineering, such as fairness and privacy. The goal is to help users become experts in prompt engineering and build efficient models. The section provides practical exercises for advanced prompt engineering models, including handling different types of prompts, fine-tuning pre-trained models, and data pre-processing and cleaning.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: Prompt engineering is a field that involves designing and optimizing prompts to elicit specific responses from language models.\n","Q: What are the different types of prompts?\n","A: Prompts can be text-based, image-based, or audio-based.\n","Q: What is multitask learning?\n","A: Multitask learning is a technique that involves training a model on multiple tasks simultaneously to improve its robustness and generalizability.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Machine Learning, Pre-trained Models, Fine-tuning, Multitask Learning, Distillation, Data Pre-processing, Tokenization, Normalization, Ethical Considerations, Fairness, Privacy\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The lecture discusses the concept of eigenfaces and singular value decomposition (SVD) in the context of image classification. The speaker uses examples of images of Arnold Schwarzenegger and Sylvester Stallone to demonstrate how eigenfaces can be used to cluster and classify images. The speaker also explores the idea of projecting images into a lower-dimensional space using SVD and how this can be used for image classification. Additionally, the speaker discusses the limitations of this approach and how it can be improved by incorporating 3D geometry and depth information. The lecture also touches on the idea that simple image classification algorithms can be shallow and may not always produce accurate results. The speaker uses the example of Taylor Swift and Arnold Schwarzenegger to illustrate how eigenfaces can sometimes produce unexpected results due to correlations in the data.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the main concept discussed in the lecture?\n","A: Eigenfaces and singular value decomposition (SVD)\n","Q: How are images classified using eigenfaces?\n","A: By projecting them into a lower-dimensional space using SVD\n","Q: What is a limitation of simple image classification algorithms?\n","A: They can be shallow and may not always produce accurate results\n","\n","KEY CONCEPTS:\n"," Eigenfaces, Singular Value Decomposition, Image Classification, Principal Component Analysis, Deep Learning, 3D Geometry, Depth Information, Correlations, Shallow Algorithms\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a framework that acts as a bridge between large language models (LLMs) and the real world, enabling them to interact with APIs, databases, and other external systems. This allows LLMs to go beyond just reasoning and generating text, and to actually take actions in the real world. With LangChain, developers can build applications that leverage the power of LLMs while also integrating with external systems. The framework provides a flexible and modular architecture, making it easy to switch out different LLMs or add new functionality. By using LangChain, developers can create more powerful and interactive AI applications.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: A framework that connects LLMs to the real world\n","Q: What is the main limitation of LLMs?\n","A: They cannot interact with the real world on their own\n","Q: What can LangChain enable LLMs to do?\n","A: Access APIs, databases, send emails, and more\n","\n","KEY CONCEPTS:\n"," LangChain, Large Language Models, LLMs, Artificial Intelligence, Natural Language Processing, APIs, Databases, Real-world interactions, AI applications, Framework architecture\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," This video discusses residual analysis in time series forecasting, focusing on how to use residuals to improve forecasting methods. Residuals are the difference between fitted values and actual values, and analyzing them can help identify issues such as autocorrelation and bias in the model. The video uses Python to demonstrate how to perform residual analysis on a time series dataset, including plotting autocorrelation and partial autocorrelation functions, and using the Ljung-Box test to detect correlation. The results show that the residuals have some correlation, indicating that the model may need to be refitted. The video concludes by emphasizing the importance of residual analysis in understanding and improving forecasting models.\n","\n","TOPICS:\n"," ['Time Series', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: Residuals are the difference between fitted values and actual values.\n","Q: How can residual analysis be used to improve forecasting models?\n","A: Residual analysis can help identify issues such as autocorrelation and bias in the model.\n","Q: What is the Ljung-Box test used for?\n","A: The Ljung-Box test is used to detect correlation in residuals.\n","\n","KEY CONCEPTS:\n"," Residual Analysis, Time Series Forecasting, Autocorrelation, Bias, Ljung-Box Test, Python, Machine Learning, Statistics, Data Science\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," The video demonstrates how to build a Text2SQL agent using LangGraph, Next.js, and SQLite. The agent is able to connect to a database and generate SQL queries based on natural language input. The agent is trained on a large language model and uses a ReAct agent to interact with the database. The video also shows how to set up a frontend application using Next.js and how to connect it to the agent. The agent is able to generate SQL queries and execute them against the database, returning the results to the user. The video also discusses the importance of guardrails to prevent the agent from having unlimited control over the database.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the purpose of the Text2SQL agent?\n","A: To generate SQL queries based on natural language input\n","Q: What technology is used to build the ReAct agent?\n","A: LangGraph\n","Q: What database is used in the video?\n","A: SQLite\n","\n","KEY CONCEPTS:\n"," Text2SQL agent, LangGraph, ReAct agent, Natural Language Processing, SQLite, Next.js, Large Language Model, Database, SQL queries, Guardrails\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses the field of prompt engineering, a specialized area within natural language processing that focuses on building models to generate high-quality text outputs in response to prompts. Prompt engineering is important for applications such as chatbots, language translation, and content generation, where output quality significantly impacts user experience. The field has limitations, including struggling with complex prompts and generating biased outputs. The course aims to provide a comprehensive introduction to prompt engineering, covering basics and advanced techniques. By the end of the course, learners will have a solid understanding of prompt engineering and its importance.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A field within natural language processing that focuses on building models to generate high-quality text outputs in response to prompts.\n","Q: What are the benefits of prompt engineering?\n","A: Generating text outputs that are more accurate, coherent, and contextually appropriate than traditional approaches.\n","Q: What are the limitations of prompt engineering?\n","A: Struggling with complex and ambiguous prompts, and generating biased and inaccurate outputs.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Chatbots, Language Translation, Content Generation, Pre-trained Models, Large Language Models, Fine-tuning, Model Architecture, User Experience\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," This episode of Code Emporium discusses Q-learning, a popular concept in reinforcement learning. Q-learning is a value-based method that determines a value function to quantify total reward and find the optimal policy. The value function can be a state value function or a state-action value function, with Q-learning focusing on the latter. The goal of Q-learning is to learn the Q-values that maximize the total reward. The process involves initializing a Q-table, choosing actions based on an exploration policy, calculating observed Q-values using the Bellman equation, and updating the Q-table based on the temporal difference error.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is Q-learning?\n","A: A value-based reinforcement learning method\n","Q: What is the goal of Q-learning?\n","A: To learn Q-values that maximize total reward\n","Q: What is the Bellman equation used for?\n","A: To calculate observed Q-values\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement Learning, Value Function, State Value Function, State-Action Value Function, Q-table, Bellman Equation, Temporal Difference Error, Exploration Policy, Optimal Policy\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier is a type of linear classifier that uses a linear function to generate predictions from input data. The model takes inputs, denoted as X, and applies a matrix multiply with weights W and bias b to produce scores. These scores are then turned into probabilities using a softmax function, which ensures that the probabilities sum to 1. The goal of training the model is to find the optimal values for the weights and bias. The softmax function plays a crucial role in converting scores into probabilities, allowing for proper classification.\n","\n","TOPICS:\n"," ['Machine Learning', 'Natural Language Processing', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is a type of linear classifier.\n","Q: What is the purpose of the softmax function?\n","A: To turn scores into probabilities.\n","Q: What is the goal of training a logistic classifier?\n","A: To find the optimal values for the weights and bias.\n","\n","KEY CONCEPTS:\n"," logistic classifier, linear function, matrix multiply, softmax function, machine learning, weights, bias, probabilities, logits, classification\n","Saved row 29\n","DONE. Final file: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_zero_shot_full_output.xlsx\n","\n","Zero-shot Groq pipeline completed ✓\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_zero_shot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.3-70b-versatile/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["050391c6e5cd4c8ab95395f760170dd1","f30ab506d77545238ce7e4c55431c8d2","31b7345d39c542be8b8fac3650ffec6b","f44414cbde0746c6b0d7859bae4c389f","17b55af2e2b944feb24fcc8a005512d6","ebaa883535774c608cd7293c2eebad1b","f3a17c0968b14936a739c1416c4ebe71","496c0f7e306742e78316686d795e1cc5","7a9d1f81922243ad8bcc82ae8a89de29","e5bbef69d7424fb3840e8c741e587aa5","a61cdd613e2240639d9d4ce5c5ab53b3","4bcaaba7086645dd8a49146ff3749ebf","2fa7b1fb105a4f75aeb4ca738f3a862b","03c450eee64143609b241415f84ade28","c631be0157a04a0ead15a817a02d4c93","108192aacffd4fcba24d0f40fd9c0700","946afc29a2f5424f9ef26bb1ec616e7d","71adfe0dfb014bfdaf93ad0661f1303c","0788c1895f184834bb9db4e3df18a20a","46b4abecda8044928b2c072ca6eee89e","71cdfc9b2645432593c63bff2b4c872d","da128057565f4cee98b74894a8dd4520","48a49564a3ca4ade85825b320456d6f1","2005a4d73c92410dae4ea908691a4943","e7135e6c742b4cc18c71f1e400bff324","b97f4ebb0f284e48aaac54b521a725eb","a36c1881eaa2451d9e40b0cc7d1b122f","9dad9eaf65004592a2be91e3b0f921ed","43c5fa0d70ce43bfb64f066961abf45a","3257d54cc0024ea7804c13326ff598f8","4ec48c1ddc3a454bbdce1f5b6046a6c4","96c74c7304514914995cdc68556a6c2d","80db653454de430bb79eb749e5d17525","80fac788659d4cba969e3517f3cdcdf7","1310c704463f4ea7920229282d84bdde","665877973a224e72a2d4e1aba8dfe0bd","5036e76ea0e34de4b66d224b47627e40","154f8be5a4e84dc69f8df310f63ba9ca","ffe23e61f9ec488db52fda19e26f765d","0a6f2a7e665d4677a80aea0e6f5401ec","1c89de83424a4a86aecafe44c80cb981","106d62b4a7014017abd40cdabf083928","d3ac840580b14f4e96979f593b3b6a10","2648f4d72a694d04ac10a568886a3a98","cbb985d68cac41f0946d7732457dbc79","2065093cb91843afbd0be1c45802ebd7","413660129ab845b4b1e1f46071f3dec6","bb95fa5149454527aa7243297abd77f1","54716ee5430340a3a119e08401aa0aa0","b53efd333ce64eac9d544cbfcb97734f","7846bac9b9f245cdaee4257b9264b882","af8f066bf9c44244a9f60057f483da80","7f46b299988b4a23a2151b8c54c9d452","ea5e9f78f7a946b296b67fbce3d98333","ad8168f19feb41f1a4bedd5cadb98fe1","f411e638ae84463e9961f3998e7018b1","abe5cdcb66af4ed292814bd6b6c7a7d7","cb020a6301b2413d962ee4bf9e3b4158","16cba2102f7b46f5b8fd3c1eaaf068f2","b6b753d5c078486e8039650665a02dd0","39bf2d0de2384d568d3aa27f52f14f38","a45ba7821eb9421da4a325ae14a18d78","e6d8063fecf94e659c9fd4b4d9f3950b","c3c9fb7e4e46481f9980fe0d0bf68359","d35c623a71464ad4b072cd9050e8f313","10875899115b40b6b2b35b9059d91ff9"]},"id":"0kE6XtTBvn2e","executionInfo":{"status":"ok","timestamp":1763365317390,"user_tz":-330,"elapsed":139677,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"cecf55e6-32bf-4970-cb38-47237af738b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_zero_shot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050391c6e5cd4c8ab95395f760170dd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bcaaba7086645dd8a49146ff3749ebf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48a49564a3ca4ade85825b320456d6f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80fac788659d4cba969e3517f3cdcdf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbb985d68cac41f0946d7732457dbc79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f411e638ae84463e9961f3998e7018b1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2986\n","  - BLEU: 0.0594\n","  - BERTScore F1: 0.8872\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3471\n","  - Micro F1: 0.4789\n","  - Macro F1: 0.4271\n","  - Weighted F1: 0.4257\n","\n","Q&A Generation:\n","  - BLEU: 0.0486\n","  - Diversity: 0.7193\n","  - Answerability: 0.7111\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4967\n","  - Recall@10: 0.1987\n","  - F1@10: 0.2838\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/llama-3.3-70b-versatile/evaluation_final.json\n"]}]}]}