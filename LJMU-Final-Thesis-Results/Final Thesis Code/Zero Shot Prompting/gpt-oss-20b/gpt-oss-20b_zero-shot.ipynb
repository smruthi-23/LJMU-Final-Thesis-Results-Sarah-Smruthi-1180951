{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOW7IAnV/v7Zp39DE7TadEu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8da6243e40644c06bc7446a18189cacd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a70ebfc5afa644d69c7417e3bc4a341a","IPY_MODEL_28ef9ae880584977bade087257b52ca4","IPY_MODEL_402e31801eb74025805f06cc1ed97f82"],"layout":"IPY_MODEL_a974be5bca0d49a0b2eb0578f576be43"}},"a70ebfc5afa644d69c7417e3bc4a341a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cacf2d7710eb46c5978dfc0086405804","placeholder":"​","style":"IPY_MODEL_9004ad3442254c74b25b915a58be72c5","value":"tokenizer_config.json: 100%"}},"28ef9ae880584977bade087257b52ca4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dbaf3d72794f4136820fa87e781b9954","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cabde3978e2f496a9f56de726104ab0e","value":25}},"402e31801eb74025805f06cc1ed97f82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2961aa3f33d4c46b642def8cfbe311c","placeholder":"​","style":"IPY_MODEL_f156118cb88d4e5787e05558b37b9776","value":" 25.0/25.0 [00:00&lt;00:00, 1.11kB/s]"}},"a974be5bca0d49a0b2eb0578f576be43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cacf2d7710eb46c5978dfc0086405804":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9004ad3442254c74b25b915a58be72c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dbaf3d72794f4136820fa87e781b9954":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cabde3978e2f496a9f56de726104ab0e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2961aa3f33d4c46b642def8cfbe311c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f156118cb88d4e5787e05558b37b9776":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21909bab91b1455c93f5cacfc5096ded":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30969ebafcae4e87947328c133da9ae1","IPY_MODEL_cf39023ddd6043828ebc3d8035ae7df4","IPY_MODEL_4da659e9d3f34a7f91c1874ac1a3007c"],"layout":"IPY_MODEL_3b07959bf64343fa98b6eb9177fe4bd7"}},"30969ebafcae4e87947328c133da9ae1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb77613ad89f4d97a1502d3326081cb1","placeholder":"​","style":"IPY_MODEL_503434a1da1e4d79af1ef0381ae4c1be","value":"config.json: 100%"}},"cf39023ddd6043828ebc3d8035ae7df4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f694a11e6341f3ae6d16b94680b633","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ce1e037dd0ca40c3a2cc4c1675d161dc","value":482}},"4da659e9d3f34a7f91c1874ac1a3007c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b73dee9e33a64f6e8fb92723bbc8cd5e","placeholder":"​","style":"IPY_MODEL_d672712a908644f5ab5f11fa19ce274d","value":" 482/482 [00:00&lt;00:00, 11.5kB/s]"}},"3b07959bf64343fa98b6eb9177fe4bd7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb77613ad89f4d97a1502d3326081cb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"503434a1da1e4d79af1ef0381ae4c1be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44f694a11e6341f3ae6d16b94680b633":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce1e037dd0ca40c3a2cc4c1675d161dc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b73dee9e33a64f6e8fb92723bbc8cd5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d672712a908644f5ab5f11fa19ce274d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f58caeefadfa462781d9af873e9a8830":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_570517e04f174b47a6f288f788c28d04","IPY_MODEL_6a655705f98743f3877c9de550e99291","IPY_MODEL_5f269c85536f49ebbe33eddffddfcac9"],"layout":"IPY_MODEL_c5e271da24de45b399a33b9eaa4d1953"}},"570517e04f174b47a6f288f788c28d04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ff9b8fc99c543ca809ffa747e17773f","placeholder":"​","style":"IPY_MODEL_6a24646c8ac343ef810329e2077e03a1","value":"vocab.json: 100%"}},"6a655705f98743f3877c9de550e99291":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea04c0eb36ba4cfbafe35367b3d3dd51","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6011fd3fa3245d69208e6d7c44ffbb2","value":898823}},"5f269c85536f49ebbe33eddffddfcac9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1301aa1190eb4bdaa70160319875301c","placeholder":"​","style":"IPY_MODEL_c1b21c5b45b740c0941b7cbbfda9c564","value":" 899k/899k [00:00&lt;00:00, 2.55MB/s]"}},"c5e271da24de45b399a33b9eaa4d1953":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ff9b8fc99c543ca809ffa747e17773f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a24646c8ac343ef810329e2077e03a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea04c0eb36ba4cfbafe35367b3d3dd51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6011fd3fa3245d69208e6d7c44ffbb2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1301aa1190eb4bdaa70160319875301c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1b21c5b45b740c0941b7cbbfda9c564":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7860fccfac54e9d9a83e2890ba5be8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_56110283016f46bebe59ec43196be5ae","IPY_MODEL_bb4d68f6fd494ee2b5ea423d6b1ac309","IPY_MODEL_86e91fffb2a94c4b8016ce77b5f5d2f3"],"layout":"IPY_MODEL_4bda3c134e144d48bde6b1215a2c5ea6"}},"56110283016f46bebe59ec43196be5ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb144386484b4b379e60b966e7bf8b00","placeholder":"​","style":"IPY_MODEL_0c0e579d516d445f84e08e3c21ad642c","value":"merges.txt: 100%"}},"bb4d68f6fd494ee2b5ea423d6b1ac309":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f49ba3ee576423c8ec27d1d25bb1232","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b492f82e24004750822d73309b6a5da5","value":456318}},"86e91fffb2a94c4b8016ce77b5f5d2f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba135050ef7a406cb589ddf4e8ae9459","placeholder":"​","style":"IPY_MODEL_a7340fcc012e48848d9b7183d6e30ca0","value":" 456k/456k [00:00&lt;00:00, 1.29MB/s]"}},"4bda3c134e144d48bde6b1215a2c5ea6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb144386484b4b379e60b966e7bf8b00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c0e579d516d445f84e08e3c21ad642c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f49ba3ee576423c8ec27d1d25bb1232":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b492f82e24004750822d73309b6a5da5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba135050ef7a406cb589ddf4e8ae9459":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7340fcc012e48848d9b7183d6e30ca0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2efdd410e9d47c28f675d31f3f7f60c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c4f10fc1527242a295ab1380c018cab8","IPY_MODEL_c2846ab077ca45c38e06e74e220829dd","IPY_MODEL_372856861eea4207b36ccbd1d1c1f8db"],"layout":"IPY_MODEL_b7c30399db3e4fa1b292292785d16fea"}},"c4f10fc1527242a295ab1380c018cab8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19afd474e00f487d81bc9120265616f0","placeholder":"​","style":"IPY_MODEL_b4dfdb08373346fd8a0b22065712e3e8","value":"tokenizer.json: 100%"}},"c2846ab077ca45c38e06e74e220829dd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4c0ddd2918f4edbaa6bd109927e121f","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90dff10508804aeb93b206f527873327","value":1355863}},"372856861eea4207b36ccbd1d1c1f8db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3324dc9eda8849de806f60185fea95a4","placeholder":"​","style":"IPY_MODEL_9359be2779c1475097716e6fe66671b5","value":" 1.36M/1.36M [00:00&lt;00:00, 3.73MB/s]"}},"b7c30399db3e4fa1b292292785d16fea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19afd474e00f487d81bc9120265616f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4dfdb08373346fd8a0b22065712e3e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4c0ddd2918f4edbaa6bd109927e121f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90dff10508804aeb93b206f527873327":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3324dc9eda8849de806f60185fea95a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9359be2779c1475097716e6fe66671b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b68193fc95c84288b612c495931dcb16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2dec275bb4e84374ab55eca573610c06","IPY_MODEL_318b66f42a9d4290b8ef91cb00a5cba7","IPY_MODEL_8c76c2b14b1142bb872c4766134de427"],"layout":"IPY_MODEL_d4af741cce7f4fd8858aef59aa5bf94a"}},"2dec275bb4e84374ab55eca573610c06":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1fa4f4dbf4e452692084b747d091fc8","placeholder":"​","style":"IPY_MODEL_0606e2a1fe5149a186331493099ba0c5","value":"model.safetensors: 100%"}},"318b66f42a9d4290b8ef91cb00a5cba7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ff1c6534e804528ad6c3e2ee2e6f2c1","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c494b43e9d3d4d1c92b3bc59211ee587","value":1421700479}},"8c76c2b14b1142bb872c4766134de427":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7b719022ca1457e810c560d46989418","placeholder":"​","style":"IPY_MODEL_613b583e1dc44cd9baec025592b0e80c","value":" 1.42G/1.42G [00:17&lt;00:00, 274MB/s]"}},"d4af741cce7f4fd8858aef59aa5bf94a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1fa4f4dbf4e452692084b747d091fc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0606e2a1fe5149a186331493099ba0c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ff1c6534e804528ad6c3e2ee2e6f2c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c494b43e9d3d4d1c92b3bc59211ee587":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a7b719022ca1457e810c560d46989418":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"613b583e1dc44cd9baec025592b0e80c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"D-x5duSNaoY3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763909268111,"user_tz":-330,"elapsed":31574,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"5f456ec2-ff69-48b9-9263-37995fad0eb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=19a5fac90c76f0dfb85606f0a216aafcdbcf6325caaeb0c4697a73f3ed9c263d\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"KweYMrr5LIdO","executionInfo":{"status":"ok","timestamp":1763909268147,"user_tz":-330,"elapsed":31,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"790c497a-c9cf-4db6-ed62-edc2c977a479"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_zero_shot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key4.txt\"\n","\n","def load_key(path: str):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"openai/gpt-oss-20b\"\n","MAX_CHARS      = 2300\n","GLOBAL_MIN_GAP = 45   # Groq is faster, but we keep a safety gap\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\"\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t)\n","    t = re.sub(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\", \" \", t)\n","    t = re.sub(r\"\\[.*?\\]|\\(.*?\\)\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","\n","    t = re.sub(r\"\\bNLP\\b\", \"Natural Language Processing (NLP)\", t)\n","    t = re.sub(r\"\\bML\\b\", \"Machine Learning (ML)\", t)\n","    t = re.sub(r\"\\bAI\\b\", \"Artificial Intelligence (AI)\", t)\n","    return t.strip()\n","\n","\n","def chunk_text(text: str, max_chars=MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    try:\n","        return json.loads(text[start:end+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (GLOBAL WAIT + RETRIES)\n","#####################################################################\n","def groq_call(prompt: str, temperature=0.15, retries=3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s before next request\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ZERO-SHOT PROMPT (ALL TASKS IN ONE CALL)\n","#####################################################################\n","def generate_zero_shot(transcript: str):\n","    topics_short = \", \".join(VALID_TOPICS)\n","\n","    prompt = (\n","        \"You are an expert NLP assistant.\\n\"\n","        \"Perform ALL tasks concisely using ONLY the transcript.\\n\\n\"\n","        \"Respond in a JSON-like object with EXACTLY these keys:\\n\"\n","        \"generated_summary, predicted_topics, generated_questions, key_concepts.\\n\\n\"\n","        \"Requirements:\\n\"\n","        \"- generated_summary: 3–5 sentence abstractive summary.\\n\"\n","        f\"- predicted_topics: 1–3 most relevant from [{topics_short}]. Use exact labels.\\n\"\n","        \"- generated_questions: 3 short Q&A pairs with keys q and a.\\n\"\n","        \"- key_concepts: 6–10 short noun phrases, no duplicates.\\n\"\n","        \"- No explanations outside the JSON.\\n\\n\"\n","        f\"Transcript:\\n\\\"\\\"\\\"{transcript}\\\"\\\"\\\"\"\n","    )\n","\n","    out = groq_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    summary = j.get(\"generated_summary\", \"\").strip()\n","\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","    topics = [t for t in topics if t in VALID_TOPICS] or [\"Other\"]\n","\n","    qas = j.get(\"generated_questions\", [])\n","    qa_lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = qa.get(\"q\", \"\").strip()\n","            a = qa.get(\"a\", \"\").strip()\n","            if q: qa_lines.append(f\"Q: {q}\")\n","            if a: qa_lines.append(f\"A: {a}\")\n","\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    concepts = \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","    return summary, topics, \"\\n\".join(qa_lines), concepts\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — ZERO SHOT (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming — {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary, topics, qa_text, concepts = generate_zero_shot(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error on row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"DONE. Final file:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nZero-shot Groq pipeline completed ✓\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6hWv1qTQLIi8","executionInfo":{"status":"ok","timestamp":1763910636900,"user_tz":-330,"elapsed":1368649,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"bc4333f8-6901-41c7-e241-49e19428778c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gpt-oss-20b\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," The video explains how reinforcement learning (RL) can be enhanced with human feedback, using a simple grid‑world example where a character named Frank learns to reach a reward spot. It shows that humans can act as mentors, nudging Frank toward better actions and speeding up learning. The speaker then describes how ChatGPT is trained with a reward model that scores responses, and how proximal policy optimization uses those scores to fine‑tune the model. Throughout, quizzes test viewers on RL concepts and the role of human feedback. The final recap emphasizes that human input guides RL algorithms, improving decision quality in systems like ChatGPT.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: Which algorithms can be used with human feedback in RL?\n","A: All of the above (Q‑learning, DQ learning, proximal policy optimization).\n","Q: How does human feedback affect the learning process?\n","A: It accelerates the learning process.\n","Q: What is the primary purpose of the reward model in ChatGPT training?\n","A: To assess and score the quality of answers generated by ChatGPT.\n","\n","KEY CONCEPTS:\n"," grid world, human feedback, reinforcement learning algorithm, reward model, proximal policy optimization, ChatGPT fine‑tuning, decision‑making, learning process, human mentor, iterative training\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," The video explains how to use CVXopt to solve quadratic programming problems for support vector machines (SVMs) and demonstrates the impact of different kernels on classification. It walks through example code that defines linear, polynomial, and Gaussian kernels, and shows how to train hard‑margin and soft‑margin SVMs. The presenter visualizes the effect of kernels by projecting data into higher dimensions and back to 2D, illustrating how nonlinear separability is achieved. The discussion also touches on practical aspects such as using libsvm for production and planning to cover scikit‑learn SVM parameters in a future tutorial. Overall, the tutorial serves as an educational guide to kernel methods in SVMs.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What is CVXopt used for in the tutorial?\n","A: It solves the quadratic programming problem that underlies the SVM training.\n","Q: Which kernel is the default in the provided code?\n","A: The default kernel is the linear kernel.\n","Q: How does the video show the effect of a kernel?\n","A: By transforming data to a higher dimension, fitting a hyperplane, and projecting back to 2D for visualization.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, CVXopt, kernel functions, quadratic programming, hard margin, soft margin, linear kernel, polynomial kernel, data visualization, support vectors\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains that prompts are the inputs to large language models and serve as the foundation of prompt engineering. It outlines seven types of prompts, such as question prompts, statement prompts, and prompts with constraints, and emphasizes how the choice of prompt type affects output quality and complexity. Key features of prompts include length, specific language, context, constraints, tone, and style, all of which help define what is expected and how it should be delivered. The speaker demonstrates examples, showing how adding constraints like \"one word answer\" or \"500 words\" changes the model’s response. Finally, the transcript discusses deconstructing prompts to identify their components and constraints, preparing viewers for building prompt engineering models with pre‑trained models in future lessons.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What are the seven types of prompts mentioned?\n","A: Question prompts, statement prompts, prompts with multiple inputs, prompts with constraints, and four other unspecified types.\n","Q: Why is defining constraints important in a prompt?\n","A: Constraints guide the model to produce outputs that meet specific requirements like word count, tone, or format.\n","Q: What does deconstructing a prompt involve?\n","A: Breaking it into components such as language, expectation, and constraints to better understand its structure.\n","\n","KEY CONCEPTS:\n"," prompt types, prompt features, prompt constraints, prompt deconstruction, large language models, output quality, context and constraints, tone and style, specific language, example prompts\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains AI agents as autonomous problem solvers that can decide their own steps, contrasting them with chains and routers that follow predefined instructions. It introduces tools—specific functions like calculators or search engines—that agents use to complete tasks. The React agent pattern, standing for Reasoning + Acting, is described as a loop where an LLM thinks, chooses an action, executes a tool, observes the result, and repeats until a solution is found. The speaker outlines how LangChain orchestrates tool execution and feeds results back to the LLM, enabling the agent to maintain context. Finally, the speaker previews coding a basic React agent and discusses its limitations before moving to LangGraph.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Natural Language Processing', 'LangChain']\n","\n","Q&A:\n"," Q: What is an AI agent?\n","A: An autonomous problem solver that can decide its own steps.\n","Q: What does the React agent pattern stand for?\n","A: Reasoning + Acting, a loop of think, action, observe.\n","Q: How do tools work in an agent?\n","A: They are functions the agent calls with arguments to perform tasks.\n","\n","KEY CONCEPTS:\n"," AI agents, tools, React agent pattern, reasoning, acting, observation, LangChain, LLM, tool execution, autonomous decision-making\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The speaker demonstrates how to trace a reflection agent system using LangSmith to monitor the interaction between a generation agent and a reflect agent that iteratively refine a tweet. By setting up API keys and environment variables, the system logs each step, showing the generation of an initial tweet, its critique, and successive revisions until a viral tweet is produced. The process highlights the power of reflection agents to deeply analyze and improve outputs through multiple cycles of feedback and generation.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'LangChain']\n","\n","Q&A:\n"," Q: What is a reflection agent?\n","A: An agent that critiques and refines outputs from another agent through iterative feedback.\n","Q: How does LangSmith help in this setup?\n","A: It captures and visualizes each operation, allowing developers to trace runs, threads, and individual traces.\n","Q: What role does the generation agent play?\n","A: It creates the initial tweet and later revises it based on feedback from the reflect agent.\n","\n","KEY CONCEPTS:\n"," reflection agent, tracing project, LangSmith integration, generation agent, reflect agent, tweet refinement, iterative workflow, LangChain, API key, run trace\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains how to set up and use LangChain's ChatOpenAI model with OpenAI's API in a Python environment. It covers installing the necessary packages, importing the model, initializing it with a specific GPT version, and invoking the LLM to make API calls. The speaker demonstrates handling missing API keys by creating a .env file and loading it with python-dotenv, then shows how to extract the content from the LLM response. Additionally, it touches on managing API usage limits and billing, and hints at future sections that will involve conversation history.\n","\n","TOPICS:\n"," ['LangChain', 'Python Programming', 'Generative AI']\n","\n","Q&A:\n"," Q: What keyword is used to call the LLM?\n","A: invoke\n","Q: Which file stores the OpenAI API key?\n","A: .env\n","Q: Which GPT model is used in the example?\n","A: gpt-4\n","\n","KEY CONCEPTS:\n"," LangChain chat model, OpenAI API key, .env file, python-dotenv, LLM invoke, API response content, token usage, billing top-up\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains how Python’s sort method handles lists of strings, noting that uppercase strings are sorted before lowercase ones alphabetically. It demonstrates that reversing the sorted list places lowercase strings in reverse alphabetical order followed by uppercase strings. The speaker then shows that Python can sort lists containing both numbers and strings, with numbers appearing first. Reversing such a mixed list moves the number to the end. The video concludes with a call to action for viewers to like and subscribe.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: How does Python's sort method order uppercase and lowercase strings?\n","A: Uppercase strings are sorted first alphabetically, then lowercase strings.\n","Q: What happens when sorting a list that contains both numbers and strings?\n","A: Numbers are placed before strings in the sorted list.\n","Q: What is the effect of reversing a sorted list?\n","A: It reverses the order, putting lowercase strings in reverse alphabetical order followed by uppercase strings.\n","\n","KEY CONCEPTS:\n"," list of strings, uppercase letters, lowercase letters, sort method, reverse order, mixed data types, Python sorting behavior, insertion at index, alphabetical order, list manipulation\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses how to decide whether a fraud alert should be handled by a human, an AI, or a combination of both. It explains that AI performs best when it is highly confident, while humans excel when AI is uncertain, and that a hybrid approach—augmented intelligence—often yields the highest success rate. The presentation of AI recommendations matters; forced displays can cause automation bias, whereas optional displays allow humans to form independent judgments first. Trust issues arise when AI shows its accuracy percentage, leading humans to ignore the recommendation. Ultimately, the best decision maker depends on the confidence level and how the AI’s input is presented to minimize bias.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Other']\n","\n","Q&A:\n"," Q: What determines when AI should handle fraud alerts?\n","A: The AI’s confidence score threshold.\n","Q: What is automation bias?\n","A: The tendency to favor AI suggestions over human judgment.\n","Q: How does optional display reduce bias?\n","A: It lets humans decide first before seeing the AI recommendation.\n","\n","KEY CONCEPTS:\n"," AI confidence score, human cognitive bias, automation bias, augmented intelligence, fraud detection alerts, performance curve, forced display, optional display, trust in AI, accuracy percentage\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Demitrius Case, a product manager at Google Cloud AI, discusses the launch of six new Vertex AI APIs designed to simplify building generative applications for enterprises. The APIs cover document understanding, improved embeddings, scalable vector search, a ranking service, grounded generation with citations, and a grounding check for fact‑verification. He emphasizes the focus on quality, leveraging Google’s deep expertise, and the ease of integration through stateless primitives and popular frameworks like LangChain and LlamaIndex.\n","\n","TOPICS:\n"," ['Generative AI', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the main focus of Demitrius's talk?\n","A: Launching six new Vertex AI APIs to help developers build generative applications faster and better.\n","Q: Which API helps with document understanding?\n","A: The Document Understanding API.\n","Q: How does the Ranking API improve answer quality?\n","A: It re‑ranks retrieved results based on how well each answer the question, boosting the relevance of the final response.\n","\n","KEY CONCEPTS:\n"," Vertex AI APIs, Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Check Grounding API, Hybrid search\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The lecture explains the singular value decomposition (SVD) of a matrix, emphasizing that the matrices U and V are unitary and preserve angles and lengths. It discusses the geometric interpretation of SVD, showing how a unit sphere is transformed into an ellipsoid whose axes are determined by the singular values and oriented by the singular vectors. The speaker also contrasts real and complex data, noting that complex matrices use conjugate transposes. Additionally, the concept of the economy‑size SVD is introduced, and the role of unitary transformations like the Fourier transform is highlighted as a coordinate rotation that simplifies data representation.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What property do unitary matrices U and V preserve?\n","A: They preserve angles and lengths between vectors.\n","Q: How does the SVD transform a unit sphere?\n","A: It maps the sphere into an ellipsoid whose axes lengths are the singular values.\n","Q: What is the difference between a transpose and a conjugate transpose?\n","A: A conjugate transpose takes the transpose and complex conjugates each element; for real matrices it reduces to the ordinary transpose.\n","\n","KEY CONCEPTS:\n"," singular value decomposition, unitary matrix, left singular vectors, right singular vectors, singular values, ellipsoid, unit sphere, Fourier transform, complex conjugate transpose, economy SVD\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Krishn demonstrates how to build generative AI applications using Google Gemini Pro 1.5, highlighting its multimodal capabilities and massive 1‑million‑token context window. He walks through a demo with a 402‑page Apollo 11 PDF, showing text and image queries, and then explains how to obtain and configure an API key in Google AI Studio. The video covers installing the Python client, selecting the correct model, and using both text and image prompts, including streaming responses and prompt feedback. Krishn also compares Gemini 1.5 Pro to earlier versions, noting the jump in context length from 32k to 1M tokens, and hints at future end‑to‑end projects like PDF‑query RAG applications.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is Gemini 1.5 Pro?\n","A: A multimodal generative AI model from Google with a 1‑million‑token context window.\n","Q: How do I obtain an API key for Gemini 1.5 Pro?\n","A: Visit ai.google.com/app/apikey, click \"Get API key\", and create a key.\n","Q: What is the main difference between Gemini 1.0 Pro and Gemini 1.5 Pro?\n","A: Gemini 1.5 Pro has a much larger context length (≈1 M tokens vs. 32 k tokens).\n","\n","KEY CONCEPTS:\n"," Gemini 1.5 Pro, multimodal model, API key, context length, PDF query RAG, image generation, text generation, Google AI Studio, streaming response, prompt feedback\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses evaluating and testing prompt engineering models, highlighting key metrics such as perplexity, accuracy, and human evaluation. It explains how to compute these metrics using a custom evaluation function on a small dataset, noting an example result of 100% accuracy. The speaker then covers debugging strategies, like analyzing common errors and fine‑tuning, and stresses the importance of testing models on varied datasets to assess generalization. Visualization and cross‑validation are mentioned as tools to aid this process. Continuous evaluation is emphasized as essential for maintaining model performance over time.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What metrics are used to evaluate prompt engineering models?\n","A: Perplexity, accuracy, and human evaluation.\n","Q: How can debugging improve a prompt engineering model?\n","A: By analyzing generated responses for common errors and fine‑tuning the model.\n","Q: Why is testing on different datasets important?\n","A: It shows the model’s ability to generalize to new or unseen data.\n","\n","KEY CONCEPTS:\n"," prompt engineering models, evaluation metrics, perplexity, accuracy, human evaluation, debugging techniques, cross-validation, visualization tools, generalization, large language model\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains the progression from simple generative AI, which creates content based on a large language model, to AI agents that can use tools and APIs to perform tasks, and finally to agentic AI systems that coordinate multiple agents for complex, multi-step goals. Generative AI relies on a knowledge cutoff and can be extended with web search or API calls to provide up-to-date information. An AI agent adds tool access and memory, enabling it to act on user requests like booking flights. Agentic AI expands this by orchestrating several agents, each with specialized capabilities, to handle tasks such as travel planning and visa checks. The discussion also mentions frameworks and tools like N8N and Langraph for building such systems.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is generative AI?\n","A: AI that creates new content such as text, images, or videos based on patterns learned from existing data.\n","Q: How does an AI agent differ from a simple generative AI?\n","A: An AI agent can use tools and APIs, has memory, and can act to complete tasks beyond answering questions.\n","Q: What is agentic AI?\n","A: A system where one or more autonomous agents coordinate to achieve complex, multi-step goals.\n","\n","KEY CONCEPTS:\n"," generative AI, large language model, knowledge cutoff, web search, API integration, tool usage, memory, autonomous decision making, multi-step reasoning, agentic AI system\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains covariance as a statistical measure that quantifies the relationship between two random variables, such as house size and price. They derive the covariance formula, compare it to variance, and illustrate how positive covariance indicates that both variables increase together while negative covariance indicates opposite trends. The discussion highlights covariance’s usefulness in data preprocessing and analysis, but also its limitation in not indicating the strength of the relationship. The speaker mentions that Pearson correlation coefficient will be covered next to address this limitation. Overall, the video serves as an introductory guide to understanding covariance in statistical data analysis.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is covariance?\n","A: Covariance measures how two random variables vary together.\n","Q: When is covariance positive?\n","A: When both variables increase or decrease together.\n","Q: Why use Pearson correlation?\n","A: To quantify the strength and direction of the relationship beyond covariance.\n","\n","KEY CONCEPTS:\n"," covariance, variance, random variables, mean, Pearson correlation coefficient, positive covariance, negative covariance, data preprocessing, data analysis, relationship between variables\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," The video explains how to define the objective in reinforcement learning, emphasizing that the goal is to learn an optimal policy that maximizes cumulative reward. It illustrates this with examples such as a manager setting a deadline for a research task and the classic Tic‑Tac‑Toe game, where rewards are assigned for winning, losing, or drawing. The speaker also discusses continuous tasks like stock trading, where the reward function can be based on profit, loss, or risk‑adjusted metrics. Overall, the focus is on how to design reward signals and objectives to guide an agent’s learning process.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Reinforcement Learning']\n","\n","Q&A:\n"," Q: What is the main objective of reinforcement learning?\n","A: To learn an optimal policy that maximizes cumulative reward.\n","Q: How are rewards defined in a Tic‑Tac‑Toe example?\n","A: +1 for a win, -1 for a loss, and 0 for a draw.\n","Q: What can be used to parameterize the objective in continuous tasks like trading?\n","A: A reward function based on profit, loss, or risk‑adjusted measures such as the Sharpe ratio.\n","\n","KEY CONCEPTS:\n"," objective definition, reward signal, optimal policy, episodic task, continuous task, trial and error learning, value-based method, policy-based method, reward function, cumulative reward\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains how to create and manipulate dictionaries in Python, highlighting that dictionaries consist of key-value pairs separated by colons and commas, and are defined within curly brackets. It covers accessing items, keys, and values using methods like items(), keys(), and values(), and demonstrates how to build a dictionary from two lists using zip and dict. The speaker also shows how to retrieve, modify, and delete entries, as well as how to determine the dictionary's length. Additionally, it mentions converting dictionary keys and values back into lists and notes the importance of dictionaries for data science tasks, especially with pandas.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A dictionary is a collection of key-value pairs stored in curly brackets.\n","Q: How can you create a dictionary from two lists?\n","A: Use the zip function to pair the lists and then pass the result to dict() to create the dictionary.\n","Q: How do you delete an entry from a dictionary?\n","A: Use the del keyword with the dictionary name and the key, e.g., del d[5].\n","\n","KEY CONCEPTS:\n"," dictionary, key-value pair, items method, keys method, values method, zip function, dict constructor, immutable key\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses how AI and machine learning, particularly User Behavior Analytics (UBA), can reduce the time to detect and contain data breaches by about 108 days, according to IBM's 2023 report. It explains that UBA learns user patterns over a minimum of seven days and flags anomalies to help security analysts prioritize insider threat investigations. The IBM Security No Text SIM platform, integrated with the Q radar app, automates threat detection, maps MITRE ATT&CK tactics, and provides natural language insights to accelerate investigations from hours to minutes. Human feedback is used to refine Q radar’s analysis through reinforcement learning, enabling analysts to focus on proactive defense. Overall, the solution streamlines security operations and enhances the organization’s defensive posture.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Other']\n","\n","Q&A:\n"," Q: What does UBA stand for?\n","A: User Behavior Analytics.\n","Q: How many days does UBA need to learn user patterns?\n","A: Seven days.\n","Q: What tool is used for automated investigation in the demo?\n","A: Q radar.\n","\n","KEY CONCEPTS:\n"," User Behavior Analytics, AI and automation, Insider threats, IBM Security No Text SIM, Q radar, MITRE ATT&CK mapping, IOC, risk dashboard, human feedback, reinforcement learning\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Krishak announces the release of Meta’s open‑source Llama 3, highlighting its impressive performance metrics and larger training data compared to Llama 2. The model comes in 8B and 70B parameter variants, supports an 8K context length, and excels in tasks like translation, dialogue, reasoning, and code generation. Benchmarks show Llama 3 outperforms many paid LLMs on human evaluation, GSM, and math tasks, while maintaining low refusal rates. The video explains how to access the model via Meta’s website, Hugging Face, and GitHub, and outlines steps for downloading and running it locally. Krishak promises a follow‑up tutorial on practical usage.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: What are the two parameter sizes available for Llama 3?\n","A: 8 billion and 70 billion parameters.\n","Q: Which platform does Meta use to provide Llama 3 access?\n","A: Meta’s website, Hugging Face, and GitHub.\n","Q: What is the context length supported by Llama 3?\n","A: 8,000 tokens, double that of Llama 2.\n","\n","KEY CONCEPTS:\n"," Llama 3, open source LLM, Meta AI, instruction tuned version, 8 billion parameters, 70 billion parameters, benchmark performance, token training data, context length, Meta Lama Guard\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains how they wrote Python code to create a decision boundary using scikit-learn's Gaussian Naive Bayes algorithm. They emphasize a step‑by‑step approach, starting with a Google search to find documentation and examples. The process includes understanding the Naive Bayes formula, selecting the Gaussian variant, and implementing the classifier in code. By the end of the lesson, viewers should be able to write similar code themselves.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: Which Python library is used for the classifier?\n","A: scikit-learn\n","Q: What specific Naive Bayes variant is implemented?\n","A: Gaussian Naive Bayes\n","Q: What resource does the speaker use to learn how to use the library functions?\n","A: Google search for documentation\n","\n","KEY CONCEPTS:\n"," scikit-learn, Naive Bayes, Gaussian Naive Bayes, decision boundary, Python code, Google documentation, algorithm derivation, classifier implementation\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains Gaussian and log normal distributions, highlighting their shapes, empirical percentages within standard deviations, and how they appear in real data such as heights, income, and product review lengths. They describe the bell curve symmetry of Gaussian data and the right‑skewed tail of log normal data. The discussion covers how to identify a log normal distribution by taking logs and checking for normality, and how to convert both Gaussian and log normal data to a standard normal distribution using scaling. This standardization process, called log normalization, aligns different features (e.g., R&D, marketing, campaign spend) onto the same scale, improving model accuracy. The speaker concludes by emphasizing the importance of understanding distribution types for effective data preprocessing.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the empirical rule for a Gaussian distribution?\n","A: About 68% within 1σ, 95% within 2σ, and 99.7% within 3σ.\n","Q: How can you determine if data follows a log normal distribution?\n","A: Take the logarithm of the data and check if the transformed values form a Gaussian distribution.\n","Q: Why is standard scaling important for model accuracy?\n","A: It puts all features on the same scale, reducing bias and improving learning.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, log normal distribution, bell curve, standard normal distribution, standard scaler, empirical rule, data scaling, model accuracy\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," The video outlines an end‑to‑end deep learning project for detecting early and late blight in potato plants, aiming to help farmers reduce economic losses. It covers data collection, preprocessing with data augmentation, and building a convolutional neural network model in TensorFlow. The model is exported and served via TensorFlow Serving and FastAPI, then deployed to Google Cloud Functions, with a React Native mobile app for farmers to take pictures and receive predictions. The workflow also includes converting the model to a quantized TensorFlow Lite format for faster inference on edge devices. The series promises to guide viewers through each step, from architecture design to deployment, and encourages building similar projects for other crops.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What diseases are the project targeting?\n","A: Early blight and late blight in potato plants.\n","Q: Which neural network architecture is used for classification?\n","A: A convolutional neural network (CNN).\n","Q: Where is the trained model deployed for inference?\n","A: On Google Cloud Functions using TensorFlow Serving and FastAPI.\n","\n","KEY CONCEPTS:\n"," potato disease detection, deep learning project, convolutional neural network, TensorFlow Serving, FastAPI server, React Native mobile app, Google Cloud Functions, quantization, TensorFlow Lite model, data augmentation\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains the progression of autonomy in LLM applications, starting from deterministic code to single LLM calls, chains, routers, state machines, and fully autonomous agents. Each level adds complexity: chains break tasks into specialist steps, routers let the LLM decide routing, and state machines introduce loops, memory, and human approval for iterative refinement. LangGraph is highlighted as a tool for building state machines that enable agents to manage sub-agents and tools. The discussion emphasizes the difference between human-driven flows and agent-driven flows, noting that only agents can make dynamic decisions and refine outputs. Future sections promise deeper exploration of AI agents.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is the first level of autonomy described?\n","A: Zero autonomy, fully deterministic code.\n","Q: How does a chain improve over a single LLM call?\n","A: It splits tasks into specialist steps, reducing confusion.\n","Q: What feature distinguishes a state machine from a router?\n","A: It allows loops, memory, and human-in-loop refinement.\n","\n","KEY CONCEPTS:\n"," levels of autonomy, LLM call, chain, router, state machine, agent, LangGraph, human in loop, tool integration, iterative refinement\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses advanced prompt engineering, covering handling of text, image, and audio prompts, and fine‑tuning pre‑trained large language models. It explains techniques such as multitask learning and model distillation to improve performance and efficiency. Data preprocessing steps like tokenization and normalization are highlighted, along with best practices for deploying models using TensorFlow Serving or Flask APIs. Ethical considerations, including bias, fairness, and privacy, are emphasized as crucial when building and deploying these models. The speaker also outlines practical exercises and future applications of advanced prompt engineering.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What advanced techniques are mentioned for fine‑tuning large language models?\n","A: Multitask learning and model distillation.\n","Q: Which preprocessing steps are recommended for text data?\n","A: Tokenization and normalization.\n","Q: What deployment options are discussed?\n","A: TensorFlow Serving and Flask API.\n","\n","KEY CONCEPTS:\n"," prompt engineering, pre‑trained large language models, multitask learning, model distillation, data preprocessing, tokenization, normalization, deployment, ethical considerations, image‑based prompts\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The lecture demonstrates how to use singular value decomposition (SVD) to compute eigenfaces from a set of action‑hero images and then project new faces into this reduced space for classification. The instructor loads 20 images each of Arnold Schwarzenegger and Sylvester Stallone, computes their average face, subtracts it, and performs an economy SVD to obtain the principal components (eigenfaces). By projecting each image onto the first three eigenfaces, the two actors form distinct clusters, and test images can be classified by proximity to these clusters. The same process is repeated with Taylor Swift versus Stallone, revealing that Arnold and Taylor overlap more than the other pair, illustrating limitations of simple correlation‑based methods.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are eigenfaces?\n","A: Linear combinations of training face images obtained via SVD that capture dominant facial features.\n","Q: Why do Arnold and Taylor overlap in the eigenface space?\n","A: Both have similar skin tone and hair color, leading to higher correlation in pixel values.\n","Q: How is SVD used in this example?\n","A: It decomposes the mean‑centered image matrix into orthogonal components, providing principal directions for projection and compression.\n","\n","KEY CONCEPTS:\n"," eigenfaces, principal component analysis, SVD, action hero dataset, face clustering, image projection, average face, principal components, classification, three‑dimensional geometry\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," The speaker explains LangChain as a framework that connects large language models (LLMs) to real-world APIs and services, enabling applications to perform tasks like booking flights or sending emails. They illustrate this with a vacation-planning example where an LLM can suggest plans but cannot directly book, highlighting the need for a bridge between AI reasoning and external actions. LangChain allows developers to swap underlying models (e.g., from GPT-4 to a free Hugging Face model) without changing application code, making AI more versatile and cost-effective. The framework supports accessing travel APIs, databases, browsing the web, and scraping sites, effectively extending the capabilities of LLMs beyond text generation. The speaker invites learners to explore more use cases in the course.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'LangChain']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: A framework that connects LLMs to real-world APIs and services.\n","Q: Why can't LLMs book flights directly?\n","A: They lack the ability to interact with external systems.\n","Q: How does LangChain enable model switching?\n","A: It abstracts the model layer so code can use different LLMs without changes.\n","\n","KEY CONCEPTS:\n"," LangChain framework, large language models, real-world APIs, flight booking, restaurant recommendation, AI reasoning, model abstraction, web scraping, email sending, database access\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," The video explains residual analysis for time series forecasting, defining residuals as the difference between fitted and actual values. It highlights how to detect autocorrelation and bias in residuals using autocorrelation plots, partial autocorrelation, and the Ljung-Box test. The presenter demonstrates these concepts with a Holt-Winters exponential smoothing model on airline passenger data in Python. By examining residual patterns, one can identify model shortcomings and adjust forecasts accordingly. The session concludes with a recap of residuals’ role in improving forecasting accuracy.\n","\n","TOPICS:\n"," ['Time Series', 'Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: Residuals are the differences between the fitted values from a model and the actual observed values.\n","Q: Which tests are used to check for autocorrelation in residuals?\n","A: Autocorrelation and partial autocorrelation plots, along with the Ljung-Box test, are used.\n","Q: Why should the mean of residuals be zero?\n","A: A zero mean indicates an unbiased forecast; a non-zero mean suggests systematic over- or under-forecasting.\n","\n","KEY CONCEPTS:\n"," residuals, fitted values, autocorrelation, partial autocorrelation, Ljung-Box test, bias, forecasting model, Holt-Winters\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," The video demonstrates building a text-to-SQL agent that can query a database using natural language. It uses LangGraph to create a ReAct agent, Next.js for the frontend, and watsonx.ai for LLM inference, while an in-memory SQLite database stores sample data. The agent is trained to generate SQL queries, execute them via a custom GetFromDB tool, and return results to the user. The tutorial covers setting up the project, configuring environment variables, creating the database schema, and integrating the agent with the frontend. Finally, it showcases the agent answering queries about customer data and performing joins between tables.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'LangChain']\n","\n","Q&A:\n"," Q: What framework is used to build the ReAct agent?\n","A: LangGraph\n","Q: Which database technology is used for the in-memory database?\n","A: SQLite\n","Q: What tool does the agent call to execute SQL queries?\n","A: GetFromDB\n","\n","KEY CONCEPTS:\n"," ReAct agent, LangGraph, Next.js frontend, watsonx.ai models, SQLite in-memory database, Text2SQL agent, SQL query generation, GetFromDB tool, database schema, system prompt\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," The transcript introduces prompt engineering as a specialized area of natural language processing that enhances the quality of text outputs from large language models like GPT, BERT, and Hugging Face Transformers. It explains how prompt engineering improves accuracy, coherence, and contextual relevance compared to rule‑based methods, benefiting applications such as chatbots, translation, and content generation. The speaker outlines the course structure, covering prompt analysis, deconstruction, key features, constraints, and the benefits and limitations of the approach. The course promises a comprehensive introduction, from fundamentals to advanced fine‑tuning techniques, and invites learners to begin exploring this exciting field.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A specialized NLP field that builds models to generate high‑quality text responses to prompts.\n","Q: Why is prompt engineering important?\n","A: It produces more accurate, coherent, and contextually appropriate outputs than rule‑based methods.\n","Q: What are some limitations of prompt engineering?\n","A: It can struggle with complex or ambiguous prompts and may produce biased or inaccurate outputs.\n","\n","KEY CONCEPTS:\n"," prompt engineering, large language models, GPT, fine‑tuning, text generation, user experience, bias, ambiguity, pre‑trained models, chatbots\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," The video explains Q-learning, a popular value-based reinforcement learning method that learns state-action values to maximize cumulative rewards. It reviews the three machine learning paradigms—supervised, unsupervised, and reinforcement learning—and distinguishes value-based from policy-based approaches. Q-learning uses a Q-table updated via the Bellman equation, with temporal difference error guiding adjustments based on a learning rate and discount factor. The speaker demonstrates the algorithm on a simple grid world, showing how random exploration (behavior policy) leads to learning an optimal policy (target policy). The discussion concludes by noting Q-learning is an off-policy method that decouples data collection from policy execution.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is Q-learning?\n","A: A value-based reinforcement learning method that learns state-action values.\n","Q: What role does the Bellman equation play?\n","A: It calculates the observed Q value for a state-action pair.\n","Q: What is temporal difference error?\n","A: The difference between the observed and expected Q values.\n","\n","KEY CONCEPTS:\n"," Q-learning, value-based methods, state-action value function, Bellman equation, temporal difference error, discount factor, learning rate, behavior policy\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains how to train a logistic classifier, a linear model that maps input features to predictions via a matrix multiplication. Inputs are represented as a vector X, multiplied by a weight matrix W and added to a bias b to produce scores for each class. These scores, or logits, are converted into probabilities using the softmax function, ensuring they sum to one and reflect confidence in each class. The goal of training is to adjust W and b so that the probability of the correct class approaches one while others approach zero. This process exemplifies basic machine learning techniques for classification tasks.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence', 'Data Science']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A linear classifier that uses a linear function to generate predictions.\n","Q: How are probabilities obtained from the model's scores?\n","A: By applying the softmax function to the scores.\n","Q: Which parameters are learned during training?\n","A: The weight matrix W and the bias term b.\n","\n","KEY CONCEPTS:\n"," logistic classifier, linear function, weight matrix, bias term, softmax function, probabilities, logits, input vector, output class, training\n","Saved row 29\n","DONE. Final file: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gpt-oss-20b/gpt-oss-20b_zero_shot_full_output.xlsx\n","\n","Zero-shot Groq pipeline completed ✓\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gpt-oss-20b/gpt-oss-20b_zero_shot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gpt-oss-20b/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["8da6243e40644c06bc7446a18189cacd","a70ebfc5afa644d69c7417e3bc4a341a","28ef9ae880584977bade087257b52ca4","402e31801eb74025805f06cc1ed97f82","a974be5bca0d49a0b2eb0578f576be43","cacf2d7710eb46c5978dfc0086405804","9004ad3442254c74b25b915a58be72c5","dbaf3d72794f4136820fa87e781b9954","cabde3978e2f496a9f56de726104ab0e","c2961aa3f33d4c46b642def8cfbe311c","f156118cb88d4e5787e05558b37b9776","21909bab91b1455c93f5cacfc5096ded","30969ebafcae4e87947328c133da9ae1","cf39023ddd6043828ebc3d8035ae7df4","4da659e9d3f34a7f91c1874ac1a3007c","3b07959bf64343fa98b6eb9177fe4bd7","cb77613ad89f4d97a1502d3326081cb1","503434a1da1e4d79af1ef0381ae4c1be","44f694a11e6341f3ae6d16b94680b633","ce1e037dd0ca40c3a2cc4c1675d161dc","b73dee9e33a64f6e8fb92723bbc8cd5e","d672712a908644f5ab5f11fa19ce274d","f58caeefadfa462781d9af873e9a8830","570517e04f174b47a6f288f788c28d04","6a655705f98743f3877c9de550e99291","5f269c85536f49ebbe33eddffddfcac9","c5e271da24de45b399a33b9eaa4d1953","9ff9b8fc99c543ca809ffa747e17773f","6a24646c8ac343ef810329e2077e03a1","ea04c0eb36ba4cfbafe35367b3d3dd51","b6011fd3fa3245d69208e6d7c44ffbb2","1301aa1190eb4bdaa70160319875301c","c1b21c5b45b740c0941b7cbbfda9c564","f7860fccfac54e9d9a83e2890ba5be8d","56110283016f46bebe59ec43196be5ae","bb4d68f6fd494ee2b5ea423d6b1ac309","86e91fffb2a94c4b8016ce77b5f5d2f3","4bda3c134e144d48bde6b1215a2c5ea6","eb144386484b4b379e60b966e7bf8b00","0c0e579d516d445f84e08e3c21ad642c","8f49ba3ee576423c8ec27d1d25bb1232","b492f82e24004750822d73309b6a5da5","ba135050ef7a406cb589ddf4e8ae9459","a7340fcc012e48848d9b7183d6e30ca0","f2efdd410e9d47c28f675d31f3f7f60c","c4f10fc1527242a295ab1380c018cab8","c2846ab077ca45c38e06e74e220829dd","372856861eea4207b36ccbd1d1c1f8db","b7c30399db3e4fa1b292292785d16fea","19afd474e00f487d81bc9120265616f0","b4dfdb08373346fd8a0b22065712e3e8","f4c0ddd2918f4edbaa6bd109927e121f","90dff10508804aeb93b206f527873327","3324dc9eda8849de806f60185fea95a4","9359be2779c1475097716e6fe66671b5","b68193fc95c84288b612c495931dcb16","2dec275bb4e84374ab55eca573610c06","318b66f42a9d4290b8ef91cb00a5cba7","8c76c2b14b1142bb872c4766134de427","d4af741cce7f4fd8858aef59aa5bf94a","c1fa4f4dbf4e452692084b747d091fc8","0606e2a1fe5149a186331493099ba0c5","1ff1c6534e804528ad6c3e2ee2e6f2c1","c494b43e9d3d4d1c92b3bc59211ee587","a7b719022ca1457e810c560d46989418","613b583e1dc44cd9baec025592b0e80c"]},"id":"YQG9hGwFLInj","executionInfo":{"status":"ok","timestamp":1763910843498,"user_tz":-330,"elapsed":206601,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"38510662-5c61-486b-b4b9-5b7f03bbc538"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gpt-oss-20b/gpt-oss-20b_zero_shot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da6243e40644c06bc7446a18189cacd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21909bab91b1455c93f5cacfc5096ded"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f58caeefadfa462781d9af873e9a8830"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7860fccfac54e9d9a83e2890ba5be8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2efdd410e9d47c28f675d31f3f7f60c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b68193fc95c84288b612c495931dcb16"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2736\n","  - BLEU: 0.0407\n","  - BERTScore F1: 0.8803\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3205\n","  - Micro F1: 0.4525\n","  - Macro F1: 0.4265\n","  - Weighted F1: 0.4084\n","\n","Q&A Generation:\n","  - BLEU: 0.0304\n","  - Diversity: 0.8439\n","  - Answerability: 0.7444\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4833\n","  - Recall@10: 0.1933\n","  - F1@10: 0.2762\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Zero Shot Prompting/gpt-oss-20b/evaluation_final.json\n"]}]}]}