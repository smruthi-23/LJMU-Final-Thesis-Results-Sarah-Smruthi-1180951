{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSk2vnxldQWQGNhh3P348W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9d578b1f3b34415b995e72b4b857251b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7c27b10ef13a4acc913fa2f9a0d280c1","IPY_MODEL_1f41a88bbfe24265b9a9072d0a79a7f6","IPY_MODEL_3e1a70e3fb674067bde7f386182f73fd"],"layout":"IPY_MODEL_b5b93f5a9014484c892b8d43f62ba723"}},"7c27b10ef13a4acc913fa2f9a0d280c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9411c45f4fd4af59afedf1b0c3c2cfc","placeholder":"​","style":"IPY_MODEL_75e450d06c5d4615a3f20a80799648b0","value":"tokenizer_config.json: 100%"}},"1f41a88bbfe24265b9a9072d0a79a7f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1131f6e755a04ae6a4a138532f28df09","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f78d758c2d9544dba2bb8a6148e72502","value":25}},"3e1a70e3fb674067bde7f386182f73fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcbbc8597eeb42a192ab681fe5655952","placeholder":"​","style":"IPY_MODEL_725a20ec17fd4e0b8d22f64a5c071c3a","value":" 25.0/25.0 [00:00&lt;00:00, 2.50kB/s]"}},"b5b93f5a9014484c892b8d43f62ba723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9411c45f4fd4af59afedf1b0c3c2cfc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e450d06c5d4615a3f20a80799648b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1131f6e755a04ae6a4a138532f28df09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f78d758c2d9544dba2bb8a6148e72502":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcbbc8597eeb42a192ab681fe5655952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"725a20ec17fd4e0b8d22f64a5c071c3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3b7b13f9531469aa31909badcf22d7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4662290953254d1cba4b00d0f59961f7","IPY_MODEL_a2f3a717cf00495395832e2ffff0c041","IPY_MODEL_0eacd161300348eca587dbb67c1281cf"],"layout":"IPY_MODEL_f73f38d109154562bac342c69bef57f2"}},"4662290953254d1cba4b00d0f59961f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abaab6f02b7043c5b54af6d8ce56d2c6","placeholder":"​","style":"IPY_MODEL_8006bda3741c4a24af671f1f3a096b65","value":"config.json: 100%"}},"a2f3a717cf00495395832e2ffff0c041":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e619fef64734dd0b1d05089da836897","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b36a3a60c7ad4c9081e9a98f7f820cef","value":482}},"0eacd161300348eca587dbb67c1281cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4961eeb9a0f144dc98195e2dedc0f1bf","placeholder":"​","style":"IPY_MODEL_2ac6359e51f34a66acb0a9ae7980dc95","value":" 482/482 [00:00&lt;00:00, 49.7kB/s]"}},"f73f38d109154562bac342c69bef57f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abaab6f02b7043c5b54af6d8ce56d2c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8006bda3741c4a24af671f1f3a096b65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e619fef64734dd0b1d05089da836897":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b36a3a60c7ad4c9081e9a98f7f820cef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4961eeb9a0f144dc98195e2dedc0f1bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ac6359e51f34a66acb0a9ae7980dc95":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ca39904cb114df6873fd4c75cb8c139":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef344693125c43af9202d1c3c8b3047a","IPY_MODEL_a9c24386b2fe48a09f47309b9d31b3dc","IPY_MODEL_fb6a1dea868e404296d5a10224aba590"],"layout":"IPY_MODEL_d19716106ac84e5ea48eb4373de29f98"}},"ef344693125c43af9202d1c3c8b3047a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40cb8e1f520b45a892ce643b594d8f04","placeholder":"​","style":"IPY_MODEL_e7b230811baa4e548e916d526aff207c","value":"vocab.json: 100%"}},"a9c24386b2fe48a09f47309b9d31b3dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3dea37305664a80aa0452afb15d035f","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec8fc8cd059945eca3909065a7f42fc8","value":898823}},"fb6a1dea868e404296d5a10224aba590":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44f5656e248e4ed7b9dd92b22c2fd618","placeholder":"​","style":"IPY_MODEL_7b84167a62164aa6904fa131de9f4280","value":" 899k/899k [00:00&lt;00:00, 37.9MB/s]"}},"d19716106ac84e5ea48eb4373de29f98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40cb8e1f520b45a892ce643b594d8f04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7b230811baa4e548e916d526aff207c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b3dea37305664a80aa0452afb15d035f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec8fc8cd059945eca3909065a7f42fc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44f5656e248e4ed7b9dd92b22c2fd618":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b84167a62164aa6904fa131de9f4280":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60d6a44970774b93b4399828844468a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b73c3dacf454dd7b83a98e1b163ffd3","IPY_MODEL_73301de31e684a9b84a3bf3f537895e2","IPY_MODEL_f9e5ef18b702406d871a2281460f99c7"],"layout":"IPY_MODEL_74660e5e998f488ab0fc9b34b39f0665"}},"2b73c3dacf454dd7b83a98e1b163ffd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96214d95e73d4c7d8f7ccbb42519bcab","placeholder":"​","style":"IPY_MODEL_f853285c5d1c4902b369ec6d1bbb4b0a","value":"merges.txt: 100%"}},"73301de31e684a9b84a3bf3f537895e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_189a8ba4072449cdb6a36bcc8516a6b7","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc57624edad84b41846032796ad95ea5","value":456318}},"f9e5ef18b702406d871a2281460f99c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f7c154094c94ad2889eda1290d0f337","placeholder":"​","style":"IPY_MODEL_8d90c8ffd60046d99adb8b8acd771aee","value":" 456k/456k [00:00&lt;00:00, 27.0MB/s]"}},"74660e5e998f488ab0fc9b34b39f0665":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96214d95e73d4c7d8f7ccbb42519bcab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f853285c5d1c4902b369ec6d1bbb4b0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"189a8ba4072449cdb6a36bcc8516a6b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc57624edad84b41846032796ad95ea5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8f7c154094c94ad2889eda1290d0f337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d90c8ffd60046d99adb8b8acd771aee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea5add688fc24c35bc06a551b32bb09a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3058fac880ca4f9997b853afdd392946","IPY_MODEL_c6fabb57d57841ab98307b6f331c70a0","IPY_MODEL_b90a152815024ae0a8d3a615def44e64"],"layout":"IPY_MODEL_3ccb6e7aa20d4c918bbe23bc1a75208b"}},"3058fac880ca4f9997b853afdd392946":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbec10ffc42b4f01a4095b5778f04eb7","placeholder":"​","style":"IPY_MODEL_dcc6573321454681a6674ceb0fe53141","value":"tokenizer.json: 100%"}},"c6fabb57d57841ab98307b6f331c70a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b8f6757110446e1a4b5222b0008479d","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_64e75f0e25ac49079ccd6aa7d68eec59","value":1355863}},"b90a152815024ae0a8d3a615def44e64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_231bfc4c49f44e6e8e9e195d607148da","placeholder":"​","style":"IPY_MODEL_57c08a84338f4991b417fac3e103f4ee","value":" 1.36M/1.36M [00:00&lt;00:00, 27.8MB/s]"}},"3ccb6e7aa20d4c918bbe23bc1a75208b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbec10ffc42b4f01a4095b5778f04eb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcc6573321454681a6674ceb0fe53141":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b8f6757110446e1a4b5222b0008479d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64e75f0e25ac49079ccd6aa7d68eec59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"231bfc4c49f44e6e8e9e195d607148da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57c08a84338f4991b417fac3e103f4ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c53b07d8a9e74ca1a32ea1d90ea5d723":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18dd887b54924a76a5dfbbdfbca1dbb8","IPY_MODEL_956deed24cb6486c9723c623f6763c33","IPY_MODEL_58f17c4d12a14a5c8985fa4f6a4ee5cf"],"layout":"IPY_MODEL_0b35dcc27d85418f9aef5e3306515205"}},"18dd887b54924a76a5dfbbdfbca1dbb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5dc97eaca9ce4b7593f6655305c20900","placeholder":"​","style":"IPY_MODEL_771418105c364dd28f185b87159698b5","value":"model.safetensors: 100%"}},"956deed24cb6486c9723c623f6763c33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3766d1705bc84de4ac84a1d2772dbd47","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_593431f67ee64e76888d610cc21b0a4d","value":1421700479}},"58f17c4d12a14a5c8985fa4f6a4ee5cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e8bd14041e4b69accc5aae652a20a9","placeholder":"​","style":"IPY_MODEL_a74bd4ed868344979111e010b7e6a50d","value":" 1.42G/1.42G [00:14&lt;00:00, 76.6MB/s]"}},"0b35dcc27d85418f9aef5e3306515205":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dc97eaca9ce4b7593f6655305c20900":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"771418105c364dd28f185b87159698b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3766d1705bc84de4ac84a1d2772dbd47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"593431f67ee64e76888d610cc21b0a4d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73e8bd14041e4b69accc5aae652a20a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a74bd4ed868344979111e010b7e6a50d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"DR5_oOO1qY--","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763883576541,"user_tz":-330,"elapsed":21606,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"20fdc4ea-d639-4ad6-b67c-38b0351724a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=1ad0205a04e777277a1ba30941befe1c0e023c72ad3118d9b8bd53344a3f558d\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"VPPcwOw1uv2e","executionInfo":{"status":"ok","timestamp":1763883576576,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"6374708e-0314-4ada-8191-28e021dee9e8"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","import google.generativeai as genai\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-pro_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key9.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Gemini key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"gemini-2.5-pro\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 70       # seconds between calls (soft global wait)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GEMINI CALL WITH GLOBAL WAIT\n","#####################################################################\n","def gemini_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    cfg = {\"temperature\": temperature}\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = model.generate_content(prompt, generation_config=cfg)\n","            LAST_TS = time.time()\n","            return (getattr(resp, \"text\", \"\") or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Gemini call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Gemini failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASKS — INSTRUCTION-BASED PROMPTING\n","#####################################################################\n","\n","# 8.1 INSTRUCTION-BASED SUMMARISATION (hierarchical)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","Return ONLY a single-line JSON object:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = gemini_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip()\n","        if not chunk_summary:\n","            chunk_summary = out.strip()[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","Return ONLY a single-line JSON object:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = gemini_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 INSTRUCTION-BASED MULTI-LABEL TOPIC CLASSIFICATION\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide and the transcript chunk for details.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","Return ONLY a single-line JSON object:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    # keep unique, keep order, max 3, fallback Other\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 8.3 INSTRUCTION-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Generate five question–answer pairs based on the transcript content.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples discussed in the transcript.\n","\n","GUIDELINES:\n","• Create EXACTLY five (5) question–answer pairs.\n","• Each pair should begin with a different question type:\n","  1. What – factual or definitional\n","  2. Why – reasoning or purpose\n","  3. How – process or mechanism\n","  4. When – timing or condition\n","  5. Who – person, system, or entity\n","• Each answer must be directly supported by information in the transcript.\n","• Keep answers concise (maximum 25 words).\n","• Avoid generic or meta questions.\n","• Ensure all questions are technically relevant and educational.\n","\n","OUTPUT FORMAT:\n","Return ONLY a one-line JSON object:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}, ...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 INSTRUCTION-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list the core terminology, methods, or technical phrases that capture the essence of the discussion.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","• Exclude generic or non-technical words (e.g., \"video\", \"people\", \"lesson\").\n","• Capitalise each concept in Title Case.\n","• Do not repeat near-duplicates.\n","• Ensure the list is concise and domain-specific.\n","\n","OUTPUT FORMAT:\n","Return ONLY a one-line JSON object:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    cleaned = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based generation pipeline completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ByGidCYEuv-p","executionInfo":{"status":"ok","timestamp":1763899824606,"user_tz":-330,"elapsed":15892013,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"bf63b47b-ab42-4b53-d303-93f38d39967e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro\n","Gemini key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement Learning with Human Feedback (RLHF) is a framework for aligning large language models (LLMs) with human preferences to improve response quality. The process involves two primary stages. First, a separate reward model is trained on a dataset of human-ranked responses, enabling it to act as a proxy for human judgment by scoring generated text. In the second stage, the main LLM is fine-tuned using a reinforcement learning algorithm, such as Proximal Policy Optimization (PPO). During this phase, the reward model provides the critical reward signal, guiding the LLM to optimize its outputs for human preference. This methodology effectively integrates nuanced human feedback into the training loop, accelerating learning and enhancing the model's ability to generate helpful, safe, and high-quality responses.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is reinforcement learning with human feedback (RLHF)?\n","A: A framework integrating human feedback into a reinforcement learning algorithm's training process to guide and accelerate learning.\n","Q: Why is human feedback used in reinforcement learning?\n","A: It is used to help the agent learn faster and to produce responses that are more human-favored.\n","Q: How is the reward model for ChatGPT trained?\n","A: Humans rank multiple generated responses to a question, and this ranking data is then used to train the model.\n","Q: When is the reward model's score used during ChatGPT's fine-tuning?\n","A: After a response is generated, the score is used as a reward in the loss function for backpropagation.\n","Q: Who provides the ranking data used to train ChatGPT's reward model?\n","A: Humans provide the data by ranking multiple generated responses to a given question from best to worst.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning With Human Feedback, Proximal Policy Optimization, Reward Model, Grid World, Fine-Tuning, GPT Architecture, Back Propagation, Loss Function, Deep Q-Learning, Q-Learning, Iterative Training Process\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This transcript provides an educational demonstration of Support Vector Machines (SVMs) using the CVXOPT library to solve the underlying quadratic programming problem. The process involves minimizing a specific objective function (½xᵀPx + qᵀx) to find Lagrange multipliers (alphas), which in turn determine the support vectors, weight vector (w), and intercept (b). The code illustrates the impact of different kernels, such as linear and polynomial, on classifying non-linear data by visualizing the kernel trick. It also explains the distinction between hard-margin and soft-margin classifiers, where a penalty parameter C manages overlapping data points. The series concludes by exploring the practical application of these concepts within scikit-learn's Support Vector Classifier (SVC) and discussing strategies for adapting the inherently binary SVM for multi-class classification tasks.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What is the CVXOPT library described as in the tutorial?\n","A: A quadratic programming solver that minimizes a specific equation subject to constraints.\n","Q: Why is CVXOPT used in this tutorial if it's not practical for real-world use?\n","A: It is used for educational purposes to directly see the impact of a kernel on the support vector machine formulation.\n","Q: How does the quadratic programming solver function according to the transcript?\n","A: It minimizes the equation 1/2 x^T P x + q^T x, subject to the constraints Gx <= h and Ax = b.\n","Q: When would a developer likely use libsvm instead of CVXOPT?\n","A: When writing their own support vector machine for practical use, rather than for purely educational purposes.\n","Q: Who is credited with creating the example code used in the tutorial?\n","A: The code was taken from Matthew Blondell's GitHub and is also related to Christopher Bishop's pattern recognition book.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, Kernels, CVXopt, Quadratic Programming Solver, Soft Margin, LibSVM, Quadratic Programming, Optimization Constraints, Nonlinear Visualization, Pattern Recognition, Mathematical Formulation\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Gemini call failed (1/3): Invalid operation: The `response.text` quick accessor requires the response to contain a valid `Part`, but none were returned. The candidate's [finish_reason](https://ai.google.dev/api/generate-content#finishreason) is 2.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," This transcript explores prompt engineering, focusing on prompts as the foundational inputs that guide large language models (LLMs). It explains that prompts provide the necessary context and constraints for text generation, with features like length, language, and specific limitations on tone, style, or word count defining the task. A key technique demonstrated is the improvement of output accuracy by adding precise constraints, such as requesting a one-word answer. The transcript also introduces prompt deconstruction, the methodical process of breaking down a prompt into its core requirements and constraints. This analytical approach enables a deeper understanding of a prompt's structure and purpose, leading to more effective model interaction and refined outputs. By mastering these elements, users can better control and optimize the performance of LLMs for specific applications.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the process of deconstructing a prompt?\n","A: It is breaking a prompt down into its individual components to better understand its key features and constraints.\n","Q: Why is it important to understand different types of prompts?\n","A: It helps you choose the right prompt for your desired output and can impact the complexity and quality of the generated text.\n","Q: How can a user get a more accurate, single-word answer from a model?\n","A: By adding a constraint to the prompt, such as specifying \"give a one word answer,\" to better define the expected output.\n","Q: When might a user include constraints in a prompt?\n","A: When they want to set specific requirements for the output, such as the tone, style, or a specific word count.\n","Q: Who or what are mentioned as examples of large language models?\n","A: The transcript mentions \"chair GPT or Google bard\" as examples of large language models that can be used with prompts.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Large Language Models, Text Generation, Types Of Prompts, Prompt Constraints, Key Features Of Prompts, Prompt Deconstruction, Pre-trained Models, Context And Constraints, Prompts With Multiple Inputs, SEO Optimization\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," This discussion details the creation and operation of autonomous AI agents using the ReAct (Reasoning + Acting) framework, which emulates human cognitive processes. These agents solve problems through an iterative \"Think-Action-Observe\" loop, where a large language model (LLM) reasons about a task, selects a tool like a search function, executes an action, and analyzes the resulting observation. The implementation, demonstrated with LangChain, involves defining tools, creating a specialized prompt template to structure the LLM's reasoning, and initializing an `AgentExecutor` to manage the cycle. This executor runs the loop until the task is complete, providing a transparent view of the agent's internal monologue as it autonomously gathers information and formulates a final answer. This process highlights how agents can independently use tools to accomplish complex goals.\n","\n","TOPICS:\n"," ['Agentic AI', 'Artificial Intelligence', 'LangChain']\n","\n","Q&A:\n"," Q: What does the 'ReAct' in the ReAct agent pattern stand for?\n","A: It stands for 'Reasoning plus Acting,' a pattern that mimics human thinking by combining thought with tool-based actions to solve problems.\n","Q: Why does the ReAct pattern operate in a loop of 'think, action, observation'?\n","A: The loop repeats for complex, multi-step problems, allowing the agent to continue thinking and acting until it finds the final answer.\n","Q: How does an agent execute a tool within the ReAct pattern's control flow?\n","A: The LLM suggests the tool and its input arguments, but the system (like LangChain) actually executes it and returns the output.\n","Q: When does the agent's 'think, action, observation' cycle end in the ReAct pattern?\n","A: The cycle ends when the agent observes a tool's output and determines it has successfully found the final answer to the problem.\n","Q: Who or what provides the reasoning ability or 'brain' for an AI agent?\n","A: The Large Language Model (LLM) acts as the 'brain,' providing the core reasoning ability for the agent to think and make decisions.\n","\n","KEY CONCEPTS:\n"," AI Agents, Autonomous Decisions, Agent Tools, React Agent Pattern, Reasoning Plus Acting, Think Action Observation Loop, Action Input, Large Language Model (LLM), Control Flow, Multi-Step Problem, LangChain, LangGraph\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial demonstrates using LangSmith to trace and visualize the workflow of a reflection agent system designed to refine content. The process requires setting specific environment variables to stream execution data from a LangChain application into a LangSmith project for detailed monitoring. The agent's core logic follows an iterative \"generate-reflect\" cycle: a generation node creates an initial output, like a tweet, and a reflection node critiques it, providing feedback for subsequent improvements. LangSmith captures the entire multi-step execution as a single \"run,\" which contains nested \"traces\" detailing the inputs, outputs, and performance of each component. This deep operational analysis provides a clear, visual understanding of how the agent's components interact to progressively enhance the final output, such as optimizing a tweet for virality.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the ultimate goal of the reflection agent system being traced?\n","A: To deliver a final, refined viral tweet.\n","Q: Why is the speaker tracing the agent system?\n","A: To understand how its two internal systems work together.\n","Q: How will the speaker trace the system's operations?\n","A: By going to the website named \"smith. chain.\"\n","Q: When will the trace of the reflection agent system occur?\n","A: In the current section of the demonstration.\n","Q: Who or what is described as working together within the agent system?\n","A: Two systems are working together to deliver the final tweet.\n","\n","KEY CONCEPTS:\n"," Reflection Agent System, System Tracing, LangSmith, LangChain, Multi-Agent System, Iterative Refinement, AI Agent, LLM Observability, Agentic Workflow, Self-Reflection\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," This guide details the integration and use of OpenAI chat models within the LangChain framework, beginning with the initial setup. The process involves installing the `langchain-openai` package, importing the `ChatOpenAI` class, and initializing a model instance like `gpt-4o`. Authentication is managed by setting the `OPENAI_API_KEY` in a `.env` file and loading it using the `python-dotenv` library. To interact with the model, the `.invoke()` method sends a query, and the generated text is accessed via the `content` property of the returned response object. Common issues like API key errors or insufficient billing funds are also addressed. The summary concludes by explaining the transition from single-string prompts to passing a structured conversation history, which provides the LLM with essential context for generating more coherent and relevant responses in a multi-turn dialogue.\n","\n","TOPICS:\n"," ['LangChain', 'Python Programming', 'Generative AI']\n","\n","Q&A:\n"," Q: What package must be installed to use LangChain with OpenAI's chat models?\n","A: The `langchain-openai` package must be installed to use the chat models.\n","Q: Why was the `gpt-4o` model chosen for the demonstration?\n","A: It was chosen because it is the latest and most advanced model released by OpenAI.\n","Q: How is a specific model like `gpt-4o` selected when initializing the `ChatOpenAI` class?\n","A: It is selected by passing the model name as a keyword parameter during class initialization.\n","Q: When might a user prefer the `gpt-3` model over a newer one?\n","A: A user might prefer `gpt-3` if they are \"a little short on cash,\" as newer models can be more expensive.\n","Q: Who is the provider of the `gpt-4o` model used in the example?\n","A: The provider of the `gpt-4o` model is OpenAI (Open Artificial Intelligence).\n","\n","KEY CONCEPTS:\n"," LangChain Chat Models, OpenAI APIs, ChatOpenAI Class, Langchain-OpenAI Package, Model Initialization, Large Language Model (LLM), Keyword Parameter, GPT-4o Model, GPT-3 Model, Python Package Installation, Module Import\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," This analysis details the behavior of Python's list `sort()` method, focusing on its application to lists of strings and mixed data types. It clarifies that the default sorting for strings is case-sensitive due to its reliance on ASCII/Unicode code points, which results in all uppercase words being ordered before any lowercase words. To achieve a true case-insensitive alphabetical sort, the recommended approach is to utilize a key function, such as `str.lower`, to normalize the strings during the comparison process. The discussion also examines sorting lists with mixed types, explaining that Python's comparison rules inherently place all numeric types before string types. Consequently, applying a reverse sort (`reverse=True`) inverts this order, positioning the strings first. A comprehensive understanding of these nuances is essential for predictable and accurate data manipulation in Python.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What is the default sorting behavior for a list of strings with mixed capitalization?\n","A: The sort method places words starting with an uppercase letter first, sorted alphabetically, followed by the lowercase words, also sorted alphabetically.\n","Q: Why might a developer convert all strings to a single case before sorting a list?\n","A: To ensure a purely alphabetical sort, avoiding the default behavior of separating uppercase and lowercase words into two sorted groups.\n","Q: How does Python's sort method handle a list containing both strings and numbers?\n","A: It places the numbers at the beginning of the list, before the strings.\n","Q: When does a number appear at the end of a sorted list containing both strings and numbers?\n","A: A number appears at the end of the list when the sort is performed in reverse order.\n","Q: Who or what is responsible for placing numbers before strings in a sorted mixed-type list?\n","A: Python's sort method is responsible for this behavior; it is just how the method works.\n","\n","KEY CONCEPTS:\n"," List Sort Method, String List Sorting, Mixed-Case String Sorting, Alphabetical Sorting, Reverse Alphabetical Order, Mixed-Type List, Sorting Mixed-Type Lists, Sort Order Precedence, List Insert Method, In-Place Sorting, Uppercase And Lowercase Letters\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," This analysis explores the optimal allocation of decision-making tasks between humans and AI, a framework known as augmented intelligence. Using performance curves that plot success rate against a prediction's confidence score, it is shown that AI excels at high-confidence tasks, while humans are more effective in ambiguous, mid-confidence scenarios where they can apply external context. The primary challenge in implementing this hybrid model is mitigating human cognitive biases, particularly automation bias—the tendency to over-rely on automated suggestions. Effective system design is crucial for success; strategies include having humans form an initial judgment before viewing an AI's input. The ultimate goal is to quantify the most effective decision-maker for any given task and optimize the presentation of AI assistance to improve overall outcomes, ensuring the combined system achieves superior results.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What do the X and Y axes on the performance graph represent?\n","A: The X-axis represents the confidence score, while the Y-axis tracks the prediction's success rate.\n","Q: Why are the financial analysts in the fraud detection example overwhelmed?\n","A: They are overwhelmed because 90 percent of the thousands of daily alerts they review are false positives.\n","Q: How does an AI's performance curve typically differ from a human's?\n","A: An AI's curve has high success at high/low confidence, while a human's curve is flatter and better when the AI is unsure.\n","Q: When is a human likely to make a better decision than an AI?\n","A: A human is likely to perform better when the AI is unsure, such as at a 50 percent confidence level.\n","Q: Who reviews the alerts of potentially fraudulent transactions in the example system?\n","A: Financial analysts review each alert generated by the fraud detection system.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Fraud Detection System, False Positives, Confidence Score, Success Rate, AI Performance Curve, Human Performance Curve, Human Bias, Holistic Curves, Fraudulent Transactions\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Vertex AI has launched a suite of new APIs to accelerate the development of enterprise generative applications by addressing the critical challenge of grounding models in proprietary data. This collection of high-quality services leverages Google's internal technologies to provide developers with simple, standalone primitives. Key components include a Document Understanding API for parsing complex formats, an enhanced Vector Search with hybrid capabilities for improved retrieval, a Ranking API to prioritize the most relevant information, and a Grounded Generation API that produces accurate, cited answers directly from source documents. To ensure seamless adoption and integration into existing developer workflows, these powerful APIs are also accessible through popular frameworks such as LangChain and LlamaIndex.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'LangChain']\n","\n","Q&A:\n"," Q: What is the key purpose of 'grounding' in generative AI applications for enterprises?\n","A: It ensures applications reliably access enterprise data to produce accurate and consistent responses.\n","Q: Why are the new Vertex AI APIs considered unique?\n","A: They embed Google's know-how and technology used in planet-scale applications like Google Search, YouTube, and Google Ads.\n","Q: How does the Ranking API improve the quality of LLM-generated answers?\n","A: It checks how well retrieved results answer a question, bubbling up the most relevant information for the model to use.\n","Q: When would a developer use the Check Grounding API?\n","A: To fact-check a statement, produced by a human or a language model, against a set of provided evidence or facts.\n","Q: Who benefits from the integration of the new APIs into frameworks like LangChain?\n","A: Developers, who can then easily prototype and combine the APIs with other third-party or open-source tools to build their solutions.\n","\n","KEY CONCEPTS:\n"," Grounding, Document Understanding API, Embedding API, Vector Search, Hybrid Search, Ranking API, Grounded Generation API, Fine-Tuned Model, Answers With Citations, Check Grounding API, Document AI, Embedding Retrieval\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," This analysis explores the Singular Value Decomposition (SVD), a fundamental matrix factorization that provides a geometric interpretation of linear transformations. The SVD decomposes a matrix X using unitary matrices U and V, which represent transformations that preserve vector lengths and angles, effectively acting as rotations. A key property of these matrices is that their inverse is their transpose for real data (UᵀU = I) or their conjugate transpose for complex data. Geometrically, the SVD demonstrates how the matrix X maps a unit sphere of vectors from its row space into an ellipsoid within its column space. The orientation of this resulting ellipsoid's principal axes is defined by the left singular vectors (the columns of U), while the lengths of these axes are determined by the corresponding singular values in the diagonal matrix Σ, offering a powerful visualization of the matrix's action.\n","\n","TOPICS:\n"," ['Data Science', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What fundamental properties of vectors are preserved by a unitary transformation?\n","A: Unitary transformations preserve the angles between vectors and the lengths of vectors.\n","Q: Why is the Fourier transform described as a key example of a unitary transformation?\n","A: It is a famous, widely used coordinate transformation that rotates a vector space into a new representation where things become simpler.\n","Q: How does a non-unitary matrix X geometrically transform a sphere of unit vectors?\n","A: It maps the sphere into an ellipsoid, with singular values defining the axis lengths and singular vectors defining the orientation.\n","Q: When is the complex conjugate transpose used in place of the standard transpose for the SVD?\n","A: The complex conjugate transpose is used when the data matrix contains complex-valued numbers rather than only real numbers.\n","Q: Which components of the SVD define the orientation of the ellipsoid formed by multiplying a sphere of vectors by matrix X?\n","A: The left singular vectors (columns of U) and right singular vectors (columns of V) define the orientation of the resulting ellipsoid.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Unitary Matrices, Unitary Transformations, Economy Size SVD, Complex Conjugate Transpose, Inner Product Preservation, Geometric Interpretation, Principal Axes, Singular Values, Left Singular Vectors, Coordinate Transformation, Fourier Transform\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial demonstrates how to build generative AI applications using Google's Gemini Pro 1.5, a unified multimodal model that processes both text and images. It highlights the model's defining feature: a one-million-token context window, which enables advanced reasoning over extensive documents like a 330,000-token transcript. The technical process involves generating a free API key, installing the `google-generativeai` library, and configuring the `gemini-1.5-pro-latest` model. Developers can then use the `generate_content` method to submit prompts, seamlessly combining text and image inputs in a single request. This streamlined approach, which replaces the need for separate vision models, simplifies development and is enhanced by features like response streaming for faster, incremental output. The model's capabilities empower developers to upgrade projects with sophisticated, multimodal functionalities.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What type of model is Google Gemini Pro 1.5 described as in the video?\n","A: It is described as a multimodal model, meaning it can work with both text and images.\n","Q: Why does the speaker plan to show the Google demo video at the beginning?\n","A: To provide an idea of what Google Gemini Pro 1.5 is capable of doing.\n","Q: How will the speaker demonstrate the model's capabilities after the demo video?\n","A: By showing a hands-on application, running code, and working with both images and text.\n","Q: According to the transcript, when is a model considered multimodal?\n","A: A model is considered multimodal when it is able to work with both text and images.\n","Q: Who created the demo video that the speaker plans to show?\n","A: The demo video was created by Google.\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence, Google Gemini Pro 1.5, AI-Powered Application, End-To-End Projects, Multi-Model, Text And Images, API Key, Long Context Understanding, Experimental Feature, Hands-On Application\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," The evaluation, testing, and debugging of prompt engineering models is a critical, iterative process for ensuring high performance. This process relies on quantitative metrics like perplexity, which measures predictive capability, and accuracy, which assesses correctness, alongside qualitative human evaluation for nuanced quality. The debugging phase involves analyzing model outputs to identify error patterns that inform targeted fine-tuning. To ensure robust generalization, models are rigorously tested on diverse datasets and tasks using techniques such as cross-validation. This continuous evaluation cycle is fundamental to maintaining and progressively enhancing a model's effectiveness and reliability over time.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What does the perplexity metric measure in a language model?\n","A: It measures how well a language model predicts a sequence of words, where a lower score indicates better performance.\n","Q: Why is it important to test prompt engineering models on different datasets?\n","A: It helps determine the model's ability to generalize its performance on new or unseen data.\n","Q: How can developers debug and improve their prompt engineering models?\n","A: By analyzing generated responses to identify common errors or patterns, which allows for fine-tuning the model.\n","Q: When should a developer evaluate and test their prompt engineering models?\n","A: It is an ongoing process; models should be continuously evaluated as they are used to generate more responses.\n","Q: Who rates the quality of responses during a human evaluation?\n","A: Human evaluators are used to rate the quality of the model's generated responses.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering Models, Model Evaluation, Evaluation Metrics, Perplexity, Accuracy, Human Evaluation, Large Language Model, Model Debugging, Model Fine-Tuning, Cross Validation, Model Generalization, Pre-trained Large Language Models\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," This transcript explains the evolution from Generative AI to more advanced AI Agents and Agentic AI systems. Generative AI, based on a Large Language Model (LLM), creates content but is limited by a knowledge cutoff. The next stage, an AI Agent, enhances this by integrating the LLM with external tools like APIs, enabling it to autonomously perform specific tasks such as booking a flight. The most advanced form, Agentic AI, involves a complex system where one or more agents collaborate to achieve multi-step goals. This requires sophisticated planning, reasoning, and coordination to navigate complex workflows. This progression signifies a critical shift from simple content generation and Q&A toward highly autonomous systems capable of complex, goal-oriented problem-solving and dynamic decision-making in real-world environments.\n","\n","TOPICS:\n"," ['Agentic AI', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is generative AI?\n","A: An AI that can create new content, such as text, images, or video, based on patterns learned from existing data.\n","Q: Why can't a basic LLM provide the price of a flight for tomorrow?\n","A: Because it has a knowledge cutoff date and lacks access to real-time information from external tools like a travel API.\n","Q: How does an AI agent handle a complex travel request with multiple criteria?\n","A: It uses different tools, like a weather API and a travel API, to check conditions and find options that meet all criteria.\n","Q: When does a system become an agentic AI system?\n","A: When one or more agents work autonomously on complex, multi-step tasks that require planning, coordination, and decision-making to achieve a goal.\n","Q: Who or what is responsible for making autonomous decisions in the flight booking example?\n","A: The AI agent is responsible; it independently decides which flight is cheapest among several options and then books it.\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence, Large Language Model, Knowledge Cutoff Date, AI Agent, Agentic AI, Tool Usage, Autonomous Decision Making, Multi-Step Reasoning, Multi-Step Planning, Multi-Agent System, Agentic Frameworks, Human In The Loop\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Covariance is a statistical measure that quantifies the directional relationship between two random variables. It is calculated as the average product of the deviations of each variable from their respective means. A positive covariance indicates a direct relationship, where variables tend to increase together, while a negative value signifies an inverse relationship. An important property is that the covariance of a variable with itself is equivalent to its variance. However, a primary limitation of covariance is that its magnitude is not standardized, revealing only the direction but not the strength of the linear association. This necessitates the use of normalized metrics, such as the Pearson correlation coefficient, to properly assess the strength of the relationship between the variables.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What does covariance quantify?\n","A: It quantifies the relationship between two random variables, indicating if one tends to increase or decrease as the other changes.\n","Q: Why is covariance a positive value when two variables increase together?\n","A: Because the product of their deviations from their respective means will be consistently positive, either from multiplying two positive or two negative numbers.\n","Q: How is the covariance of a variable with itself related to its variance?\n","A: The covariance of a variable with itself, Cov(X, X), is mathematically equivalent to the variance of that variable, Var(X).\n","Q: When will the covariance between two variables be negative?\n","A: Covariance will be negative in a scenario where as one variable increases, the other variable consistently decreases.\n","Q: Who or what technique is mentioned to overcome the disadvantage that covariance doesn't measure the strength of a relationship?\n","A: The Pearson correlation coefficient is the technique mentioned to overcome this disadvantage by measuring the magnitude of the relationship.\n","\n","KEY CONCEPTS:\n"," Covariance, Variance, Random Variables, Data Pre-Processing, Data Analysis, Covariance Formula, Mean Of A Random Variable, Positive Covariance, Negative Covariance, Pearson Correlation Coefficient, Deviation From The Mean\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning (RL) is a computational approach where an agent learns an optimal policy to maximize a cumulative numerical reward signal over time. This is achieved through a trial-and-error process of interaction with an environment, where the agent takes actions and receives rewards as feedback on their quality. The agent then updates its policy to favor actions that lead to higher long-term rewards. This framework is applicable to diverse task structures. In episodic tasks, such as Tic-Tac-Toe, rewards are defined by discrete outcomes like winning (+1) or losing (-1). In continuous tasks, like algorithmic stock trading, the objective is parameterized by a reward function, which can be a mathematical expression for profit or a sophisticated risk-adjusted metric such as the Sharpe ratio, guiding the agent's learning process.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is the primary objective of a reinforcement learning agent?\n","A: To learn an optimal policy that maximizes the cumulative numerical reward signal over time.\n","Q: Why is a reward signal provided to an agent?\n","A: It provides feedback to the agent about the quality of its actions, guiding its learning process.\n","Q: How does an agent learn to maximize its cumulative reward?\n","A: Through trial and error, by exploring the environment, observing outcomes, and updating its policy accordingly.\n","Q: In an episodic task like Tic-Tac-Toe, when does the agent receive a reward?\n","A: The agent receives a reward at the end of the game, based on whether it wins, loses, or draws.\n","Q: Who or what is the entity that learns from its environment to maximize rewards?\n","A: An agent, which is a software program that learns from its environment and takes actions to maximize its rewards.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Episodic Task, Continuous Task, Optimal Policy, Reward Signal, Cumulative Reward, Trial and Error Learning, State-Action Pair, Value-Based Method, Policy-Based Method, Reward Function, Risk-Adjusted Measure\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," This summary details the Python dictionary, a fundamental data structure for mapping data using key-value pairs, or items. Enclosed in curly brackets, each item links an immutable key, such as a string or number, to a corresponding value. Dictionaries can be instantiated directly or created by zipping two lists with the `dict()` function. Essential operations include accessing or modifying values with square bracket notation (`dict[key]`) and deleting items using the `del` keyword. Furthermore, the `.keys()`, `.values()`, and `.items()` methods are used to retrieve the dictionary's respective components, which is crucial for effective data handling and mapping tasks.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is a primary requirement for a data type to be used as a dictionary key?\n","A: The key must be an immutable data type, such as a string or number, and cannot be a mutable type like a list.\n","Q: Why are dictionaries considered an important structure for those interested in data science?\n","A: They are important for data science because they work very well with the pandas library, which is commonly used in that field.\n","Q: How can a specific key-value pair be removed from a dictionary?\n","A: A pair is removed using the `del` function, followed by the dictionary variable and the specific key in square brackets.\n","Q: When might you use the zip() and dict() functions together?\n","A: You would use them together to create a new dictionary by pairing corresponding elements from two separate lists of keys and values.\n","Q: Who or what is responsible for retrieving a list of only the keys from a dictionary?\n","A: The `.keys()` method, when called on a dictionary object, is responsible for returning a list of all its keys.\n","\n","KEY CONCEPTS:\n"," Key Value Pairs, Immutable Key, Dictionary Data Type, Dict() Function, Zip() Function, Items() Method, Keys() Method, Values() Method, Accessing Values By Key, Modifying Dictionary Values, Deleting Dictionary Items, Length of a Dictionary\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence (AI) significantly enhances organizational security by accelerating threat detection and containment. A primary application is User Behavior Analytics (UBA), which uses machine learning to identify costly insider threats by detecting anomalies in user activity. Integrated within SIEM solutions like IBM QRadar, UBA establishes a baseline of normal behavior to flag suspicious actions, presenting analysts with risk-prioritized dashboards and detailed user timelines. AI further automates investigations by correlating events and mapping threats to the MITRE ATT&CK framework. According to IBM's 2023 report, this extensive use of AI and automation can reduce breach containment time by an average of 108 days, enabling security teams to shift from reactive alert management to a proactive defense posture.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What was the average cost of an insider threat for an organization, according to the transcript?\n","A: According to the transcript, the average cost of an insider threat for an organization was $4.\n","Q: Why is AI considered beneficial for improving an organization's security posture?\n","A: AI helps security professionals stay ahead of emerging threats and significantly reduces the time to identify and contain data breaches.\n","Q: How can AI and machine learning help security teams address insider threats?\n","A: They help detect and respond to insider threats quickly and precisely through user behavior analytics (UBA).\n","Q: When was a 108-day reduction in breach containment time observed?\n","A: It was observed when organizations extensively used AI and automation compared to those that did not.\n","Q: Who published the 2023 report that found AI usage reduced breach containment time?\n","A: The report was published by IBM and was based on a survey of over 500 organizations.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Machine Learning, User Behavior Analytics, Insider Threats, Data Breach, Emerging Threats, Security Posture, AI and Automation, Faster Containment, Cost of a Data Breach\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta's Llama 3 is a state-of-the-art open-source large language model, released in 8B and 70B parameter versions. Key advancements over its predecessor include training on a massive 15 trillion token dataset, a doubled 8K context length, and refined post-training for improved alignment. Benchmark evaluations show Llama 3 is highly competitive, outperforming other open-source models and rivaling proprietary systems like GPT-4 and Gemini Pro, particularly in complex reasoning and code generation. Accessing the model requires submitting a request on the official Meta Llama website or platforms like Hugging Face. Upon approval, users receive a signed URL to download the model weights and tokenizer. The official GitHub repository provides comprehensive code and instructions, enabling users to download the necessary components from Hugging Face and perform model inference locally.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the name of the person speaking in the transcript?\n","A: The speaker's name is Krishak.\n","Q: Why is the speaker making this introduction?\n","A: To welcome the audience to his YouTube channel.\n","Q: How does the speaker begin his video?\n","A: By stating his name and welcoming viewers to his YouTube channel.\n","Q: When is the speaker recording the video?\n","A: The speaker is recording the video at 2 a.m.\n","Q: Who is the host of the YouTube channel mentioned?\n","A: A person named Krishak is the host of the YouTube channel.\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," This transcript details a practical workflow for implementing machine learning algorithms using the scikit-learn (sklearn) library in Python. It highlights a common and effective methodology for developers: leveraging search engines to find the official documentation for a desired algorithm. The speaker provides a concrete example, demonstrating how a search for \"sklearn Naive Bayes\" directs the user to the relevant library pages. This process allows for the correct identification and implementation of the appropriate function, such as the Gaussian Naive Bayes classifier. The demonstration shows how this classifier is used to create a decision boundary for a given dataset. The overarching objective is to empower learners with the ability to independently research, locate, and apply scikit-learn's powerful tools, fostering self-sufficiency in machine learning projects.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What specific algorithm from the scikit-learn library was used to create the decision boundary?\n","A: The Gaussian Naive Bayes algorithm was used, which is a specific type of Naive Bayes classifier available in the library.\n","Q: Why does the speaker use Google in the demonstration?\n","A: To find the scikit-learn library's documentation and figure out how to use its functions for the Naive Bayes algorithm.\n","Q: How did the speaker find the specific classifier they used?\n","A: By searching Google for \"sklearn Naive Bayes\" and navigating the results, which led to the page for Gaussian Naive Bayes.\n","Q: When will the speaker explain the exact details of how Naive Bayes works?\n","A: The speaker will explain the algorithm's details later, after first showing the viewer how to run the code.\n","Q: Who or what provides the implementation of the Gaussian Naive Bayes classifier?\n","A: The scikit-learn (sk-learn) Python library provides the implementation of the classifier.\n","\n","KEY CONCEPTS:\n"," Scikit-learn, Naive Bayes, Gaussian Naive Bayes, Decision Boundary, Classifier, Python Library, Algorithm, Naive Bayes Formula\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The log-normal distribution is a continuous probability distribution of a random variable whose natural logarithm is normally (Gaussian) distributed. Characterized by a right-skewed curve, it contrasts with the symmetrical bell shape of a Gaussian distribution and is commonly observed in phenomena like income levels or the length of product reviews. A key application is in machine learning for feature scaling through a process called log normalization. This technique involves applying a log transformation to convert log-normally distributed data into a normal distribution. Following this transformation, standardization can be applied to bring features with disparate scales onto a common ground, a crucial preprocessing step that can significantly improve the performance and accuracy of many machine learning models.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What defines a log-normal distribution for a random variable X?\n","A: A random variable X has a log-normal distribution if the logarithm of its values, log(X), follows a Gaussian or normal distribution.\n","Q: Why is it beneficial to scale different features to the same scale before training a machine learning model?\n","A: Scaling features to the same scale before training a model helps in achieving higher accuracy from the algorithm.\n","Q: How can a feature with a log-normal distribution be transformed into a standard normal distribution?\n","A: First, apply a log transformation to make it Gaussian, then apply standard scaling using the formula (Xᵢ - μ) / σ.\n","Q: When is the technique of log normalization particularly useful for a dataset?\n","A: It is useful when a feature follows a log-normal distribution and needs to be scaled down to improve a model's performance.\n","Q: Who or what provides real-world data, like product reviews, that often follows a log-normal distribution?\n","A: Companies like Amazon and Flipkart provide data, such as product review comment lengths, that typically follows a log-normal distribution.\n","\n","KEY CONCEPTS:\n"," Gaussian Distribution, Log Normal Distribution, Standard Normal Distribution, Empirical Formula, Standard Deviation, Bell Curve, Random Variable, Standard Scaler, Log Normalization, Binomial Distribution, Bernoulli's Distribution\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This project presents an end-to-end deep learning system for potato plant disease classification, aimed at mitigating farmers' economic losses by identifying early and late blight. The workflow involves collecting and pre-processing leaf images using tf.data.Dataset and data augmentation, followed by training a Convolutional Neural Network (CNN). For deployment, the model is optimized into a TensorFlow Lite format using quantization and hosted on Google Cloud Platform with a serverless backend (Google Cloud Functions). A React Native mobile application allows users to photograph a plant leaf and receive a real-time classification from the cloud-based API. While on-device inference is an alternative, this cloud architecture demonstrates a complete MLOps cycle. The project serves as a valuable portfolio piece for machine learning engineers, showcasing practical skills and adaptability for other applications like tomato plant diseases.\n","\n","TOPICS:\n"," ['Deep Learning', 'Mlops', 'Data Science']\n","\n","Q&A:\n"," Q: What are the two common potato plant diseases mentioned in the project?\n","A: The two common diseases are early blight, caused by a fungus, and late blight, caused by a specific microorganism.\n","Q: Why is it important to accurately identify the specific type of blight on a potato plant?\n","A: Because the treatments for early blight and late blight are different, requiring accurate identification to apply the appropriate one.\n","Q: How will the mobile application determine if a potato plant is diseased?\n","A: A farmer takes a picture, and the app uses a deep learning convolutional neural network to identify if the plant is healthy or diseased.\n","Q: When can a farmer prevent economic loss from potato diseases?\n","A: A farmer can prevent economic loss if they detect diseases early and apply the appropriate treatment.\n","Q: Who is responsible for building the end-to-end application in this project scenario?\n","A: A data scientist working for AtliQ Agriculture is responsible for building the entire application from end to end.\n","\n","KEY CONCEPTS:\n"," End-to-End Deep Learning, ML Ops, TensorFlow Serving, FastAPI, Model Deployment, Google Cloud Platform (GCP), Google Cloud Functions, React Native, Artificial Intelligence (AI), Deep Learning, Convolutional Neural Network, Technical Architecture\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," This transcript outlines a spectrum of autonomy in Large Language Model (LLM) applications, detailing a progression from human-driven to agent-executed systems. The hierarchy begins with deterministic code and single LLM calls for simple tasks. Greater complexity is managed by chains, which are fixed sequences of specialized LLM calls for multi-step processes. A higher level of autonomy is introduced with routers, where an LLM dynamically selects the appropriate chain based on user input. The most advanced level is the agent, or state machine, in which the LLM controls the entire workflow, enabling loops, memory, and iterative refinement. This evolution marks a significant shift from rigid, pre-defined logic to dynamic, autonomous systems that can handle complex, multi-stage operations with greater flexibility and intelligence.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Langraph']\n","\n","Q&A:\n"," Q: What is the fundamental difference between a chain and an agent in LLM applications?\n","A: A chain is one-directional, whereas an agent (state machine) can have cycles and its control flow is managed by the LLM itself.\n","Q: Why are chains described as having low autonomy, like a “rigid assembly line”?\n","A: Chains are considered rigid because they always follow the same fixed sequence of steps that are predefined by a human developer.\n","Q: How does a router architecture decide which path to take for a given input?\n","A: An LLM analyzes the user's input to determine the intent and then routes the request to the appropriate tool or chain for the job.\n","Q: When is an LLM application, such as a state machine, considered an agent?\n","A: An application is considered an agent when its control flow, including the ability to loop and go back, is controlled by an LLM.\n","Q: Who or what decides the next steps to take in a router-based architecture?\n","A: The AI itself decides the next steps to take, unlike in chains where the sequence is predefined by a human.\n","\n","KEY CONCEPTS:\n"," Levels Of Autonomy, Single LLM Call, Chains, Router, Control Flow, State Machine, AI Agents, LangGraph, Human In The Loop, Multi-Agent Systems, Agent Executed, Adaptive Learning\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This overview of advanced prompt engineering details techniques for developing robust and efficient AI models capable of handling diverse inputs like text, image, and audio. It covers sophisticated fine-tuning methods, including multitask learning, which trains a model on several tasks simultaneously for versatile representations, and distillation, where a smaller model mimics a larger one for improved efficiency. The summary emphasizes the critical role of data pre-processing through tokenization and normalization for model accuracy. Additionally, it discusses practical deployment strategies using frameworks such as Flask and TensorFlow Serving. The presentation concludes by underscoring the paramount importance of addressing ethical considerations, including bias, fairness, and data privacy, throughout the entire model development lifecycle to ensure responsible AI implementation.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Deep Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is distillation as an advanced fine-tuning technique?\n","A: It is a technique that involves training a smaller model to mimic the behavior of a larger model, making it more efficient and faster.\n","Q: Why is text normalization considered a best practice in data pre-processing?\n","A: It helps the model avoid confusion between words that are spelled the same but have different meanings due to factors like capitalization.\n","Q: How does multitask learning improve a model's ability to generalize?\n","A: By training a model on multiple tasks simultaneously, it helps the model learn more robust representations that can generalize to different uses.\n","Q: When does the ethical issue of bias arise in prompt engineering models?\n","A: Bias can occur when the data used to train the model is not representative of the whole population, leading to discriminatory outcomes.\n","Q: Who or what pre-trained models were used in the example for image-based dog breed identification?\n","A: The models used were ResNet50, VGG16, Xception, and Inception, along with logistic regression.\n","\n","KEY CONCEPTS:\n"," Fine-Tuning Pre-Trained Models, Multitask Learning, Model Distillation, Data Pre-Processing, Tokenization, Text Normalization, Deploying Models In Production, Cross-Entropy Loss, Adam Optimizer, Self-Supervised Learning, Data Augmentation, Bias and Fairness\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," This lecture demonstrates the application of Singular Value Decomposition (SVD) for facial recognition via the eigenfaces method. The process begins by reshaping facial images into vectors and subtracting the average face to perform Principal Component Analysis. SVD is then applied to this mean-centered data matrix to compute the principal components, or eigenfaces, which capture the most significant variations in the dataset. By projecting each image onto a few dominant eigenfaces, high-dimensional data is effectively reduced to a low-dimensional coordinate system for clustering. An experiment with celebrity images successfully formed distinct clusters for individuals like Stallone and Schwarzenegger. However, it also revealed the method's limitations, as superficial features like skin tone caused a closer grouping between Schwarzenegger and Taylor Swift, highlighting the technique's potential shallowness.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What preprocessing step is performed on all images before computing the SVD for principal component analysis?\n","A: The average face, computed from all images in the dataset, is subtracted from each individual image to center the data.\n","Q: Why did the eigenface clusters for Arnold Schwarzenegger and Taylor Swift show significant overlap?\n","A: They shared shallow features like fair skin and blonde hair, which created high pixel-wise correlation despite other differences.\n","Q: How is a new test image classified using the generated eigenface space?\n","A: The image is projected into the eigenface coordinates, and it is classified based on which person's data cluster it is closest to.\n","Q: When did Facebook's facial recognition accuracy improve to match human levels, according to the speaker?\n","A: It improved when the system began inferring the 3D geometry of a person's head from 2D images, not just analyzing 2D pixels.\n","Q: Who or what is responsible for computing the eigenfaces from the preprocessed image data matrix?\n","A: The Singular Value Decomposition (SVD) algorithm is used, where the resulting columns of the U matrix represent the eigenfaces.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Average Face, Column Vector Representation, Economy SVD, Projection Into Eigenface Space, Image Classification, Training Data Set, Feature Space Clustering, Test Image\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a framework designed to augment Large Language Models (LLMs) by bridging their powerful reasoning capabilities with the ability to interact with the external world. It functions by providing LLMs with agency, enabling them to connect with and utilize external resources such as APIs, private databases, and search engines. This allows the AI to execute tangible tasks, including making bookings, sending emails, and scraping websites for information. A core feature of LangChain is its modular architecture, which grants developers the flexibility to switch between different LLM providers without significant code alterations. Ultimately, LangChain transforms LLMs from isolated reasoning engines into proactive agents capable of performing complex, real-world actions, thereby expanding their practical applications and development efficiency.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is LangChain's primary role in AI application development?\n","A: LangChain acts as a bridge, or framework, that connects large language models (LLMs) with real-world systems like APIs and databases.\n","Q: Why are LLMs like ChatGPT unable to perform tasks like booking a flight on their own?\n","A: Because they are just reasoning \"brains\" trained on data and cannot interact with the real world to perform actions or access live APIs.\n","Q: How does LangChain allow for flexibility in choosing an LLM?\n","A: It enables developers to easily switch out LLMs, like replacing GPT-4 with a Hugging Face model, without touching the application code.\n","Q: When is a framework like LangChain necessary?\n","A: It is necessary when an application needs both an LLM's reasoning ability and the capability to communicate with real-world APIs or databases.\n","Q: Who or what enables an LLM-powered application to access external tools like a flight booking API?\n","A: LangChain enables this by acting as a bridge, allowing the AI to access and communicate with various real-world APIs for different services.\n","\n","KEY CONCEPTS:\n"," LangChain Framework, Large Language Models (LLMs), Real-World API Integration, LLM Application Development, LLM Reasoning Ability, Chat Application Interface, Model Abstraction, External Databases, Limitations Of Large Language Models, Hugging Face LLM\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residual analysis is a critical diagnostic tool for evaluating and improving time series forecasting models. Residuals, defined as the difference between actual observations and the model's fitted values, are examined to identify deficiencies. For a well-specified model, residuals should have a zero mean, indicating no systematic bias, and exhibit no autocorrelation, meaning all predictable information has been captured. Diagnostic methods include plotting the Autocorrelation and Partial Autocorrelation Functions (ACF/PACF), analyzing residual histograms for bias, and conducting statistical tests like the Ljung-Box test. A significant Ljung-Box test result (low p-value) reveals remaining autocorrelation, suggesting the model has missed underlying patterns. This iterative process of diagnosing residuals is crucial for understanding model performance and making targeted improvements to enhance forecasting accuracy.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: The difference between the fitted value and the actual value of the time series data that the model was trained on.\n","Q: Why should the mean of residuals be zero?\n","A: A non-zero mean indicates the model is biased, meaning it is generally over-forecasting or under-forecasting.\n","Q: How can you test for serial correlation in residuals?\n","A: By plotting autocorrelation functions (ACF/PACF) or using a statistical method like the Ljung-Box test to quantify if correlation exists.\n","Q: When does the Ljung-Box test suggest a model has missed information?\n","A: When it returns a low p-value (in the significance zone), rejecting the null hypothesis and indicating the presence of serial correlation.\n","Q: Who (or what system) is used to perform the Ljung-Box test in the Python example?\n","A: The `statsmodels` Python package is used by calling the `acorr_ljungbox` function on the model's residuals.\n","\n","KEY CONCEPTS:\n"," Residual Analysis, Fitted Value, Autocorrelation Function, Partial Autocorrelation Function, Holt-Winters Model, Exponential Smoothing Model, Ljung-Box Test, Serial Correlation, Biased Forecast, Independently Distributed Residuals, Trend, Seasonality, And Level\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This project details the development of a text-to-SQL application by building a ReAct agent with LangGraph, watsonx.ai models, and a Next.js frontend. The process involves managing UI state for chat history and loading indicators using React's `useState` hook. User queries are serialized and sent to a server-side function where the agent processes them. A key technical component is a custom LangChain tool that enables the LLM to execute commands against an in-memory SQLite database, which is seeded with customer and order data on startup. The system prompt is refined to guide the model in generating valid SQLite syntax. The final application successfully translates complex natural language questions, including those requiring table joins, into executable SQL queries, demonstrating a controlled method for database interaction.\n","\n","TOPICS:\n"," ['Agentic AI', 'Langraph', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What specific database technology is used for the in-memory database in this project?\n","A: The project uses an in-memory database running with SQLite.\n","Q: Why is Tailwind CSS included in the project setup?\n","A: Tailwind is used so the developer does not have to write any CSS for styling the application.\n","Q: How is the Next.js development server started after the initial setup?\n","A: The development server is started by running the command `npm run dev` in the project directory.\n","Q: When should the developer change into the newly created project directory?\n","A: The developer must move into the project directory before attempting to start the application.\n","Q: Who or what system is responsible for building the ReAct agent?\n","A: The ReAct agent is built using the LangGraph framework.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence Agent, Large Language Models, Text2SQL Agent, LangGraph, ReAct Agent, Watsonx.ai, In-Memory Database, SQLite, Next.js, Tailwind CSS, Client-Side Component\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," This summary outlines the principles of prompt engineering, a specialized discipline within Natural Language Processing (NLP) dedicated to designing optimal inputs for large language models (LLMs) such as GPT and BERT. The core objective is to elicit high-quality, accurate, and contextually coherent text, offering significant advantages over traditional rule-based systems in applications like advanced chatbots and automated translation. Despite its benefits, the field grapples with significant limitations, including the difficulty of crafting unambiguous prompts and the risk of generating biased or factually incorrect outputs stemming from the underlying training data and model architecture. The discussion progresses from foundational concepts, such as systematic prompt analysis, to advanced techniques like model fine-tuning, which are essential for overcoming these challenges and maximizing LLM performance.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A specialized NLP field for building models that generate high-quality text outputs from prompts using pre-trained large language models.\n","Q: Why is prompt engineering considered beneficial?\n","A: It generates more accurate, coherent, and contextually appropriate text outputs than traditional rule-based or keyword-based approaches.\n","Q: How does the course introduce prompt analysis?\n","A: It teaches how to deconstruct prompts and identify their key features and constraints to understand their structure and content.\n","Q: When might a prompt engineering model struggle?\n","A: It may struggle with complex and ambiguous prompts or generate biased outputs due to its underlying data or model architecture.\n","Q: Who provides the pre-trained models mentioned as a basis for prompt engineering?\n","A: The transcript mentions models from OpenAI (GPT), Google (BERT), and Hugging Face (Transformers).\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Pre-trained Large Language Models, GPT, BERT, Transformer Models, Fine-Tuning, Prompt Analysis, Rule-Based Approaches, Model Architecture, Model Bias, Content Generation\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is a value-based, off-policy reinforcement learning algorithm designed to find an optimal policy by learning a state-action value function (Q-function). This function estimates the total expected reward for taking an action in a given state, with values stored in a Q-table. The learning process is iterative: an agent explores an environment, and the Q-table is updated using a rule derived from the Bellman equation. This update is driven by the temporal difference (TD) error, which is the discrepancy between the current Q-value and a new estimate based on the immediate reward and the discounted maximum Q-value of the next state. Scaled by a learning rate, this error refines the Q-table. Through multiple episodes, the Q-values converge, yielding a stable target policy that maximizes rewards.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What does the state-action value function (or Q-value) quantify?\n","A: It quantifies how good it is to be in a state and then take a specific action from that state.\n","Q: Why does the agent initially take actions based on an exploration policy instead of the highest Q-value?\n","A: The agent's actions are based on random chance to explore the environment, rather than exploiting potentially arbitrary initial Q-values.\n","Q: How is the observed Q-value for a state-action pair calculated in the learning process?\n","A: It is calculated using the Bellman equation, which sums the immediate reward with the discounted maximum future Q-value of the next state.\n","Q: When does the agent receive a reward signal from the environment?\n","A: The agent receives a reward after taking an action and transitioning from its current state to a new state in the environment.\n","Q: Who or what determines the agent's actions while it is learning the optimal policy?\n","A: The behavior policy, which is an exploration policy based on random chance, determines the agent's actions during the learning phase.\n","\n","KEY CONCEPTS:\n"," Q-Learning, Reinforcement Learning, Value Based Methods, Policy Based Methods, Optimal Policy, State Value Functions, State Action Value Function, Q Value, Q Table, Bellman Equation, Discount Factor, Behavior Policy\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," The logistic classifier is a fundamental linear model in machine learning that generates predictions by applying a linear function to an input vector (X). This operation uses a weights matrix (W) and a bias vector (b) to produce raw output scores, known as logits. The core objective of the machine learning process is training, which involves finding the optimal values for W and b through an optimization process. For classification tasks, these logits are then transformed into a valid probability distribution using the softmax function. This function normalizes the scores, ensuring they are all positive and sum to one. The ultimate goal of training is to adjust the model's parameters to maximize the assigned probability for the correct class label while simultaneously minimizing the probabilities for all incorrect classes.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are the output scores from a logistic classifier often called?\n","A: In the context of logistic regression, scores are often also called logits.\n","Q: Why is a softmax function used on the scores from a logistic classifier?\n","A: It is used to turn any kind of scores into proper probabilities that sum to 1.\n","Q: How does a logistic classifier generate its predictions from the input?\n","A: It applies a linear function, a matrix multiply, to the input vector (X) using weights (W) and a bias (b).\n","Q: When are the probabilities for a class considered small after applying the softmax function?\n","A: Probabilities are small when the scores are comparatively smaller.\n","Q: Who (or what) holds the learned parameters in a logistic classifier?\n","A: The weights (W) and the bias (b) are the parameters that are trained to find good values for performing predictions.\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Classifier, Linear Function, Matrix Multiply, Weights And Biases, Model Training, Softmax Function, Logits, Input Vector, Output Class, Bias Term\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n","\n","Instruction-based generation pipeline completed (NO evaluation).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["9d578b1f3b34415b995e72b4b857251b","7c27b10ef13a4acc913fa2f9a0d280c1","1f41a88bbfe24265b9a9072d0a79a7f6","3e1a70e3fb674067bde7f386182f73fd","b5b93f5a9014484c892b8d43f62ba723","f9411c45f4fd4af59afedf1b0c3c2cfc","75e450d06c5d4615a3f20a80799648b0","1131f6e755a04ae6a4a138532f28df09","f78d758c2d9544dba2bb8a6148e72502","bcbbc8597eeb42a192ab681fe5655952","725a20ec17fd4e0b8d22f64a5c071c3a","f3b7b13f9531469aa31909badcf22d7f","4662290953254d1cba4b00d0f59961f7","a2f3a717cf00495395832e2ffff0c041","0eacd161300348eca587dbb67c1281cf","f73f38d109154562bac342c69bef57f2","abaab6f02b7043c5b54af6d8ce56d2c6","8006bda3741c4a24af671f1f3a096b65","5e619fef64734dd0b1d05089da836897","b36a3a60c7ad4c9081e9a98f7f820cef","4961eeb9a0f144dc98195e2dedc0f1bf","2ac6359e51f34a66acb0a9ae7980dc95","5ca39904cb114df6873fd4c75cb8c139","ef344693125c43af9202d1c3c8b3047a","a9c24386b2fe48a09f47309b9d31b3dc","fb6a1dea868e404296d5a10224aba590","d19716106ac84e5ea48eb4373de29f98","40cb8e1f520b45a892ce643b594d8f04","e7b230811baa4e548e916d526aff207c","b3dea37305664a80aa0452afb15d035f","ec8fc8cd059945eca3909065a7f42fc8","44f5656e248e4ed7b9dd92b22c2fd618","7b84167a62164aa6904fa131de9f4280","60d6a44970774b93b4399828844468a5","2b73c3dacf454dd7b83a98e1b163ffd3","73301de31e684a9b84a3bf3f537895e2","f9e5ef18b702406d871a2281460f99c7","74660e5e998f488ab0fc9b34b39f0665","96214d95e73d4c7d8f7ccbb42519bcab","f853285c5d1c4902b369ec6d1bbb4b0a","189a8ba4072449cdb6a36bcc8516a6b7","bc57624edad84b41846032796ad95ea5","8f7c154094c94ad2889eda1290d0f337","8d90c8ffd60046d99adb8b8acd771aee","ea5add688fc24c35bc06a551b32bb09a","3058fac880ca4f9997b853afdd392946","c6fabb57d57841ab98307b6f331c70a0","b90a152815024ae0a8d3a615def44e64","3ccb6e7aa20d4c918bbe23bc1a75208b","fbec10ffc42b4f01a4095b5778f04eb7","dcc6573321454681a6674ceb0fe53141","7b8f6757110446e1a4b5222b0008479d","64e75f0e25ac49079ccd6aa7d68eec59","231bfc4c49f44e6e8e9e195d607148da","57c08a84338f4991b417fac3e103f4ee","c53b07d8a9e74ca1a32ea1d90ea5d723","18dd887b54924a76a5dfbbdfbca1dbb8","956deed24cb6486c9723c623f6763c33","58f17c4d12a14a5c8985fa4f6a4ee5cf","0b35dcc27d85418f9aef5e3306515205","5dc97eaca9ce4b7593f6655305c20900","771418105c364dd28f185b87159698b5","3766d1705bc84de4ac84a1d2772dbd47","593431f67ee64e76888d610cc21b0a4d","73e8bd14041e4b69accc5aae652a20a9","a74bd4ed868344979111e010b7e6a50d"]},"id":"joxlkw1fvYWe","executionInfo":{"status":"ok","timestamp":1763900452520,"user_tz":-330,"elapsed":142235,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"695385ce-cae3-44b3-e951-9d8fbe527e0f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/gemini-2.5-pro_instruction_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d578b1f3b34415b995e72b4b857251b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3b7b13f9531469aa31909badcf22d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ca39904cb114df6873fd4c75cb8c139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60d6a44970774b93b4399828844468a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5add688fc24c35bc06a551b32bb09a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c53b07d8a9e74ca1a32ea1d90ea5d723"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2887\n","  - BLEU: 0.0669\n","  - BERTScore F1: 0.8887\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.3878\n","  - Micro F1: 0.5181\n","  - Macro F1: 0.4884\n","  - Weighted F1: 0.5111\n","\n","Q&A Generation:\n","  - BLEU: 0.0219\n","  - Diversity: 0.7490\n","  - Answerability: 0.7067\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5533\n","  - Recall@10: 0.2213\n","  - F1@10: 0.3162\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-pro/evaluation_final.json\n"]}]}]}