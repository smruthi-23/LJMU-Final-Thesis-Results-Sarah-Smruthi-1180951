{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzOW1BR1I+43QNX1KYTgnX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5ff703bee29840e993401a97bf011e00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6123953f140b4b7fa630b4b2c850f528","IPY_MODEL_60aa548784684d57b6d950dc04197df2","IPY_MODEL_4c731ec0c3814cd99c5fc06e9066bbec"],"layout":"IPY_MODEL_95abdc7d93da421eb31250f20639a831"}},"6123953f140b4b7fa630b4b2c850f528":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2caa34526c7a4ec9aa4563c38bf1c496","placeholder":"​","style":"IPY_MODEL_b86d9230ecd8480cbce2b9d183d102e7","value":"tokenizer_config.json: 100%"}},"60aa548784684d57b6d950dc04197df2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_845bdacd463d43b0b461d72cbc453c5f","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca96fb62d2db464eb1c13af08fc644ae","value":25}},"4c731ec0c3814cd99c5fc06e9066bbec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_603dd3eda4f64232b65a3b252cfd21c8","placeholder":"​","style":"IPY_MODEL_d493e15a65a541ac889005137b405aec","value":" 25.0/25.0 [00:00&lt;00:00, 1.36kB/s]"}},"95abdc7d93da421eb31250f20639a831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2caa34526c7a4ec9aa4563c38bf1c496":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86d9230ecd8480cbce2b9d183d102e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"845bdacd463d43b0b461d72cbc453c5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca96fb62d2db464eb1c13af08fc644ae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"603dd3eda4f64232b65a3b252cfd21c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d493e15a65a541ac889005137b405aec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd99258641014e6292383c9cd950d0aa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66edfec19b38452c85833140cf66209c","IPY_MODEL_0a7677f77c2a407391313c235578aa54","IPY_MODEL_9a13ce25c2244984ae2e74d33b6d5c14"],"layout":"IPY_MODEL_e0549945615243cab2accd922438289c"}},"66edfec19b38452c85833140cf66209c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_044ff34456234374b6cb1eb5f507f5e7","placeholder":"​","style":"IPY_MODEL_9794cb5987674b708553b1ccc5f119b0","value":"config.json: 100%"}},"0a7677f77c2a407391313c235578aa54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4602783c811743629e22c1926b03f4ba","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ec4e76b44474084ab2d51a2aa5c042d","value":482}},"9a13ce25c2244984ae2e74d33b6d5c14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84103d78898e4ccaabf09c9e931e5e81","placeholder":"​","style":"IPY_MODEL_02b8c8905fe54641bbaa44554501b65e","value":" 482/482 [00:00&lt;00:00, 35.9kB/s]"}},"e0549945615243cab2accd922438289c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"044ff34456234374b6cb1eb5f507f5e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9794cb5987674b708553b1ccc5f119b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4602783c811743629e22c1926b03f4ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ec4e76b44474084ab2d51a2aa5c042d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84103d78898e4ccaabf09c9e931e5e81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02b8c8905fe54641bbaa44554501b65e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96a39ff5c5554f329667475d3ac715de":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b47791320daa4b1bb29f4bbd350506dd","IPY_MODEL_4da5e9759fcd429d8fd6e0526abc65b3","IPY_MODEL_c40162d1285845d9844ffc03618726b2"],"layout":"IPY_MODEL_fca9e9220ea04c8e86c8f14847692bc8"}},"b47791320daa4b1bb29f4bbd350506dd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24cc47c066854174bd6098be0b07da8d","placeholder":"​","style":"IPY_MODEL_1619b19e2571498c859c64e2b9dd9359","value":"vocab.json: 100%"}},"4da5e9759fcd429d8fd6e0526abc65b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebe83a5b48d64797833e795b18eeb528","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4d19a1bd1e014c3fa616ac36e30b02c3","value":898823}},"c40162d1285845d9844ffc03618726b2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d2bc05478c44259b09cc299e156ada4","placeholder":"​","style":"IPY_MODEL_4af191ff6b9f4a02bee86d3cb2b212de","value":" 899k/899k [00:00&lt;00:00, 7.04MB/s]"}},"fca9e9220ea04c8e86c8f14847692bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24cc47c066854174bd6098be0b07da8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1619b19e2571498c859c64e2b9dd9359":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebe83a5b48d64797833e795b18eeb528":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d19a1bd1e014c3fa616ac36e30b02c3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5d2bc05478c44259b09cc299e156ada4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4af191ff6b9f4a02bee86d3cb2b212de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5171e694b83547089ab7221383961823":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9a6b5269f3494f64bf05ef2c6f0bce63","IPY_MODEL_9435bcb4cbf3416ab9342e20f5c8dc97","IPY_MODEL_358d22acb05b418680c5c0dec1a4c65a"],"layout":"IPY_MODEL_4879f1a5d6ea4f1fa36e50435796e8c2"}},"9a6b5269f3494f64bf05ef2c6f0bce63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c9d766ffee24107b5bc74291db1eca4","placeholder":"​","style":"IPY_MODEL_465b24580c6b4d6ab9e8b8709eee25da","value":"merges.txt: 100%"}},"9435bcb4cbf3416ab9342e20f5c8dc97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0531780c0e9248b8b9fc26d0a8ecd158","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_93bc675cfadf4806a5334e40a855c1f0","value":456318}},"358d22acb05b418680c5c0dec1a4c65a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65ff71538ab445ba82467fc5db9e03ed","placeholder":"​","style":"IPY_MODEL_19389217277e47ac9ecc99c9a5b94940","value":" 456k/456k [00:00&lt;00:00, 7.23MB/s]"}},"4879f1a5d6ea4f1fa36e50435796e8c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c9d766ffee24107b5bc74291db1eca4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"465b24580c6b4d6ab9e8b8709eee25da":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0531780c0e9248b8b9fc26d0a8ecd158":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93bc675cfadf4806a5334e40a855c1f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65ff71538ab445ba82467fc5db9e03ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19389217277e47ac9ecc99c9a5b94940":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0e2fa664ab446dba15fc04bdc37ec0c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6f6f8142f6c64f898195a1dd2f279992","IPY_MODEL_20058af26e414b2a8f0bfca59c3dc3bb","IPY_MODEL_8c50285d49f444cba8c700820fc23a34"],"layout":"IPY_MODEL_375a148599e9425b9b82f7075d89aadd"}},"6f6f8142f6c64f898195a1dd2f279992":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8009200299ae42a4b3784424a25e4363","placeholder":"​","style":"IPY_MODEL_ddb49600a4d44c938d19b7f5bc9eae39","value":"tokenizer.json: 100%"}},"20058af26e414b2a8f0bfca59c3dc3bb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39d853c436b342099b7a6f28e0d67b9b","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf9fad8f66fe454786dbffcf11bf35a1","value":1355863}},"8c50285d49f444cba8c700820fc23a34":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08477133c9c949a89d265a21c0d9101d","placeholder":"​","style":"IPY_MODEL_b5bfb7832d884a259f88e2aca3708077","value":" 1.36M/1.36M [00:00&lt;00:00, 18.8MB/s]"}},"375a148599e9425b9b82f7075d89aadd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8009200299ae42a4b3784424a25e4363":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddb49600a4d44c938d19b7f5bc9eae39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39d853c436b342099b7a6f28e0d67b9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9fad8f66fe454786dbffcf11bf35a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08477133c9c949a89d265a21c0d9101d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5bfb7832d884a259f88e2aca3708077":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5562a84fb0bf401f880a0182b153ab80":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d0e6800da06466f8f3003622594022d","IPY_MODEL_e984cc7f6a2447ab9a6921c7bae1beaf","IPY_MODEL_358c6847509b4fa3ac59055f8c8b53cb"],"layout":"IPY_MODEL_162e3303056f49919236d0060d4818f6"}},"1d0e6800da06466f8f3003622594022d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a03df4b6a0a44e55bd391f0b7ee1900d","placeholder":"​","style":"IPY_MODEL_3606d06b0037499e82aa916317ab6efe","value":"model.safetensors: 100%"}},"e984cc7f6a2447ab9a6921c7bae1beaf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_803058e60ba54964a20ab6903ef04c95","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6945b8f3ba544d29956e245fdae276e","value":1421700479}},"358c6847509b4fa3ac59055f8c8b53cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8848c030f4a14aabbb8bd799cb295351","placeholder":"​","style":"IPY_MODEL_da8b34f84ded4dcbb1f55719edc51c02","value":" 1.42G/1.42G [00:23&lt;00:00, 103MB/s]"}},"162e3303056f49919236d0060d4818f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a03df4b6a0a44e55bd391f0b7ee1900d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3606d06b0037499e82aa916317ab6efe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"803058e60ba54964a20ab6903ef04c95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6945b8f3ba544d29956e245fdae276e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8848c030f4a14aabbb8bd799cb295351":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da8b34f84ded4dcbb1f55719edc51c02":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"id":"_ysJ0lDsejiu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763872022743,"user_tz":-330,"elapsed":6267,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"2a26d2f3-463a-4308-98f6-553bdb5cc7ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.36.0)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n","Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"44Hwtn1qsiSi","executionInfo":{"status":"ok","timestamp":1763872022774,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"c4256820-72d8-4bfd-cc4a-36ac89d77113"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED ONLY THIS\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"openai/gpt-oss-20b\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25        # SAFE FOR FREE COLAB\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    s = text.find(\"{\")\n","    e = text.rfind(\"}\")\n","    if s == -1 or e == -1 or e <= s:\n","        return {}\n","    try:\n","        return json.loads(text[s:e+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (RATE LIMIT SAFE)\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"Groq failed after all retries\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. INSTRUCTION-BASED TASKS (PROMPTS UNCHANGED)\n","#####################################################################\n","\n","##########################\n","# 8.1 SUMMARISATION\n","##########################\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, 1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = groq_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip() or out[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(partial_summaries)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\").strip() or out2[:900]\n","\n","\n","##########################\n","# 8.2 TOPIC CLASSIFICATION\n","##########################\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...] }}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","\n","##########################\n","# 8.3 Q&A GENERATION\n","##########################\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an AI assistant that MUST return STRICT JSON with no additional text.\n","\n","INSTRUCTION:\n","Generate exactly five (5) question–answer pairs based on the transcript.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples.\n","\n","GUIDELINES:\n","• EXACTLY 5 pairs.\n","• Types (in order):\n","  1. What\n","  2. Why\n","  3. How\n","  4. When\n","  5. Who\n","• Each answer must be ≤25 words.\n","• Answers must come directly from the transcript.\n","• No generic filler.\n","• NO explanations outside JSON.\n","\n","IMPORTANT JSON RULES:\n","• Start your response with '{' and end with '}'.\n","• Use ONLY this format:\n","{{\n","  \"generated_questions\": [\n","    {{\"q\": \"...\", \"a\": \"...\"}},\n","    ...\n","  ]\n","}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    # Call Groq\n","    out = groq_call(prompt, temperature=0.0)\n","\n","    # Try strict JSON parse\n","    j = extract_json(out)\n","    qa_list = j.get(\"generated_questions\", [])\n","\n","    # If JSON is invalid or empty → fallback extraction\n","    if not qa_list:\n","        logger.warning(\"JSON parsing failed. Using fallback regex extraction.\")\n","\n","        # Regex to capture Q and A even in messy output\n","        pattern = r'(?:Q:?\\s*)(.*?)(?:\\n|\\r)+(?:A:?\\s*)(.*?)(?=\\nQ|\\Z)'\n","        matches = re.findall(pattern, out, flags=re.DOTALL)\n","\n","        qa_list = []\n","        for q, a in matches:\n","            q = q.strip().replace(\"\\n\", \" \")\n","            a = a.strip().replace(\"\\n\", \" \")\n","            if q and a:\n","                qa_list.append({\"q\": q, \"a\": a})\n","\n","    # If still empty → final fallback: return empty string\n","    if not qa_list:\n","        return \"\"\n","\n","    # Convert to text format for output Excel\n","    lines = []\n","    for qa in qa_list:\n","        q = qa.get(\"q\", \"\").strip()\n","        a = qa.get(\"a\", \"\").strip()\n","        if q: lines.append(f\"Q: {q}\")\n","        if a: lines.append(f\"A: {a}\")\n","\n","    return \"\\n\".join(lines)\n","\n","##########################\n","# 8.4 KEY CONCEPT EXTRACTION\n","##########################\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list core terminology, methods, or technical phrases.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases.\n","• Exclude generic words.\n","• No duplicates.\n","\n","OUTPUT FORMAT:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    if not isinstance(concepts, list):\n","        concepts = []\n","\n","    return \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based (Groq) pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrVYGTgMskRJ","outputId":"5f61e033-d525-41f6-9490-2d7c8e797428","executionInfo":{"status":"ok","timestamp":1763873873823,"user_tz":-330,"elapsed":1815757,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gpt-oss-20b\n","Groq key loaded ✓\n","Resuming: 19 rows already processed.\n","Skipping row 0\n","Skipping row 1\n","Skipping row 2\n","Skipping row 3\n","Skipping row 4\n","Skipping row 5\n","Skipping row 6\n","Skipping row 7\n","Skipping row 8\n","Skipping row 9\n","Skipping row 10\n","Skipping row 11\n","Skipping row 12\n","Skipping row 13\n","Skipping row 14\n","Skipping row 15\n","Skipping row 16\n","Skipping row 17\n","Skipping row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The video focuses on Gaussian and log‑normal distributions, explaining their definitions, properties, and practical relevance in data preprocessing. It begins by describing the Gaussian (normal) distribution, characterized by its mean and standard deviation, and highlights the empirical 68‑95‑99.7 rule that governs the spread of data around the mean. The discussion then introduces the log‑normal distribution, noting that a variable follows this distribution when its logarithm is normally distributed, and provides real‑world examples such as income, product review length, and sentiment scores. The speaker demonstrates how to transform log‑normal data into a normal form by applying a logarithmic transformation, and subsequently standardises both the original Gaussian data and the transformed data using the z‑score formula to achieve a standard normal distribution. This standard scaling is emphasized as essential for improving machine‑learning model accuracy, as it ensures that features are on comparable scales and that the underlying data distributions are properly recognised and handled during preprocessing and modelling.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is a Gaussian distribution?\n","A: A probability distribution where X follows a bell-shaped curve, defined by mean and sigma.\n","Q: Why are we learning various distributions?\n","A: To determine profit from R&D, marketing, and campaign spending.\n","Q: How can we convert log normal distribution to standard normal distribution?\n","A: Find log of values, then apply (x - μ)/σ.\n","Q: When should we perform standard scaling?\n","A: When data follows Gaussian or log-normal distribution and we need same scale for modeling.\n","Q: Who uses these distributions for modeling?\n","A: We, the data analysts, use them to prepare data for regression or classification models.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, Normal distribution, Empirical rule (68-95-99.7 rule), Bell curve, Log-normal distribution, Log transformation, Standard normal distribution, Standard scaling (standard scaler), Mean (μ), Standard deviation (σ), Log-normal to Gaussian conversion, Normalization technique (law of normalization)\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," An end‑to‑end deep learning pipeline for plant disease detection in agriculture, focusing on early and late blight in potato plants, is presented. The workflow begins with collecting images of healthy and diseased leaves, followed by data cleaning and augmentation using TensorFlow datasets—rotations, flips, and contrast adjustments—to enlarge the training set. A convolutional neural network is trained, exported, and served with TensorFlow Serving, while a FastAPI backend exposes inference endpoints. The web interface, built in React, allows image uploads and displays predictions, and the same model is quantized to TensorFlow Lite for a React Native mobile app. Deployment occurs on Google Cloud Platform, with Cloud Functions acting as serverless inference services. The architecture demonstrates MLOps practices, edge inference, and cloud‑based scalability, and encourages customization for other crops to strengthen machine‑learning portfolios.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Mlops']\n","\n","Q&A:\n"," Q: What diseases cause economic losses to potato farmers?\n","A: Early blight and late blight.\n","Q: Why is it important to accurately identify the disease in a potato plant?\n","A: Early detection and proper treatment can prevent waste and economic loss.\n","Q: How does the mobile application determine the health status of a potato plant?\n","A: It uses deep learning with convolutional neural networks to analyze a photo taken by the farmer.\n","Q: When will the backend server be built?\n","A: After model building.\n","Q: Who is building the mobile application for farmers?\n","A: AtliQ Agriculture.\n","\n","KEY CONCEPTS:\n"," end-to-end deep learning project series, agriculture domain, data collection pipeline, model building workflow, ML Ops with TensorFlow Serving, backend server with FastAPI, deployment to Google Cloud Platform, Google Cloud Functions integration, mobile app using React Native, disease detection in potato plants, early blight and late blight classification, convolutional neural network for plant disease detection\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The section explores the spectrum of autonomy in large‑language‑model (LLM) applications, ranging from deterministic code to fully autonomous agents. At the foundational level, hard‑coded code offers no autonomy yet fails to handle real‑world complexity. A single LLM invocation processes one prompt and returns an output, but it struggles with multi‑task demands. Chains of specialist LLMs execute fixed, parallel steps, enhancing performance while remaining rigid. Routers introduce a decision‑making LLM that selects the appropriate chain based on user intent, yet they lack memory and iterative refinement. State‑machine agents combine routing with loops, human‑in‑the‑loop approval, and adaptive learning, enabling iterative refinement and true autonomous control. Fully autonomous agents aim to eliminate human oversight, but current technology is still emerging, highlighting the evolving balance between control, flexibility, and safety in LLM‑driven systems.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'LangChain', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is code autonomy level in LLM applications?\n","A: Code has zero autonomy and is 100% deterministic.\n","Q: Why does a single LLM call have limitations?\n","A: Because it tries to do everything in one shot, leading to confused or mixed up responses.\n","Q: How do chains improve LLM autonomy?\n","A: They break tasks into steps, assigning each step to a specialist LLM, creating a smarter system.\n","Q: When does a router decide which chain to use?\n","A: After an LLM analyzes the user input to determine the desired channel, the router directs to the matching chain.\n","Q: Who is responsible for approving the final draft in the agent system?\n","A: The head of the content agent reviews the draft and gives approval before publishing.\n","\n","KEY CONCEPTS:\n"," Levels of autonomy in LLM applications, Single LLM call, Chains (specialist LLM sequences), Router (intelligent routing LLM), State machine (agent with loops), LangGraph (state machine framework), Human‑in‑the‑loop approval step, Advanced memory management, Adaptive learning in agents, Multi‑agent hierarchical structure, Tool integration via Python functions, Autonomous agents (fully autonomous LLM agents)\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," The transcript discusses advanced prompt engineering for multimodal AI systems, outlining techniques for handling text, image, and audio prompts, fine‑tuning pre‑trained large language models, and deploying them in production environments. It illustrates image prompt handling through a dog‑breed classification example that combines pre‑trained convolutional neural networks—ResNet‑50, VGG‑16, and Inception—with logistic regression for final prediction. Advanced fine‑tuning methods such as multitask learning and model distillation are described, including detailed loss computation and optimization strategies. Data preprocessing best practices—tokenization, normalization, and augmentation—are highlighted to ensure high‑quality inputs across modalities. Deployment options like TensorFlow Serving and Flask APIs are mentioned, while ethical concerns around bias, fairness, and privacy in prompt‑based decision making are emphasized as critical considerations for responsible AI deployment.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What advanced techniques are explored for fine-tuning pre-trained large language models in prompt engineering?\n","A: Multitask learning, distillation, and self-supervised learning techniques are explored.\n","Q: Why is it important to ensure that the data used to train prompt engineering models is diverse and inclusive?\n","A: To avoid bias and ensure fairness and prevent discriminatory outcomes.\n","Q: How can prompt engineering models be deployed in production?\n","A: Use TensorFlow Serving or a Flask API built with the OpenAI API.\n","Q: When should data preprocessing steps such as tokenization and normalization be performed?\n","A: Before fine-tuning or training the prompt engineering models.\n","Q: Who should consider the ethical implications of prompt engineering?\n","A: Researchers, developers, and organizations deploying models.\n","\n","KEY CONCEPTS:\n"," prompt engineering, fine-tuning pre-trained large language models, multitask learning, model distillation, data preprocessing and cleaning, tokenization, text normalization, data augmentation, TensorFlow Serving deployment, Flask API deployment, ethical considerations in prompt engineering, bias and fairness in model training\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," Main overall topic: using singular value decomposition to compute eigenfaces for face clustering and classification. The lecturer loads 40 grayscale face images (20 each of Arnold Schwarzenegger and Sylvester Stallone), averages them, subtracts the mean to form matrix B, and performs an economy SVD to obtain eigenfaces (principal components). Projecting each image onto the first three eigenfaces yields a representation that separates the two actors into clusters, though some overlap remains. The method is then tested on unseen images and on alternative pairs (Taylor Swift vs. Stallone, Taylor Swift vs. Arnold), illustrating how skin tone and hair color can dominate the learned features and affect separability. The discussion concludes with a comparison to Facebook face‑recognition systems that incorporated 3‑D geometry.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the purpose of computing the average action-hero face?\n","A: To subtract the average face from each image, creating matrix B for PCA.\n","Q: Why does the algorithm project images into the first three eigen faces?\n","A: To reduce dimensionality and cluster similar faces in lower-dimensional space.\n","Q: How does the instructor load and process the face images?\n","A: By reading files, converting to grayscale, aligning, reshaping into vectors, and adding to a matrix.\n","Q: When did the instructor mention Facebook's face recognition performance?\n","A: After discussing eigenfaces, noting Facebook eventually matched or surpassed human accuracy.\n","Q: Who are the two action heroes used in the example?\n","A: Arnold Schwarzenegger and Sylvester Stallone.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Eigenfaces, Principal Component Analysis (PCA), Mean face subtraction (mean‑centering), Projection onto principal components, Image clustering in feature space, Linear combination of eigenfaces, Reshaping images into column vectors, Economy SVD, 3D geometry inference from 2D images, Skeleton mapping for face recognition, Color to grayscale conversion\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a framework that extends large language models (LLMs) from purely conversational agents into actionable AI systems capable of interacting with external services. By providing modular components, it allows developers to plug in diverse LLMs—such as GPT‑4 or open‑source Hugging Face models—without altering the core code. Built‑in integrations with travel APIs, flight and restaurant booking services, and other real‑time data sources enable the model to retrieve and act on up‑to‑date information. LangChain also supports access to private company databases, email sending, web browsing, and scraping, thereby transforming LLM reasoning into executable workflows that can query APIs, update records, or trigger external actions. This bridging of conversational AI and practical, API‑driven tasks demonstrates how LangChain turns computational intelligence into a functional agent, and the course explores a wide range of additional use cases that showcase its versatility.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is Lang chain?\n","A: Lang chain is a popular framework that bridges llms and real world, allowing them to interact with APIs and databases.\n","Q: Why cannot llms book flights directly?\n","A: Large language models cannot directly interact with the real world; they lack access to external APIs.\n","Q: How does Lang chain help llms interact with the real world?\n","A: Lang chain connects the reasoning brain of llms to external APIs, databases, and services.\n","Q: When is the query sent to an llm model?\n","A: When you press enter, the query is sent to an llm model.\n","Q: Who uses Lang chain?\n","A: Developers building applications that need llm reasoning and real world interaction.\n","\n","KEY CONCEPTS:\n"," LangChain framework, Large Language Models (LLMs), Real‑world API integration, Model switching (GPT‑4 ↔ Hugging Face), LLM reasoning capability, Bridge between LLMs and external services, Chat application interface, Travel booking APIs, Restaurant recommendation API, Email sending via API, Database connectivity, Model training data\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," This video focuses on residual analysis for time‑series forecasting, emphasizing how residuals—differences between fitted values and observed data—serve as diagnostics for model performance. It distinguishes residuals from forecast errors, noting that residuals arise from training data while errors involve unseen data. Key diagnostics include verifying that residuals have zero mean to avoid bias and checking for lack of autocorrelation using ACF/PACF plots and the Ljung‑Box test. The presenter demonstrates these techniques in Python with a Holt‑Winters exponential smoothing model applied to the Air Passengers dataset, extracting fitted values, computing residuals, and visualizing them through plots and histograms. The analysis reveals that residual patterns can expose missing trend or seasonality, guiding iterative model refinement. Overall, the discussion underscores the importance of routine residual analysis to build robust, unbiased, and well‑fitted time‑series models.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals?\n","A: The difference between a model’s fitted values and the actual observed values in a time series.\n","Q: Why is it important that residuals have zero mean?\n","A: A non-zero mean indicates bias, meaning the model is consistently under- or over-forecasting.\n","Q: How can we test residual autocorrelation?\n","A: By plotting autocorrelation and partial autocorrelation functions or using the Ljung-Box test.\n","Q: When should we check for autocorrelation in residuals?\n","A: After fitting a model, during residual analysis to detect missing information.\n","Q: Who explains residual analysis in the video?\n","A: The presenter, identified as 'Eagle', a data scientist living in London.\n","\n","KEY CONCEPTS:\n"," Residual analysis, Time series forecasting, Fitted values, Autocorrelation, Partial autocorrelation, Ljung-Box test, Exponential smoothing model, Holt-Winters model, Forecast bias correction, Residuals vs errors distinction, Mean of residuals zero, Residuals diagnostic plots\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," The video explains how to construct a text‑to‑SQL AI agent that queries an in‑memory SQLite database using a large language model hosted on watsonx.ai. The developer builds a Next.js front‑end with TypeScript, Tailwind, and React hooks to manage user input, message history, and a loading state, rendering Human and AI messages while suppressing system prompts. On the server side, an actions.ts file creates a LangChain ReAct agent, serializes message history, and connects to the watsonx chat interface, with a .env file storing API credentials. A local SQLite database is initialized via a database.ts module that defines customer and order tables, seeds mock data, and exposes an execute function. A GetFromDB tool is added to the agent, providing the LLM with schema context and a SQL input schema. Guardrails are discussed to limit database access, and the speaker points viewers to a GitHub repository for full code.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Langraph', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the agent built to do?\n","A: Use SQL knowledge to connect to your databases.\n","Q: Why use LangGraph, Next.js, and models on watsonx.ai?\n","A: LangGraph builds ReAct agent; Next.js for frontend; models run on watsonx.ai.\n","Q: How to start the Next.js application?\n","A: Run npm run dev.\n","Q: When to run the create-next-app command?\n","A: Run create-next-app at the start to set up the project.\n","Q: Who will use TypeScript?\n","A: The developer, which I will, will use TypeScript.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence agent communicating with databases, Large language models trained on SQL code, ReAct agent architecture, LangGraph framework, Next.js frontend application, Models deployed on watsonx.ai, In-memory SQLite database, Next.js CLI create-next-app, TypeScript integration in Next.js, Tailwind CSS utility framework, Client-side component rendering in Next.js, Input element event handling for LLM queries\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering in natural language processing focuses on crafting and optimizing input prompts for large language models such as GPT, BERT, and Hugging Face Transformers to elicit accurate, coherent, and contextually appropriate responses. By fine‑tuning these pre‑trained architectures, prompt engineering can outperform traditional rule‑based or keyword‑driven approaches, enhancing applications in chatbots, machine translation, and automated content creation. Nonetheless, the technique can falter when faced with ambiguous prompts and may propagate biases or inaccuracies present in the training data. The course outlined in the transcript covers the foundational principles of prompt engineering, introduces systematic prompt‑analysis methods, and evaluates the advantages and constraints of the field. Learners are equipped to design, implement, and critically assess prompt‑engineering solutions, thereby advancing the practical deployment of state‑of‑the‑art language models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A specialized NLP field that builds models generating high quality text from prompts.\n","Q: Why is prompt engineering important?\n","A: It produces more accurate, coherent, contextually appropriate text than rule-based methods.\n","Q: How does prompt engineering work?\n","A: It fine‑tunes pre‑trained large language models for specific tasks and inputs.\n","Q: When might prompt engineering struggle?\n","A: With complex or ambiguous prompts, or when biases in data cause inaccuracies.\n","Q: Who uses prompt engineering in practice?\n","A: Chatbot developers, translation services, and content generators to improve user experience.\n","\n","KEY CONCEPTS:\n"," Prompt engineering, Natural language processing, Large language models, Pre-trained models, Fine-tuning, GPT, Google BERT, Hugging Face Transformers, Prompt analysis, Deconstruct prompts, Bias in language models, Complex ambiguous prompts\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," The episode introduces Q‑learning, a value‑based reinforcement learning algorithm that seeks to maximize cumulative reward by learning a state‑action value function Q(s,a). It contrasts supervised, unsupervised, and reinforcement paradigms, emphasizing that RL derives optimal policies through value functions. Q‑learning represents Q(s,a) as a table updated via the Bellman equation, employing temporal‑difference learning: Q_new = Q_old + α[ r + γ max_a' Q(s',a') – Q_old ]. A grid‑world example demonstrates how an exploratory behavior policy drives updates, gradually steering the agent toward a +10 reward while avoiding penalties. The transcript details concrete updates, such as moving from S1 to S2 and from S2 to S6, illustrating error computation and learning rate application. Multiple episodes refine the Q‑table until convergence, after which the agent follows a target policy selecting actions with maximal Q‑values. As an off‑policy method, Q‑learning decouples data collection from the optimal policy, enabling efficient reward‑maximizing strategies.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is reinforcement learning?\n","A: Reinforcement learning is learning what to do, how to map situations to actions, to maximize a numerical reward signal.\n","Q: Why are there two types of reinforcement learning algorithms?\n","A: To either determine a value function to quantify total reward or directly determine an optimal policy that maximizes total reward.\n","Q: How does Q-learning determine the state-action values?\n","A: By maintaining a Q table mapping states to actions, updating each entry via the Bellman equation using received rewards and future Q values.\n","Q: When does the agent receive a reward in the grid world example?\n","A: When the agent transitions to a new state after taking an action, it receives the reward associated with that state.\n","Q: Who defines the behavior policy in Q-learning?\n","A: The agent uses an exploration policy, which is a random behavior policy chosen by the agent.\n","\n","KEY CONCEPTS:\n"," Q-learning, value-based reinforcement learning, state-action value function, state value function, Bellman equation, discount factor, exploration policy, target policy, grid world environment, policy-based methods, reinforcement learning paradigms, machine learning paradigms\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," The overall topic is the training of a logistic (softmax) classifier, a linear model that maps input pixel vectors to class predictions through a weight matrix and bias term. The model computes raw scores (logits) by multiplying the input vector X with a weight matrix W and adding a bias vector b. These logits are then transformed into class probabilities using the softmax function, which normalizes the scores so that they sum to one and ensures that the probability of the correct class approaches one while the probabilities of incorrect classes approach zero. This process allows the classifier to learn discriminative features from pixel data, enabling accurate multi-class predictions by iteratively adjusting W and b during training to maximize the likelihood of the correct labels.\n","\n","TOPICS:\n"," ['Machine Learning', 'Statistics', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is what's called the linear classifier.\n","Q: Why do we want the probability of the correct class close to one?\n","A: So the probability of the correct class to be very close to one and the probability for every other class to be close to zero.\n","Q: How do we turn scores into probabilities?\n","A: The way to turn scores into probabilities is to use a softmax function.\n","Q: When does each image have only one possible label?\n","A: Each image, that we have as an input can have one and only one possible label.\n","Q: Who are denoted by X, W, and b?\n","A: Throughout, we'll denote the inputs by X, the weights by W, and the biased term by b.\n","\n","KEY CONCEPTS:\n"," logistic classifier, linear classifier, linear function, matrix multiplication, input vector, weight matrix, bias term, training model, softmax function, probability scores, logit scores\n","Saved row 29\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gpt-oss-20b/gpt-oss-20b_instruction_full_output.xlsx\n","\n","Instruction-based (Groq) pipeline completed successfully.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gpt-oss-20b/gpt-oss-20b_instruction_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gpt-oss-20b/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"id":"BfMt8fgDslC6","colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["5ff703bee29840e993401a97bf011e00","6123953f140b4b7fa630b4b2c850f528","60aa548784684d57b6d950dc04197df2","4c731ec0c3814cd99c5fc06e9066bbec","95abdc7d93da421eb31250f20639a831","2caa34526c7a4ec9aa4563c38bf1c496","b86d9230ecd8480cbce2b9d183d102e7","845bdacd463d43b0b461d72cbc453c5f","ca96fb62d2db464eb1c13af08fc644ae","603dd3eda4f64232b65a3b252cfd21c8","d493e15a65a541ac889005137b405aec","cd99258641014e6292383c9cd950d0aa","66edfec19b38452c85833140cf66209c","0a7677f77c2a407391313c235578aa54","9a13ce25c2244984ae2e74d33b6d5c14","e0549945615243cab2accd922438289c","044ff34456234374b6cb1eb5f507f5e7","9794cb5987674b708553b1ccc5f119b0","4602783c811743629e22c1926b03f4ba","4ec4e76b44474084ab2d51a2aa5c042d","84103d78898e4ccaabf09c9e931e5e81","02b8c8905fe54641bbaa44554501b65e","96a39ff5c5554f329667475d3ac715de","b47791320daa4b1bb29f4bbd350506dd","4da5e9759fcd429d8fd6e0526abc65b3","c40162d1285845d9844ffc03618726b2","fca9e9220ea04c8e86c8f14847692bc8","24cc47c066854174bd6098be0b07da8d","1619b19e2571498c859c64e2b9dd9359","ebe83a5b48d64797833e795b18eeb528","4d19a1bd1e014c3fa616ac36e30b02c3","5d2bc05478c44259b09cc299e156ada4","4af191ff6b9f4a02bee86d3cb2b212de","5171e694b83547089ab7221383961823","9a6b5269f3494f64bf05ef2c6f0bce63","9435bcb4cbf3416ab9342e20f5c8dc97","358d22acb05b418680c5c0dec1a4c65a","4879f1a5d6ea4f1fa36e50435796e8c2","4c9d766ffee24107b5bc74291db1eca4","465b24580c6b4d6ab9e8b8709eee25da","0531780c0e9248b8b9fc26d0a8ecd158","93bc675cfadf4806a5334e40a855c1f0","65ff71538ab445ba82467fc5db9e03ed","19389217277e47ac9ecc99c9a5b94940","f0e2fa664ab446dba15fc04bdc37ec0c","6f6f8142f6c64f898195a1dd2f279992","20058af26e414b2a8f0bfca59c3dc3bb","8c50285d49f444cba8c700820fc23a34","375a148599e9425b9b82f7075d89aadd","8009200299ae42a4b3784424a25e4363","ddb49600a4d44c938d19b7f5bc9eae39","39d853c436b342099b7a6f28e0d67b9b","cf9fad8f66fe454786dbffcf11bf35a1","08477133c9c949a89d265a21c0d9101d","b5bfb7832d884a259f88e2aca3708077","5562a84fb0bf401f880a0182b153ab80","1d0e6800da06466f8f3003622594022d","e984cc7f6a2447ab9a6921c7bae1beaf","358c6847509b4fa3ac59055f8c8b53cb","162e3303056f49919236d0060d4818f6","a03df4b6a0a44e55bd391f0b7ee1900d","3606d06b0037499e82aa916317ab6efe","803058e60ba54964a20ab6903ef04c95","a6945b8f3ba544d29956e245fdae276e","8848c030f4a14aabbb8bd799cb295351","da8b34f84ded4dcbb1f55719edc51c02"]},"executionInfo":{"status":"ok","timestamp":1763874081607,"user_tz":-330,"elapsed":197651,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"d9172540-c588-4fcc-f065-1426ade794de"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gpt-oss-20b/gpt-oss-20b_instruction_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ff703bee29840e993401a97bf011e00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd99258641014e6292383c9cd950d0aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a39ff5c5554f329667475d3ac715de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5171e694b83547089ab7221383961823"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0e2fa664ab446dba15fc04bdc37ec0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5562a84fb0bf401f880a0182b153ab80"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2415\n","  - BLEU: 0.0355\n","  - BERTScore F1: 0.8699\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3967\n","  - Micro F1: 0.5221\n","  - Macro F1: 0.5018\n","  - Weighted F1: 0.5046\n","\n","Q&A Generation:\n","  - BLEU: 0.0246\n","  - Diversity: 0.7862\n","  - Answerability: 0.6667\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5467\n","  - Recall@10: 0.2187\n","  - F1@10: 0.3124\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gpt-oss-20b/evaluation_final.json\n"]}]}]}