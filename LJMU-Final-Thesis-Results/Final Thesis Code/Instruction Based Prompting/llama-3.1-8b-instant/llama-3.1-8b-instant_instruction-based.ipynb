{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPknPY8yK1vC+rzWocLTAsl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"53c50f7903f745189a0dbd0b1447d6e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_258e1b3a242948388298b2719864e2aa","IPY_MODEL_e034c897ff074502ab0f805962ac5ba8","IPY_MODEL_ed0968ea97cc4022adbc7d11b17df91e"],"layout":"IPY_MODEL_59ac45ec17eb442489074ca4c0db354e"}},"258e1b3a242948388298b2719864e2aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f0db5407b047c2b1447ec627af2ac3","placeholder":"​","style":"IPY_MODEL_13a9d24a598545248e7eeedc9c48efce","value":"tokenizer_config.json: 100%"}},"e034c897ff074502ab0f805962ac5ba8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_98e4d053d6d340c293d894d87b9100d0","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c7f1aaead584c2781c3ec41fb93ccbb","value":25}},"ed0968ea97cc4022adbc7d11b17df91e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_995fd2c72a12477096d6f09e314d8a9f","placeholder":"​","style":"IPY_MODEL_bdb60737aeb640ad811c412647ae85a8","value":" 25.0/25.0 [00:00&lt;00:00, 1.85kB/s]"}},"59ac45ec17eb442489074ca4c0db354e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4f0db5407b047c2b1447ec627af2ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13a9d24a598545248e7eeedc9c48efce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98e4d053d6d340c293d894d87b9100d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c7f1aaead584c2781c3ec41fb93ccbb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"995fd2c72a12477096d6f09e314d8a9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdb60737aeb640ad811c412647ae85a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5df0164ed28a4a7db9a2ae955c974071":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7f728b690d44176acc47c2c9a6d1e04","IPY_MODEL_68de1777efea4b8d8652281c71054e53","IPY_MODEL_f7cd8976df8e4882885e5eb36c61eac8"],"layout":"IPY_MODEL_c8ca043658004fe38173ff686e5b33ea"}},"c7f728b690d44176acc47c2c9a6d1e04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba72518fe52a4f10a7703e5eea9908b7","placeholder":"​","style":"IPY_MODEL_559ad073775e4360bc06e7af16de597d","value":"config.json: 100%"}},"68de1777efea4b8d8652281c71054e53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11caaf8410ea4f42b71ec243f852fb67","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d23a001f04e54848b4ba70e44b231f1b","value":482}},"f7cd8976df8e4882885e5eb36c61eac8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8435fcbcfdf4e278ec73de550b3ec8c","placeholder":"​","style":"IPY_MODEL_25a81baad95b4f338b73cdbaa7570891","value":" 482/482 [00:00&lt;00:00, 45.5kB/s]"}},"c8ca043658004fe38173ff686e5b33ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba72518fe52a4f10a7703e5eea9908b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"559ad073775e4360bc06e7af16de597d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11caaf8410ea4f42b71ec243f852fb67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d23a001f04e54848b4ba70e44b231f1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8435fcbcfdf4e278ec73de550b3ec8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25a81baad95b4f338b73cdbaa7570891":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"452136b4c424480f84b66202c8ead784":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50fb7ca9ed164a4e8e254b47403f296a","IPY_MODEL_1d98858d262d4211a04c16271f0a757f","IPY_MODEL_5df869b7be3d40cd970d465538f3f411"],"layout":"IPY_MODEL_61d5846a506c4415af1c86b2e1264e78"}},"50fb7ca9ed164a4e8e254b47403f296a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9131be4c4c3641838a6301ce38a7e4ec","placeholder":"​","style":"IPY_MODEL_611374e2915e4df99c4468eba64b98fa","value":"vocab.json: 100%"}},"1d98858d262d4211a04c16271f0a757f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_558a5e1e8ac94ae8a817f0a6d4f476bf","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ce55835140c492ca40dfa51f8b1400d","value":898823}},"5df869b7be3d40cd970d465538f3f411":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7dd168c9efc4c0a9218b42bbde406c8","placeholder":"​","style":"IPY_MODEL_8607b6c594cd4301aed29fa85afd4264","value":" 899k/899k [00:00&lt;00:00, 7.32MB/s]"}},"61d5846a506c4415af1c86b2e1264e78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9131be4c4c3641838a6301ce38a7e4ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"611374e2915e4df99c4468eba64b98fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"558a5e1e8ac94ae8a817f0a6d4f476bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ce55835140c492ca40dfa51f8b1400d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7dd168c9efc4c0a9218b42bbde406c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8607b6c594cd4301aed29fa85afd4264":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c2d2a08d83d4c28bb47be5a77d29416":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_745dbf0b16754ed78469b743a9253690","IPY_MODEL_6a69df13359f4baab8560ffdefe8cd6a","IPY_MODEL_d5db6e735ccf4bd99e035494033c54e7"],"layout":"IPY_MODEL_46b3072bd5f546f7849e3366afa6c5f9"}},"745dbf0b16754ed78469b743a9253690":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_050af71e2e704599a8714be9993283f1","placeholder":"​","style":"IPY_MODEL_71ef13b862974d5db0418b303ab56cf4","value":"merges.txt: 100%"}},"6a69df13359f4baab8560ffdefe8cd6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f905d6a3f4ac40db8c8c3b4bf8f862bf","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c16255ea6ffa4d48a39ab060888c1971","value":456318}},"d5db6e735ccf4bd99e035494033c54e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_871509879a324e1895102a016967f520","placeholder":"​","style":"IPY_MODEL_0ef9712d45fb4be3933cae2a0d2e5594","value":" 456k/456k [00:00&lt;00:00, 3.66MB/s]"}},"46b3072bd5f546f7849e3366afa6c5f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"050af71e2e704599a8714be9993283f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71ef13b862974d5db0418b303ab56cf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f905d6a3f4ac40db8c8c3b4bf8f862bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c16255ea6ffa4d48a39ab060888c1971":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"871509879a324e1895102a016967f520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ef9712d45fb4be3933cae2a0d2e5594":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3506c7dc07f44095a5f42ff16f009e6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45b71a23107249c3b33fd76f5b3a688a","IPY_MODEL_6fb64721a1c64c82ab6cd39512b80b36","IPY_MODEL_a6092d3049784570acfdef7755dfce9d"],"layout":"IPY_MODEL_9d21dcab50644d42bf8d88788cafeefa"}},"45b71a23107249c3b33fd76f5b3a688a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fc042211c8b4869b93c3529a143adac","placeholder":"​","style":"IPY_MODEL_c7e18c0eeba24a2b821084ccf79df32d","value":"tokenizer.json: 100%"}},"6fb64721a1c64c82ab6cd39512b80b36":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6991f93c7bcc4521a273ca5ab50a6888","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4a25533fefa45f4bfefdc303e127b59","value":1355863}},"a6092d3049784570acfdef7755dfce9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3be7439b912d4ff3b09f1de605a2e808","placeholder":"​","style":"IPY_MODEL_4cdcc5518a1a44859b21b03a903ae784","value":" 1.36M/1.36M [00:00&lt;00:00, 40.8MB/s]"}},"9d21dcab50644d42bf8d88788cafeefa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9fc042211c8b4869b93c3529a143adac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7e18c0eeba24a2b821084ccf79df32d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6991f93c7bcc4521a273ca5ab50a6888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4a25533fefa45f4bfefdc303e127b59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3be7439b912d4ff3b09f1de605a2e808":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cdcc5518a1a44859b21b03a903ae784":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd878591e0cb4b7d82c35a315a315801":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc065cd0e83c457b8daf84bfbabd32b1","IPY_MODEL_74de656b0a304bd3b4a5d422bf67eee0","IPY_MODEL_3d254963ddd94d7caf799349639792bb"],"layout":"IPY_MODEL_7b8d4813e9ed45f1a0099304e1c0a59f"}},"cc065cd0e83c457b8daf84bfbabd32b1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3043f76599245f680c97f2f6302306c","placeholder":"​","style":"IPY_MODEL_22368d00902b4cd8bdd4f3200ec838f6","value":"model.safetensors: 100%"}},"74de656b0a304bd3b4a5d422bf67eee0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7588b961e57d488895bcd2a3b5449c3f","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1a0574dfcf48480f8505209bb8e607a3","value":1421700479}},"3d254963ddd94d7caf799349639792bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7ff908169364b208d5486d7bf1dde9c","placeholder":"​","style":"IPY_MODEL_27b3bad8665e4b4ca983b9a22df8b835","value":" 1.42G/1.42G [00:13&lt;00:00, 101MB/s]"}},"7b8d4813e9ed45f1a0099304e1c0a59f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3043f76599245f680c97f2f6302306c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22368d00902b4cd8bdd4f3200ec838f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7588b961e57d488895bcd2a3b5449c3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a0574dfcf48480f8505209bb8e607a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7ff908169364b208d5486d7bf1dde9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27b3bad8665e4b4ca983b9a22df8b835":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"R0HMtiad6a4L","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763700112248,"user_tz":-330,"elapsed":7100,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"5699ec22-7bcc-47f8-c40a-c67c63802a22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.36.0)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n","Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"HDupT6B6I6wt","executionInfo":{"status":"ok","timestamp":1763700112269,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"6f9faa5b-9bde-4b5a-9f36-4403d5332bc3"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED ONLY THIS\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.1-8b-instant/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.1-8b-instant_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.1-8b-instant\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25        # SAFE FOR FREE COLAB\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    s = text.find(\"{\")\n","    e = text.rfind(\"}\")\n","    if s == -1 or e == -1 or e <= s:\n","        return {}\n","    try:\n","        return json.loads(text[s:e+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (RATE LIMIT SAFE)\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"Groq failed after all retries\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. INSTRUCTION-BASED TASKS (PROMPTS UNCHANGED)\n","#####################################################################\n","\n","##########################\n","# 8.1 SUMMARISATION\n","##########################\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, 1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = groq_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip() or out[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(partial_summaries)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\").strip() or out2[:900]\n","\n","\n","##########################\n","# 8.2 TOPIC CLASSIFICATION\n","##########################\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...] }}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","\n","##########################\n","# 8.3 Q&A GENERATION\n","##########################\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an AI assistant that MUST return STRICT JSON with no additional text.\n","\n","INSTRUCTION:\n","Generate exactly five (5) question–answer pairs based on the transcript.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples.\n","\n","GUIDELINES:\n","• EXACTLY 5 pairs.\n","• Types (in order):\n","  1. What\n","  2. Why\n","  3. How\n","  4. When\n","  5. Who\n","• Each answer must be ≤25 words.\n","• Answers must come directly from the transcript.\n","• No generic filler.\n","• NO explanations outside JSON.\n","\n","IMPORTANT JSON RULES:\n","• Start your response with '{' and end with '}'.\n","• Use ONLY this format:\n","{{\n","  \"generated_questions\": [\n","    {{\"q\": \"...\", \"a\": \"...\"}},\n","    ...\n","  ]\n","}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    # Call Groq\n","    out = groq_call(prompt, temperature=0.0)\n","\n","    # Try strict JSON parse\n","    j = extract_json(out)\n","    qa_list = j.get(\"generated_questions\", [])\n","\n","    # If JSON is invalid or empty → fallback extraction\n","    if not qa_list:\n","        logger.warning(\"JSON parsing failed. Using fallback regex extraction.\")\n","\n","        # Regex to capture Q and A even in messy output\n","        pattern = r'(?:Q:?\\s*)(.*?)(?:\\n|\\r)+(?:A:?\\s*)(.*?)(?=\\nQ|\\Z)'\n","        matches = re.findall(pattern, out, flags=re.DOTALL)\n","\n","        qa_list = []\n","        for q, a in matches:\n","            q = q.strip().replace(\"\\n\", \" \")\n","            a = a.strip().replace(\"\\n\", \" \")\n","            if q and a:\n","                qa_list.append({\"q\": q, \"a\": a})\n","\n","    # If still empty → final fallback: return empty string\n","    if not qa_list:\n","        return \"\"\n","\n","    # Convert to text format for output Excel\n","    lines = []\n","    for qa in qa_list:\n","        q = qa.get(\"q\", \"\").strip()\n","        a = qa.get(\"a\", \"\").strip()\n","        if q: lines.append(f\"Q: {q}\")\n","        if a: lines.append(f\"A: {a}\")\n","\n","    return \"\\n\".join(lines)\n","\n","##########################\n","# 8.4 KEY CONCEPT EXTRACTION\n","##########################\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list core terminology, methods, or technical phrases.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases.\n","• Exclude generic words.\n","• No duplicates.\n","\n","OUTPUT FORMAT:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    if not isinstance(concepts, list):\n","        concepts = []\n","\n","    return \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based (Groq) pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CoaGyxL0I9nj","executionInfo":{"status":"ok","timestamp":1763705201045,"user_tz":-330,"elapsed":3737345,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"6182084c-7b5b-4a23-b236-e4fa6d84f4d5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.1-8b-instant\n","Groq key loaded ✓\n","Resuming: 1 rows already processed.\n","Skipping row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial explores the application of CVX opt and kernels in Support Vector Machines (SVMs), examining the impact of kernels on the SVM model and visualizing nonlinear and soft margin effects. The tutorial utilizes example code from Matthew Blondell's GitHub, which is based on Christopher Bishop's pattern recognition and machine learning book. CVX opt is used to directly visualize the kernel's influence on the SVM, and the quadratic programming solver used in CVX opt is explained. The implementation of SVMs using the CVX optimization library is covered, including the initialization method, fit method, and polynomial kernel. The code generates different types of data to demonstrate the effect of kernels on SVM performance, and the process of linear separation is illustrated through visualization. The discussion revolves around the kernel in SVM classification, including the parameters of scikit-learn's Support Vector Classifier and its application to multi-class classification.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What is the topic of this tutorial?\n","A: Working with CVX opt and kernels applied to a support Vector machine\n","Q: Why is CVX opt useful in this case?\n","A: To see directly the impact of a kernel and where it's actually being injected and change and modifying the initial formal support Vector machine\n","Q: How do you visualize nonlinear and soft margin in pyit learn?\n","A: It's just a little harder to actually see the kernels impact\n","Q: When is CVX opt not something you're probably ever going to use?\n","A: Beyond this tutorial\n","Q: Who wrote the book that some information from is used in this tutorial?\n","A: Christopher Bishop\n","\n","KEY CONCEPTS:\n"," Machine Learning, Support Vector Machine, CVX Opt, Kernels, Quadratic Programming, Soft Margin, Lib SVM, PyTorch, Quadratic Equation, Support Vector, Optimization, Visualization\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," The foundation of prompt engineering lies in comprehending the concept of prompts, which are inputs given to large language models to generate text outputs. Prompts can take various forms, including question prompts, statement prompts, and those with multiple inputs or constraints. Key features of prompts include length, language, context, and constraints, which significantly impact the complexity and quality of the output. Effective prompts require defining expected outcomes and constraints such as tone, style, and specific requirements. By deconstructing prompts into individual components, understanding their key features and constraints becomes essential for prompt engineering, enabling the creation of more accurate and efficient outputs.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What are the inputs that we give to our prompt engineering models?\n","A: prompts\n","Q: Why is it important to understand the type of prompts?\n","A: it can help you choose the right prompt for your desired output\n","Q: How do you define the key features of a prompt?\n","A: the length of the prompt, the specific language used, and any context or constraints that are included\n","Q: When do you need to deconstruct a prompt?\n","A: when you want to know the elements and key features and constraints in this prompt\n","Q: Who can benefit from understanding prompt engineering?\n","A: anyone who wants to generate accurate and efficient text outputs\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Large Language Models, Prompt Types, Question Prompts, Statement Prompts, Multiple Input Prompts, Problem Prompts with Constraints, Prompt Features, Prompt Length, Specific Language, Context, Constraints, Tone, Style, Requirements, Output Format, Prompt Deconstruction, Constraint Identification, Prompt Optimization, SEO Optimization\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," The concept of Artificial Intelligence (AI) agents is discussed, where agents are problem solvers that can think and act autonomously. They utilize tools, which are specific functions that enable them to complete tasks. The React Agent Pattern is introduced, mimicking human thinking and consisting of a cycle of thinking, acting, observing, and repeating. This pattern involves an LLM that thinks about a problem, decides on an action, and observes the output of a tool, which can be a Python function or an API call. The React Agent Pattern is visualized as a brain equipped with tools, giving rise to an agent. A coding example using Lang Chain will be provided to build a basic React Agent and explore its limitations.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What are AI agents?\n","A: AI agents are problem solvers of the Artificial Intelligence (AI) World that can think on their own.\n","Q: Why do AI agents take it a step further than chains and routers?\n","A: AI agents can decide for themselves what steps to take on their own.\n","Q: How can we create an agent?\n","A: We can create an agent using the React Agent Pattern, which stands for reasoning plus acting.\n","Q: When does the cycle of think, action, observation, think, action, observation repeat?\n","A: The cycle repeats until we get the final answer.\n","Q: Who equips the brain with tools to make an agent?\n","A: We equip the brain with tools to make an agent.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI) Agents, Problem Solvers, Autonomous Decisions, React Agent Pattern, Reasoning Plus Acting, Think-Action-Observation Loop, LLM (Large Language Model), Tools, Functions, Action Input, Control Flow, Lang Chain, Agent, Brain (Reasoning Ability), API Calls, Google Search, Python Function, Agent Pattern, Autonomous Systems, Multi-Step Problems, Complex Problems, Context, Output, Input Arguments, Function Execution, System Function, Tool Execution, Output of Tool, Context Availability, Cycle of Think-Action-Observation\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The reflection agent system is a key component in delivering refined viral tweets. To understand its functionality and integration with other systems, the process is examined in detail on the website smith.chain. LangChain, a library supporting LSmith, is used to generate and refine a tweet through multiple iterations of reflection and generation. LSmith is a tracing system that captures and visualizes the execution of a program. The instructor demonstrates how to use the reflection agent to provide feedback to the generation agent, resulting in a final, refined tweet. This process highlights the power of reflection agents in thinking deeply and generating high-quality output, setting the stage for the next section on reflexion agents.\n","\n","TOPICS:\n"," ['LangChain', 'Prompt Engineering', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the purpose of tracing the reflection agent system?\n","A: to understand exactly what is happening where\n","Q: Why is it necessary to understand the reflection agent system?\n","A: to understand how both systems are working together\n","Q: How will tracing the reflection agent system help?\n","A: to deliver our final refined viral tweet\n","Q: When will the tracing of the reflection agent system take place?\n","A: in this section\n","Q: Who is responsible for tracing the reflection agent system?\n","A: I'm just going to go ahead\n","\n","KEY CONCEPTS:\n"," Reflection Agent System, Viral Tweet, Final Refined Output, Delivery System, Agent System, Chain, Section, Trace, Particular Website, Smith Chain, Systems Working Together, Final Output\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," This section focuses on working with LangChain chat models, specifically open Artificial Intelligence (AI) APIs. The necessary package, LChain Das Open AI, is installed and imported, allowing for the selection of a specific model, such as GBT-40 or GPT-3. The 'invoke' keyword is used to interact with Open AI's APIs, but an error occurs due to a missing API key. To resolve this, an EnV file is created and the API key is copied and pasted into it, and the 'python-dotenv' package is installed to provide access to the EnV file's contents. The Open AI API is used to access its Language Model (LLM) and retrieve content from an EnV file. The LLM's response is accessed, and the content property is used to extract the desired information. This process demonstrates how to use Lang chains chat models to communicate with various APIs, such as the Open AI API, by providing the desired model and accessing its interface. The discussion also focuses on utilizing a Large Language Model (LLM) to generate responses based on conversation history, including human and AI interactions, to enhance its response generation capabilities and provide awareness of previous events.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the primary focus of this section?\n","A: Working with Lang chains chat models\n","Q: Why are we predominantly working with open Artificial Intelligence (AI) APIs?\n","A: No specific reason mentioned\n","Q: How do you install the L chain Das open aai package?\n","A: Remove the percentage sign and install it\n","Q: When did the installation of the L chain Das open aai package complete?\n","A: After a few seconds\n","Q: Who released the GBT 40 model by open aai?\n","A: No specific person mentioned\n","\n","KEY CONCEPTS:\n"," Open Artificial Intelligence (AI) APIs, Lang Chain Chat Models, Chat Open AI, LLM (Large Language Model), Model Initialization, Model Parameters, Model Selection, Open AAI, GBT 40 Model, GPT 3 Model, Package Installation, Module Importation, Command Line Interface (CLI)\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explores the application of the sort method in Python, specifically its behavior when sorting lists containing strings. It highlights that Python prioritizes uppercase letters over lowercase letters, resulting in two separate sorts. Additionally, the transcript demonstrates the sort method's ability to handle mixed data types, placing numbers before strings. The reverse of the sorted list is also shown, with numbers at the end. This behavior has implications for real-world applications, where ensuring consistent sorting may require standardizing string case. The transcript provides a comprehensive understanding of the sort method's capabilities and limitations when working with strings and mixed data types.\n","\n","TOPICS:\n"," ['Python Programming', 'Statistics', 'Data Science']\n","\n","Q&A:\n"," Q: What happens when you sort a list that contains a number of strings?\n","A: It sorts the words that have a capital uppercase letter first and then the ones with the lowercase first letter alphabetically.\n","Q: Why might you want to make sure all strings are either all lowercase or all uppercase when sorting?\n","A: Because you might not want the two sorts going on, with uppercase and then lowercase.\n","Q: How does Python handle the sort method when a list contains both strings and numbers?\n","A: It puts the numbers first and the strings.\n","Q: When does Python put numbers first and strings when sorting a list?\n","A: When a list contains both strings and numbers.\n","Q: Who might benefit from watching this video?\n","A: Anyone who wants to learn about sorting lists in Python.\n","\n","KEY CONCEPTS:\n"," Multi-word technical phrases, List comprehension, String sorting, Uppercase and lowercase sorting, Alphabetical order, Reverse alphabetical order, Insertion method, Inserting into a list, Handling mixed data types, Sorting mixed data types, Python sort method, List manipulation, Data type handling, String manipulation\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The decision-making process involves determining whether to rely on human judgment or artificial intelligence (AI) for a particular task. A combination of holistic curves and human bias is employed when AI outperforms humans statistically. A fraud detection system is used as an example, where AI generates alerts and financial analysts review them. The performance of AI systems is influenced by their confidence levels, with high confidence leading to better predictions and low confidence allowing humans to outperform AI by incorporating additional information. Augmented intelligence, which combines human decision-making with AI, falls between the two and has the highest success rate for low and high confidence scores. However, human cognitive bias can affect the effectiveness of augmented intelligence, particularly when presenting AI information to human decision-makers. To optimize the integration of AI recommendations into human decision-making processes, individuals should form their own opinions before consulting AI suggestions, mitigating automation bias and enabling humans to consider AI recommendations in a more informed manner. By combining human and AI capabilities, 'augmented intelligence' can be achieved, leading to improved decision-making outcomes and minimizing cognitive bias.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What should decide a single decision?\n","A: A fascinating combination of holistic curves and human bias.\n","Q: Why is a human likely to do a better job than an AI at a 50 percent confidence level?\n","A: Often not quite as accurate as a very confident AI algorithm, but a little better at making the right decision when the AI is unsure.\n","Q: How does an AI performance curve look?\n","A: Very low confidence scores, this is not a real alert, and very high confidence scores, this is a real alert.\n","Q: When is a human likely to do a better job than an AI?\n","A: At a 50 percent confidence level.\n","Q: Who should handle alerts in a fraud detection system?\n","A: An AI system could help alleviate the workload, but which alerts should the AI handle, and which should be processed by a skilled financial analyst?\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Fraud Detection System, False Positives, Confidence Score, Success Rate, Performance Curve, Human Bias, Holistic Curves, Financial Analysts, Decision Making, Machine Learning Algorithm, Alerts Generation, Decision Threshold\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:JSON parsing failed. Using fallback regex extraction.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," The launch of six new Vertex AI APIs by Google Cloud AI addresses technical challenges in building generative applications for enterprises. These APIs, designed to improve document understanding, embedding, vector search, ranking, grounded generation, and fact-checking, offer high-quality results and unique solutions. Key features include simplicity, standalone functionality, and the incorporation of Google's expertise in AI and machine learning. The APIs are intended as primitives that can be combined with other services to build custom solutions, with Google investing in integrating them with popular frameworks for seamless integration.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Check Grounding API, Vertex AI, Generative Applications, Enterprise Data, Artificial Intelligence (AI), Google Magic, Primitives, Stateless APIs, Clear Interfaces, Chain or Llama Index, Prototyping, Combining APIs, Google Know-How, Planet-Scale Applications, Deep Understanding of Document Processing, Hybrid Search, Fine-Tuned Models, Specialized Models, Citation-Based Answers, Fact-Checking, Evidence-Based Statements, Contradicting Evidence\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The singular value decomposition (SVD) of a matrix X is discussed, focusing on unitary matrices U and V. Unitary matrices preserve angles and lengths of vectors, and are defined as matrices whose transpose is their inverse. The SVD is expressed as X = UΣV^T, where U and V are unitary matrices, and Σ is a diagonal matrix containing the singular values of X. The economy size SVD reduces the size of matrices U and V. A geometric interpretation of SVD is presented, where X maps a sphere of unit length vectors in the row space to an ellipsoid in the column space, with singular values determining elongation and left singular vectors determining orientation. This interpretation is used to understand the effect of SVD on data and simplify complex data by rotating it into a new coordinate system.\n","\n","TOPICS:\n"," ['Deep Learning', 'Statistics', 'Machine Learning']\n","\n","Q&A:\n"," Q: What are unitary matrices?\n","A: Matrices U and V are unitary matrices, where U U transpose equals U transpose U equals an identity matrix.\n","Q: Why are unitary matrices important in science and engineering?\n","A: They preserve the angles between any two vectors in the vector space that they're transforming.\n","Q: How do unitary transformations preserve angles and lengths of vectors?\n","A: They just rotate vectors, they just rotate vectors but they don't change the length or the angles between them.\n","Q: When is the complex conjugate transpose used instead of the transpose?\n","A: When dealing with complex valued data, where X is complex and we denote that by it being in the set of complex numbers.\n","Q: Who uses unitary transformations?\n","A: Scientists and engineers, who use them to rotate vector spaces so that angles and lengths are preserved.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Unitary Matrices, Economy Size SVD, Identity Matrix, Inner Product, Complex Conjugate Transpose, Unitary Transformations, Geometric Interpretation, Column Space, Row Space, Principal Axes, Singular Values, Left Singular Vectors, Right Singular Vectors, Coordinate Transformation, Data-Driven Generalization of Fourier Transform, Matrix Multiplication, Vector Space, Vector Rotation, Ellipsoid, Principal Directions\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," This educational video focuses on building a generative Artificial Intelligence (AI) powered application using Google Gemini Pro 1.5. The presenter, Krishn, showcases the capabilities of Gemini Pro 1.5 through a 1-minute demo video and hands-on applications, including text and image processing. The model can work with both text and images, enabling long context understanding and experimental features. With a vast capacity of 1 million tokens, the application can create complex generative applications, such as 1-hour videos and 30,000 lines of code. To use the application, one can create an API key for free and integrate it into the model. The presenter demonstrates the capabilities of the 1.5 Pro latest model, using the 'generate content' method to answer user queries and displaying markdown text. The model's performance is expected to become more efficient as it is used further, making it suitable for public use. The discussion also revolves around setting up prompt feedback, utilizing the Gemini Pro model for generating text from images, and creating end-to-end projects using Google Gemin Pro 1.5.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the topic of this YouTube video?\n","A: Building generative Artificial Intelligence (AI) powered application using Google Gemini Pro 1.5\n","Q: Why is Google Gemini Pro 1.5 considered amazing?\n","A: It's quite amazing with respect to the kind of application that I've actually checked\n","Q: How will the video demonstrate Google Gemini Pro 1.5 capabilities?\n","A: By showing a demo video and Hands-On applications with code\n","Q: When will the video discuss creating an API key and using it?\n","A: We'll be discussing about it after the demo\n","Q: Who is the host of this YouTube video?\n","A: Krishn\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Google Gemini Pro, Multi-Model, API Key, Long Context Understanding, Experimental Feature, End-to-End Projects, Hands-On Application, Code Implementation, Text-to-Image, Image-to-Text, Model Gemini\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Evaluating and testing prompt engineering models is a critical step in assessing their performance and accuracy. Metrics such as perplexity, accuracy, and human evaluation are used to measure a model's ability to generate meaningful responses. Techniques for debugging and improvement involve analyzing generated responses, fine-tuning models, and testing on different data sets or tasks to determine generalizability. Tools like visualization and cross-validation aid in this process. Ongoing evaluation and testing are necessary to ensure the model's continued performance. Advanced concepts in prompt engineering, including ongoing evaluation and testing, will be explored to refine model performance and accuracy.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What are some commonly used matrices for evaluating prompt engineering models?\n","A: Perplexity, accuracy, and human evaluation\n","Q: Why is it important to have a good understanding of the matrices used to evaluate prompt engineering models?\n","A: To measure the performance of the model and its ability to generate accurate and meaningful responses\n","Q: How can you fine-tune a prompt engineering model?\n","A: By analyzing the generated response and identifying common errors or patterns\n","Q: When should you continue to evaluate and test a prompt engineering model?\n","A: As you continue to use your model and generate responses\n","Q: Who can rate the quality of the responses in human evaluation?\n","A: Humans\n","\n","KEY CONCEPTS:\n"," Prompt Engineering Models, Matrices for Evaluating Prompt Engineering Models, Perplexity, Accuracy, Human Evaluation, Language Model Prediction, Sequence of Words, Generated Responses, Debugging and Improving Models, Cross Validation, Model Evaluation and Testing, Generalization on New Data, Visualization Tools, Cross Validation Techniques, Model Validation, Model Fine-Tuning, Large Language Models, Pre-Trained Language Models, Prompt Engineering, Model Performance Measurement, Model Response Analysis, Common Errors and Patterns, Model Testing on Different Data Sets, Model Testing on Different Tasks, Model Evaluation Metrics, Model Debugging Techniques, Model Improvement Techniques\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The topic revolves around the differences and complexities of various Artificial Intelligence (AI) systems, including generative AI, AI agents, and agentic AI. Generative AI creates new content based on patterns learned from data, while AI agents take input, think, and act to complete tasks using tools, memory, and knowledge. Agentic AI involves multiple AI agents working autonomously to achieve complex goals, often requiring multi-step reasoning and planning. The speaker highlights the increasing sophistication of tasks as one moves from generative AI to AI agents to agentic AI, emphasizing the importance of tools and frameworks, such as N8N and Agno, in building agentic AI systems that can tackle complex tasks.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Generative AI', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is the main difference between generative Artificial Intelligence (AI) and Artificial Intelligence (AI) agents?\n","A: Generative AI creates new content, while AI agents can perform tasks and make decisions using tools and other agents.\n","Q: Why is it difficult for generative AI to answer questions about future events?\n","A: It has a knowledge cutoff date and cannot access current information.\n","Q: How does an AI agent with access to tools and memory perform a complex task?\n","A: It uses the tools and memory to perform the task, such as booking a flight and suggesting hotels and airport taxis.\n","Q: When is it necessary to give control to an AI agent and limit its autonomy?\n","A: When the task requires sensitive information, such as a bank password.\n","Q: Who is a creator of an Agno framework that defines agentic systems into five levels?\n","A: A friend of the speaker\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence (AI), Large Language Model (LLM), Knowledge Cutoff Date, Artificial Intelligence (AI) Agent, Autonomous Decision Making, Multi-Step Reasoning, Multi-Step Planning, Agentic Artificial Intelligence (AI), Tool Usage, Autonomous Action, N8N Workflow Diagram, Agno Framework, Human-in-the-Loop, Artificial Intelligence (AI) Boot Camp, Machine Learning (ML) Server, Front-end and Back-end Architecture, Chatbot Development, Natural Language Processing (NLP), Artificial Intelligence (AI) System Components, Agent with Tools and Instruction, Agent with Knowledge and Memory, Agent with Autonomy and Decision Making\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explores the concepts of variance and covariance in data analysis, highlighting their significance in understanding the spread and relationship between variables. Variance measures the spread of a single variable, while covariance quantifies the relationship between two variables. The covariance equation is presented as 1/n summation of (X_i - μ_X) * (Y_i - μ_Y), demonstrating its equivalence to the variance equation for a single variable. A positive covariance indicates a positive relationship, while a negative covariance indicates a negative relationship, but does not provide information on the strength of the relationship. The transcript concludes by emphasizing the importance of covariance in data analysis, particularly in machine learning and statistics, and sets the stage for the discussion of the Pearson correlation coefficient in the next video.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the topic of discussion in this video?\n","A: Variance and covariance\n","Q: Why is covariance important in data analysis?\n","A: It helps to quantify the relationship between random variables in a particular data set\n","Q: How is covariance calculated?\n","A: Using the formula: 1/n summation of (X - μX)(Y - μY)\n","Q: When is the covariance positive?\n","A: When X is increasing and Y is increasing\n","Q: Who should watch this video?\n","A: People who want to learn statistics and machine learning\n","\n","KEY CONCEPTS:\n"," Variance-Covariance, Data Pre-processing, Data Analysis, Covariance, Variance, Random Variables, Mean, Covariance Formula, Covariance Matrix, Correlation Coefficient, Pearson Correlation Coefficient, Machine Learning, Statistics, Data Set, Relationship Quantification, Positive Covariance, Negative Covariance, Variance-Covariance Matrix, Random Variable Relationship\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning is a problem-solving approach where an agent aims to achieve a defined objective, typically maximizing a numerical reward signal, by interacting with an environment, making decisions based on observations, and receiving feedback in the form of rewards. The ultimate goal is to learn an optimal policy that maximizes cumulative rewards over time through trial and error learning. This can be achieved in various ways, such as episodic tasks like Tic-Tac-Toe or continuous tasks like stock market trading. The reward function is a mathematical expression that assigns a numerical value to each state or action, reflecting the agent's goal and preferences. By maximizing the expected cumulative reward, the agent can achieve its objective and improve its performance over time.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the ultimate goal of reinforcement learning?\n","A: to learn the optimal policy that maximizes a numerical reward signal\n","Q: Why is trial and error learning used in reinforcement learning?\n","A: to explore the environment by taking action and observing the resulting State and reward and update its policy accordingly\n","Q: How is the optimal policy learned in reinforcement learning?\n","A: by learning the value for each state and action pair based on its experience\n","Q: When is the reward received in the Tic-Tac-Toe game?\n","A: at the end of the game, either winning or losing\n","Q: Who receives a positive reward in the Tic-Tac-Toe game?\n","A: the agent for winning the game\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Objective, Episodic Task, Continuous Task, Reward Signal, Feedback, Trial and Error Learning, Optimal Policy, Value-Based Method, Policy-Based Method, Episodic Reward, Continuous Reward, Reward Function, Expected Cumulative Reward, Risk-Adjusted Measure, Sharp Ratio, Certino Ratio, State-Action Pair, Numerical Reverse Signal, Agent-Environment Interaction, Exploration-Exploitation Trade-off\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," The transcript focuses on the data type 'dictionary' in Python, which is a collection of key-value pairs. A dictionary is declared within curly brackets, with key-value pairs separated by colons and items by commas. The key must be immutable, such as a string or number, and cannot be a list. Dictionaries are useful for mapping one item to another, like stock prices. The transcript covers various dictionary functions, including 'items', 'keys', and 'values', which return lists of key-value pairs, keys, and values, respectively. It also explains how to create a dictionary from two lists using the 'zip' function and how to access, modify, and delete key-value pairs in a dictionary.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A dictionary consists of key-value pairs, each key-value pair is referred to as an item.\n","Q: Why are keys in a dictionary immutable?\n","A: Keys have to be immutable so they can't change.\n","Q: How do you create a dictionary from two lists?\n","A: You can use the zip function to pair the lists together and then turn that into a dictionary.\n","Q: When can you access the value associated with a key in a dictionary?\n","A: You can access the value associated with a key in a dictionary by writing the name of the dictionary and the key in square brackets.\n","Q: Who can benefit from understanding dictionaries in Python?\n","A: Data scientists can benefit from understanding dictionaries in Python, especially when working with pandas.\n","\n","KEY CONCEPTS:\n"," Immutable, Key-Value Pairs, Dictionary, Items, Colon Separation, Comma Separation, Curly Brackets, Zip Function, Tuple, Dictionary Methods, Dictionary Functions, List Comprehension, Data Science, Pandas, Data Type, Mapping, Key, Value, Dictionary Length, Dictionary Keys, Dictionary Values, Dictionary Items, Dictionary Declaration, Dictionary Creation, Dictionary Manipulation, Dictionary Deletion\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," The integration of Artificial Intelligence (AI) and User Behavior Analytics (UBA) significantly enhances an organization's security posture by expediting the detection and containment of Insider threats. AI-powered UBA quickly identifies and responds to threats, reducing breach identification and containment time by 108 days. UBA uses machine learning to analyze user behavior, identifying anomalies and potential threats. When integrated with a Security Information and Event Management (SIM) solution, UBA assists security professionals in detecting and responding to Insider threats. Q Radar SIM showcases AI and automation capabilities, streamlining security operations, enhancing skills, and providing actionable insights. By leveraging UBA and AI, security analysts can stay ahead of emerging threats and fortify their organization's defenses, addressing costly and critical Insider threats with an average cost of $4 million per incident.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the potential impact of using AI on identifying and containing a data breach?\n","A: It would take 108 fewer days on average.\n","Q: Why is AI and automation important for security posture?\n","A: It has the potential to make a big difference.\n","Q: How can user Behavior analytics with AI and machine learning help with Insider threats?\n","A: It can help detect and respond to Insider threats quickly and precisely.\n","Q: When was the IBM's cost of a data breach report 2023 based on a survey of over 500 organizations?\n","A: This is not specified in the transcript.\n","Q: Who is affected by Insider threats?\n","A: Organizations of all sizes.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), User Behavior Analytics (UBA), Machine Learning, Insider Threats, Data Breach, Security Posture, Emerging Threats, Containment, Cost of a Data Breach Report, Security Team, Artificial Intelligence and Automation, Data Breach Report 2023\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," This transcript discusses the recent announcement of Llama 3, an open-source large language model developed by Meta, which is available in two variants with 8 billion and 70 billion pre-trained parameters. The model has been integrated into Meta's AI assistant, demonstrating state-of-the-art performance in language nuances, contextual understanding, and complex tasks. Llama 3 has been trained on a massive dataset of 50 trillion tokens and achieves high accuracy in benchmarks, outperforming paid LLM models. The discussion also focuses on the performance of the MML model, which demonstrates superior performance compared to other models. The transcript provides a comprehensive approach to responsibility taken by companies, specifically mentioning the DUPTEI system level framework, and highlights the addition of Meta Llama 3, which provides transparency into the model's architecture and development. The speaker guides the audience through the process of accessing and downloading the Meta Llama 3 model, including signing up for an account and downloading the original checkpoints and tokenizer. The transcript also discusses accessing and utilizing the Llama 3 model, specifically highlighting the need for Hugging Face access and providing download options in Transformers and Native Llama 3 formats.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is your name?\n","A: Krishak\n","Q: Why are you making this video?\n","A: No specific reason mentioned\n","Q: How late is it?\n","A: 2 a.m.\n","Q: When is this video being made?\n","A: Right now\n","Q: Who are you addressing?\n","A: Guys\n","\n","KEY CONCEPTS:\n"," YouTube channel, multi-word technical phrases, transcript, technical concepts, key technical concepts, distinct concepts, multi-word phrases, generic words, technical phrases, core terminology, methods, technical terms\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The implementation of a Naive Bayes classifier using the scikit-learn library in Python is reviewed. The instructor accesses the library through Google, utilizing its documentation to understand available functions for the Naive Bayes algorithm, specifically Gaussian Naive Bayes. This algorithm is used to create a decision boundary, enabling viewers to write their own code and implement the Naive Bayes classifier in their projects. By the end of the next video or two, viewers will have the necessary skills to apply this classifier in their own work, leveraging the capabilities of the scikit-learn library and the Gaussian Naive Bayes algorithm.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What library are you going to be using a lot in this lesson?\n","A: scikit-learn\n","Q: Why are you using Google to help you use the documentation of the library?\n","A: to figure out how to use some of the functions that it has\n","Q: How do you plan to help the viewers write the code themselves?\n","A: by the end of the next video or two, you will be able to write this code yourself\n","Q: When will you be explaining what Naive Bayes does exactly?\n","A: we'll go back in a little bit\n","Q: Who is the author of the Naive Bayes formula?\n","A: there's a page on Naive Bayes here that gives you a derivation\n","\n","KEY CONCEPTS:\n"," Decision Boundary, Python Library, Scikit-learn, Naive Bayes, Google Documentation, Functions, Algorithm, Gaussian Naive Bayes, Derivation, Use Cases, Classifier, Code Writing, Python Code\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion on statistics focuses on the Gaussian distribution, also known as the normal distribution, characterized by a bell curve with 50% of the data on either side of the mean. The empirical formula states that 68% of the data falls within one standard deviation, 95% within two standard deviations, and 99.7% within three standard deviations. The log normal distribution is another type of distribution, where the logarithm of the data follows a normal distribution, commonly observed in real-world data such as income and product reviews. Standard scaling is a technique used to normalize data by converting it to a standard normal distribution, allowing for more accurate modeling and analysis, particularly useful when dealing with log normal distributed data.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the name of the distribution that follows a bell curve?\n","A: Gaussian distribution or normal distribution\n","Q: Why do we learn various distributions?\n","A: To determine what kind of distribution a particular data follows\n","Q: How do we convert a log normal distribution to a standard normal distribution?\n","A: By finding the log of each value and then applying the formula X of I minus mu divided by Sigma\n","Q: When should we use standard scaling?\n","A: When we want to scale down values to the same scale as another distribution\n","Q: Who can benefit from learning about statistical concepts?\n","A: Anyone who wants to improve their understanding of data and machine learning\n","\n","KEY CONCEPTS:\n"," Gaussian Distribution, Normal Distribution, Empirical Formula, Standard Deviation, Bell Curve, Log Normal Distribution, Standard Normal Distribution, Log Normalization, Standard Scaler, Regression Algorithm, Classification Algorithm, Domain Knowledge, Mean, Sigma, Standard Deviation, Normal Deviate, Log of X, Logarithmic Transformation, Probability Distribution, Statistical Concepts\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This video series is part of an end-to-end deep learning project in agriculture, focusing on developing a mobile application for farmers to detect potato diseases using deep learning and convolutional neural networks. The project involves data collection, model building, and deployment on Google Cloud, utilizing technologies such as TensorFlow Serving, FastAPI, and React Native. The application will enable farmers to accurately identify diseases and apply appropriate treatments, reducing economic losses. The project covers technical architecture discussions, data collection, data cleaning and pre-processing, machine learning operations, and deployment of a TensorFlow Lite model on edge devices or Google Cloud. The technology stack includes TensorFlow, CNN, data augmentation, TF dataset, quantization, TF Serving, FastAPI, DXJS, React Native, and Google Cloud Platform. The speaker advises viewers to focus on videos 14 to 20 in the series, which cover essential skills for machine learning engineers and data scientists.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the main problem faced by farmers who grow potatoes?\n","A: Farmers who grow potatoes are facing lot of economic losses every year because of various diseases that can happen to a potato plant.\n","Q: Why is it important to accurately identify the disease in a potato plant?\n","A: The treatments for early blight and late blight are little different so it's important that you accurately identify what kind of disease is there in that potato plant.\n","Q: How will the mobile application help farmers?\n","A: the mobile application will tell them whether the potato plant is healthy or it has one of these diseases\n","Q: When will the series of videos on deep learning project in agriculture domain be completed?\n","A: The series will have total seven to eight videos\n","Q: Who is working on the project to build the mobile application?\n","A: you are a data scientist working for AtliQ Agriculture\n","\n","KEY CONCEPTS:\n"," End-to-end deep learning project, Machine Learning Ops, TF serving, Fast API, Google Cloud Platform (GCP), Google Cloud Functions, React Native, Convolutional Neural Network (CNN), Artificial Intelligence (AI), Deep Learning, Data Collection, Model Building, Backend Server, Mobile Application Development, Data Analytics Solution\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explores the levels of autonomy in Large Language Model (LLM) applications, ranging from zero autonomy (code) to maximum autonomy. It discusses six levels of autonomy, from deterministic code to completely autonomous agents, highlighting the limitations and capabilities of each level. The levels progress from a single LLM call to a state machine that combines previous levels and adds features such as loops, human review, and refinement. The transcript also distinguishes between human-driven and agent-executed systems, with the latter being more intelligent and autonomous. The development of completely autonomous agents is still in progress, aiming to enable agents to make decisions without human input.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the first level of autonomy in LLM applications?\n","A: Code with zero autonomy and 100% determinism.\n","Q: Why is it difficult to handle real-life complexity with code?\n","A: Because you would need to write rules for every possible scenario.\n","Q: How does a single LLM call work?\n","A: It processes user input and gives back an output, like a chatbot responding to a message.\n","Q: When does a router make a decision based on user input?\n","A: It looks at the user input and directs the control flow to the appropriate chain.\n","Q: Who is responsible for making decisions in an Agent?\n","A: The Artificial Intelligence (AI) itself decides the output of the step and which steps to take.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI) Agents, Levels of Autonomy, Code, Llm Call, Chains, Router, State Machine, Agent, Human-Driven, Agent-Executed, Autonomous Agents, Land Graph, Cognitive Architecture, Deterministic, Decision Making, Control Flow, Loops, Cycles, Multi-Agent Systems, Advanced Memory Management, Adaptive Learning, Tools, Functions, Python Functions, Endpoints, Hierarchical Structure, LLM Applications, Autonomy, Artificial Intelligence (AI)\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This course advances the topic of prompt engineering, covering complex techniques and topics such as handling various types of prompts, including text-based, image-based, and audio-based prompts. Advanced techniques for fine-tuning pre-trained large language models, such as multitask learning and distillation, are explored. Best practices for data preprocessing and cleaning, including tokenization and normalization, are discussed. The deployment of prompt engineering models in production environments using frameworks such as TensorFlow Serving and Flask is covered. Additionally, the course touches on the ethical considerations of prompt engineering, including fairness and privacy concerns, and the importance of diverse and inclusive data for training models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What are some advanced topics in prompt engineering?\n","A: Multitasking learning, distillation, self-supervised learning techniques, and data pre-processing cleaning.\n","Q: Why is tokenization important in prompt engineering?\n","A: It helps the model understand the meaning of the text more accurately.\n","Q: How does multitasking learning work in prompt engineering?\n","A: It involves training a model on multiple tasks simultaneously to learn more robust representations.\n","Q: When is it necessary to deploy prompt engineering models in production?\n","A: Once you have built a prompt engineering model to make it accessible to the vast majority of users.\n","Q: Who should consider the ethical implications of prompt engineering?\n","A: Everyone involved in prompt engineering, as it has the potential to influence decision making in many areas.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Multitask Learning, Distillation, Data Pre-processing, Data Cleaning, Tokenization, Normalization, Data Augmentation, Pre-trained Large Language Models, Fine-tuning, Machine Learning Models, Cross Entropy Loss, Adam Optimizer, TensorFlow Serving, Flask App, Artificial Intelligence (AI) API, Ethical Considerations, Fairness, Privacy, Self-supervised Learning, Multimodal Prompts, Text-based Prompts, Image-based Prompts, Audio-based Prompts, Data Augmentation Techniques, Logistic Regression, Multinomial Regression, ResNet 50, BERT 16, Inception, T5 Small, Open AI API, Flask Framework, Web Frameworks, APIs, Machine Learning Deployment, Production Environment, Model Serving, Model Optimization, Model Evaluation, Model Interpretability, Explainability, Model Fairness, Model Bias, Model Robustness, Model Generalizability\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The application of Singular Value Decomposition (SVD) and eigenfaces in image classification is discussed. A dataset of images of Arnold Schwarzenegger and Sylvester Stallone is used, with the images loaded into a matrix, converted to greyscale, and processed to create a matrix B. The SVD is computed on B, resulting in eigenfaces that are linear combinations of the original images. The images are projected into the first three eigenfaces and plotted, demonstrating good separation between the two actors in some cases. The lecturer highlights the importance of considering three-dimensional geometry in face classification and discusses the limitations of this approach. The results show that some pairs of actors are more similar in eigenface space than others, indicating potential for this method in image classification.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is the purpose of finding the eigenfaces of a bunch of faces?\n","A: to cluster two different people in face space\n","Q: Why do we subtract the average face from all the faces?\n","A: to do principal components and compute the SVD\n","Q: How do we project each image into the first three eigenfaces?\n","A: by taking the image vector times the library (transposed)\n","Q: When do we see better separation between Taylor Swift and Stallone?\n","A: in the first three eigenface coordinates\n","Q: Who are the two action heroes used in the example?\n","A: Arnold Schwarzenegger and Sylvester Stallone\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Eigenfaces, Principal Component Analysis (PCA), Image Classification, Eigen Action Heroes, Image Compression, Principal Components, Image Features, Face Recognition, Deep Neural Network Architectures, 3D Face Geometry, Stereo Vision, Image Representation, Face Classification, Human Accuracy, Image Processing, Image Analysis, Feature Extraction, Dimensionality Reduction, Image Projection, Image Clustering\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," The Lang chain framework bridges the gap between large language models (LLMs) and the real world, enabling applications to leverage LLM reasoning abilities while interacting with external APIs, databases, and systems. It acts as an interface between LLMs and the external world, facilitating tasks such as booking flights, sending emails, and accessing APIs. Lang chain is particularly useful for building applications requiring both LLM intelligence and physical world interaction. As the most popular framework for building LLM-based applications, it offers flexibility and ease of use, allowing developers to switch between different LLMs without modifying code. The Lang chain enables AI to interact with the real world by accessing private databases, sending emails, browsing the internet, and scraping websites, giving it the ability to perform various tasks. Throughout the course, the Lang chain's capabilities will be explored in detail, with numerous use cases demonstrated.\n","\n","TOPICS:\n"," ['LangChain', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is Lang chain?\n","A: Lang chain acts as a bridge between the llms and the real world.\n","Q: Why do we need Lang chain?\n","A: We need Lang chain to have a framework that helps build applications using llms and communicate with the real world.\n","Q: How does Lang chain work?\n","A: Lang chain helps build applications using llms and communicate with the real world.\n","Q: When can we switch out GP4 with another LLM?\n","A: We can easily switch out GP4 with another LLM in the future without touching the code.\n","Q: Who can access a lot of APIs with Lang chain?\n","A: The Artificial Intelligence (AI) that we're working with can access a lot of APIs with Lang chain.\n","\n","KEY CONCEPTS:\n"," Large Language Models (LLMs), LLM Model, Chat Application, Interface, Real-World Interaction, APIs, Databases, Email Communication, Framework, Lang Chain, LLM Brain, Real-World APIs, Booking APIs, GP4, Hugging Face LLM, Code Switching, Artificial Intelligence (AI), Real-World Integration\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residual analysis is a crucial step in time series forecasting, used to diagnose model performance and identify areas for improvement. It involves checking for autocorrelation and partial autocorrelation, ensuring the mean of residuals is zero, and verifying if residuals are independently distributed using the Young Box test. A Holt-Winters model was fitted to the air passenger data set, and residual analysis revealed some form of correlation, indicating the model had missed information in the data. Statistical tests and plots confirmed the presence of serial correlation in the residuals. Residual analysis is essential for understanding model performance and improving forecasts in subsequent iterations, and it can indicate autocorrelation, bias, and other issues through correlation tests, the Ljung-Box test, and residual histograms.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: The difference between the fitted value and the actual value of the time series.\n","Q: Why is residual analysis important?\n","A: To diagnose the performance of a model and improve it by detecting trends or inconsistencies.\n","Q: How do we check for autocorrelation in residuals?\n","A: By plotting the partial autocorrelation function and the autocorrelation function for the residuals.\n","Q: When should the mean of the residuals be zero?\n","A: When the residuals are not biased, meaning they are not shifted to the left or right.\n","Q: Who can use residual analysis to improve their forecasting methods?\n","A: Data scientists like myself, Eagle, who can use Python to analyze residuals and improve their models.\n","\n","KEY CONCEPTS:\n"," Time Series Crash Course, Residual Analysis, Forecasting Methods, Python, Residuals, Fitted Values, Actual Values, Autocorrelation, Partial Autocorrelation, Young Box Test, Holt Winters Model, Exponential Smoothing Model, Trend, Seasonality, Level, Residual Plot, Correlation Analysis, Statistical Significance\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," The development of an Artificial Intelligence (AI) agent capable of interacting with a database is discussed. A ReAct agent is built using LangGraph, Next.js for the frontend application, and models running on WatsonX AI. An in-memory database using SQLite is utilized. The process involves setting up a Next.js project, creating a component called Home, and implementing a ReAct agent in a VS Code environment. The LangGraph and LangChain libraries are installed to enable the creation of a ReAct agent and connection to models on WatsonX.ai. A WatsonX AI project is set up, specifically using the Mistral Large model, and a database is created with environment variables and connected to a React frontend. The application includes state variables to manage user input and send messages to the language model, and a sendMessage function is created to send messages to the LangGraph agent. The application also includes a loading state to prevent confusion when sending messages to a large language model. The transcript discusses the implementation of message rendering, database setup, and establishing a foreign key relationship between tables. A TXS SQL agent is built by creating a function to execute SQL queries against a database, and a tool is defined using the 'tool' function from LangChain. The instructor demonstrates the model's ability to generate SQL queries and execute them on a database, including joining multiple tables. The importance of implementing guardrails is emphasized to prevent the Data Lineage Management (DLM) from having unrestricted control over the database.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the purpose of building an Artificial Intelligence (AI) agent that can talk to your database?\n","A: Most large language models have been trained on code, including SQL.\n","Q: Why are we using LangGraph to build a ReAct agent?\n","A: We're going to build an agent that's able to use that SQL knowledge to connect to your databases.\n","Q: How do we set up the boilerplate project in VS Code?\n","A: I'll be running create-next-app at latest, together with the name of my project.\n","Q: When do we start the Next.js application?\n","A: After putting in the code for the messages, I'm going to format my code and start the Next.js application.\n","Q: Who is running the models on watsonx.ai?\n","A: We also will be running an in-memory database using SQLite.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), SQL, LangGraph, ReAct agent, Next.js, watsonx.ai, SQLite, VS Code, Tailwind, TypeScript, client-side component, server-side code, large language model, database connection, Text2SQL agent, CLI (Command Line Interface), boilerplate project, npm (Node Package Manager), npm run dev, browser application\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a specialized field within Natural Language Processing (NLP) that focuses on building models to generate high-quality text outputs in response to prompts or input. Based on pre-trained large language models, such as GPT or Transformers, these models are fine-tuned for specific tasks and inputs. The key benefit of prompt engineering is generating accurate, coherent, and contextually appropriate text outputs in applications like chatbots, language translation, and content generation. However, prompt engineering models may struggle with complex prompts or generate biased outputs due to underlying data or model architecture. This comprehensive introduction covers the basics, benefits, and limitations, as well as advanced techniques for fine-tuning pre-trained models to achieve optimal results.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: a specialized field within natural language processing that focuses on building models that can generate high quality text outputs in response to the prompts or our input\n","Q: Why is prompt engineering important?\n","A: it allows us to generate text outputs that are more accurate, coherent and contextually appropriate than traditional rule based or keyword based approaches\n","Q: How do prompt engineering models work?\n","A: they are basically based on pre-trained large language models such as open AI GPT, Google BERT or hugging face Transformers that are fine-tuned for specific tasks and inputs\n","Q: When will you learn about the basics of prompt engineering?\n","A: in the first part of this course\n","Q: Who can benefit from learning prompt engineering?\n","A: those interested in Natural Language Processing (NLP) and applications such as chatbots, language translation and content generation\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing (NLP), Pre-trained Large Language Models, Fine-tuning, Chatbots, Virtual Assistants, Translation Software, Text Generation, Rule-based Approaches, Keyword-based Approaches, Contextually Appropriate Text Outputs, Large Language Models (LLMs), Artificial Intelligence (AI), Transformer Models, Open AI GPT, Hugging Face Transformers, Language Translation, Content Generation, User Experience, Engagement, Prompt Analysis, Prompt Deconstruction, Key Features and Constraints, Core Structure and Content, Model Architecture, Underlying Data, Biased and Inaccurate Outputs\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning is a machine learning paradigm where an agent learns to map situations to actions to maximize a numerical reward signal. Q-learning is a value-based reinforcement learning method that determines a value function, known as the Q-value, to maximize total rewards. The Q-value is calculated using the Bellman equation, which defines a recursive relationship between Q-values. The Q-learning algorithm involves updating a Q-table based on the temporal difference error, which is the difference between the observed and expected Q-values. The update rule is applied iteratively until the sequence of steps is complete, known as an episode. Multiple episodes are performed to learn the Q-values, which dictate the policy to achieve the target reward. Q-learning is an off-policy algorithm, allowing the behavior policy to collect data independently from the target policy. The Q-table is updated using the update rule, and the Q-values become more stable with repeated episodes, enabling an agent to take actions based on the highest Q-value to achieve the target reward. This process is demonstrated in the example of a grid world where an agent learns to navigate to a reward spot.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is Q-learning?\n","A: A value-based reinforcement learning method to solve problems.\n","Q: Why do we use Q-learning?\n","A: To learn an optimal policy that maximizes the total reward.\n","Q: How does Q-learning work?\n","A: By learning the state-action value function using the Bellman equation.\n","Q: When is the Q value updated?\n","A: When the agent transitions into a new state and receives a reward.\n","Q: Who determines the optimal policy in Q-learning?\n","A: The agent, using the Q table and the Bellman equation.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Q-Learning, Value-Based Methods, Policy-Based Methods, Value Function, State Value Function (V), State-Action Value Function (Q), Q-Value, Bellman Equation, Exploration Policy, Behavior Policy, Optimal Policy, Target Policy, Grid World, Supervised Learning, Unsupervised Learning, Machine Learning Paradigms\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," Logistic classification is a fundamental process in machine learning that enables accurate predictions from input data. It involves applying a linear function, represented as a matrix multiplication of the input vector X with a weight matrix W and a bias term b, to generate scores or logits. These scores are then converted into probabilities using the softmax function S, ensuring that the probabilities sum to 1 and are large for high scores and small for low scores. The weights and bias are learned through machine learning, where the goal is to find optimal values that enable accurate predictions. This process is essential in logistic regression and is a crucial step in training a logistic classifier.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A linear classifier that takes input and applies a linear function to generate predictions.\n","Q: Why are the weights and bias important in a logistic classifier?\n","A: They are where the machine learning comes in, and we need to train them to find good values.\n","Q: How do we use scores to perform classification in a logistic classifier?\n","A: We turn the scores into probabilities using a softmax function.\n","Q: When do we want the probability of the correct class to be close to one in a logistic classifier?\n","A: When the image has one and only one possible label.\n","Q: Who determines the values for the weights and bias in a logistic classifier?\n","A: We, through the process of training the model.\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Classifier, Matrix Multiply, Giant Matrix Multiply, Linear Function, Machine Learning, Weight Matrix, Bias Term, Softmax Function, Logits, Proper Probabilities, Logistic Regression, Vector Notation (X), Weight Notation (W), Bias Notation (b), Score to Probability Conversion, Classification Task, Label Prediction, Prediction Scores\n","Saved row 29\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_instruction_full_output.xlsx\n","\n","Instruction-based (Groq) pipeline completed successfully.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_instruction_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.1-8b-instant/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["53c50f7903f745189a0dbd0b1447d6e5","258e1b3a242948388298b2719864e2aa","e034c897ff074502ab0f805962ac5ba8","ed0968ea97cc4022adbc7d11b17df91e","59ac45ec17eb442489074ca4c0db354e","e4f0db5407b047c2b1447ec627af2ac3","13a9d24a598545248e7eeedc9c48efce","98e4d053d6d340c293d894d87b9100d0","1c7f1aaead584c2781c3ec41fb93ccbb","995fd2c72a12477096d6f09e314d8a9f","bdb60737aeb640ad811c412647ae85a8","5df0164ed28a4a7db9a2ae955c974071","c7f728b690d44176acc47c2c9a6d1e04","68de1777efea4b8d8652281c71054e53","f7cd8976df8e4882885e5eb36c61eac8","c8ca043658004fe38173ff686e5b33ea","ba72518fe52a4f10a7703e5eea9908b7","559ad073775e4360bc06e7af16de597d","11caaf8410ea4f42b71ec243f852fb67","d23a001f04e54848b4ba70e44b231f1b","a8435fcbcfdf4e278ec73de550b3ec8c","25a81baad95b4f338b73cdbaa7570891","452136b4c424480f84b66202c8ead784","50fb7ca9ed164a4e8e254b47403f296a","1d98858d262d4211a04c16271f0a757f","5df869b7be3d40cd970d465538f3f411","61d5846a506c4415af1c86b2e1264e78","9131be4c4c3641838a6301ce38a7e4ec","611374e2915e4df99c4468eba64b98fa","558a5e1e8ac94ae8a817f0a6d4f476bf","7ce55835140c492ca40dfa51f8b1400d","c7dd168c9efc4c0a9218b42bbde406c8","8607b6c594cd4301aed29fa85afd4264","2c2d2a08d83d4c28bb47be5a77d29416","745dbf0b16754ed78469b743a9253690","6a69df13359f4baab8560ffdefe8cd6a","d5db6e735ccf4bd99e035494033c54e7","46b3072bd5f546f7849e3366afa6c5f9","050af71e2e704599a8714be9993283f1","71ef13b862974d5db0418b303ab56cf4","f905d6a3f4ac40db8c8c3b4bf8f862bf","c16255ea6ffa4d48a39ab060888c1971","871509879a324e1895102a016967f520","0ef9712d45fb4be3933cae2a0d2e5594","3506c7dc07f44095a5f42ff16f009e6e","45b71a23107249c3b33fd76f5b3a688a","6fb64721a1c64c82ab6cd39512b80b36","a6092d3049784570acfdef7755dfce9d","9d21dcab50644d42bf8d88788cafeefa","9fc042211c8b4869b93c3529a143adac","c7e18c0eeba24a2b821084ccf79df32d","6991f93c7bcc4521a273ca5ab50a6888","a4a25533fefa45f4bfefdc303e127b59","3be7439b912d4ff3b09f1de605a2e808","4cdcc5518a1a44859b21b03a903ae784","fd878591e0cb4b7d82c35a315a315801","cc065cd0e83c457b8daf84bfbabd32b1","74de656b0a304bd3b4a5d422bf67eee0","3d254963ddd94d7caf799349639792bb","7b8d4813e9ed45f1a0099304e1c0a59f","f3043f76599245f680c97f2f6302306c","22368d00902b4cd8bdd4f3200ec838f6","7588b961e57d488895bcd2a3b5449c3f","1a0574dfcf48480f8505209bb8e607a3","e7ff908169364b208d5486d7bf1dde9c","27b3bad8665e4b4ca983b9a22df8b835"]},"id":"xr-65oi4DXem","executionInfo":{"status":"ok","timestamp":1763705948550,"user_tz":-330,"elapsed":170413,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"94f92725-95ff-4ae9-a09d-3a6878f6c303"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_instruction_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c50f7903f745189a0dbd0b1447d6e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5df0164ed28a4a7db9a2ae955c974071"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"452136b4c424480f84b66202c8ead784"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c2d2a08d83d4c28bb47be5a77d29416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3506c7dc07f44095a5f42ff16f009e6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd878591e0cb4b7d82c35a315a315801"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3024\n","  - BLEU: 0.0702\n","  - BERTScore F1: 0.8880\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9000\n","  - Jaccard Index: 0.3282\n","  - Micro F1: 0.4629\n","  - Macro F1: 0.3961\n","  - Weighted F1: 0.4319\n","\n","Q&A Generation:\n","  - BLEU: 0.0287\n","  - Diversity: 0.7467\n","  - Answerability: 0.6467\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5567\n","  - Recall@10: 0.2227\n","  - F1@10: 0.3181\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.1-8b-instant/evaluation_final.json\n"]}]}]}