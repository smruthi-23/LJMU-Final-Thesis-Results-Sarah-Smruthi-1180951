{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPO+zLYFfEHp+NrZ7fxRvp7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"41f263b925b54829943f6182ffd59b54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a924caea8abb41fcaf07b192974ccd88","IPY_MODEL_9b67435e7591429d9bad588f9aed4885","IPY_MODEL_8e0db39422e947cca9cbeaec4f74fd3e"],"layout":"IPY_MODEL_70cd4490a310498c88bb3a373a5fef82"}},"a924caea8abb41fcaf07b192974ccd88":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d90c80799ba245f8aa4f88c91b8a52c7","placeholder":"​","style":"IPY_MODEL_f7d455f67b1d4ca4afd43fbaea3870df","value":"tokenizer_config.json: 100%"}},"9b67435e7591429d9bad588f9aed4885":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47cec66701404bbba788ec29951e8e0e","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4f8356d0c6804e3ea3bb0bc27eae5cbd","value":25}},"8e0db39422e947cca9cbeaec4f74fd3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9dd6f41e3bcc417cbdb616580aaec49b","placeholder":"​","style":"IPY_MODEL_3008bc2a91a7403da5cf900be214316e","value":" 25.0/25.0 [00:00&lt;00:00, 1.25kB/s]"}},"70cd4490a310498c88bb3a373a5fef82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d90c80799ba245f8aa4f88c91b8a52c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7d455f67b1d4ca4afd43fbaea3870df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47cec66701404bbba788ec29951e8e0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f8356d0c6804e3ea3bb0bc27eae5cbd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9dd6f41e3bcc417cbdb616580aaec49b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3008bc2a91a7403da5cf900be214316e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3badbf3448814eb085a0cae5cb576969":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_322d66b2975e48b59815b5aa764ec912","IPY_MODEL_29a9d076fb1e468d9f064100ee56eaff","IPY_MODEL_406e7a2293304dbcb5ed5a2f0dabcfae"],"layout":"IPY_MODEL_ca35addcc66841be802ddd8c91ee7a89"}},"322d66b2975e48b59815b5aa764ec912":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bc7a9ee29bc4b699865dd2e80045b8a","placeholder":"​","style":"IPY_MODEL_3c64e329e60145cda0b3b3add9b08f9a","value":"config.json: 100%"}},"29a9d076fb1e468d9f064100ee56eaff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d47d8881945c453894304187eb7c6c24","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7de3a2091f9946439df7d2415263d64f","value":482}},"406e7a2293304dbcb5ed5a2f0dabcfae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f6e8c6a22b048adbed71c6a0ab4c0b5","placeholder":"​","style":"IPY_MODEL_329cc85ed2ae40afb1355395927cc650","value":" 482/482 [00:00&lt;00:00, 7.73kB/s]"}},"ca35addcc66841be802ddd8c91ee7a89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bc7a9ee29bc4b699865dd2e80045b8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c64e329e60145cda0b3b3add9b08f9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d47d8881945c453894304187eb7c6c24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7de3a2091f9946439df7d2415263d64f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f6e8c6a22b048adbed71c6a0ab4c0b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"329cc85ed2ae40afb1355395927cc650":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0560a89435e54c42a1cdb8d7f1e029a9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83a1cc328d254fe3a8f055ed085163d2","IPY_MODEL_33b53857981c4b76a8fa7057c8310ac3","IPY_MODEL_ba66de31a8a642cdaecb12d3d9f9a570"],"layout":"IPY_MODEL_b30fb535e586408daf8b665db5ba0fa0"}},"83a1cc328d254fe3a8f055ed085163d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e90a6ff06d0942ee9ac25e23db9e2952","placeholder":"​","style":"IPY_MODEL_5943ee36f9304473b9991b36678f17a4","value":"vocab.json: 100%"}},"33b53857981c4b76a8fa7057c8310ac3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_411992e17baf4a44960176ab39e327f8","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ead8b57225ba4685bd103f4cb6848c6a","value":898823}},"ba66de31a8a642cdaecb12d3d9f9a570":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c9d758e5e3f4e96a4e1ba611cc0ff13","placeholder":"​","style":"IPY_MODEL_79d8b8e8277e4c74bc64523b230c8b46","value":" 899k/899k [00:00&lt;00:00, 8.78MB/s]"}},"b30fb535e586408daf8b665db5ba0fa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e90a6ff06d0942ee9ac25e23db9e2952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5943ee36f9304473b9991b36678f17a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"411992e17baf4a44960176ab39e327f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ead8b57225ba4685bd103f4cb6848c6a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c9d758e5e3f4e96a4e1ba611cc0ff13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79d8b8e8277e4c74bc64523b230c8b46":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0828eac3f6346858d0d3157fc3fe4d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2e5226ebcbc74bd5be82a853da51674c","IPY_MODEL_ece78841acca41cd9524945e9bca66e2","IPY_MODEL_83bb9348b87a4661b40fdaf6edaf8dcf"],"layout":"IPY_MODEL_21f376e573624ee890e127a5de68d5eb"}},"2e5226ebcbc74bd5be82a853da51674c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86de845fe1564fb688e17f144e455303","placeholder":"​","style":"IPY_MODEL_29cb46cf1a634dacbb9195d58e4db51c","value":"merges.txt: 100%"}},"ece78841acca41cd9524945e9bca66e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5619ab85fdb4474a94c4a2349152f8e","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca11102b2a2f4a6a84017d5da32baa69","value":456318}},"83bb9348b87a4661b40fdaf6edaf8dcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46f5bda9a34245f190b9c7f4b6ab6ece","placeholder":"​","style":"IPY_MODEL_1c1af764078c45ca9f609a9c48d8d5d3","value":" 456k/456k [00:00&lt;00:00, 26.3MB/s]"}},"21f376e573624ee890e127a5de68d5eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86de845fe1564fb688e17f144e455303":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29cb46cf1a634dacbb9195d58e4db51c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5619ab85fdb4474a94c4a2349152f8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca11102b2a2f4a6a84017d5da32baa69":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"46f5bda9a34245f190b9c7f4b6ab6ece":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c1af764078c45ca9f609a9c48d8d5d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eac8838d2b0a44e28bd932c0f8a53c48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_285853b826f1417eba63a2e1b476980f","IPY_MODEL_2e0941b7cb30472590dacb4528ad09c2","IPY_MODEL_d95cd9a035324bb6b0028a9e13124e1d"],"layout":"IPY_MODEL_ef8d0ad7d1c643c3b800fba1025929b5"}},"285853b826f1417eba63a2e1b476980f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ab44fc4da23446d99fbfe264a57c4b3","placeholder":"​","style":"IPY_MODEL_69b110f4ddf94ac8b3dda3488403b30c","value":"tokenizer.json: 100%"}},"2e0941b7cb30472590dacb4528ad09c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_917d0be098bf4da6865580735dd13d37","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16aee070f4fd4f728883f9e7a98739f6","value":1355863}},"d95cd9a035324bb6b0028a9e13124e1d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b474266701ce4b5c87e7558c503c680b","placeholder":"​","style":"IPY_MODEL_ce8621c5d0614450b997c8a33bb34cbb","value":" 1.36M/1.36M [00:00&lt;00:00, 35.1MB/s]"}},"ef8d0ad7d1c643c3b800fba1025929b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ab44fc4da23446d99fbfe264a57c4b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69b110f4ddf94ac8b3dda3488403b30c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"917d0be098bf4da6865580735dd13d37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16aee070f4fd4f728883f9e7a98739f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b474266701ce4b5c87e7558c503c680b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce8621c5d0614450b997c8a33bb34cbb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab7af517e8a04aadb8647019e35bcd52":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ae773874eaf4fe3a9ed175a1c1fcb15","IPY_MODEL_9f51a6ae7c3945e1b1194a8ca1ebb477","IPY_MODEL_63cd2fc1a0fb45929d121108edc077a9"],"layout":"IPY_MODEL_dcea63fd352e47f5bb73eb5510364db3"}},"0ae773874eaf4fe3a9ed175a1c1fcb15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9b834a5c9494d00a2e826916a9edb3b","placeholder":"​","style":"IPY_MODEL_7881ae7ec2864466b470d06e2db814ba","value":"model.safetensors: 100%"}},"9f51a6ae7c3945e1b1194a8ca1ebb477":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ea0218d42c3414c95d92a311577b2d0","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fc78fea047d14ce6bad2303e3108efe5","value":1421700479}},"63cd2fc1a0fb45929d121108edc077a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9260687374334094acb6faa0fd7f7135","placeholder":"​","style":"IPY_MODEL_3e9adc83f05c40a9a500d1e59983fc11","value":" 1.42G/1.42G [00:15&lt;00:00, 242MB/s]"}},"dcea63fd352e47f5bb73eb5510364db3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9b834a5c9494d00a2e826916a9edb3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7881ae7ec2864466b470d06e2db814ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ea0218d42c3414c95d92a311577b2d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc78fea047d14ce6bad2303e3108efe5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9260687374334094acb6faa0fd7f7135":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e9adc83f05c40a9a500d1e59983fc11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DR5_oOO1qY--","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763214153091,"user_tz":-330,"elapsed":22711,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"12e68c3a-b5b2-4ef2-99b0-6c9cf5dbd35c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.10.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f03decc35cdae195642bcf34b0d9f2cab51184b764a9858290daca01d9fbf330\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"VPPcwOw1uv2e","executionInfo":{"status":"ok","timestamp":1763214153130,"user_tz":-330,"elapsed":29,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"377eb676-6ae9-48cc-846f-4e34e1e14d32"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","import google.generativeai as genai\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-flash_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Gemini key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"gemini-2.5-flash\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 70       # seconds between calls (soft global wait)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GEMINI CALL WITH GLOBAL WAIT\n","#####################################################################\n","def gemini_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    cfg = {\"temperature\": temperature}\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = model.generate_content(prompt, generation_config=cfg)\n","            LAST_TS = time.time()\n","            return (getattr(resp, \"text\", \"\") or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Gemini call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Gemini failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASKS — INSTRUCTION-BASED PROMPTING\n","#####################################################################\n","\n","# 8.1 INSTRUCTION-BASED SUMMARISATION (hierarchical)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","Return ONLY a single-line JSON object:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = gemini_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip()\n","        if not chunk_summary:\n","            chunk_summary = out.strip()[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","Return ONLY a single-line JSON object:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = gemini_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 INSTRUCTION-BASED MULTI-LABEL TOPIC CLASSIFICATION\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide and the transcript chunk for details.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","Return ONLY a single-line JSON object:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    # keep unique, keep order, max 3, fallback Other\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 8.3 INSTRUCTION-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Generate five question–answer pairs based on the transcript content.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples discussed in the transcript.\n","\n","GUIDELINES:\n","• Create EXACTLY five (5) question–answer pairs.\n","• Each pair should begin with a different question type:\n","  1. What – factual or definitional\n","  2. Why – reasoning or purpose\n","  3. How – process or mechanism\n","  4. When – timing or condition\n","  5. Who – person, system, or entity\n","• Each answer must be directly supported by information in the transcript.\n","• Keep answers concise (maximum 25 words).\n","• Avoid generic or meta questions.\n","• Ensure all questions are technically relevant and educational.\n","\n","OUTPUT FORMAT:\n","Return ONLY a one-line JSON object:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}, ...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 INSTRUCTION-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list the core terminology, methods, or technical phrases that capture the essence of the discussion.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","• Exclude generic or non-technical words (e.g., \"video\", \"people\", \"lesson\").\n","• Capitalise each concept in Title Case.\n","• Do not repeat near-duplicates.\n","• Ensure the list is concise and domain-specific.\n","\n","OUTPUT FORMAT:\n","Return ONLY a one-line JSON object:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    cleaned = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based generation pipeline completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"ByGidCYEuv-p","executionInfo":{"status":"ok","timestamp":1763228490172,"user_tz":-330,"elapsed":14337038,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"c4a43468-0c40-461c-81a8-afe2b6b06b98"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash\n","Gemini key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement Learning with Human Feedback (RLHF) is a framework that integrates human input into the training of reinforcement learning algorithms, such as Q-learning or Proximal Policy Optimization. As demonstrated by an agent like Frank navigating a grid world, human feedback acts as a mentor, accelerating learning and guiding the agent towards more human-favored decisions. For practical applications like ChatGPT, RLHF involves two key stages: first, training a reward model by having humans rank multiple generated responses; and second, using this reward model alongside an algorithm such as Proximal Policy Optimization to fine-tune ChatGPT. The reward model scores responses, providing a crucial signal for iterative backpropagation, thereby enhancing ChatGPT's ability to generate high-quality, human-aligned answers.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Deep Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the primary purpose of the rewards model in the ChatGPT process?\n","A: Its primary purpose is to assess and score the quality of answers generated by ChatGPT.\n","Q: Why is human feedback provided to Frank during his learning process?\n","A: It allows Frank to learn faster and to give responses that are more human-favored.\n","Q: How do humans train the rewards model for ChatGPT?\n","A: Humans rank multiple unique answers from a pre-trained ChatGPT based on their quality (best to worst).\n","Q: When is human feedback provided to Frank in the grid world example?\n","A: Human feedback is provided while Frank is learning with a reinforcement learning algorithm.\n","Q: Who helps explain the concept of reinforcement learning through human feedback using a grid world?\n","A: Frank, a character in a grid world environment, helps explain the concept.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning With Human Feedback, Reinforcement Learning Algorithm, Proximal Policy Optimization, Reward Model, GPT Architecture, Fine-Tune Chat GPT, Grid World, Human Favored Responses, Accelerates The Learning Process, Back Propagation, Loss Function, Iterative Training Process\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial series explores Support Vector Machines (SVMs), focusing on the application of kernel functions for nonlinear classification and soft margins. It demonstrates how kernels modify the SVM framework, using CVXopt for educational purposes to directly observe kernel injection and solve the underlying quadratic programming problem: `1/2 x^T P x + q^T x` subject to `G x <= h` and `A x = b`. While `libsvm` is noted for practical implementations, the discussion covers initializing SVM objects with polynomial, linear, or Gaussian kernels and the 'C' parameter for hard or soft margins. The fitting process involves solving for alphas, support vectors, intercept, and W, enabling prediction by projecting data and returning the sign. Visualizations illustrate how kernels transform data for linear separation. The linear kernel uses a dot product, with `self.W` being zero, while other kernels require feature set translation and kernel application during prediction. Future topics include scikit-learn's `SupportVectorClassifier` and multi-class strategies.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What is the primary focus of this machine learning tutorial?\n","A: The tutorial focuses on working with CVX opt, applying kernels to Support Vector Machines, and visualizing their impact.\n","Q: Why is CVX opt used in this specific tutorial?\n","A: CVX opt is used to directly see the impact of a kernel and where it's injected and modifies the initial formal Support Vector Machine.\n","Q: How does the quadratic programming solver mentioned in the transcript work?\n","A: It minimizes `1/2 x^T P x + q^T x` subject to constraints `G x <= h` and `A x = b`.\n","Q: When is CVX opt considered useful according to the speaker?\n","A: It's useful in this case purely to see the direct impact of a kernel and its injection into the Support Vector Machine.\n","Q: Who is credited with the example code used in this tutorial?\n","A: The code is from Matthew Blondell's GitHub, with references to Christopher Bishop's pattern recognition and machine learning book.\n","\n","KEY CONCEPTS:\n"," CVX Opt, Kernels, Support Vector Machine, Lib SVM, Nonlinear Classification, Soft Margin, Pyit Learn, Quadratic Programming Solver, Quadratic Programming Objective Function, Quadratic Programming Constraints\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompts serve as fundamental inputs for prompt engineering models, providing essential context and constraints for large language models to generate desired text outputs. These inputs vary in complexity, categorized as questions, statements, or those with specific constraints. Effective prompts are characterized by their length, the specificity of language used, and any embedded context or constraints, which collectively dictate both the expected output and its desired execution. Precise prompting, exemplified by specifying a one-word answer or a specific word count for an essay, significantly refines the generated text's quality and adherence to requirements. A crucial process involves deconstructing prompts into their core components—identifying requirements, constraints, and specific language—to enhance understanding and systematically improve prompt design, thereby optimizing model interaction and output.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are prompts in prompt engineering?\n","A: Prompts are the inputs given to prompt engineering models, serving as the starting point for generating text outputs.\n","Q: Why is understanding prompts important for generating text outputs?\n","A: Understanding prompts is important because they provide the context and constraints for large language models to generate the required output.\n","Q: How does deconstructing a prompt help in understanding it?\n","A: Deconstructing a prompt involves breaking it down into individual components to better understand its key features and constraints.\n","Q: When is prompt engineering needed according to the example given?\n","A: Prompt engineering is needed when a model gives more information than desired, like providing context instead of a one-word answer.\n","Q: Who or what are examples of large language models that use prompts?\n","A: Examples of large language models that use prompts include ChatGPT, Google Bard, or any other large language model.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Prompts, Large Language Models, Text Outputs, Type Of Prompts, Prompt Constraints, Key Features Of Prompts, Prompt Context, Specific Language, Deconstruction Of Prompts, Pre-trained Models, Accurate Output\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents are autonomous problem solvers capable of independent decision-making, distinct from instruction-bound chains. They leverage specialized \"tools,\" such as calculators or search engines, to complete tasks. A key methodology for constructing these agents is the ReAct pattern (Reasoning + Acting), designed to mimic human thought processes. This pattern involves a cyclical loop where the LLM \"thinks\" about a problem, determines an \"action\" potentially using a tool, provides necessary \"action input\" arguments, and then \"observes\" the tool's output. This think-action-observation cycle repeats until a satisfactory solution is achieved, effectively combining the LLM's reasoning capabilities with external specialized tools.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Langraph']\n","\n","Q&A:\n"," Q: What is the primary role of an AI agent in the AI world?\n","A: Agents are the problem solvers of the Artificial Intelligence world, capable of making autonomous decisions.\n","Q: Why is the React agent pattern considered one of the best known patterns for building AI agents?\n","A: It mimics how human beings think by cycling through thinking, acting, and observing to solve problems.\n","Q: How does an AI agent decide whether to use a tool during the React pattern's action phase?\n","A: The LLM decides if it can answer by itself or if it should use a particular tool.\n","Q: When does the control flow return to LangChain during the execution of a tool by an agent?\n","A: The control flow returns to LangChain after the LLM decides on proper arguments for a tool.\n","Q: Who is responsible for executing the tool with the arguments suggested by the LLM?\n","A: LangChain (our system) executes the tool and returns the output back to the LLM.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence Agents, Autonomous Decisions, Agent Tools, React Agent Pattern, Reasoning Plus Acting, Think Action Observation Loop, Action Input Arguments, Control Flow Management, Large Language Model (LLM), LangChain Framework, LangGraph\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," This educational segment meticulously traces the reflection agent system, detailing its integration with LangChain and LangSmith for iterative refinement in generating highly refined and potentially viral tweets. A comprehensive understanding of its internal workings and collaborative dynamics is achieved through tracing on smith.chain, which visualizes the system's execution. Setting up LangSmith involves configuring an API key and environment variables to enable streaming and capturing operational traces, where a \"run\" represents a full application execution and \"traces\" provide granular details of individual components. The core workflow involves a \"generation agent\" creating an initial tweet, which a \"reflect agent\" critiques with improvement suggestions. This feedback loop drives multiple revision iterations, ultimately yielding a sophisticated tweet, effectively showcasing the power of reflection agents for complex, iterative tasks.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI', 'Generative AI']\n","\n","Q&A:\n"," Q: What system will be traced in this section?\n","A: The reflection agent system that was built will be traced.\n","Q: Why is the reflection agent system being traced?\n","A: To understand what is happening and how both systems work together to deliver the final refined viral tweet.\n","Q: How will the tracing of the system be initiated?\n","A: By going to the website smith.chain.\n","Q: When will the understanding of system interaction be achieved?\n","A: After tracing the reflection agent system to understand what is happening where.\n","Q: Who built the reflection agent system being traced?\n","A: The speaker and their team built the reflection agent system.\n","\n","KEY CONCEPTS:\n"," Reflection Agent System, Refined Viral Tweet, Smith.chain\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," This transcript details the integration of LangChain's chat models with OpenAI APIs, beginning with the installation of `langchain-openai` and `python-dotenv`. It outlines initializing `ChatOpenAI` with models like `gpt-4o` or `gpt-3` and managing `OPENAI_API_KEY` via a `.env` file loaded using `load_dotenv`. API calls are made using `llm.invoke()`, demonstrated by a query for the square root of 49. The process addresses common issues such as missing API keys and insufficient account balance, advising users to top up their OpenAI account. While full API responses contain extensive metadata, the summary emphasizes extracting the 'content' property for practical use. The discussion then transitions from simple single-turn interactions to a more sophisticated approach, where the LLM is provided with an entire conversation history, including human and AI exchanges, to enable the generation of more informed and contextually relevant responses.\n","\n","TOPICS:\n"," ['LangChain', 'Generative AI', 'Python Programming']\n","\n","Q&A:\n"," Q: What package needs to be installed to work with LangChain's OpenAI chat models?\n","A: The `langchain-openai` package needs to be installed to work with LangChain's OpenAI chat models.\n","Q: Why is the `gpt-4o` model chosen for the example in the transcript?\n","A: The `gpt-4o` model is chosen because it is the latest and most advanced model released by OpenAI.\n","Q: How is the `ChatOpenAI` class imported into the file?\n","A: The `ChatOpenAI` class is imported using `from langchain_openai import ChatOpenAI`.\n","Q: When might a user choose to use the `gpt-3` model instead of `gpt-4o`?\n","A: A user might choose `gpt-3` if they are short on cash, as `gpt-4o` can be more expensive.\n","Q: Who released the `gpt-4o` model mentioned in the transcript?\n","A: The `gpt-4o` model was released by OpenAI.\n","\n","KEY CONCEPTS:\n"," LangChain Chat Models, OpenAI APIs, Chat OpenAI Class, L-Chain-OpenAI Package, Package Installation, Module Import, Model Initialization, Large Language Model (LLM), Keyword Parameter, GPT-4o Model, GPT-3 Model\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," This summary details Python's `sort()` method behavior when applied to lists containing various data types. For lists of strings, `sort()` establishes a specific order, prioritizing words beginning with uppercase letters, which are sorted alphabetically, followed by words starting with lowercase letters, also sorted alphabetically. Reversing this sort maintains the uppercase-then-lowercase precedence while inverting the alphabetical order within each group. When `sort()` is applied to mixed lists containing both strings and numbers, it first places all numerical elements at the beginning of the list, sorted numerically. Subsequently, the string elements are sorted alphabetically. Reversing the sort on such mixed-type lists results in the numbers being positioned at the end of the list.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What is the default sorting behavior for a Python list containing strings with mixed cases?\n","A: It sorts words starting with uppercase letters alphabetically first, then words starting with lowercase letters alphabetically.\n","Q: Why might a user need to make sure all strings are lowercase or uppercase before sorting?\n","A: To achieve a specific sort order if the default mixed-case sorting behavior is not desired.\n","Q: How does Python's `sort` method handle a list that contains both strings and numbers?\n","A: It puts the numbers first in the sorted list, followed by the strings.\n","Q: When sorting a list with mixed-case strings, when do words starting with lowercase letters appear?\n","A: They appear after all words starting with uppercase letters have been sorted alphabetically.\n","Q: Who or what system determines the sorting order for lists containing mixed types like strings and numbers?\n","A: Python's built-in `sort` method determines this order.\n","\n","KEY CONCEPTS:\n"," Python Lists, List Of Strings, Sort Method, Alphabetical Sorting, Uppercase Character Sorting, Lowercase Character Sorting, Reverse Sorting Order, Mixed Data Type Lists, List Element Insertion, Numeric Sorting Precedence\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," This discussion explores optimizing human-AI collaboration in decision-making, particularly for fraud detection systems overwhelmed by false positives. It introduces a method comparing human and AI performance using success rate versus confidence score graphs. AI excels at extreme confidence levels but struggles with uncertainty, where humans often outperform by integrating context. Augmented intelligence, combining human and AI decisions, offers superior success rates but requires careful design to mitigate human cognitive biases like automation bias. Strategies include having humans form initial judgments before reviewing AI recommendations and using optional AI displays. Counterintuitively, presenting AI accuracy percentages can reduce adoption. Ultimately, quantifying the optimal decision-maker (human, AI, or augmented) for specific scenarios is crucial, emphasizing that effective AI assistance presentation is key to enhancing overall decision-making outcomes.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is the primary function of the fraud detection system described in the transcript?\n","A: The fraud detection system generates alerts for potentially fraudulent transactions, which financial analysts then review.\n","Q: Why might an AI system be introduced into a fraud detection process?\n","A: An AI system could alleviate the workload of financial analysts who are overwhelmed by a high percentage of false positive alerts.\n","Q: How is the performance of an AI system typically visualized in the context of fraud detection?\n","A: Performance is visualized on a graph where the Y-axis tracks success rate and the X-axis represents the confidence score of a prediction.\n","Q: When is a human likely to outperform an AI in making a decision, according to the transcript?\n","A: A human is likely to do a better job than an AI when the confidence level of a prediction is around 50 percent.\n","Q: Who is responsible for reviewing alerts generated by the fraud detection system?\n","A: Skilled financial analysts are responsible for reviewing each alert generated by the fraud detection system.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Fraud Detection System, False Positives, Success Rate, Confidence Score, AI Performance Curve, Human Performance Curves, Holistic Curves, Human Bias, Fraudulent Transactions, Prediction\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Demitrius from Google Cloud AI introduced new Vertex AI APIs designed to accelerate and enhance enterprise generative AI application development. These APIs specifically address critical challenges such as grounding applications to reliable data for accurate responses and overcoming common technical hurdles. Key launches include a Document Understanding API for complex document processing, enhanced Embedding and Vector Search APIs to improve retrieval, a Ranking API for refining search results, a Grounded Generation API providing cited answers, and a Check Grounding API for robust fact-checking. Leveraging Google's extensive expertise, these solutions ensure high quality and unique performance, facilitating seamless integration into developer workflows and popular frameworks like LangChain.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary purpose of the Document Understanding API?\n","A: It processes large, complex documents to improve retrieval and answer generation quality for generative applications.\n","Q: Why is grounding a key aspect for generative applications in enterprises?\n","A: To ensure applications reliably access the right Enterprise data, producing accurate and consistent responses.\n","Q: How does the Ranking API improve the quality of answers produced by an LLM model?\n","A: It checks how good retrieved results are at answering a question, bubbling up the most relevant information for the LLM.\n","Q: When is the Check Grounding API used?\n","A: It is used to fact-check a statement, whether human-produced or by a language model, against provided evidence.\n","Q: Who uses the underlying technology for Google's Vector Search for planet-scale applications?\n","A: Google, YouTube, and Google Ads use this technology for their planet-scale applications.\n","\n","KEY CONCEPTS:\n"," Vertex Artificial Intelligence (AI), Generative Applications, Grounding, Document Understanding API, Embedding API, Vector Search, Hybrid Search, Ranking API, Grounded Generation API, Check Grounding API, LangChain, LlamaIndex\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The Singular Value Decomposition (SVD) fundamentally relies on unitary matrices U and V, which are indispensable in various scientific and engineering applications. These unitary matrices are crucial because they preserve the angles and lengths of vectors, effectively acting as rotations, a principle also seen in the Fourier transform. For complex-valued data, the standard transpose is appropriately replaced by the complex conjugate transpose. Geometrically, the matrix X in SVD transforms a unit sphere from its row space into an ellipsoid within its column space. The singular values precisely dictate the lengths of this ellipsoid's principal axes, while the left singular vectors define its orientation. Unlike the unitary U and V, the matrix X itself is not unitary, which accounts for the deformation and stretching of the sphere into an ellipsoid. Ultimately, these unitary transformations are vital for maintaining the fundamental geometric structure of data, such as angles and lengths, throughout coordinate transformations.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the primary geometric property preserved by unitary matrices?\n","A: Unitary matrices preserve the angles between any two vectors and the lengths of vectors in the vector space they transform.\n","Q: Why are unitary transformations considered important for understanding data geometry?\n","A: They preserve the basic geometric structure, like angles and lengths, which is key for understanding how vectors are related in a vector space.\n","Q: How does a matrix X transform a sphere of unit vectors in its row space?\n","A: Multiplying a sphere of unit vectors by X maps it into an ellipsoid in the column space, with principal axes lengths given by singular values.\n","Q: When is the complex conjugate transpose (X star) used instead of the regular transpose for a matrix X?\n","A: The complex conjugate transpose is used when the data in matrix X is complex valued, rather than real valued.\n","Q: Who or what determines the orientation of the ellipsoid formed when a sphere of unit vectors is mapped through matrix X?\n","A: The orientation of the ellipsoid is determined by the left singular vectors (U) of the matrix X.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Unitary Matrices, Economy Size SVD, Unitary Transformations, Fourier Transform, Inner Product Preservation, Complex Conjugate Transpose, Geometric Interpretation, Ellipsoid Mapping, Singular Values, Left Singular Vectors\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," This educational segment focuses on developing generative AI applications using Google Gemini Pro 1.5, a highly capable multimodal model. Gemini Pro 1.5 stands out with its impressive 1 million token context window, enabling it to process extensive data like 402-page PDFs, 1-hour videos, or 700k words. It unifies text and image processing, eliminating the need for separate models, and demonstrates advanced capabilities such as abstract detail recognition and accurate citation. The practical implementation involves obtaining a free API key, installing the `google-generativeai` library, and configuring the `model/gemini-1.5-pro-latest`. Developers utilize `model.generate_content` for text-based queries and directly handle multimodal inputs via `model.generate(image)`, even combining textual prompts with images to generate content like blog posts. This advancement simplifies multimodal tasks, encouraging developers to leverage its features for diverse AI applications, underscoring Google's innovation in large language models.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is Google Gemini Pro 1.5 described as in the transcript?\n","A: It is described as a multimodal model capable of working with both text and images.\n","Q: Why is Google Gemini Pro 1.5 considered a multimodal model?\n","A: It is multimodal because it will be able to work with both text and images.\n","Q: How will the speaker demonstrate Google Gemini Pro 1.5 in the video?\n","A: By showing a demo video, running code for hands-on applications, and explaining API key creation and usage.\n","Q: When will the hands-on application part of the video begin?\n","A: After the 1-minute demo video, the speaker will show hands-on applications by running code.\n","Q: Who is the speaker creating this video about Google Gemini Pro 1.5?\n","A: The speaker creating this video is Krishn.\n","\n","KEY CONCEPTS:\n"," Generative AI Powered Application, Google Gemini Pro 1.5, Multimodal Model, Text And Image Processing, API Key Creation, Long Context Understanding, Experimental Feature, End-to-End Projects, Hands-On Application, Code Execution\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," This section outlines the comprehensive evaluation and testing methodology for prompt engineering models. Key performance metrics include perplexity, which quantifies a language model's predictive capability (with lower values indicating better performance), accuracy for assessing response correctness, and human evaluation to rate the quality of generated outputs. An illustrative example demonstrates the application of an `evaluate_translation` function on a dataset to compute these metrics. Following initial evaluation, models undergo a crucial debugging and improvement phase, involving error analysis of generated responses and subsequent fine-tuning. Continuous testing on diverse datasets and tasks, potentially leveraging visualization tools or cross-validation, is essential to rigorously assess their generalization capabilities and ensure sustained performance over time.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What are the commonly used matrices for evaluating prompt engineering models?\n","A: The commonly used matrices include perplexity, accuracy, and human evaluation.\n","Q: Why is it important to test prompt engineering models on different datasets or tasks?\n","A: It determines the model's ability to generalize on new or unseen data.\n","Q: How does perplexity measure the performance of a language model?\n","A: Perplexity measures how well a language model predicts a sequence of words; lower perplexity indicates better performance.\n","Q: When should prompt engineering models be evaluated and tested?\n","A: Evaluation and testing is an ongoing process, continuing as the model is used and generates responses.\n","Q: Who rates the quality of responses during human evaluation?\n","A: Human evaluation involves having humans rate the quality of the responses.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering Models, Evaluation Matrices, Perplexity, Accuracy, Human Evaluation, Debugging And Improvement, Model Testing, Pre-Trained Large Language Models, Fine-Tuning Models, Model Generalization, Cross Validation\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The discussion outlines the progression of Artificial Intelligence capabilities, starting with Generative AI, which utilizes Large Language Models (LLMs) trained on extensive datasets to create new content like text or images, primarily for Q&A tasks, albeit with knowledge cutoffs. Building upon this, an AI Agent integrates LLMs with external tools (APIs) and memory, enabling real-time information access and autonomous execution of narrow, specific tasks, such as flight bookings. The most advanced stage, Agentic AI, involves one or more AI agents autonomously tackling complex, multi-step goals. These sophisticated systems demonstrate multi-step reasoning, planning, and coordination, often leveraging multiple tools and even other agents to achieve intricate objectives, showcasing a significant increase in task complexity and autonomy beyond basic Generative AI.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Langraph']\n","\n","Q&A:\n"," Q: What is generative Artificial Intelligence (AI)?\n","A: Generative AI is AI that creates new content like text, images, or video based on patterns learned from existing data.\n","Q: Why can an AI agent answer a flight price question that a simple LLM cannot?\n","A: An AI agent can access external APIs (like Xedia or MakeMyTrip) to fetch real-time, latest information, unlike a simple LLM with a knowledge cutoff.\n","Q: How does an AI agent handle a complex travel request involving weather and flights?\n","A: It accesses weather APIs to find sunny days, then uses travel APIs to search flights within budget, potentially suggesting hotels and taxis.\n","Q: When does an AI system demonstrate the highest level of autonomous decision-making?\n","A: Autonomous decision-making is highest when it comes to Agentic Artificial Intelligence (AI) systems, which handle multi-step goals with planning and coordination.\n","Q: Who defines agentic systems into five levels, according to the speaker?\n","A: The speaker's friend, who is the creator of the Agno framework, defines agentic systems into five levels.\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence (AI), Large Language Model (LLM), Knowledge Cutoff Date, Tool Access, Artificial Intelligence (AI) Agent, Autonomous Decision Making, Multi-step Reasoning, Multi-step Planning, Agentic Artificial Intelligence (AI), AI Frameworks, Agent Memory, Complex Goal Autonomy\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion introduces covariance as a vital concept in data analysis, quantifying the linear relationship between two random variables, such as house size and price. It is formally defined by the formula Cov(X, Y) = (1/n) Σ[(Xi - μX)(Yi - μY)], which represents the average of the product of deviations from their respective means. A notable property is that the covariance of a variable with itself equals its variance. Interpreting its value, a positive covariance indicates that variables tend to increase or decrease together, while a negative covariance suggests an inverse relationship. However, covariance's utility is limited as it only reveals the direction of the relationship, not its strength or magnitude, a limitation that necessitates the use of the Pearson correlation coefficient for a comprehensive understanding.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary purpose of covariance in data analysis?\n","A: Covariance helps quantify the relationship between two random variables in a dataset, like the size and price of a house.\n","Q: Why is covariance considered an important topic in data pre-processing?\n","A: It is important because it helps quantify the relationship between features or random variables in a dataset.\n","Q: How does the covariance formula determine if a relationship is positive or negative?\n","A: It multiplies the deviations of X and Y from their means; consistent signs yield positive, opposite signs yield negative.\n","Q: When does covariance yield a positive value?\n","A: Covariance yields a positive value when both random variables (X and Y) increase or decrease together.\n","Q: Who or what is discussed as a solution to overcome the disadvantage of covariance?\n","A: The Pearson correlation coefficient is discussed as a solution to overcome covariance's disadvantage.\n","\n","KEY CONCEPTS:\n"," Covariance, Data Pre-processing, Data Analysis, Random Variables, Covariance Equation, Variance Of X, Mean Of Random Variable, Positive Covariance, Negative Covariance, Pearson Correlation Coefficient, Feature Relationship Quantification\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning (RL) focuses on enabling an agent to learn an optimal policy that maximizes a numerical reward signal, specifically the cumulative reward over time, within defined episodic or continuous tasks. Agents achieve this through a trial-and-error process, interacting with an environment by making decisions based on observations, taking actions, and receiving feedback via reward signals. This iterative learning involves exploration, observing resulting states and rewards, and subsequently updating the agent's policy. The nature of rewards varies by task: episodic tasks, such as Tic-Tac-Toe, utilize discrete rewards (e.g., +1 for winning), while continuous tasks, like stock market trading, aim for profit maximization, often parameterized by reward functions reflecting profit/loss or risk-adjusted metrics like the Sharpe ratio.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the objective of reinforcement learning?\n","A: The objective is to learn the optimal policy that maximizes a numerical reward signal.\n","Q: Why does the reward signal provide feedback to the agent?\n","A: It provides feedback to the agent about the quality of its actions.\n","Q: How does an agent learn in reinforcement learning?\n","A: An agent learns through trial and error, exploring the environment, taking actions, observing states and rewards, and updating its policy.\n","Q: When does an agent typically receive a reward in an episodic task like Tic-Tac-Toe?\n","A: The agent receives the reward at the end of the game, either for winning or losing.\n","Q: Who interacts with the environment and makes decisions in reinforcement learning?\n","A: The agent interacts with the environment and makes decisions based on observations.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning Problem, Optimal Policy, Expected Cumulative Reward, Agent Environment Interaction, Episodic Task, Continuous Task, Reward Function, Trial And Error Learning, Value Based Method, Policy Based Method, State Action Pair\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," Python dictionaries are fundamental data structures composed of key-value pairs, known as items, enclosed in curly brackets. Keys, which must be immutable types like strings or numbers, uniquely map to values that can be of any data type. Dictionaries are essential for mapping one item to another and are created either directly or by zipping two lists together using `dict(zip(keys, values))`. Core operations include accessing values via `dictionary[key]`, modifying items by assigning new values, and deleting items using `del dictionary[key]`. Utility functions like `d.items()`, `d.keys()`, `d.values()`, and `len()` provide insights into dictionary content and size. Their ability to efficiently store and retrieve mapped data makes them crucial for various data science applications.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is a Python dictionary primarily composed of?\n","A: A Python dictionary consists of key-value pairs, referred to as items, declared within curly brackets.\n","Q: Why are dictionaries considered a useful structure in Python?\n","A: Dictionaries are useful for mapping one item to another, like associating stock prices (values) with 'open', 'high', and 'close' keys.\n","Q: How can you create a dictionary from two separate lists in Python?\n","A: Create a dictionary from two lists by zipping them together using `zip()`, then converting the result with the `dict()` function.\n","Q: When would you use the `del` function with a dictionary?\n","A: You would use the `del` function when you want to remove a specific key-value pair, or entry, from a dictionary.\n","Q: What system or library works very well with Python dictionaries for data science?\n","A: Python dictionaries work very well with the pandas library, especially for those pursuing data science applications.\n","\n","KEY CONCEPTS:\n"," Python Dictionary, Key Value Pairs, Immutable Key, Dict Function, Zip Function, Dictionary Items Method, Dictionary Keys Method, Dictionary Values Method, Accessing Dictionary Values, Changing Dictionary Values, Deleting Dictionary Entries, Length Of A Dictionary\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence (AI) is a critical tool for enhancing organizational security posture and proactively addressing emerging threats, significantly reducing data breach identification and containment times by an average of 108 days. Specifically, User Behavior Analytics (UBA), an AI and machine learning subset, is crucial for detecting and responding to costly insider threats, which average $4 million per incident. UBA establishes normal user and peer group patterns over a minimum of seven days using rules and machine learning to identify behavioral anomalies. When integrated with SIEM solutions like IBM Security QRadar, the UBA app provides dashboards for prioritizing risks, monitoring employees, and viewing alerts detailing individual behavior, offenses, and Indicators of Compromise. QRadar further leverages AI to map MITRE ATT&CK tactics, accelerate investigations with natural language insights and visual graphs, and is refined by human feedback, enabling a shift towards proactive defense.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is a key finding for organizations extensively using AI and automation regarding data breaches?\n","A: They experienced 108 fewer days on average to identify and contain a data breach compared to those that didn't.\n","Q: Why is Artificial Intelligence considered to have the potential to make a big difference in security?\n","A: AI has the potential to significantly improve an organization's security posture and help stay ahead of emerging threats.\n","Q: How can User Behavior Analytics (UBA) with AI and machine learning help organizations?\n","A: UBA with AI and machine learning can help organizations detect and respond to Insider threats quickly and precisely.\n","Q: When was the IBM report on the cost of a data breach published, which highlighted AI's impact?\n","A: The report mentioned, IBM's cost of a data breach report 2023, was published in 2023.\n","Q: Who conducted the survey that found the average cost of an Insider threat for an organization?\n","A: IBM's cost of a data breach report 2023, based on a survey of over 500 organizations, provided this finding.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Security Posture, Emerging Threats, Data Breach, AI And Automation, User Behavior Analytics (UBA), Machine Learning, Insider Threats, Threat Detection And Response, Faster Containment\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta has announced Llama 3, an open-source large language model available in 8 billion and 70 billion parameter versions, both pre-trained and instruction-tuned. Trained on 24K GPU clusters using over 50 trillion tokens and supporting an 8K context length, Llama 3 demonstrates state-of-the-art performance. It excels in language nuances, contextual understanding, and complex tasks such as translation, reasoning, and code generation. Benchmarks indicate it significantly outperforms Llama 2 and competes strongly with proprietary LLMs, featuring refined post-training processes that reduce false refusals and improve response alignment. Comprehensive responsibility guidelines for Llama 3 encompass model-level development steps and system-level additions like Meta Llama Guard for transparency. Access is provided through the Meta Llama website, Hugging Face, and Kaggle, allowing users to download model weights in Transformers or Native Llama 3 formats for local inference, with additional examples available on GitHub.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Generative AI', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the name of the speaker?\n","A: The speaker's name is Krishak.\n","Q: Why does Krishak welcome people to his YouTube channel?\n","A: He is welcoming viewers to his personal YouTube channel.\n","Q: How does Krishak greet his audience at the beginning?\n","A: Krishak greets his audience by saying \"hello\" and \"welcome to my YouTube channel.\"\n","Q: When is Krishak recording or speaking according to the transcript?\n","A: Krishak is speaking at 2 a.m.\n","Q: Who owns the YouTube channel mentioned in the transcript?\n","A: The YouTube channel is owned by Krishak.\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," This transcript segment outlines the practical implementation of a decision boundary within a machine learning context using Python, specifically utilizing the scikit-learn (sklearn) library. The instructional approach emphasizes navigating scikit-learn's comprehensive documentation, often accessed via Google, to effectively understand and apply its various functions. The primary machine learning algorithm explored is Naive Bayes, with a particular focus on its Gaussian variant, which is crucial for classification tasks. The overarching objective is to empower learners to confidently replicate and adapt the provided classifier code, previously demonstrated to employ Gaussian Naive Bayes from the scikit-learn library, thereby fostering a deeper understanding of model construction and application.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What Python library will be used extensively in this lesson?\n","A: The Python library that will be used a lot in this lesson is scikit-learn, often abbreviated as sklearn.\n","Q: Why does the speaker use Google at the start of the process?\n","A: The speaker uses Google to help find and use the documentation of the scikit-learn library to understand its functions.\n","Q: How did the speaker find information about the Naive Bayes algorithm?\n","A: The speaker searched Google for \"sklearn Naive Bayes\" to find documentation, including derivation and use cases.\n","Q: When will the audience be able to write the Python code themselves?\n","A: The audience will be able to write the Python code themselves by the end of the next video or two.\n","Q: Who (or what system/library) provides the Naive Bayes algorithm the speaker is using?\n","A: The scikit-learn (sklearn) Python library provides the Naive Bayes algorithm, specifically Gaussian Naive Bayes, that the speaker used.\n","\n","KEY CONCEPTS:\n"," Decision Boundary, Python Code, Python Library, Scikit-learn, Naive Bayes, Naive Bayes Formula, Gaussian Naive Bayes, Classifier, Library Documentation, Machine Learning Algorithm\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion focuses on Gaussian and Log-Normal distributions, defining Gaussian as a symmetrical bell curve characterized by specific empirical rule percentages, exemplified by human height. In contrast, the Log-Normal distribution describes variables whose logarithms are normally distributed, resulting in a right-skewed curve, often seen in data like income or product review lengths. A fundamental understanding of these distributions is vital for effective data preprocessing. This involves converting diverse data distributions into a standard normal distribution (mean 0, standard deviation 1) using techniques such as log transformation and standardization. This process, termed log normalization, is critical for significantly enhancing machine learning model accuracy by ensuring features are on a comparable scale.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the defining characteristic of a log normal distribution?\n","A: A random variable belongs to a log normal distribution if its logarithm is normally distributed.\n","Q: Why is it important to learn about different data distributions like Gaussian and log normal?\n","A: Understanding distributions helps in scaling data for machine learning models, leading to higher accuracy.\n","Q: How can a log normal distribution be converted into a standard normal distribution?\n","A: First, take the log of the values to get a Gaussian distribution, then apply the formula (X_i - mu) / sigma.\n","Q: When might a dataset typically follow a log normal distribution?\n","A: Datasets like income of people, product comment lengths, or data for sentiment analysis often follow a log normal distribution.\n","Q: Who benefits from understanding data distributions for scaling purposes in machine learning?\n","A: Machine learning practitioners and data scientists benefit, as it helps improve model accuracy by scaling data to a common range.\n","\n","KEY CONCEPTS:\n"," Gaussian Distribution, Empirical Formula, Standard Deviation, Bell Curve, Log Normal Distribution, Random Variable, Standard Normal Distribution, Standard Scaler, Log Normalization, Model Accuracy\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This educational series outlines an end-to-end deep learning project focused on developing a React Native mobile application for potato disease detection, aiming to mitigate significant economic losses for farmers. The project utilizes Convolutional Neural Networks (CNNs) for supervised image classification, identifying healthy plants or specific diseases like early and late blight. The technical pipeline involves collecting and augmenting diverse image datasets using `tf.data`, building and exporting CNN models with TensorFlow, and optimizing them via quantization for `TF Lite` conversion to reduce size. For deployment, `TF Serving` hosts model versions accessed through a `FastAPI` backend, with `Google Cloud Functions` on GCP facilitating the mobile app's operation. This comprehensive training emphasizes practical skills for aspiring ML engineers, encouraging project customization and application of deep learning for real-world agricultural challenges.\n","\n","TOPICS:\n"," ['Deep Learning', 'Mlops', 'Data Science']\n","\n","Q&A:\n"," Q: What are the two common diseases affecting potato plants mentioned in the transcript?\n","A: The two common diseases are early blight and late blight.\n","Q: Why is early detection and accurate identification of potato plant diseases important for farmers?\n","A: Early detection and accurate identification can save waste and prevent significant economic losses for farmers.\n","Q: How will the mobile application help farmers identify potato plant diseases?\n","A: Farmers take a picture; the app, using deep learning, identifies if the plant is healthy or diseased.\n","Q: When can farmers prevent economic losses due to potato plant diseases?\n","A: Farmers can prevent economic losses if they detect diseases early and apply appropriate treatment.\n","Q: Who is developing the mobile application to help farmers detect potato plant diseases?\n","A: AtliQ Agriculture, an AI company, is developing the mobile application.\n","\n","KEY CONCEPTS:\n"," End-to-End Deep Learning Project, Data Collection, Model Building, Machine Learning Ops, TF Serving, Fast API, Google Cloud Platform, Google Cloud Functions, React Native, Convolutional Neural Network, Artificial Intelligence\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," ```json\n","{\"generated_summary\":\"The transcript outlines the increasing levels of autonomy in LLM applications, evolving from zero-autonomy deterministic code to advanced agentic systems. It starts with basic single LLM calls for one-shot tasks, limited by their inability to handle complexity. \"Chains\" introduce sequential, human-defined LLM calls, where specialized models perform distinct steps but lack flexibility due to fixed paths. \"Routers\" enhance autonomy by allowing an LLM to dynamically select processing paths, though without memory for past interactions. The highest level, \"State Machines\" or \"Agents,\" features LLM-controlled flow, iterative loops, memory, human-in-the-loop capabilities, and adaptive learning. This enables complex, multi-step tasks and refinement, distinguishing these \"agent-executed\" systems, where the LLM actively manages control flow, from more \"human-driven\" c\n","\n","TOPICS:\n"," ['Agentic AI', 'Langraph', 'LangChain']\n","\n","Q&A:\n"," Q: What is the primary characteristic of \"code\" in LLM applications regarding autonomy?\n","A: Code has zero autonomy and is 100% deterministic, with everything hard-coded.\n","Q: Why do chains offer a smarter system than a single LLM call?\n","A: Chains break big problems into smaller, manageable pieces, using multiple specialists for different steps.\n","Q: How does a router LLM application determine the next step?\n","A: The AI itself decides what steps to take next, routing the request to the right tool or chain.\n","Q: When is an LLM application considered an \"agent\"?\n","A: An LLM application is considered an agent whenever the control flow is controlled by the LLM itself.\n","Q: Who defines the fixed sequences and steps in a \"chains\" LLM application?\n","A: The fixed sequences and steps in a \"chains\" LLM application are defined by the human.\n","\n","KEY CONCEPTS:\n"," Levels Of Autonomy, Hardcoded Rules, LLM Call, Chains, Router, Control Flow, State Machine, Agent, Human In Loop, Multi-Agent Systems, Adaptive Learning, Agent Tools\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This section comprehensively covers advanced prompt engineering, detailing techniques applicable to diverse prompt types, including text, image, and audio. It thoroughly explores advanced fine-tuning methods for pre-trained large language models, such as multitask learning for simultaneous training on multiple tasks and distillation, which involves training a smaller model to efficiently mimic a larger one. Key data preprocessing steps like tokenization and normalization are emphasized for maintaining high data quality. The discussion also addresses the deployment of these prompt engineering models using frameworks like TensorFlow Serving or Flask, alongside crucial ethical considerations regarding bias, fairness, and privacy in model development and application.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Deep Learning', 'Generative AI']\n","\n","Q&A:\n"," Q: What is multitask learning in prompt engineering?\n","A: It involves training a model on multiple tasks simultaneously to learn robust representations that generalize to different users.\n","Q: Why is distillation used in prompt engineering?\n","A: Distillation trains a smaller model to mimic a larger one, making the smaller model more efficient and faster.\n","Q: How do tokenization and normalization help in data preprocessing for prompt engineering?\n","A: Tokenization breaks text into smaller units for accurate understanding. Normalization converts text to a standard format, like lowercase, to avoid confusion.\n","Q: When should a prompt engineering model be deployed in production?\n","A: A prompt engineering model should be deployed in production once it is built, to make it accessible to users.\n","Q: Who or what can be used to deploy prompt engineering models in production?\n","A: TensorFlow Serving or Flask can be used to deploy prompt engineering models in a production environment.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering Models In Production, Ethical Considerations In Prompt Engineering, Fine Tuning Pre-trained Large Language Models, Multitask Learning, Model Distillation, Self-Supervised Learning Techniques, Data Pre-processing And Cleaning, Tokenization, Text Normalization, Data Augmentation, Image-Based Prompts, Audio-Based Prompts\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," This lecture demonstrates the application of Singular Value Decomposition (SVD) and eigenfaces for facial clustering and classification. The methodology involves collecting, aligning, and converting images to grayscale, then reshaping them into high-dimensional vectors. An average face is computed and subtracted before applying SVD to derive eigenfaces. Images are subsequently projected into a lower-dimensional eigenface space for clustering. Initial experiments with \"eigen heroes\" like Arnold Schwarzenegger and Sylvester Stallone showed good separation and correct classification of test images. Further analysis revealed better separation between Taylor Swift and Stallone, but surprisingly, significant overlap between Taylor Swift and Arnold Schwarzenegger, attributed to shared features such as skin tone. This outcome underscores potential limitations of naive correlation-based classification methods.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence', 'Data Science']\n","\n","Q&A:\n"," Q: What is the primary technique demonstrated in this lecture using action hero faces?\n","A: The lecture demonstrates Singular Value Decomposition (SVD) and eigenfaces, applied to cluster different people in eigenface space.\n","Q: Why were the images of action heroes cropped and aligned before processing?\n","A: Images were cropped and aligned so faces were in the same place and filled the same box for consistent processing.\n","Q: How are the eigenfaces (or eigen heroes) computed in this example?\n","A: Eigenfaces are computed by performing an economy SVD on the B matrix, where the columns of U represent the eigen heroes.\n","Q: When did Facebook's face classification algorithms achieve human-level accuracy?\n","A: Facebook's algorithms achieved human-level accuracy when they started inferring 3D geometry of human heads from 2D images.\n","Q: Who were the two initial action heroes used in the eigenfaces example?\n","A: The two initial action heroes used were Arnold Schwarzenegger and Sylvester Stallone.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Eigen Face Space, Image Classification, Average Face Subtraction, Image Column Vector, Feature Space Projection, Training Data Set, Deep Neural Network Architectures, Three-Dimensional Geometry Inference\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a crucial framework that addresses a key limitation of Large Language Models (LLMs) like ChatGPT: their inherent inability to interact with the real world. While LLMs excel at reasoning and planning, they cannot directly perform actions such as making bookings or sending emails. LangChain serves as a bridging framework, connecting LLMs to external services, APIs, and databases, thereby enabling real-world interaction. It is the most popular framework for building LLM-powered applications, offering flexibility to swap different LLM models without altering core application code. This significantly enhances AI capabilities, allowing models to perform practical tasks like accessing booking APIs, querying private company databases for customer inquiries, sending emails, browsing public web resources, and scraping websites. Essentially, LangChain empowers AI with autonomous action within external environments, expanding their utility beyond mere computational intelligence.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: LangChain is the most popular framework that helps build applications using Large Language Models (LLMs) by acting as a bridge to the real world.\n","Q: Why is a framework like LangChain needed for LLM applications?\n","A: A framework like LangChain is needed because LLMs alone cannot interact with the real world, access APIs, or send emails.\n","Q: How does LangChain enable LLMs to interact with the real world?\n","A: LangChain acts as a bridge, allowing LLMs to access real-world APIs like flight and restaurant booking services.\n","Q: When can a developer easily switch out an LLM model in an application?\n","A: A developer can easily switch out an LLM model when the application is built using the LangChain framework.\n","Q: Who or what provides the reasoning ability in an application built with LangChain?\n","A: The Large Language Models (LLMs) provide the reasoning ability, acting as the \"brains\" of the application.\n","\n","KEY CONCEPTS:\n"," LangChain Framework, Large Language Models (LLMs), LLM Limitations, Real World Interaction, External APIs, Database Communication, LLM Application Development, Reasoning Ability, Model Interchangeability, Application Bridge\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residual analysis is a crucial method for improving time series forecasting. Residuals, defined as the difference between a model's fitted values and actual time series values, are distinct from forecast errors and are essential for diagnosing model performance and identifying inconsistencies. Ideal residuals should exhibit no autocorrelation or partial autocorrelation and have a zero mean, indicating an unbiased forecast that has captured all available information. The Ljung-Box test and autocorrelation/partial autocorrelation function plots are key tools for assessing these properties, while a histogram assesses bias. An example using a Holt-Winters model on air passenger data revealed significant correlation in residuals via the Ljung-Box test (p-values < 0.05), suggesting missed information. A slight negative mean in the residual histogram also indicated a minor negative forecasting bias.\n","\n","TOPICS:\n"," ['Time Series', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: Residuals are the difference between the fitted value (y-hat) and the actual value (y) of the time series.\n","Q: Why is analyzing residuals important for forecasting methods?\n","A: It allows diagnosis of model performance and helps detect trends or inconsistencies that can be improved upon.\n","Q: How should ideal residuals behave to indicate a good model fit?\n","A: Ideal residuals should have no autocorrelation and their mean should be zero, indicating no bias.\n","Q: When is the Ljung-Box test used in residual analysis?\n","A: The Ljung-Box test is used to quantify if there is indeed correlation present within the residuals.\n","Q: Who is the speaker introducing this time series crash course video?\n","A: The speaker is Eagle, a data scientist living in London.\n","\n","KEY CONCEPTS:\n"," Residual Analysis, Forecasting Methods, Residuals, Fitted Value, Actual Value, Autocorrelation, Partial Autocorrelation, Young Box Test, Zero Mean Residuals, Biased Forecast, Holt Winters Model, Test Train Split\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This transcript details the construction of an AI agent capable of interacting with databases using SQL knowledge derived from large language models. The architecture integrates LangGraph for a ReAct agent, a Next.js frontend, watsonx.ai for model execution (Mistral Large), and an in-memory SQLite database. The development process involved setting up the Next.js UI with React components for input and message display, configuring watsonx.ai credentials, and managing conversational state. A crucial step was defining and seeding the SQLite database with `customer` and `order` tables. An `execute` function was then established in `database.ts` and integrated as a `GetFromDB` tool within the LangChain agent in `actions.ts`. The system prompt was carefully engineered to guide the LLM in generating precise SQLite queries, ensuring proper tool usage and field quoting. Demonstrations showcased the agent's ability to generate SQL jokes, count customers, and perform complex multi-table joins. The importance of implementing guardrails for secure and controlled database interaction was also highlighted.\n","\n","TOPICS:\n"," ['Agentic AI', 'Langraph', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the primary goal of the AI agent being built in this video?\n","A: The primary goal is to build an AI agent that can talk to a database using its SQL knowledge. (20 words)\n","Q: Why are large language models suitable for building a database-interacting AI agent?\n","A: Most large language models have been trained on code, including SQL, which provides the necessary knowledge. (19 words)\n","Q: How is the boilerplate Next.js project set up in VS Code?\n","A: It's set up by running the `create-next-app@latest` CLI command and answering configuration questions like using TypeScript and Tailwind. (22 words)\n","Q: When can the Next.js application be started after the CLI setup?\n","A: The application can be started after the CLI finishes and the user moves into the Text2SQL agent directory. (19 words)\n","Q: Who or what provides the models running for the AI agent?\n","A: The models running for the AI agent are provided by watsonx.ai. (13 words)\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence Agent, Large Language Models, SQL, LangGraph, ReAct Agent, Next.js, watsonx.ai, In-Memory Database, SQLite, TypeScript, Tailwind, Text2SQL Agent\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a specialized field within Natural Language Processing (NLP) focused on developing models capable of generating high-quality text outputs in response to specific prompts. These advanced models are built upon fine-tuned pre-trained large language models, such as GPT or BERT, leveraging their sophisticated understanding of language. A primary advantage of prompt engineering lies in its ability to produce more accurate, coherent, and contextually appropriate text compared to traditional rule-based or keyword-based methods, proving crucial for diverse applications like chatbots and language translation. Nevertheless, prompt engineering models face challenges, including difficulties with complex or ambiguous prompts and the potential to generate biased or inaccurate information. This course provides a comprehensive introduction to the discipline, thoroughly covering prompt engineering fundamentals, effective prompt analysis techniques, and a detailed examination of its significant benefits and inherent limitations.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: It's a specialized field in NLP focused on building models that generate high-quality text outputs in response to prompts or inputs.\n","Q: Why is prompt engineering considered important?\n","A: It generates text outputs that are more accurate, coherent, and contextually appropriate than traditional rule-based or keyword-based methods.\n","Q: How do prompt engineering models generate text outputs?\n","A: They are based on pre-trained large language models like GPT or Google Bard, fine-tuned for specific tasks and inputs.\n","Q: When might prompt engineering models struggle or generate inaccurate outputs?\n","A: They may struggle with complex/ambiguous prompts or generate biased/inaccurate outputs due to underlying data or model architecture.\n","Q: Who are some examples of pre-trained large language models used in prompt engineering?\n","A: Examples include OpenAI GPT, Google Bard, and Hugging Face Transformers.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Prompt Engineering Models, Pre-Trained Large Language Models, Fine-Tuning, OpenAI GPT, Hugging Face Transformers, Rule-Based Approaches, Keyword-Based Approaches, Model Architecture, Prompt Analysis\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is a prominent value-based reinforcement learning (RL) method where agents learn to map situations to actions to maximize a numerical reward signal. Unlike policy-based methods, Q-learning focuses on learning the state-action value function, or Q-values, which quantify the desirability of being in a given state and taking a particular action, typically represented in a Q-table. The learning process involves an agent exploring an environment, taking actions based on a behavior policy, transitioning between states, receiving rewards, and iteratively updating Q-values. This update uses the Bellman equation to calculate an observed Q-value, which is then compared to the current expected Q-value to determine the Temporal Difference (TD) error. This error is subsequently used in a gradient-like update rule, incorporating a learning rate, to adjust the Q-table entries. This iterative process, repeated over multiple episodes, allows Q-values to stabilize, defining the \"target policy\" for optimal action selection. Q-learning is characterized as an \"off-policy\" algorithm because it separates the behavior policy used for exploration from the target policy derived from the optimal Q-table.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Agentic AI', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary objective of reinforcement learning?\n","A: The primary objective is learning what to do—how to map situations to actions—to maximize a numerical reward signal.\n","Q: Why is the state-action value function (Q-value) important for Q-learning?\n","A: Q-learning is interested in learning the state-action value function because it quantifies how good it is to be in a state and take an action.\n","Q: How does a value-based reinforcement learning method determine an optimal policy?\n","A: Value-based methods determine a value function that quantifies total reward, and then use this function to determine the optimal policy.\n","Q: When does an agent use an exploration policy in the grid world example?\n","A: An agent uses an exploration policy when it takes an action based on random chance, rather than choosing a state with a high Q-value.\n","Q: Who or what defines a recursive relationship between Q-values?\n","A: The Bellman equation defines a recursive relationship between Q-values, used to calculate the observed Q-value.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Q-Learning, Machine Learning Paradigms, Supervised Learning, Unsupervised Learning, Value Based Methods, Policy Based Methods, Optimal Policy, State Action Value Function, Bellman Equation, Discount Factor, Exploration Policy\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," Training a logistic classifier, a type of linear classifier, involves applying a linear function—specifically, matrix multiplication—to input data (X) using learned weights (W) and a bias (b) to generate raw prediction scores. The core machine learning objective is to optimize these W and b parameters to achieve highly accurate predictions. For single-label classification, these raw prediction scores, also known as logits, are subsequently converted into probabilities via a softmax function (S). The softmax function is crucial as it normalizes these scores, ensuring that the output probabilities sum to one, with higher initial scores directly correlating to higher probabilities for their corresponding classes, ultimately aiming for the correct class to have a probability approaching one.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is a linear classifier that applies a linear function to inputs, like image pixels, to generate predictions.\n","Q: Why is machine learning involved in training a logistic classifier?\n","A: Machine learning is used to find optimal values for the weights (W) and bias (b) that perform well in generating predictions.\n","Q: How does a logistic classifier generate predictions from inputs?\n","A: It applies a linear function, a matrix multiply, taking inputs (X) and multiplying them with a weight matrix (W) to generate predictions.\n","Q: When are probabilities generated by the softmax function considered \"proper probabilities\"?\n","A: Probabilities are proper when they sum to 1, are large for large scores, and small for comparatively smaller scores.\n","Q: Who or what are denoted by X, W, and b in the context of a logistic classifier?\n","A: X denotes the inputs, W denotes the weights of the matrix, and b denotes the biased term.\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Classifier, Linear Function, Matrix Multiply, Input Vector, Output Class, Weights, Bias Term, Model Training, Softmax Function, Logistic Regression, Logits\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n","\n","Instruction-based generation pipeline completed (NO evaluation).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["41f263b925b54829943f6182ffd59b54","a924caea8abb41fcaf07b192974ccd88","9b67435e7591429d9bad588f9aed4885","8e0db39422e947cca9cbeaec4f74fd3e","70cd4490a310498c88bb3a373a5fef82","d90c80799ba245f8aa4f88c91b8a52c7","f7d455f67b1d4ca4afd43fbaea3870df","47cec66701404bbba788ec29951e8e0e","4f8356d0c6804e3ea3bb0bc27eae5cbd","9dd6f41e3bcc417cbdb616580aaec49b","3008bc2a91a7403da5cf900be214316e","3badbf3448814eb085a0cae5cb576969","322d66b2975e48b59815b5aa764ec912","29a9d076fb1e468d9f064100ee56eaff","406e7a2293304dbcb5ed5a2f0dabcfae","ca35addcc66841be802ddd8c91ee7a89","9bc7a9ee29bc4b699865dd2e80045b8a","3c64e329e60145cda0b3b3add9b08f9a","d47d8881945c453894304187eb7c6c24","7de3a2091f9946439df7d2415263d64f","4f6e8c6a22b048adbed71c6a0ab4c0b5","329cc85ed2ae40afb1355395927cc650","0560a89435e54c42a1cdb8d7f1e029a9","83a1cc328d254fe3a8f055ed085163d2","33b53857981c4b76a8fa7057c8310ac3","ba66de31a8a642cdaecb12d3d9f9a570","b30fb535e586408daf8b665db5ba0fa0","e90a6ff06d0942ee9ac25e23db9e2952","5943ee36f9304473b9991b36678f17a4","411992e17baf4a44960176ab39e327f8","ead8b57225ba4685bd103f4cb6848c6a","8c9d758e5e3f4e96a4e1ba611cc0ff13","79d8b8e8277e4c74bc64523b230c8b46","e0828eac3f6346858d0d3157fc3fe4d5","2e5226ebcbc74bd5be82a853da51674c","ece78841acca41cd9524945e9bca66e2","83bb9348b87a4661b40fdaf6edaf8dcf","21f376e573624ee890e127a5de68d5eb","86de845fe1564fb688e17f144e455303","29cb46cf1a634dacbb9195d58e4db51c","e5619ab85fdb4474a94c4a2349152f8e","ca11102b2a2f4a6a84017d5da32baa69","46f5bda9a34245f190b9c7f4b6ab6ece","1c1af764078c45ca9f609a9c48d8d5d3","eac8838d2b0a44e28bd932c0f8a53c48","285853b826f1417eba63a2e1b476980f","2e0941b7cb30472590dacb4528ad09c2","d95cd9a035324bb6b0028a9e13124e1d","ef8d0ad7d1c643c3b800fba1025929b5","1ab44fc4da23446d99fbfe264a57c4b3","69b110f4ddf94ac8b3dda3488403b30c","917d0be098bf4da6865580735dd13d37","16aee070f4fd4f728883f9e7a98739f6","b474266701ce4b5c87e7558c503c680b","ce8621c5d0614450b997c8a33bb34cbb","ab7af517e8a04aadb8647019e35bcd52","0ae773874eaf4fe3a9ed175a1c1fcb15","9f51a6ae7c3945e1b1194a8ca1ebb477","63cd2fc1a0fb45929d121108edc077a9","dcea63fd352e47f5bb73eb5510364db3","b9b834a5c9494d00a2e826916a9edb3b","7881ae7ec2864466b470d06e2db814ba","2ea0218d42c3414c95d92a311577b2d0","fc78fea047d14ce6bad2303e3108efe5","9260687374334094acb6faa0fd7f7135","3e9adc83f05c40a9a500d1e59983fc11"]},"id":"joxlkw1fvYWe","executionInfo":{"status":"ok","timestamp":1763229111916,"user_tz":-330,"elapsed":147993,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"72d37f54-61a6-4bef-f2c8-ff6d15467249"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/gemini-2.5-flash_instruction_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41f263b925b54829943f6182ffd59b54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3badbf3448814eb085a0cae5cb576969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0560a89435e54c42a1cdb8d7f1e029a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0828eac3f6346858d0d3157fc3fe4d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eac8838d2b0a44e28bd932c0f8a53c48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7af517e8a04aadb8647019e35bcd52"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3653\n","  - BLEU: 0.1170\n","  - BERTScore F1: 0.8981\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.3813\n","  - Micro F1: 0.5175\n","  - Macro F1: 0.4905\n","  - Weighted F1: 0.5254\n","\n","Q&A Generation:\n","  - BLEU: 0.0236\n","  - Diversity: 0.7353\n","  - Answerability: 0.7467\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5967\n","  - Recall@10: 0.2387\n","  - F1@10: 0.3410\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/gemini-2.5-flash/evaluation_final.json\n"]}]}]}