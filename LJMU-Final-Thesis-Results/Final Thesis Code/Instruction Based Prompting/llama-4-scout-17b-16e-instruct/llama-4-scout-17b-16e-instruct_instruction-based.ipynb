{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPujG6xL13k6nKrGE2OrvWE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a9f2a3e2b08a435e8611a6e51a5a92bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc2f99dbad5345edb3a1971982b2ee68","IPY_MODEL_dda24fc23bc940de89a6bf42f7f075b9","IPY_MODEL_7e6066ac48ac43edb78264086eebe000"],"layout":"IPY_MODEL_b7280dcafc834d9ea6a3c489acd02683"}},"dc2f99dbad5345edb3a1971982b2ee68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e01c00f117c44fcb9503ba21d0a7f20f","placeholder":"​","style":"IPY_MODEL_283f5dab1c364337ab7d0a942d1e22f3","value":"tokenizer_config.json: 100%"}},"dda24fc23bc940de89a6bf42f7f075b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f395fa3b02e64e16b06c04dc1e6eb44b","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6a7b1f324d2f498fb8e3cf6907a54a18","value":25}},"7e6066ac48ac43edb78264086eebe000":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6a6c3b6129a4d6ebec116e911b7b0a2","placeholder":"​","style":"IPY_MODEL_ca2933f3b7634a1a99f9d69a3b30b19c","value":" 25.0/25.0 [00:00&lt;00:00, 3.10kB/s]"}},"b7280dcafc834d9ea6a3c489acd02683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e01c00f117c44fcb9503ba21d0a7f20f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"283f5dab1c364337ab7d0a942d1e22f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f395fa3b02e64e16b06c04dc1e6eb44b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a7b1f324d2f498fb8e3cf6907a54a18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6a6c3b6129a4d6ebec116e911b7b0a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca2933f3b7634a1a99f9d69a3b30b19c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3a27ea0bd9f434e93dd0439793069cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9606e6409e3143c4bfbe52df0d2dc208","IPY_MODEL_d106b0cd87a344b682708ac12a792e6d","IPY_MODEL_534d167e7d9147f587014d395cf5d74e"],"layout":"IPY_MODEL_84bf5e05c2774ab29d05089644ae916a"}},"9606e6409e3143c4bfbe52df0d2dc208":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e13bc3886e9346fb9422f7604e2f4752","placeholder":"​","style":"IPY_MODEL_69b5202c870f4db8bcc58c17ca03814a","value":"config.json: 100%"}},"d106b0cd87a344b682708ac12a792e6d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_996901eccf504d06aa8aedd0f257d658","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_95e5f2e0b9be4f0fa3a67712ae094581","value":482}},"534d167e7d9147f587014d395cf5d74e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ae992a609ed4ff4a0f39452a518aef4","placeholder":"​","style":"IPY_MODEL_689d9f8836e043118ba8ab71f92a89a0","value":" 482/482 [00:00&lt;00:00, 67.5kB/s]"}},"84bf5e05c2774ab29d05089644ae916a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e13bc3886e9346fb9422f7604e2f4752":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69b5202c870f4db8bcc58c17ca03814a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"996901eccf504d06aa8aedd0f257d658":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95e5f2e0b9be4f0fa3a67712ae094581":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ae992a609ed4ff4a0f39452a518aef4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"689d9f8836e043118ba8ab71f92a89a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"44018be0b5cb47099768ad021a2b29f5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52cfa15a029449dba029962576566424","IPY_MODEL_70915a5507a545408189d87e25bd61ac","IPY_MODEL_d758a104dcca48c5801b80c95c6ac766"],"layout":"IPY_MODEL_ba3af11c2fae42968b07feb090cceb1b"}},"52cfa15a029449dba029962576566424":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5236bf652ccf448fa544925811e47c1b","placeholder":"​","style":"IPY_MODEL_a6e9748051054f039364d76d5aef5858","value":"vocab.json: 100%"}},"70915a5507a545408189d87e25bd61ac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7f139cbeb334313a62cee5aeacf9886","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c280045ac752448096978578e14d90b9","value":898823}},"d758a104dcca48c5801b80c95c6ac766":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecb86368d5f348079ef95b4b8d7f2542","placeholder":"​","style":"IPY_MODEL_9d0c7d3541de4822af8af217a60df127","value":" 899k/899k [00:00&lt;00:00, 6.52MB/s]"}},"ba3af11c2fae42968b07feb090cceb1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5236bf652ccf448fa544925811e47c1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6e9748051054f039364d76d5aef5858":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7f139cbeb334313a62cee5aeacf9886":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c280045ac752448096978578e14d90b9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ecb86368d5f348079ef95b4b8d7f2542":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d0c7d3541de4822af8af217a60df127":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1ef18b6c555406793972e9cb178f404":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72dd07b912b843e18094098fec454cfb","IPY_MODEL_17cbd6ae6c84465c820286c4cac6efba","IPY_MODEL_c98ad4fb2a2c448d81e6a0cced02232a"],"layout":"IPY_MODEL_826d5f0dbced4dffaf050107798696e3"}},"72dd07b912b843e18094098fec454cfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d4b5036234e498fb90125a889be96ba","placeholder":"​","style":"IPY_MODEL_67bec66db81f48ae950a272033cab1a0","value":"merges.txt: 100%"}},"17cbd6ae6c84465c820286c4cac6efba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89f2584fa15e4a83931c50952ea87de5","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8b307f8f485a4947964c660f18528e44","value":456318}},"c98ad4fb2a2c448d81e6a0cced02232a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7ca379d3e484c85957e42ccbbd6e4dd","placeholder":"​","style":"IPY_MODEL_90cc018d9f514548a71de63e77a39bf8","value":" 456k/456k [00:00&lt;00:00, 40.8MB/s]"}},"826d5f0dbced4dffaf050107798696e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d4b5036234e498fb90125a889be96ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67bec66db81f48ae950a272033cab1a0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89f2584fa15e4a83931c50952ea87de5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b307f8f485a4947964c660f18528e44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d7ca379d3e484c85957e42ccbbd6e4dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90cc018d9f514548a71de63e77a39bf8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ebb8636f8304dafb60d8c4cd20841dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1282ca1fd37a4b60a25542aee5879d4f","IPY_MODEL_65a09105618a4d20babbd55d8f8a5ada","IPY_MODEL_ce1879042ebd40088999344427c0c5f3"],"layout":"IPY_MODEL_2f5d4cb5e2544d9095a7ac3c2443023d"}},"1282ca1fd37a4b60a25542aee5879d4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf2d12aaafe34600b06d8e298ffc21c6","placeholder":"​","style":"IPY_MODEL_00117b6a6acd4ec9b5236971cc8b8185","value":"tokenizer.json: 100%"}},"65a09105618a4d20babbd55d8f8a5ada":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_68e30d438fdd4b1f853a4fe5790d50c2","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9cd3db296e484d6aad79cd347a4c78b3","value":1355863}},"ce1879042ebd40088999344427c0c5f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65c5ca5391c948d48428d9e453bc5d4a","placeholder":"​","style":"IPY_MODEL_afb20128bf434fde9a1502321c5f798f","value":" 1.36M/1.36M [00:00&lt;00:00, 33.6MB/s]"}},"2f5d4cb5e2544d9095a7ac3c2443023d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf2d12aaafe34600b06d8e298ffc21c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00117b6a6acd4ec9b5236971cc8b8185":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68e30d438fdd4b1f853a4fe5790d50c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cd3db296e484d6aad79cd347a4c78b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65c5ca5391c948d48428d9e453bc5d4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afb20128bf434fde9a1502321c5f798f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23be720b3ddc43b48ace7a8443af7a9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ee344a6e5764f5fbb812c7930d3a3cf","IPY_MODEL_59298cbc39a4477b935231827f614d4d","IPY_MODEL_db37c8bdcc5c48479891abef5765764d"],"layout":"IPY_MODEL_703d4b830fcb483aa4c87cabfe3aa54f"}},"7ee344a6e5764f5fbb812c7930d3a3cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b3e60e4a7a04ad98540cea961a0120d","placeholder":"​","style":"IPY_MODEL_7f69fc91c7a448bc8c00c190f2f915df","value":"model.safetensors: 100%"}},"59298cbc39a4477b935231827f614d4d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_40ddb5c632ae435bbcccb4304ee844fa","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aa88369449304b82986770fa22118f53","value":1421700479}},"db37c8bdcc5c48479891abef5765764d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b0b6c8f61284c91a9baa8ac873b490c","placeholder":"​","style":"IPY_MODEL_109c20aab0754f6abf89c01695122b3d","value":" 1.42G/1.42G [00:23&lt;00:00, 106MB/s]"}},"703d4b830fcb483aa4c87cabfe3aa54f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b3e60e4a7a04ad98540cea961a0120d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f69fc91c7a448bc8c00c190f2f915df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40ddb5c632ae435bbcccb4304ee844fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa88369449304b82986770fa22118f53":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b0b6c8f61284c91a9baa8ac873b490c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"109c20aab0754f6abf89c01695122b3d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"sSXCc7Fy6bGs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763812021871,"user_tz":-330,"elapsed":15652,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"f110d9f3-7665-457a-d413-d58df293b7d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=86c25580db6975e90b6d3007d8b294836fef4b34d29292b4c11baba250750448\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"6VHwpssgYJmx","executionInfo":{"status":"ok","timestamp":1763812021903,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"eb27e661-3e1e-467e-dfd2-9a3af8e0848d"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED ONLY THIS\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-4-scout-17b-16e-instruct/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-4-scout-17b-16e-instruct_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25        # SAFE FOR FREE COLAB\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    s = text.find(\"{\")\n","    e = text.rfind(\"}\")\n","    if s == -1 or e == -1 or e <= s:\n","        return {}\n","    try:\n","        return json.loads(text[s:e+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (RATE LIMIT SAFE)\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"Groq failed after all retries\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. INSTRUCTION-BASED TASKS (PROMPTS UNCHANGED)\n","#####################################################################\n","\n","##########################\n","# 8.1 SUMMARISATION\n","##########################\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, 1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = groq_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip() or out[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(partial_summaries)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\").strip() or out2[:900]\n","\n","\n","##########################\n","# 8.2 TOPIC CLASSIFICATION\n","##########################\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...] }}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","\n","##########################\n","# 8.3 Q&A GENERATION\n","##########################\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Generate five question–answer pairs based on the transcript content.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples discussed in the transcript.\n","\n","GUIDELINES:\n","• Create EXACTLY five (5) question–answer pairs.\n","• Each pair should begin with a different question type:\n","  1. What – factual or definitional\n","  2. Why – reasoning or purpose\n","  3. How – process or mechanism\n","  4. When – timing or condition\n","  5. Who – person, system, or entity\n","• Each answer must be directly supported by information in the transcript.\n","• Keep answers concise (maximum 25 words).\n","• Avoid generic or meta questions.\n","• Ensure all questions are technically relevant and educational.\n","\n","OUTPUT FORMAT:\n","Return ONLY a one-line JSON object:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}, ...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    lines = []\n","    for qa in j.get(\"generated_questions\", []):\n","        q = qa.get(\"q\", \"\").strip()\n","        a = qa.get(\"a\", \"\").strip()\n","        if q: lines.append(f\"Q: {q}\")\n","        if a: lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines)\n","\n","\n","##########################\n","# 8.4 KEY CONCEPT EXTRACTION\n","##########################\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list core terminology, methods, or technical phrases.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases.\n","• Exclude generic words.\n","• Capitalise each concept.\n","• No duplicates.\n","\n","OUTPUT FORMAT:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    if not isinstance(concepts, list):\n","        concepts = []\n","\n","    return \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based (Groq) pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kpZoKTyVYNPY","executionInfo":{"status":"ok","timestamp":1763816769126,"user_tz":-330,"elapsed":4747164,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"c47fa359-8a41-4ccf-acde-24fabe63514d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-4-scout-17b-16e-instruct\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning with human feedback is an approach that integrates human input into the training process of a reinforcement learning algorithm, guiding and accelerating the learning process. This approach can be applied to various algorithms, including Q-learning, DQ learning, and proximal policy optimization. A notable application of this approach is seen in Chat GPT, where human feedback is provided through a rewards model that assesses and scores the quality of generated answers. The rewards model and proximal policy optimization are used in conjunction to fine-tune Chat GPT, enabling it to generate high-quality responses. By leveraging human feedback, this framework enhances the capabilities of reinforcement learning algorithms, allowing them to make more informed decisions and improve their overall performance.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the goal for Frank in the grid world?\n","A: To get to the +10 reward spot.\n","Q: Why is human feedback used in reinforcement learning?\n","A: To guide and accelerate the learning process.\n","Q: How does human feedback contribute to reinforcement learning as illustrated with Frank's grid world adventure?\n","A: It accelerates the learning process.\n","Q: When is human feedback used in the training process of a reinforcement learning algorithm?\n","A: During the training process to guide and accelerate learning.\n","Q: Who or what is used to assess and score the quality of answers generated by chat GPT?\n","A: The rewards model.\n","\n","KEY CONCEPTS:\n"," REINFORCEMENT LEARNING, HUMAN FEEDBACK, GRID WORLD, REINFORCEMENT LEARNING ALGORITHM, Q LEARNING, DEEP Q LEARNING, PROXIMAL POLICY OPTIMIZATION, REWARDS MODEL, GPT ARCHITECTURE, CHAT GPT, ITERATIVE TRAINING PROCESS\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial on machine learning explores the application of kernels to Support Vector Machines (SVMs) using CVX Opt, a quadratic programming solver. SVMs use different kernels, including linear, polynomial, and radial basis function (RBF) kernels, to find a hyperplane that maximally separates classes. The kernel type determines the transformation of data, and its choice significantly impacts the performance of SVMs. The tutorial demonstrates the use of linear, nonlinear, and soft margin classifiers on different datasets, showcasing the effects of kernel choice and regularization. The process involves solving a quadratic programming problem to find support vectors and using the learned model to make predictions. The discussion also touches on the use of scikit-learn and modern practices, as well as the issue of classifying multiple categories with SVMs. The tutorial aims to facilitate understanding of quadratic programming and kernel methods in SVMs.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is CVX opt used for in this tutorial?\n","A: To see the direct impact of a kernel on a Support Vector Machine.\n","Q: Why is CVX opt used in this tutorial?\n","A: For educational purposes to visualize kernels and their impact.\n","Q: How does the CVX opt solver work?\n","A: It minimizes 1/2 x^T P x + q^T x subject to constraints G x <= h and A x = b.\n","Q: When would you use CVX opt in practice?\n","A: Rarely, mostly for educational purposes; libsvm is preferred for Support Vector Machines.\n","Q: Who provided the example code used in this tutorial?\n","A: Matthew Blondell from his GitHub.\n","\n","KEY CONCEPTS:\n"," Machine Learning, Support Vector Machine, CVX Opt, Quadratic Programming, Kernel Methods, Nonlinear Visualization, Soft Margin, Pyit Learn, Lib SVM, Pattern Recognition, Machine Learning Tutorial, GitHub Repository\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," The foundation of prompt engineering lies in understanding prompts, which are inputs given to large language models to generate text outputs. These prompts vary in complexity, context, and type, including question prompts, statement prompts, and prompts with constraints. The key features of prompts, such as length, language, context, and constraints, play a crucial role in defining the output and its quality. Effective prompt engineering involves choosing the right prompt type and features to achieve the desired output. To optimize output, it is essential to deconstruct a prompt by breaking it down into individual components to comprehend its key features and constraints, ultimately enabling better comprehension and optimization of the output.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are prompts in the context of prompt engineering?\n","A: Inputs given to prompt engineering models to generate text outputs.\n","Q: Why is understanding prompts important?\n","A: To choose the right prompt for the desired output and impact complexity and quality.\n","Q: How can prompts be defined?\n","A: By what you expect and how you want it to be done, including constraints like tone, style, and specific requirements.\n","Q: When might deconstruction of a prompt be necessary?\n","A: When you already have a prompt and want to identify its elements, key features, and constraints.\n","Q: Who or what generates text outputs based on prompts?\n","A: Large language models like ChatGPT, Google Bard, or other similar models.\n","\n","KEY CONCEPTS:\n"," PROMPT ENGINEERING, LARGE LANGUAGE MODELS, PROMPTS, QUESTION PROMPTS, STATEMENT PROMPTS, MULTIPLE INPUTS, CONSTRAINTS, PROMPT FEATURES, PROMPT DECONSTRUCTION, PRE-TRAINED MODELS, PROMPT ENGINEERING MODELS, TEXT GENERATION\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence (AI) agents are problem solvers that can think autonomously and make decisions by utilizing tools, which are specific functions that enable them to complete tasks. The React Agent pattern, a popular approach to creating AI agents, enables agents to mimic human thinking through a cyclical process. This process involves agents thinking about a problem, deciding on an action, taking that action using a tool, and observing the result. Through repeated iterations of this cycle, agents can solve complex problems by refining their approach until a solution is found. This pattern allows AI agents to iteratively adapt and improve their problem-solving capabilities, making them effective in tackling a wide range of challenges.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Agentic AI', 'LangChain']\n","\n","Q&A:\n"," Q: What are agents in Artificial Intelligence (AI)?\n","A: Agents are problem solvers in AI that can think and make autonomous decisions.\n","Q: Why is the React Agent Pattern used in AI?\n","A: The React Agent Pattern is used to mimic human thinking, which involves thinking, acting, and observing to solve problems.\n","Q: How does the React Agent Pattern work?\n","A: The pattern involves a cycle of thinking, taking action, observing the result, and repeating until the problem is solved.\n","Q: When does the LLM observe the output of a tool?\n","A: The LLM observes the output of a tool after the control flow returns to the system and the tool is executed.\n","Q: Who or what is equipped with tools to make an agent?\n","A: An LLM (Large Language Model) is equipped with tools, such as API calls or Python functions, to make an agent.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), React Agent Pattern, Reasoning Plus Acting, Think Action Observation, Lang Chain, Large Language Model (LLM), Action Input, Tool Execution, API Calls, Agent Architecture\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The focus is on tracing a reflection agent system that integrates with another system to produce a refined viral tweet. The system, built using LangChain and LSmith, facilitates this integration to deliver a final output. The process involves generating, critiquing, and revising a tweet through multiple iterations, demonstrating the power of reflection agents in AI. An API key is generated and environment variables are set to integrate LSmith, allowing for tracing and streaming of operations. This exercise provides insight into the system's functionality, enabling in-depth analysis of AI operations and decision-making processes. The goal is to clarify the system's components and interactions, ultimately producing a refined viral tweet through the combined efforts of the two systems.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the purpose of tracing the reflection agent system?\n","A: To understand how both systems work together to deliver a refined viral tweet.\n","Q: Why is tracing the reflection agent system necessary?\n","A: To understand exactly what is happening where.\n","Q: How will tracing the reflection agent system help?\n","A: By understanding how both systems work together.\n","Q: When will tracing the reflection agent system be useful?\n","A: In delivering a final refined viral tweet.\n","Q: Who is guiding the tracing of the reflection agent system?\n","A: The speaker.\n","\n","KEY CONCEPTS:\n"," \n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The transcript focuses on working with Lang chain's chat models, specifically with Open AI APIs, to interact with large language models (LLMs). The process begins with installing the necessary package and initializing a chat model, specifically GPT 4, as an LLM. The LLM is used to make API calls, such as calculating the square root of 49, but requires an API key to function. To resolve access issues, an environment file (.env) is created to store the API key, and the `python-dotenv` package is installed. The discussion also highlights the importance of providing conversation history to the LLM to improve response accuracy, including sharing previous human and AI interactions to provide context. By doing so, the LLM can generate more informed responses, enhancing its ability to understand and respond to complex conversations.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the package that needs to be installed for Lang chains chat models?\n","A: L chain Das open aai\n","Q: Why is the latest model, gbt 40, used in this example?\n","A: Because it's the latest model released by open AI and is the most advanced\n","Q: How is the chat model imported in the file?\n","A: From longchain open Artificial Intelligence (AI), importing the chat open Artificial Intelligence (AI) class\n","Q: When might someone choose not to use the latest model, gbt 40?\n","A: When they are short on cash, as the latest models can be expensive\n","Q: Who or what provides the open AI apis used in this example?\n","A: Open Artificial Intelligence (AI)\n","\n","KEY CONCEPTS:\n"," \n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," The sorting of lists in Python containing strings and numbers was discussed. Python's sort method handles lists of strings by prioritizing uppercase letters over lowercase letters, but this can be resolved by converting lists to all lowercase or uppercase. When sorting lists with both strings and numbers, Python prioritizes numbers over strings, placing them first, followed by strings in alphabetical order. The sort method can effectively handle mixed-type lists, and its behavior can be reversed using the reverse method. Overall, understanding these sorting behaviors is essential for effective list management in Python, particularly when working with lists that contain a mix of data types.\n","\n","TOPICS:\n"," ['Python Programming', 'Other']\n","\n","Q&A:\n"," Q: What happens when you sort a list of strings with both uppercase and lowercase letters in Python?\n","A: Python sorts uppercase letters first, then lowercase letters, alphabetically.\n","Q: Why does Python sort lists with both uppercase and lowercase letters in a specific order?\n","A: It sorts them to prioritize uppercase over lowercase letters alphabetically.\n","Q: How does Python handle sorting a list that contains both strings and numbers?\n","A: It puts numbers first, then strings, when sorting the list.\n","Q: When does Python put numbers before strings in a sorted list?\n","A: When the list contains both numbers and strings, and is sorted.\n","Q: Who or what can handle lists that contain both strings and numbers when using the sort method in Python?\n","A: The sort method in Python can handle such lists.\n","\n","KEY CONCEPTS:\n"," Python Lists, Sort Method, Alphabetical Order, Case Sensitivity, Uppercase Letters, Lowercase Letters, List Sorting, Reverse Sorting, Type Handling, List Manipulation, String Sorting, Numeric Sorting\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The use of Artificial Intelligence (AI) in decision-making, particularly in fraud detection systems, is explored in relation to human decision-making. A graph is used to determine which alerts should be handled by AI and which by financial analysts, based on confidence scores and success rates. AI excels in high-certainty situations, while humans outperform AIs in complex or rare cases by incorporating additional context. Augmented intelligence, combining human and AI decision-making, offers a promising approach. However, its effectiveness is influenced by human cognitive bias, which can be mitigated by careful presentation of AI-generated information. The display of AI recommendations significantly impacts human decision-making. By combining human and AI capabilities and careful presentation of AI-generated information, decision-making outcomes can be improved. Effective collaboration between humans and AI algorithms can lead to better results, optimizing decision-making processes.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is the task objective in the fraud detection system example?\n","A: To determine which alerts should be handled by AI and which by financial analysts.\n","Q: Why do humans perform better than AI at a 50 percent confidence level?\n","A: Because humans make better decisions when AI is unsure.\n","Q: How do AI performance curves typically behave?\n","A: High success rates at low and high confidence scores, lower success rates when unsure.\n","Q: When do humans outperform AI in decision-making?\n","A: At a 50 percent confidence level, when AI is unsure.\n","Q: Who should handle alerts with a 50 percent confidence level?\n","A: A human, a skilled financial analyst.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Fraud Detection System, Holistic Curves, Human Bias, Confidence Score, Success Rate, Performance Curve, False Positives, Prediction Accuracy, Human Performance Curve, AI Performance Curve, Decision Making\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Building generative AI applications with Vertex AI presents challenges, particularly in ensuring reliability, accuracy, and access to enterprise data. To address these issues, Google Cloud AI is introducing six new Vertex AI APIs: document understanding, embedding, vector search, ranking, grounded generation, and check grounding APIs. These APIs leverage Google's AI and document processing expertise to enhance the quality and efficiency of generative AI applications. Designed to be simple, standalone, and stateless, they facilitate easy integration into developer workflows, enabling the creation of more reliable and accurate applications. The APIs aim to improve grounding and data access, ultimately streamlining the development of generative AI applications. This launch underscores Google's commitment to supporting developers in overcoming the challenges of building high-quality generative AI applications.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What does Demetrius do at Google?\n","A: He is a product manager within Cloud AI, focusing on search and document Artificial Intelligence (AI).\n","Q: Why are the new Vertex AI APIs being launched?\n","A: To help developers build generative applications faster and better, and to solve technical challenges in grounding and accessing enterprise data.\n","Q: How do the new Vertex AI APIs work together?\n","A: They are designed as primitives, simple, standalone, stateless APIs with clear interfaces, and can be combined with other APIs and frameworks.\n","Q: When can developers use the new Vertex AI APIs?\n","A: Now, as they are being launched, and can be integrated into popular frameworks like Chain or Llama Index.\n","Q: Who is the speaker, Demetrius?\n","A: A product manager within Cloud AI at Google, focusing on search and document Artificial Intelligence (AI).\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Vertex Artificial Intelligence (AI), Generative Applications, Document Understanding API, Embedding API, Vector Search, Hybrid Search, Ranking API, Grounded Generation API, Check Grounding API, Gemini Model, Language Models\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The singular value decomposition (SVD) of a matrix X is a fundamental concept in science and engineering, represented by X = UΣV^T, where U and V are unitary matrices that preserve angles and lengths of vectors. These unitary matrices can be thought of as rotating vectors in a vector space without changing their geometry. Geometrically, the SVD can be interpreted as transforming X into ellipsoids in both the column and row spaces of X, through multiplication by V and U^T, respectively. The singular values of X determine the lengths of the principal axes of these ellipsoids, providing crucial information about the matrix structure. Overall, SVD provides a powerful tool for understanding and analyzing matrices in various scientific and engineering applications.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is a unitary matrix?\n","A: A matrix that satisfies U U transpose equals U transpose U equals an identity matrix.\n","Q: Why are unitary transformations important?\n","A: They preserve angles and lengths of vectors, which is crucial in understanding the geometry of vector spaces.\n","Q: How do unitary transformations affect vectors?\n","A: They rotate vectors but do not change their lengths or angles with respect to other vectors.\n","Q: When would the complex conjugate transpose be used instead of the transpose?\n","A: When dealing with complex-valued data, the complex conjugate transpose (denoted by a star) is used.\n","Q: Who or what is an example of a widely used unitary transformation?\n","A: The Fourier transform is a widely used unitary transformation that takes a vector space and rotates it into a new representation.\n","\n","KEY CONCEPTS:\n"," SINGULAR VALUE DECOMPOSITION, UNITARY MATRICES, ECONOMY SVD, FOURIER TRANSFORM, COMPLEX CONJUGATE TRANSPOSE, INNER PRODUCT, VECTOR SPACE, GEOMETRIC INTERPRETATION, PRINCIPAL AXES, SINGULAR VALUES, UNITARY TRANSFORMATIONS\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," The video tutorial focuses on building generative Artificial Intelligence (AI) powered applications using Google Gemini Pro 1.5, a multi-modal model that can work with both text and images. The model's capabilities include processing large contexts of up to 1 million multimodal tokens, generating text based on prompts and images, and handling large inputs such as 1 hour of video, 11 hours of audio, and 30k lines of code. The tutorial covers hands-on applications, including code implementation and API key creation, to enable viewers to build their own generative AI applications. The Google Gemini Pro API is a powerful tool for developing generative applications, and its adoption by major clients like Mercedes demonstrates its potential for providing an enhanced user experience. Overall, Google Gemini Pro 1.5 offers advanced features for generating text based on prompts and images, making it a valuable tool for building innovative AI-powered applications.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the name of the YouTube channel host?\n","A: Krishn\n","Q: Why is Google Gemini Pro 1.5 considered amazing?\n","A: Due to its capabilities with text and images\n","Q: How does Google Gemini Pro 1.5 work with different data types?\n","A: As a multi-model, it works with both text and images\n","Q: When will the host show hands-on applications of Google Gemini Pro 1.5?\n","A: After the demo video\n","Q: Who will be demonstrating the use of Google Gemini Pro 1.5?\n","A: Krishn, the YouTube channel host\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Google Gemini Pro, Multi-Model, Long Context Understanding, Experimental Feature, Gemini 1.5, API Key, Generative AI, Hands-On Application, Multimodel Application, Text and Image Processing\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Evaluating and testing prompt engineering models is crucial to ensure their performance and accuracy. To measure a model's ability to generate accurate and meaningful responses, various metrics are employed, including perplexity, which assesses a language model's prediction of a sequence of words, and accuracy, which evaluates the correctness of generated responses. Additionally, human evaluation is used, involving rating the quality of responses. To debug and improve models, techniques such as analyzing generated responses, identifying common errors, and fine-tuning models are utilized. Furthermore, testing on diverse data sets and tasks helps determine a model's ability to generalize to new or unseen data, ultimately ensuring the reliability and effectiveness of prompt engineering models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What are some commonly used matrices for evaluating prompt engineering models?\n","A: Perplexity, accuracy, and human evaluation\n","Q: Why is testing prompt engineering models on different data sets or tasks important?\n","A: To determine the model's ability to generalize on new or unseen data\n","Q: How can we analyze generated responses to debug and improve prompt engineering models?\n","A: By identifying common errors or patterns\n","Q: When should we continue to evaluate and test prompt engineering models?\n","A: As we continue to use the model and generate responses\n","Q: Who is involved in evaluating the quality of responses in human evaluation?\n","A: Human raters\n","\n","KEY CONCEPTS:\n"," Prompt Engineering Models, Evaluating And Testing, Perplexity, Accuracy, Human Evaluation, Model Evaluation, Debugging And Improving, Cross Validation, Visualization Tools, Large Language Models, Pre-Trained Language Models, Model Generalization\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The distinction between Generative Artificial Intelligence (AI), AI agents, and agentic AI is explored. Generative AI refers to systems like large language models (LLMs) that create new content based on patterns learned from existing data. AI agents are autonomous programs that utilize tools, memory, and knowledge to complete tasks. Agentic AI systems, comprising one or more AI agents, work autonomously to achieve complex goals through multi-step reasoning, planning, and tool usage. This enables more sophisticated task performance. Overall, these concepts highlight the progression from simple content generation to complex autonomous task execution, showcasing advancements in AI capabilities and applications.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Agentic AI', 'Generative AI']\n","\n","Q&A:\n"," Q: What is Generative Artificial Intelligence (AI)?\n","A: AI that can create new content (text, image, or video) based on patterns learned from existing data.\n","Q: Why can't a simple LLM answer questions like 'What is the price of a flight ticket tomorrow?'\n","A: Because it has a knowledge cutoff date and can't access real-time information.\n","Q: How does an AI agent become more intelligent than a simple LLM?\n","A: By having access to tools (APIs) and memory, enabling it to perform actions and make decisions autonomously.\n","Q: When is an AI system considered 'agentic'?\n","A: When one or more AI agents work autonomously, making decisions using tools and other agents to reach a complex goal.\n","Q: Who or what can be used to build agentic AI systems?\n","A: Various tools and frameworks, such as N8N, Agnu, and others, which can incorporate generative AI as a core component.\n","\n","KEY CONCEPTS:\n"," GENERATIVE ARTIFICIAL INTELLIGENCE (AI), LARGE LANGUAGE MODEL (LLM), ARTIFICIAL INTELLIGENCE (AI) AGENT, AGENTIC ARTIFICIAL INTELLIGENCE (AI), MULTI-STEP REASONING, AUTONOMOUS DECISION MAKING, TOOL USAGE, KNOWLEDGE CUT-OFF DATE, API INTEGRATION, IMMIGRATION ARTIFICIAL INTELLIGENCE (AI) AGENT, FLIGHT BOOKING ARTIFICIAL INTELLIGENCE (AI) AGENT, N8N WORKFLOW DIAGRAM\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion focuses on covariance, a crucial concept in data analysis and preprocessing that measures the relationship between two random variables. Covariance is calculated using the equation 1/n * Σ[(xi - μx)(yi - μy)], where xi and yi are individual data points, and μx and μy are the means of the variables. A positive covariance indicates that as one variable increases, the other also increases, while a negative covariance indicates that as one variable increases, the other decreases. This statistical measure helps quantify relationships between variables, although it does not provide the magnitude of the relationship. Overall, covariance is an essential tool in understanding how variables relate to each other, which is vital in data analysis and preprocessing.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is covariance?\n","A: A measure to quantify the relationship between two random variables.\n","Q: Why is covariance used?\n","A: To determine if a relationship exists between two variables and its direction.\n","Q: How is covariance calculated?\n","A: Using the formula: 1/n * Σ[(xi - μx)(yi - μy)].\n","Q: When does covariance become positive?\n","A: When both variables increase or decrease together.\n","Q: Who uses covariance?\n","A: Data analysts and statisticians to analyze relationships between variables.\n","\n","KEY CONCEPTS:\n"," VARIANCE COVARIANCE, DATA PRE-PROCESSING, DATA ANALYSIS, COVARIANCE EQUATION, RANDOM VARIABLES, CORRELATION BETWEEN VARIABLES, QUANTIFYING RELATIONSHIP, VARIANCE OF X, COVARIANCE FORMULA, PEARSON CORRELATION COEFFICIENT, STATISTICS IN MACHINE LEARNING, DATA SET ANALYSIS\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning is a goal-oriented learning process where an agent, which can be a human or software program, learns an optimal policy to maximize a numerical reward signal. Through interactions with an environment, the agent makes decisions based on observations and receives feedback in the form of a reward signal indicating the quality of its actions. The agent's ultimate objective is to maximize cumulative reward over time via trial and error learning. A reward function, a mathematical expression, assigns numerical values to states or actions, reflecting the agent's trading goals and preferences. This process enables the agent to adapt and make informed decisions to achieve its objectives.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the objective of reinforcement learning?\n","A: To learn the optimal policy that maximizes a numerical reward signal.\n","Q: Why is feedback important in reinforcement learning?\n","A: Feedback provides the agent with information about the quality of its actions.\n","Q: How does an agent learn in reinforcement learning?\n","A: Through trial and error learning, exploring the environment, and updating its policy.\n","Q: When is the reward received in an episodic task like Tic-Tac-Toe?\n","A: At the end of the game, either winning (+1), losing (-1), or drawing (0).\n","Q: Who or what defines the reward function in reinforcement learning?\n","A: The reward function is defined by the user, reflecting their trading goals and preferences.\n","\n","KEY CONCEPTS:\n"," REINFORCEMENT LEARNING, OPTIMAL POLICY, CUMULATIVE REWARD, TRIAL AND ERROR LEARNING, REWARD FUNCTION, VALUE BASED METHOD, POLICY BASED METHOD, EPISODIC TASK, CONTINUOUS TASK, OBJECTIVE DEFINITION, PARAMETERIZE OBJECTIVE\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," In Python, dictionaries are a data type consisting of key-value pairs, where each key is immutable and maps to a specific value. They are declared within curly brackets and are useful for mapping one item to another. Dictionaries comprise items, which are key-value pairs separated by colons and commas. Key functions associated with dictionaries include `.items()`, `.keys()`, and `.values()`, which retrieve key-value pairs, keys, and values, respectively. Dictionaries can be created from lists using the `zip` and `dict` functions. Their elements can be accessed, modified, or deleted using square brackets, assignment, and the `del` function, making dictionaries a versatile and essential data structure in Python programming.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A dictionary in Python consists of key-value pairs, where each key-value pair is referred to as an item.\n","Q: Why are dictionaries useful?\n","A: Dictionaries are useful for mapping one item to another, such as stock prices with open, high, and close as keys.\n","Q: How is a dictionary created from two lists?\n","A: A dictionary can be created from two lists using the zip function and the dict function, which pairs corresponding values from each list.\n","Q: When would you use the del function with a dictionary?\n","A: You would use the del function to delete a particular entry within a dictionary, specifying the key to be deleted.\n","Q: Who or what can be a key in a dictionary?\n","A: A key in a dictionary must be immutable, such as a string or a number, but not a list, and can be a tuple.\n","\n","KEY CONCEPTS:\n"," KEY VALUE PAIRS, DICTIONARY DECLARATION, IMMUTABLE KEYS, DICTIONARY ITEMS, D DOT ITEMS, D DOT KEYS, D DOT VALUES, ZIP FUNCTION, DICT FUNCTION, KEY ACCESS, VALUE UPDATE, DELETE FUNCTION, DICTIONARY LENGTH\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," The application of Artificial Intelligence (AI) and machine learning significantly enhances an organization's security posture by improving threat detection and response. According to IBM's 2023 Cost of a Data Breach report, AI and automation can reduce the time to identify and contain data breaches by 108 days on average. User Behavior Analytics (UBA), powered by AI and machine learning, is crucial in detecting and responding to insider threats, which have an average cost of $4 million per incident. When integrated with a Security Information and Event Management (SIEM) solution, such as IBM Security's QRadar SIEM, UBA enables security professionals to analyze user behavior, detect anomalies, and prioritize employees by risk. By leveraging AI and automation, organizations can streamline security operations, accelerate investigations, and stay ahead of emerging threats, ultimately improving their overall security and reducing the risk of insider threats.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the average number of days to identify and contain a data breach for organizations that extensively use AI and automation?\n","A: 108 fewer days\n","Q: Why is AI important for security professionals?\n","A: To stay ahead of emerging threats and improve organization security posture\n","Q: How can user Behavior analytics (UBA) with AI and machine learning help security teams?\n","A: Detect and respond to Insider threats quickly and precisely\n","Q: When did IBM publish its Cost of a Data Breach report?\n","A: 2023\n","Q: Who conducted the survey for the Cost of a Data Breach report?\n","A: IBM\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, User Behavior Analytics, Machine Learning, Automation, Insider Threats, Data Breach, Security Posture, Emerging Threats, IBM's Cost of a Data Breach Report, User Behavior Analytics with AI and Machine Learning, Threat Containment\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Krishak discusses Meta's Llama 3, an open-source large language model (LLM) with impressive performance metrics, available in 8 billion and 70 billion parameter variants. Trained on 50 trillion tokens of data, Llama 3 excels in language nuances, contextual understanding, and complex tasks. Its performance surpasses paid LLM models, making it a significant advancement. The model's responsible development is guided by Meta's comprehensive approach, including determining use cases, defining, preparing, training, evaluating, and improving the system. Llama 3 is accessible through Meta's website, Hugging Face, and Kaggle, with model weights and tokenizer downloadable following specific instructions. The speaker plans to demonstrate usage in a subsequent video, enabling users to quickly get started with Llama 3 models, which have been evaluated using human evaluation metrics, showing comparable or superior performance to other models like MML, GPQ, and Gemini Pro 1.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the speaker's name?\n","A: Krishak\n","Q: Why is the speaker mentioning the current time?\n","A: To share the current time\n","Q: How does the speaker introduce himself?\n","A: By stating his name\n","Q: When does the speaker say it is currently?\n","A: 2 a.m.\n","Q: Who is the speaker addressing?\n","A: Guys (his YouTube audience)\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The discussion focuses on creating a decision boundary using Python code with the scikit-learn library, specifically exploring the Naive Bayes algorithm and its Gaussian variant. The goal is to enable understanding and replication of the code by leveraging key resources such as the scikit-learn library and Google documentation. Gaussian Naive Bayes is used to write the classifier. By referencing the library's documentation, the aim is to provide a detailed explanation of the algorithm and its implementation, allowing for replication of the code by the end of the next video or two.\n","\n","TOPICS:\n"," ['Python Programming', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the Python library used in the lesson?\n","A: scikit-learn (sk-learn)\n","Q: Why is Google used in the lesson?\n","A: To access the documentation of the scikit-learn library\n","Q: How does the instructor find the relevant information on the scikit-learn library?\n","A: By searching Google for sklearn and the algorithm name\n","Q: When does the instructor plan to explain the Naive Bayes algorithm?\n","A: After having the students run the code\n","Q: Who or what provides the Gaussian Naive Bayes function used in the lesson?\n","A: The scikit-learn library\n","\n","KEY CONCEPTS:\n"," \n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion focuses on statistics, specifically exploring various probability distributions. A key concept is the Gaussian distribution, also known as the normal distribution, characterized by a symmetrical bell curve with data points clustered around the mean. Another important distribution is the log normal distribution, which is often skewed and occurs when the logarithm of a random variable follows a normal distribution. Examples of log normal distributions include income and product review lengths. Understanding these distributions, such as Gaussian and log normal, is essential for data analysis and modeling, as it allows for standardization and scaling of data, ultimately leading to improved model accuracy. This knowledge is crucial for working with data and enables more precise modeling and analysis.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the percentage of total distribution that falls within 1 standard deviation in a Gaussian distribution?\n","A: 68 percent\n","Q: Why are we learning various distributions like Gaussian and log normal distribution?\n","A: To scale down values to the same scale and increase model accuracy\n","Q: How do we convert a log normal distribution to a standard normal distribution?\n","A: By finding the log of the values and then applying the formula (X - μ) / σ\n","Q: When do we use log normal distribution?\n","A: When data has a skewed distribution, e.g., income of people, product reviews\n","Q: Who is the target audience for learning about statistical distributions?\n","A: Data analysts, machine learning practitioners, and statisticians\n","\n","KEY CONCEPTS:\n"," GAUSSIAN DISTRIBUTION, LOG NORMAL DISTRIBUTION, BELL CURVE, STANDARD DEVIATION, MEAN, SYMMETRY, EMPIRICAL FORMULA, NORMAL DISTRIBUTION, LOGARITHM, STANDARD NORMAL DISTRIBUTION, SCALING, NORMALIZATION\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This video series presents an end-to-end deep learning project in agriculture, focusing on a mobile application to detect potato plant diseases, specifically early blight and late blight, using image classification. The project utilizes convolutional neural networks to classify images as healthy or diseased. The technology stack includes TensorFlow, TF serving, Fast API, and Google Cloud Platform. The process involves data collection, cleaning, pre-processing, model building, and machine learning operations. The project is deployed as a mobile application built with React Native, with the model optimized using quantization and TensorFlow Lite. The series covers the entire project lifecycle, from data collection to deployment, and aims to equip viewers with practical experience in deep learning and machine learning operations, making it ideal for those seeking to enhance their skills and resumes in machine learning engineering or data science.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the problem that farmers face in the agriculture domain?\n","A: Economic losses due to diseases in potato plants.\n","Q: Why is accurate identification of diseases in potato plants important?\n","A: To apply appropriate treatment and prevent economic loss.\n","Q: How will the mobile application identify diseases in potato plants?\n","A: Using deep learning and convolutional neural network.\n","Q: When will the farmer use the mobile application?\n","A: When they notice a potentially diseased potato plant.\n","Q: Who is building the mobile application?\n","A: AtliQ Agriculture, an AI company.\n","\n","KEY CONCEPTS:\n"," Machine Learning Ops, TF Serving, Deep Learning, Convolutional Neural Network, Fast API, Google Cloud Platform, Google Cloud Functions, React Native, Artificial Intelligence, End-to-End Application, Model Building, Data Collection\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The levels of autonomy in Large Language Model (LLM) applications range from zero to maximum autonomy. The spectrum of autonomy includes code with zero autonomy, where everything is hard-coded, to more advanced levels such as LLM call, chains, routers, state machines or agents, and fully autonomous agents. Each level offers increased autonomy, with state machines and agents enabling more complex tasks, iterative refinement, and intelligent decision-making. The progression from simple to complex systems allows for the integration of multiple specialists, human input, multi-agent systems, and adaptive learning. Overall, the levels of autonomy in LLM applications provide a framework for understanding and developing increasingly sophisticated and autonomous systems, ultimately leading to more efficient and effective decision-making and task execution.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the level of autonomy in code?\n","A: Zero autonomy, 100% deterministic\n","Q: Why is a single LLM call limited?\n","A: It often leads to confused or mixed up responses for multiple tasks\n","Q: How do chains work in LLM applications?\n","A: By breaking tasks into steps with multiple specialists, each good at one thing\n","Q: When is a router used in LLM applications?\n","A: When the AI needs to decide which steps to take next, based on user input\n","Q: Who or what is considered an agent in LLM applications?\n","A: A state machine or system where control flow is controlled by an LLM\n","\n","KEY CONCEPTS:\n"," ZERO AUTONOMY, LLM CALL, CHAINS, ROUTER, STATE MACHINE, AGENT EXECUTED, LANG GRAPH, MULTI-AGENT SYSTEMS, ADAPTIVE LEARNING, HUMAN IN LOOP, TIME TRAVEL ABILITIES, AUTONOMOUS AGENTS\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," Advanced topics in prompt engineering are explored, focusing on handling diverse prompt types, including text-based, image-based, and audio-based prompts. Techniques for fine-tuning pre-trained large language models, such as multitask learning and distillation, are covered. The significance of data pre-processing and cleaning, including tokenization and normalization, is emphasized. The deployment of prompt engineering models in production using frameworks like TensorFlow Serving and Flask is discussed, alongside ethical considerations, including bias, fairness, and privacy. Best practices for data processing and model deployment are also presented, highlighting the importance of careful consideration in the development and implementation of prompt engineering models to ensure effective and responsible use.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What are the different types of prompts that can be used in prompt engineering?\n","A: Text-based, image-based, and audio-based prompts.\n","Q: Why is data pre-processing important in prompt engineering?\n","A: To ensure the quality of data used to train models, which is crucial to their success.\n","Q: How can pre-trained large language models be fine-tuned for prompt engineering?\n","A: Using techniques such as multitask learning, distillation, and self-supervised learning.\n","Q: When is it necessary to deploy prompt engineering models in production?\n","A: After building a prompt engineering model, to make it accessible to a vast majority of users.\n","Q: Who is responsible for considering the ethical implications of prompt engineering models?\n","A: The developers and users of prompt engineering models, to ensure fairness, privacy, and transparency.\n","\n","KEY CONCEPTS:\n"," MULTITASK LEARNING, DISTILLATION, FINE-TUNING PRE-TRAINED MODELS, DATA PRE-PROCESSING, TOKENIZATION, NORMALIZATION, DATA AUGMENTATION, DEPLOYING PROMPT ENGINEERING MODELS, TENSORFLOW SERVING, FLASK FRAMEWORK, PROMPT ENGINEERING ETHICS, BIAS AND FAIRNESS, PRIVACY CONSIDERATIONS, SELF-SUPERVISED LEARNING, IMAGE-BASED PROMPTS, AUDIO-BASED PROMPTS, TEXT-BASED PROMPTS\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The singular value decomposition (SVD) and eigenfaces are used to cluster images of faces. Twenty greyscale images of each of two individuals, such as Arnold Schwarzenegger and Sylvester Stallone, are aligned, and an average face is computed and subtracted from each image. The SVD is then computed, producing eigenfaces or principal components that are linear combinations of the input images. The images are projected into the first three eigenfaces, resulting in a 3D representation that shows good separation between the two individuals, allowing for classification. This process is repeated for other pairs of individuals, including Taylor Swift and Stallone, and Taylor Swift and Arnold, with varying degrees of separation, demonstrating the effectiveness of SVD and eigenfaces in face clustering.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the main objective of the example discussed in the lecture?\n","A: To cluster images of Arnold Schwarzenegger and Sylvester Stallone using eigenfaces.\n","Q: Why is the average face computed from the images of Arnold and Stallone?\n","A: To subtract it from each image and create a matrix B for principal component analysis.\n","Q: How are the eigenfaces used in the example?\n","A: To project images into a lower-dimensional space and cluster them based on their eigenface coordinates.\n","Q: When is the SVD computed in the example?\n","A: After subtracting the average face from each image and creating matrix B.\n","Q: Who are the two individuals whose images are used to test the eigenface classification method?\n","A: Taylor Swift and Arnold Schwarzenegger.\n","\n","KEY CONCEPTS:\n"," SINGULAR VALUE DECOMPOSITION, EIGENFACES, PRINCIPAL COMPONENT ANALYSIS, ECONOMY SVD, EIGEN VECTORS, LINEAR COMBINATION, FACE SPACE, IMAGE CLASSIFICATION, CLUSTER ANALYSIS, PRINCIPAL COMPONENTS, EIGENVALUES, FACE RECOGNITION\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a framework that bridges the gap between large language models (LLMs) and the real world, enabling applications to leverage LLMs' reasoning abilities while interacting with external systems. It acts as an intermediary, allowing developers to build applications that combine LLM capabilities with real-world interactions, such as accessing APIs, databases, and sending emails. This framework enables the creation of more powerful AI applications, like booking flights and hotels, and suggesting restaurants. LangChain empowers AI to interact with the real world by accessing external resources, including private company databases, online platforms, and websites. This capability extends beyond making AI smarter, as it enables the technology to perform concrete actions. With vast potential applications, LangChain facilitates real-world actions, making it a powerful tool for creating advanced AI applications.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is Lang chain?\n","A: Lang chain is a framework that helps build applications using large language models (LLMs) and connects them to the real world.\n","Q: Why is Lang chain needed?\n","A: Lang chain is needed because LLMs can't interact with the real world, such as booking flights or hotels, on their own.\n","Q: How does Lang chain work?\n","A: Lang chain acts as a bridge between LLMs and the real world, enabling communication with APIs, databases, and more.\n","Q: When would you use Lang chain?\n","A: You would use Lang chain when building an application that needs to leverage the reasoning ability of an LLM and interact with the real world.\n","Q: Who can benefit from using Lang chain?\n","A: Developers who want to build AI applications using LLMs and connect them to real-world services, such as booking APIs, can benefit from using Lang chain.\n","\n","KEY CONCEPTS:\n"," Large Language Models, Lang Chain, LLM Model, Chat Application, Real World APIs, Artificial Intelligence, Language Model Framework, Reasoning Ability, Bridge Between LLMs And Real World, Application Development Framework, API Integration, LLM Model Training\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," This educational transcript discusses residual analysis in time series forecasting, which involves examining the differences between fitted and actual values in a model to diagnose performance and improve forecasting methods. To perform residual analysis, one should verify that residuals show no autocorrelation or partial autocorrelation and have a mean of zero. The Ljung-Box test and histograms are utilized to detect correlations and bias. A Python demonstration using the Holt-Winters model on the Air Passenger dataset illustrates residual analysis to check for correlations and bias. The goal is to ensure residuals are randomly distributed and unbiased. Through residual analysis, one can identify model shortcomings, enabling corrections and improvements in forecasting model iterations, ultimately leading to more accurate predictions.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: The difference between the fitted value and the actual value.\n","Q: Why is residual analysis important in time series forecasting?\n","A: To diagnose performance, detect trends, and improve forecasting methods.\n","Q: How can we use residual analysis to improve forecasting models?\n","A: By checking for autocorrelation and ensuring the mean of residuals is zero.\n","Q: When should residuals have no autocorrelation or partial autocorrelation?\n","A: For a good model fit, residuals should have no correlation.\n","Q: Who or what can perform the Ljung-Box test for residual analysis?\n","A: The statmods package in Python can perform the Ljung-Box test.\n","\n","KEY CONCEPTS:\n"," RESIDUAL ANALYSIS, TIME SERIES ANALYSIS, FORECASTING METHODS, RESIDUALS, AUTOCORRELATION, PARTIAL AUTOCORRELATION, YOUNG BOX TEST, Holt WINTERS MODEL, EXPONENTIAL SMOOTHING, TIME SERIES DECOMPOSITION, FITTED VALUES, ERRORS, BIAS CORRECTION\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," The main topic is building an Artificial Intelligence (AI) agent that interacts with a database using SQL knowledge. The agent is developed using LangGraph to build a ReAct agent, Next.js for the frontend, and models running on watsonx.ai, with an in-memory SQLite database. The process involves setting up a Next.js project, creating a client-side component for interaction with the large language model, and implementing functions to send user input to the LangGraph agent. The agent is enhanced with guidelines to generate SQLite queries based on natural language inputs and utilize a tool to execute these queries. A database connection is established, seeded with mock data, and a server-side function is created to connect to the SQLite database. The application enables interaction with the database using natural language queries, generating SQL queries, and executing them to produce desired results, while considering guardrails to prevent unlimited database control.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the purpose of building an AI agent in this video?\n","A: To connect to databases using SQL knowledge\n","Q: Why is LangGraph used in this project?\n","A: To build a ReAct agent\n","Q: How is the Next.js application started?\n","A: By running npm run dev\n","Q: When is a component run client-side in Next.js?\n","A: When specified as a client-side component\n","Q: Who or what is used to run models in this project?\n","A: watsonx.ai\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Large Language Models, SQL Knowledge, LangGraph, ReAct Agent, Next.js, Watsonx.ai, In-Memory Database, SQLite, TypeScript, Tailwind CSS\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a specialized field within natural language processing that focuses on building models generating high-quality text outputs in response to input prompts. It utilizes pre-trained large language models, such as GPT and Transformers, fine-tuned for specific tasks like chatbots, language translation, and content generation. The key benefit of prompt engineering is generating accurate, coherent, and contextually appropriate text outputs, which significantly impacts user experience. The field covers fundamental concepts, including prompt analysis, benefits, limitations, and techniques for fine-tuning pre-trained models. Overall, prompt engineering is crucial for improving the quality of text outputs, enabling more effective and efficient applications in various areas, and enhancing user experience through more accurate and relevant responses.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A specialized field within natural language processing that focuses on building models that can generate high-quality text outputs in response to prompts.\n","Q: Why is prompt engineering important?\n","A: It allows generating text outputs that are more accurate, coherent, and contextually appropriate than traditional approaches, impacting user experience and engagement.\n","Q: How do prompt engineering models work?\n","A: They are based on pre-trained large language models, fine-tuned for specific tasks and inputs.\n","Q: When is prompt engineering particularly crucial?\n","A: For applications such as chatbots, language translation, and content generation, where output quality significantly impacts user experience and engagement.\n","Q: Who benefits from understanding prompt engineering?\n","A: Developers and professionals working with chatbots, virtual assistants, translation software, and other natural language processing applications.\n","\n","KEY CONCEPTS:\n"," Natural Language Processing, Prompt Engineering, Pre-trained Large Language Models, Fine-Tuning, Large Language Models, Open Artificial Intelligence, GPT, Google Bird, Hugging Face Transformers, Chatbots, Language Translation, Content Generation, Rule-Based Approaches, Keyword-Based Approaches\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning is a machine learning paradigm where an agent learns to map situations to actions to maximize a numerical reward signal. Q-learning, a value-based method, determines a state-action value function, or Q-function, to solve problems by learning Q-values that maximize the total reward. The Q-function takes a state and action as input and outputs a Q-value, which quantifies the total reward. Q-learning uses a Q-table to learn Q-values and determine an optimal policy. The Q-value update is driven by the temporal difference error, calculated as the difference between observed and expected Q-values, and follows a gradient descent rule with a learning rate. Through iterative updates, Q-learning converges to optimal Q-values, enabling effective learning and decision-making. The learned Q-values dictate the target policy, decoupled from the behavior policy used for exploration, and Q-learning is an off-policy algorithm.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What are the three machine learning paradigms?\n","A: Supervised, unsupervised, and reinforcement learning\n","Q: Why is Q learning a value-based reinforcement learning method?\n","A: It determines a value function to maximize total reward\n","Q: How does Q learning learn the state-action value function?\n","A: Through a table of Q values updated using the Bellman equation\n","Q: When is the agent's goal achieved in the grid world?\n","A: When it reaches the +10 reward spot\n","Q: Who or what determines the behavior policy in Q learning?\n","A: The agent, based on random chance\n","\n","KEY CONCEPTS:\n"," REINFORCEMENT LEARNING, MACHINE LEARNING PARADIGMS, SUPERVISED LEARNING, UNSUPERVISED LEARNING, VALUE-BASED METHODS, POLICY-BASED METHODS, STATE VALUE FUNCTION, STATE ACTION VALUE FUNCTION, Q LEARNING, BELLMAN EQUATION, Q VALUE, DISCOUNT FACTOR\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," Logistic classification is a linear classification method that applies a linear function to input data to generate predictions. The linear function is a matrix multiplication of the input vector and weight matrix, plus a bias term. The goal is to train the model to find optimal values for the weight matrix and bias to make accurate predictions. The scores are converted to probabilities using a softmax function, which ensures the probabilities sum to 1 and reflect the relative magnitude of the scores. This process enables the model to output a probability distribution over classes, with the correct class having a probability close to 1, allowing for accurate classification of input data.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A linear classifier that applies a linear function to inputs to generate predictions.\n","Q: Why is a softmax function used?\n","A: To turn scores into probabilities that sum to 1 and reflect the likelihood of each class.\n","Q: How does a logistic classifier generate predictions?\n","A: By applying a linear function, a giant matrix multiply, to the input vector.\n","Q: When are probabilities used in logistic regression?\n","A: After obtaining scores, to ensure the probability of the correct class is close to 1 and others are close to 0.\n","Q: Who or what is denoted by W, X, and b in the logistic classifier?\n","A: W denotes weights, X denotes inputs, and b denotes the bias term.\n","\n","KEY CONCEPTS:\n"," \n","Saved row 29\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_instruction_full_output.xlsx\n","\n","Instruction-based (Groq) pipeline completed successfully.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_instruction_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":715,"referenced_widgets":["a9f2a3e2b08a435e8611a6e51a5a92bd","dc2f99dbad5345edb3a1971982b2ee68","dda24fc23bc940de89a6bf42f7f075b9","7e6066ac48ac43edb78264086eebe000","b7280dcafc834d9ea6a3c489acd02683","e01c00f117c44fcb9503ba21d0a7f20f","283f5dab1c364337ab7d0a942d1e22f3","f395fa3b02e64e16b06c04dc1e6eb44b","6a7b1f324d2f498fb8e3cf6907a54a18","d6a6c3b6129a4d6ebec116e911b7b0a2","ca2933f3b7634a1a99f9d69a3b30b19c","d3a27ea0bd9f434e93dd0439793069cc","9606e6409e3143c4bfbe52df0d2dc208","d106b0cd87a344b682708ac12a792e6d","534d167e7d9147f587014d395cf5d74e","84bf5e05c2774ab29d05089644ae916a","e13bc3886e9346fb9422f7604e2f4752","69b5202c870f4db8bcc58c17ca03814a","996901eccf504d06aa8aedd0f257d658","95e5f2e0b9be4f0fa3a67712ae094581","2ae992a609ed4ff4a0f39452a518aef4","689d9f8836e043118ba8ab71f92a89a0","44018be0b5cb47099768ad021a2b29f5","52cfa15a029449dba029962576566424","70915a5507a545408189d87e25bd61ac","d758a104dcca48c5801b80c95c6ac766","ba3af11c2fae42968b07feb090cceb1b","5236bf652ccf448fa544925811e47c1b","a6e9748051054f039364d76d5aef5858","e7f139cbeb334313a62cee5aeacf9886","c280045ac752448096978578e14d90b9","ecb86368d5f348079ef95b4b8d7f2542","9d0c7d3541de4822af8af217a60df127","c1ef18b6c555406793972e9cb178f404","72dd07b912b843e18094098fec454cfb","17cbd6ae6c84465c820286c4cac6efba","c98ad4fb2a2c448d81e6a0cced02232a","826d5f0dbced4dffaf050107798696e3","5d4b5036234e498fb90125a889be96ba","67bec66db81f48ae950a272033cab1a0","89f2584fa15e4a83931c50952ea87de5","8b307f8f485a4947964c660f18528e44","d7ca379d3e484c85957e42ccbbd6e4dd","90cc018d9f514548a71de63e77a39bf8","2ebb8636f8304dafb60d8c4cd20841dc","1282ca1fd37a4b60a25542aee5879d4f","65a09105618a4d20babbd55d8f8a5ada","ce1879042ebd40088999344427c0c5f3","2f5d4cb5e2544d9095a7ac3c2443023d","cf2d12aaafe34600b06d8e298ffc21c6","00117b6a6acd4ec9b5236971cc8b8185","68e30d438fdd4b1f853a4fe5790d50c2","9cd3db296e484d6aad79cd347a4c78b3","65c5ca5391c948d48428d9e453bc5d4a","afb20128bf434fde9a1502321c5f798f","23be720b3ddc43b48ace7a8443af7a9f","7ee344a6e5764f5fbb812c7930d3a3cf","59298cbc39a4477b935231827f614d4d","db37c8bdcc5c48479891abef5765764d","703d4b830fcb483aa4c87cabfe3aa54f","1b3e60e4a7a04ad98540cea961a0120d","7f69fc91c7a448bc8c00c190f2f915df","40ddb5c632ae435bbcccb4304ee844fa","aa88369449304b82986770fa22118f53","0b0b6c8f61284c91a9baa8ac873b490c","109c20aab0754f6abf89c01695122b3d"]},"id":"pjLb9FptYNiY","executionInfo":{"status":"ok","timestamp":1763817321374,"user_tz":-330,"elapsed":126276,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"d45410c7-9013-4759-dab3-221d3af34128"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_instruction_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f2a3e2b08a435e8611a6e51a5a92bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3a27ea0bd9f434e93dd0439793069cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44018be0b5cb47099768ad021a2b29f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1ef18b6c555406793972e9cb178f404"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ebb8636f8304dafb60d8c4cd20841dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23be720b3ddc43b48ace7a8443af7a9f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3166\n","  - BLEU: 0.0869\n","  - BERTScore F1: 0.8927\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.7000\n","  - Jaccard Index: 0.2817\n","  - Micro F1: 0.3837\n","  - Macro F1: 0.3915\n","  - Weighted F1: 0.3836\n","\n","Q&A Generation:\n","  - BLEU: 0.0277\n","  - Diversity: 0.7010\n","  - Answerability: 0.7467\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5567\n","  - Recall@10: 0.2227\n","  - F1@10: 0.3181\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\n"]}]}]}