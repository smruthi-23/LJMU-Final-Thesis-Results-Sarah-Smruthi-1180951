{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfDBoFPM6AQFyAOpphVQ4i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_ysJ0lDsejiu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763956377569,"user_tz":-330,"elapsed":24593,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"293efd2b-bef5-46e1-ebbb-eb5449f2ab02"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=421e54537e89ee1f608afb08d39b50b6f58de28acbd93e062085dcfba94c02a7\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"44Hwtn1qsiSi","executionInfo":{"status":"ok","timestamp":1763956377611,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"2536a272-5137-4270-8f79-164d05f0285e"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED ONLY THIS\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/kimi-k2-instruct-0905/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"kimi-k2-instruct-0905_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"moonshotai/kimi-k2-instruct-0905\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25        # SAFE FOR FREE COLAB\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    s = text.find(\"{\")\n","    e = text.rfind(\"}\")\n","    if s == -1 or e == -1 or e <= s:\n","        return {}\n","    try:\n","        return json.loads(text[s:e+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (RATE LIMIT SAFE)\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"Groq failed after all retries\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. INSTRUCTION-BASED TASKS (PROMPTS UNCHANGED)\n","#####################################################################\n","\n","##########################\n","# 8.1 SUMMARISATION\n","##########################\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, 1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = groq_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip() or out[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(partial_summaries)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\").strip() or out2[:900]\n","\n","\n","##########################\n","# 8.2 TOPIC CLASSIFICATION\n","##########################\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...] }}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","\n","##########################\n","# 8.3 Q&A GENERATION\n","##########################\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an AI assistant that MUST return STRICT JSON with no additional text.\n","\n","INSTRUCTION:\n","Generate exactly five (5) question–answer pairs based on the transcript.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples.\n","\n","GUIDELINES:\n","• EXACTLY 5 pairs.\n","• Types (in order):\n","  1. What\n","  2. Why\n","  3. How\n","  4. When\n","  5. Who\n","• Each answer must be ≤25 words.\n","• Answers must come directly from the transcript.\n","• No generic filler.\n","• NO explanations outside JSON.\n","\n","IMPORTANT JSON RULES:\n","• Start your response with '{' and end with '}'.\n","• Use ONLY this format:\n","{{\n","  \"generated_questions\": [\n","    {{\"q\": \"...\", \"a\": \"...\"}},\n","    ...\n","  ]\n","}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    # Call Groq\n","    out = groq_call(prompt, temperature=0.0)\n","\n","    # Try strict JSON parse\n","    j = extract_json(out)\n","    qa_list = j.get(\"generated_questions\", [])\n","\n","    # If JSON is invalid or empty → fallback extraction\n","    if not qa_list:\n","        logger.warning(\"JSON parsing failed. Using fallback regex extraction.\")\n","\n","        # Regex to capture Q and A even in messy output\n","        pattern = r'(?:Q:?\\s*)(.*?)(?:\\n|\\r)+(?:A:?\\s*)(.*?)(?=\\nQ|\\Z)'\n","        matches = re.findall(pattern, out, flags=re.DOTALL)\n","\n","        qa_list = []\n","        for q, a in matches:\n","            q = q.strip().replace(\"\\n\", \" \")\n","            a = a.strip().replace(\"\\n\", \" \")\n","            if q and a:\n","                qa_list.append({\"q\": q, \"a\": a})\n","\n","    # If still empty → final fallback: return empty string\n","    if not qa_list:\n","        return \"\"\n","\n","    # Convert to text format for output Excel\n","    lines = []\n","    for qa in qa_list:\n","        q = qa.get(\"q\", \"\").strip()\n","        a = qa.get(\"a\", \"\").strip()\n","        if q: lines.append(f\"Q: {q}\")\n","        if a: lines.append(f\"A: {a}\")\n","\n","    return \"\\n\".join(lines)\n","\n","##########################\n","# 8.4 KEY CONCEPT EXTRACTION\n","##########################\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list core terminology, methods, or technical phrases.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases.\n","• Exclude generic words.\n","• No duplicates.\n","\n","OUTPUT FORMAT:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    if not isinstance(concepts, list):\n","        concepts = []\n","\n","    return \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based (Groq) pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrVYGTgMskRJ","outputId":"c061ef7b-9018-458f-b24c-767c798a837d","executionInfo":{"status":"ok","timestamp":1763961187748,"user_tz":-330,"elapsed":1174658,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}}},"execution_count":3,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/kimi-k2-instruct-0905\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning from human feedback (RLHF) integrates human evaluative signals into reinforcement learning to accelerate convergence and align agents with human preferences. In grid-world navigation, an agent (Frank) explores via Q-learning, DQN or proximal policy optimisation while human mentors provide corrective rankings that compress exploration and elevate policy quality. The same paradigm scales to language models: ChatGPT samples multiple responses per prompt, human annotators rank them, and a transformer-based reward model learns to predict scalar quality scores. PPO then maximises these learned rewards, iteratively refining the policy to produce coherent, contextually appropriate outputs. RLHF thus unifies low-level control and high-level generation, demonstrating that preference-based reward modelling robustly aligns autonomous behaviour with nuanced human intent across discrete action spaces.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: What reinforcement learning algorithms can work with human feedback?\n","A: Q-learning, DQ learning, proximal policy optimization, or any other algorithm.\n","Q: Why is human feedback used alongside RL algorithms like Frank’s?\n","A: It accelerates learning and produces more human-favored responses.\n","Q: How is the rewards model trained for ChatGPT?\n","A: Humans rank multiple ChatGPT answers, then the model learns to score response quality.\n","Q: When does the rewards model assess an answer during ChatGPT fine-tuning?\n","A: After each response is generated, before backpropagation updates the network.\n","Q: Who ranks the ChatGPT answers that train the rewards model?\n","A: We as humans rank them.\n","\n","KEY CONCEPTS:\n"," reinforcement learning with human feedback, grid world reward structure, proximal policy optimization, reward model training, human preference ranking, iterative fine-tuning loop, backpropagation via reward signal, DQ learning integration, accelerated policy learning, human-in-the-loop mentoring, chat GPT reward scoring, RLHF loss function\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This instalment of the machine-learning series uses Matthew Blondell’s CVXopt-based SVM implementation to expose how quadratic programming encodes kernel mechanics: the convex objective ½xᵀPx+qᵀx subject to Gx≤h, Ax=b is solved to obtain Lagrange multipliers, support vectors, weight w and bias b, enabling prediction via sign(w·x+b). Polynomial and Gaussian-RBF kernels map linearly inseparable data to implicit high-dimensional spaces where a soft-margin hyperplane is found by tuning penalty C, while visualisations contrast decision boundaries and support-vector counts across linear, non-linear and overlapping datasets. Although CVXopt clarifies solver internals, practitioners are steered toward optimised libraries such as LIBSVM or scikit-learn; the forthcoming concluding tutorial will survey scikit-learn’s hyperparameters and multi-class strategies, consolidating a concise practical reference.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What is part 32 of the tutorial covering?\n","A: Running example code with CVX opt, kernels applied to an SVM, and visualization.\n","Q: Why is CVX opt used in this tutorial?\n","A: To directly show kernel impact on the SVM; beyond education, CVX opt is rarely useful.\n","Q: How is CVX opt described as a solver?\n","A: It minimizes ½xᵀPx+qᵀx subject to Gx≤h and Ax=b.\n","Q: When would you use libSVM instead of CVX opt?\n","A: When writing your own support vector machine; CVX opt is only for educational visualization.\n","Q: Who provided the example code?\n","A: Matthew Blondel via his GitHub.\n","\n","KEY CONCEPTS:\n"," CVXOPT quadratic programming solver, kernel trick injection, support vector machine visualization, soft margin visualization, nonlinear kernel impact, Matthew Blondell GitHub code, Christopher Bishop PRML reference, libSVM practical alternative, quadratic programming constraints, educational kernel demonstration\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is the systematic design of user-supplied inputs that steer large language models by embedding context, constraints, and explicit task specifications. Seven distinct prompt types—ranging from questions and statements to multi-input and constraint-driven formulations—modulate output complexity and fidelity. High-performance prompts optimize length, employ precise unambiguous language, and separately declare the required content (“what”) and the desired presentation (“how”), including tone, style, and word count. Practitioners deconstruct existing prompts to isolate objectives and constraints, enabling iterative refinement that calibrates responses for accuracy, brevity, or rich detail while preserving semantic integrity and adherence to formatting rules.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What are prompts?\n","A: Inputs given to prompt engineering models.\n","Q: Why is choosing the right prompt type important?\n","A: It impacts output complexity and quality.\n","Q: How can you make GPT return a one-word answer?\n","A: Write give a one-word answer in prompt.\n","Q: When does the transcript say deconstruction happens?\n","A: When you already have a prompt.\n","Q: Who provides additional context beyond the direct answer?\n","A: ChatGPT.\n","\n","KEY CONCEPTS:\n"," prompt engineering, large language models, question prompts, statement prompts, prompt constraints, prompt deconstruction, context specification, tone constraints, output format constraints, SEO-optimized prompts, pre-trained model fine-tuning\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents are autonomous systems that dynamically decide which actions to take, contrasting with fixed chains or routers. Central to their operation is the ReAct loop: the large language model reasons about the prompt, selects and parameterizes task-specific tools (e.g., calculators, search engines), observes the returned data, and repeats until the goal is met. LangChain orchestrates this cycle by invoking the chosen tool and feeding its output back into the model, maintaining full conversational context. By integrating external tools through ReAct, an LLM becomes an agent capable of multi-step, context-aware reasoning and action, enabling solutions to complex, open-ended problems beyond the scope of single-shot inference.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What are AI agents described as?\n","A: problem solvers of the AI world\n","Q: Why is the ReAct pattern effective?\n","A: it mimics how human beings think\n","Q: How does LangChain execute a tool?\n","A: LangChain executes the tool and returns the output back to the LLM\n","Q: When does the think-action-observe cycle end?\n","A: if it has found out the answer to the particular problem\n","Q: Who equips the LLM to become an agent?\n","A: whenever you equip a brain with some tools\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence agents, autonomous decision-making, tools as special functions, ReAct agent pattern, reasoning plus acting loop, think-action-observe cycle, LangChain tool execution, LLM context retention, multi-step problem solving, agent-tool integration, LangGraph problem solving\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The transcript details an end-to-end demonstration of a reflection-agent pipeline that iteratively refines a tweet for maximum virality. Integrating LangChain with LangSmith, the system logs every step to smith.chain for full observability. A generation node drafts an initial tweet, a reflection node critiques its hooks, hashtags, emojis and context, and the revised text loops back; six such cycles produce a highly optimised output. Storing the LangSmith API key in an environment file enables automatic telemetry streaming, capturing granular traces and timing data (≈46 s total). The workflow exemplifies how reflective agents systematically critique and revise content, with LangSmith providing transparent inspection of each iterative exchange within the tweet-generation pipeline.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What system is being traced in this section?\n","A: the reflection agent system\n","Q: Why are they tracing the reflection agent system?\n","A: so we can understand exactly what is happening where\n","Q: How will tracing help them?\n","A: we'll understand how both of these systems are working together\n","Q: When does the speaker go to smith.chain?\n","A: after saying \"uh to do that\"\n","Q: Who built the reflection agent system?\n","A: we\n","\n","KEY CONCEPTS:\n"," reflection agent system, trace reflection agent, refined viral tweet, systems working together, smith.chain website\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The tutorial details configuring and querying OpenAI models within LangChain, beginning with pip-installation of langchain-openai and secure loading of the OPENAI_API_KEY via python-dotenv. Instantiating ChatOpenAI with gpt-4o (or cost-effective GPT-3.5-turbo) yields an llm object whose invoke method transmits prompts to the API, returning a JSON payload containing content, metadata, and token counts; result.content isolates the textual answer. Successful calls require a funded account (≥$5). The pattern generalises to any OpenAI model selected through the same interface. To improve coherence, the instructor recommends supplying the complete human–AI dialogue history rather than isolated prompts, enabling the model to maintain contextual awareness across conversational turns and generate progressively grounded responses.\n","\n","TOPICS:\n"," ['LangChain', 'Prompt Engineering', 'Python Programming']\n","\n","Q&A:\n"," Q: What package does the speaker install for using LangChain with OpenAI?\n","A: langchain-dash-openai\n","Q: Why did the installation initially fail?\n","A: because of a percentage sign\n","Q: How is the chat model imported after installation?\n","A: from langchain_openai import ChatOpenAI\n","Q: When does the speaker remove the percentage sign?\n","A: before running the install command\n","Q: Who provides the GPT-4o model mentioned?\n","A: OpenAI\n","\n","KEY CONCEPTS:\n"," LangChain chat models, OpenAI API integration, chat OpenAI class, LangChain-OpenAI package, GPT-4o model, GPT-3 fallback, pip install langchain-openai, VS Code terminal, model initialization, keyword parameter, latest model expense\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," Python’s list.sort() method imposes a deterministic ordering on heterogeneous lists: numbers precede strings, and within strings ASCII rules apply so uppercase-initial words sort before lowercase-initial ones, each subgroup alphabetical. Reversing the list inverts these relative positions. Case-insensitive alphabetical ordering requires prior normalisation of all strings to a uniform case, because the default lexicographic comparison is case-sensitive. These behaviours are consistent and predictable, enabling developers to control collation by preprocessing elements or supplying key functions.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What two groups does the sort method create when sorting strings?\n","A: Uppercase-first strings, then lowercase-first strings, each sorted alphabetically.\n","Q: Why does apples appear before very after sorting the list?\n","A: Words starting with uppercase letters precede lowercase ones.\n","Q: How can you prevent separate uppercase/lowercase groups when sorting strings?\n","A: Convert all strings to the same case before sorting.\n","Q: When did the speaker insert 18 into the list?\n","A: After initially sorting a pure string list.\n","Q: Who demonstrated the sort behavior for mixed types?\n","A: The presenter in the tutorial video.\n","\n","KEY CONCEPTS:\n"," Python sort method, case-sensitive alphabetical ordering, mixed-type list sorting, uppercase-first sorting, reverse alphabetical ordering, string-number list handling, case normalization requirement, alphabetical ordering within case groups, reverse sorting behavior, capital letter precedence, lowercase letter grouping\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The transcript investigates optimal human–AI task allocation for binary fraud-alert triage. An AI classifier outputs 0–100 % confidence scores whose accuracy peaks at the extremes and collapses near 50 %; humans, though slightly inferior at high certainty, degrade more gracefully across the mid-range. Calibrated thresholds therefore auto-approve high-confidence alerts, auto-reject low-confidence ones, and route intermediate cases to analysts, minimising false positives while reserving expert scrutiny where AI is unreliable. Augmented intelligence—AI-assisted human review—further improves mid-range accuracy, yet its benefit is eroded by automation bias. Optional-display interfaces that reveal AI advice only on demand, and sequencing protocols that oblige reviewers to commit to an initial judgment before seeing machine guidance, mitigate over-reliance. Explicitly advertising AI fallibility paradoxically reduces uptake, so systems should quietly embed reliability cues. Treating decision authority as an empirically allocable variable rather than a preference shifts design toward interfaces that preserve human agency, curb cognitive bias, and maximise hybrid decision quality.\n","\n","TOPICS:\n"," ['Machine Learning', 'Artificial Intelligence', 'Data Science']\n","\n","Q&A:\n"," Q: What percentage of fraud-detection alerts are false positives?\n","A: 90 percent\n","Q: Why is a human often better at a 50 percent confidence prediction?\n","A: Human performance curves are flatter, outperforming AI when AI is unsure.\n","Q: How is AI performance represented on the described graph?\n","A: Low and high confidence scores correlate to high success rates.\n","Q: When does the AI algorithm effectively say \"I don't know\"?\n","A: When confidence is near the middle, yielding lower success rates.\n","Q: Who reviews the potentially fraudulent transaction alerts?\n","A: Financial analysts\n","\n","KEY CONCEPTS:\n"," fraud detection system, false positive rate, confidence score, AI performance curve, human performance curve, holistic decision curves, human bias in decisions, AI uncertainty region, financial analyst workload, alert triage threshold, human-AI collaboration, statistical decision superiority\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Vertex AI’s new stateless APIs provide a modular, enterprise-ready stack for building grounded generative applications. The Document Understanding API converts complex documents into structured data; high-performance text-gecko embeddings and hybrid Vector Search improve retrieval; a Ranking API re-orders passages by answer relevance; the Grounded Generation API, fine-tuned on Gemini, emits cited natural-language responses; Check Grounding verifies factual claims against supplied evidence. By exposing these services through simple interfaces and open frameworks such as LangChain and LlamaIndex, Google enables developers to compose retrieval, ranking, generation, and fact-checking stages with minimal code while leveraging the same large-scale search and ads infrastructure that powers Google’s consumer products. The integrated suite aims to accelerate production deployment of accurate, auditable, domain-specific generative solutions without requiring dedicated model hosting or stateful orchestration.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the document understanding API's purpose?\n","A: Using our document AI knowledge to parse structure and boost retrieval/application answer quality.\n","Q: Why embed Google know-how into these new APIs?\n","A: Tackle frequent developer hurdles with planet-scale tech only Google owns.\n","Q: How can developers integrate the APIs?\n","A: Simple, stateless APIs drop into chains, llamaindex or custom stacks.\n","Q: When were these six new Vertex AI APIs launched?\n","A: In this talk today we are announcing the six APIs.\n","Q: Who is the speaker outlining these launches?\n","A: Demitrius Case, a product manager for Cloud AI search and document AI.\n","\n","KEY CONCEPTS:\n"," document understanding API, embedding API improvements, vector search hybrid retrieval, ranking API relevance scoring, grounded generation API with citations, check grounding fact-checking API, enterprise data grounding, vertex AI generative APIs, stateless API primitives, chain/llama-index framework integration, Google planet-scale retrieval technology\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," Singular Value Decomposition (SVD) factorises a real m×n matrix X into U Σ Vᵀ, where the unitary matrices U and V contain orthonormal bases for the column and row spaces; an economy-size variant retains only the r = rank(X) columns, reducing storage while preserving exact reconstruction. Because unitary transformations conserve Euclidean norms and angles, SVD acts as a data-dependent rotation that generalises the Fourier transform. Geometrically, X deforms the unit sphere in ℝⁿ into an ellipsoid in ℝᵐ whose principal semi-axes are the singular values σᵢ and whose directions are the left singular vectors; the adjoint Xᵀ analogously maps the sphere in ℝᵐ to an ellipsoid in ℝⁿ oriented by the right singular vectors. Consequently, SVD exposes the orthogonal directions along which X imparts maximal stretching, yielding a coordinate system aligned with the data’s intrinsic variance structure and underpinning applications from low-rank approximation to principal component analysis.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What are U and V in the SVD of matrix X?\n","A: U and V are unitary matrices.\n","Q: Why are unitary transformations important for data geometry?\n","A: They preserve angles and lengths between vectors.\n","Q: How does multiplying a sphere by X change its shape?\n","A: It maps the sphere into an elongated ellipsoid.\n","Q: When would transposes in SVD become complex conjugate transposes?\n","A: When the data matrix X is complex valued.\n","Q: Who determines the orientation of the ellipsoid produced by X?\n","A: The left singular vectors in U.\n","\n","KEY CONCEPTS:\n"," singular value decomposition, economy-size SVD, unitary matrices, complex conjugate transpose, Fourier transform as unitary operator, angle and length preservation, inner-product invariance, sphere-to-ellipsoid mapping, left and right singular vectors, singular values as principal axes, data-driven coordinate rotation, rectangular matrix geometry\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial demonstrates building generative-AI applications with Google Gemini Pro 1.5, a unified multimodal large-language model that processes up to one million tokens of text, images, audio, video, or code in a single context. After obtaining a free API key from ai.google.com and installing the google-generativeai library, developers access the single endpoint model/gemini-1.5-pro-latest, eliminating the previous split between text and vision models. The workflow—configure credentials, list available models, invoke generate_content with streaming=True—yields low-latency, Markdown-formatted responses. Live experiments ingest a 402-page, 330 k-token Apollo 11 PDF and a sketch, then retrieve exact humorous quotes, identify Neil Armstrong’s first step, and cite mission times, illustrating precise cross-modal retrieval over lengthy documents. Parallel demos feed images alongside prompts such as “write a blog post,” producing coherent multimodal output without external vision modules. The session concludes that the million-token context window enables enterprise-grade retrieval-augmented generation and seamless integration of vision and language tasks, positioning Gemini 1.5 Pro ahead of GPT-4 Turbo’s 128 k limit.\n","\n","TOPICS:\n"," ['Generative AI', 'Python Programming', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What does Krishn demonstrate building in this video?\n","A: A generative AI-powered application using Google Gemini Pro 1.5.\n","Q: Why does Krishn show Google's demo video?\n","A: To first illustrate what Gemini 1.5 Pro can do.\n","Q: How will Krishn guide viewers?\n","A: Show demo video, run code on text/image samples, explain API creation and use.\n","Q: When does Krishn plan to run the code?\n","A: After presenting the one-minute Google demo video.\n","Q: Who presents the tutorial?\n","A: Krishn on his YouTube channel.\n","\n","KEY CONCEPTS:\n"," Google Gemini Pro 1.5, generative AI application, multimodal model, long context understanding, API key creation, hands-on code implementation, text and image processing, end-to-end projects, experimental feature, 1-minute demo video, Krishn YouTube channel\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Systematic evaluation and iterative refinement are essential for prompt-engineered language models to maintain robust performance across evolving data. The process employs perplexity, accuracy, and human ratings to respectively gauge predictive quality, factual correctness, and perceived response coherence. An evaluate_translation function exemplifies this by achieving perfect accuracy on a parallel corpus while simultaneously logging perplexity. Debugging strategies include error-pattern analysis, visualisation, and cross-validation across tasks to expose generalisation limits. Continuous re-evaluation loops integrate new data, update prompts, and re-measure metrics, ensuring that model behaviour remains aligned with domain requirements and prevents silent performance drift.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What matrices evaluate prompt engineering models?\n","A: Perplexity, accuracy, and human evaluation.\n","Q: Why analyze generated responses?\n","A: To identify common errors and fine-tune the model.\n","Q: How test model generalization?\n","A: Cross-validation on varied datasets or tasks.\n","Q: When repeat evaluation?\n","A: Continuously, as the model keeps generating responses.\n","Q: Who rated the translation quality?\n","A: Human evaluators.\n","\n","KEY CONCEPTS:\n"," prompt engineering models, evaluation matrices, perplexity measurement, human evaluation, evaluate_translation function, source text target text dataset, model accuracy, model debugging techniques, cross-validation testing, model generalization ability, visualization error analysis, fine-tuning models\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," The presentation delineates a three-tier hierarchy of artificial-intelligence competence. Tier-1 Generative AI, typified by GPT-4, synthesizes novel content from static training data yet remains bounded by its knowledge cutoff. Tier-2 AI agents extend large language models with tool-use and API access, autonomously executing narrowly scoped tasks such as live flight reservations. Tier-3 Agentic AI integrates multiple agents, external tools, persistent memory, and domain knowledge to conduct multi-step, goal-directed planning—illustrated by coordinating visa requirements, weather windows, and budget limits for complete itineraries—thereby achieving the highest level of autonomous decision-making and operational complexity.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is generative AI?\n","A: AI that creates new text, image, or video content learned from existing data.\n","Q: Why can’t a basic LLM give tomorrow’s flight price?\n","A: It has a knowledge cutoff date and lacks live data access.\n","Q: How does an AI agent book the cheapest flight?\n","A: It searches APIs like Expedia, compares options, and books the cheapest.\n","Q: When does AI check visa eligibility?\n","A: Before booking, it calls an immigration agent to verify visa status.\n","Q: Who defines agentic systems into levels?\n","A: The creator of the Agno framework defines them into five levels.\n","\n","KEY CONCEPTS:\n"," generative Artificial Intelligence (AI), large language model (LLM), knowledge cutoff date, tool-augmented LLM, AI agent, agentic Artificial Intelligence (AI), multi-step reasoning, autonomous decision making, travel API integration, weather API integration, immigration AI agent, LangGraph framework\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The lecture centres on covariance as a fundamental statistic for measuring the linear association between two random variables, illustrated by house size (X) and price (Y). Defined as Cov(X,Y)=1n∑(Xi−μX)(Yi−μY), covariance generalises variance, since Cov(X,X)=Var(X). Positive covariance arises when high values of X systematically coincide with high values of Y, producing predominantly positive cross-products of deviations; conversely, negative covariance emerges when high X values align with low Y values. If no systematic co-movement exists, the average product of deviations approaches zero. Because the magnitude of covariance is sensitive to the variables’ scales, it is not dimensionless and therefore cannot compare strengths across different variable pairs; this limitation motivates the introduction of the Pearson correlation coefficient as a scale-invariant, normalised measure of linear dependence.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What does covariance quantify?\n","A: covariance quantify a relationship between size and price\n","Q: Why does covariance become negative?\n","A: X increases Y decreases giving negative products\n","Q: How is covariance formula related to variance?\n","A: covariance of X with X equals variance of X\n","Q: When will covariance always be positive?\n","A: when X increasing implies Y increasing\n","Q: Who introduces the next topic after covariance?\n","A: the speaker will discuss Pearson correlation coefficient\n","\n","KEY CONCEPTS:\n"," variance-covariance matrix, random variables relationship, covariance formula, mean deviation product, positive covariance, negative covariance, quantify relationship, Pearson correlation coefficient, data preprocessing, machine learning statistics, covariance interpretation\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning seeks an optimal policy π*(s,a) that maximises expected cumulative reward through iterative interaction between an agent and its environment. Scalar reward signals encode task objectives, ranging from discrete outcomes (+1 win, −1 loss, 0 draw) in episodic games like Tic-Tac-Toe to continuous metrics such as profit or Sharpe ratio in algorithmic trading. The reward function formally specifies the designer’s goal, guiding value-based, policy-based, or actor–critic algorithms to update state–action value estimates via trial-and-error. By propagating these evaluations back through experienced trajectories, the agent progressively refines its policy toward the one yielding the highest long-term return, thereby solving the sequential decision problem.\n","\n","TOPICS:\n"," ['Reinforcement Learning']\n","\n","Q&A:\n"," Q: What is the objective of any reinforcement learning problem?\n","A: To learn the optimal policy that maximizes a numerical reward signal.\n","Q: Why does an RL agent use trial-and-error learning?\n","A: To explore actions, observe states and rewards, and update its policy.\n","Q: How is the Tic-Tac-Toe RL reward set up?\n","A: +1 for winning, –1 for losing, 0 for draw or during play.\n","Q: When does the agent receive reward in an episodic Tic-Tac-Toe game?\n","A: At the end of the game when it wins, loses, or draws.\n","Q: Who gives feedback on the agent’s action quality in RL?\n","A: The reward signal provides feedback about action quality.\n","\n","KEY CONCEPTS:\n"," reinforcement learning objective, optimal policy maximization, cumulative reward signal, trial-and-error learning, state-action value update, episodic vs continuous tasks, reward function parameterization, risk-adjusted reward metrics, Sharpe ratio reward, policy-based methods, value-based methods, Tic-Tac-Toe reward scheme\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," Python dictionaries are mutable, curly-bracketed key–value stores whose keys must be immutable. Core introspection via items(), keys(), values() exposes pairs or their components; construction proceeds literally or by zipping equal-length lists and casting to dict(). Bracket notation retrieves or mutates entries, del removes them, len() returns cardinality, and list() applied to keys or values regenerates the original sequences, providing a foundational data structure for subsequent data-science workflows with pandas.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A data type consisting of key-value pairs declared within curly brackets.\n","Q: Why must dictionary keys be immutable?\n","A: They can't change and must remain constant for reliable key-value mapping.\n","Q: How is a dictionary created from two lists?\n","A: Zip the lists and pass the result to dict().\n","Q: When is the length of a dictionary nine?\n","A: After deleting one item from the original ten-item dictionary.\n","Q: Who uses dictionaries in later data science work?\n","A: Programmers working with pandas.\n","\n","KEY CONCEPTS:\n"," key-value pair, immutable key, curly-bracket declaration, dict constructor, zip function pairing, square-bracket indexing, in-place value mutation, del statement removal, d.items() method, d.keys() method, d.values() method\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," Organisations are increasingly deploying AI-powered User and Entity Behaviour Analytics (UEBA) to compress the detection-to-containment window for costly insider threats. By continuously learning baseline user and peer-group behaviour for a minimum of seven days, machine-learning models embedded in IBM QRadar SIEM surface subtle deviations, correlate them with MITRE ATT&CK tactics, and rank employees by risk. The unified dashboard presents natural-language summaries of indicators, threat actors, and high-value assets, while analyst feedback loops refine models and shrink investigation times from hours to minutes. IBM’s 2023 survey of 550+ firms shows that extensive AI automation trims breach lifecycles by 108 days and lowers the average insider incident expense of US $4 million, demonstrating measurable gains in operational efficiency and security posture.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What potential time saving did AI offer for breach containment?\n","A: 108 fewer days on average to identify and contain a data breach\n","Q: Why did AI lead to faster containment according to IBM?\n","A: faster containment was a key finding for those that extensively used AI and automation\n","Q: How can AI help detect Insider threats?\n","A: user behavior analytics with AI and machine learning help detect and respond quickly\n","Q: When did the IBM cost of a data breach report emerge?\n","A: the 2023 cost of a data breach report\n","Q: Who faces a major Insider threat concern?\n","A: organizations of all sizes\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), user Behavior analytics (UBA), machine learning, Insider threats, data breach containment, security posture, emerging threats, cost of a data breach report, AI and automation, faster containment, security team, average cost of Insider threat\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta’s open-source Llama 3 family—8 B and 70 B parameter models pre-trained on 50 T tokens across 24k-GPU clusters—delivers doubled 8 k context, superior reasoning, code, translation and dialogue capabilities, and state-of-the-art MMLU, GSM8K, HumanEval and MATH scores that rival proprietary systems while reducing false refusals via multi-turn post-training alignment. Competitive evaluation shows Llama 3 outperforms GPT-4 on HumanEval and GSM8K, though it slightly trails on MMLU and underperforms Gemini Pro 1 on MATH. Meta enforces responsible AI through a five-stage governance pipeline—use-case definition, specification, training, evaluation and system refinement—augmented by Llama Guard and detailed model cards. Access requires registration, compliance approval and signed URLs; official weights, tokenizers and inference recipes are downloadable from Meta, Hugging Face or Kaggle, with GitHub documentation enabling local deployment and experimentation.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Other']\n","\n","Q&A:\n"," Q: What is the speaker’s name?\n","A: Krishak\n","Q: Why did Krishak greet the audience?\n","A: to welcome viewers to his YouTube channel\n","Q: How did Krishak introduce the video?\n","A: by stating his name and welcoming viewers\n","Q: When was the video recorded?\n","A: 2 a.m.\n","Q: Who is speaking in the transcript?\n","A: Krishak\n","\n","KEY CONCEPTS:\n"," Krishak YouTube channel, 2 a.m. livestream intro\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," This workshop transcript guides learners through a hands-on implementation of a Gaussian Naive Bayes classifier in Python with scikit-learn. Beginning with inspection of the finished decision boundary, the instructor deliberately decelerates the pace to expose every coding action, emphasising reproducibility and documentation literacy. Participants navigate the official scikit-learn documentation to instantiate the GaussianNB estimator from the naive_bayes module, fit the model to labelled data, generate predictions, and visualise the resulting class-separating surface. Statistical derivations and probabilistic justifications are intentionally postponed, prioritising immediate coding fluency and confidence with the API. The session thereby equips practitioners to rapidly prototype Naive Bayes solutions while laying groundwork for subsequent deeper dives into algorithmic theory.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What Python library does the instructor rely on for the lesson?\n","A: scikit-learn, often abbreviated sk-learn.\n","Q: Why does the instructor search Google before writing code?\n","A: To use the documentation and figure out how to use library functions.\n","Q: How did the instructor implement the classifier demonstrated?\n","A: Using Gaussian Naive Bayes from scikit-learn.\n","Q: When will learners be able to write the code themselves?\n","A: By the end of the next video or two.\n","Q: Who is the intended audience for this walkthrough?\n","A: Learners who will soon write the Python code themselves.\n","\n","KEY CONCEPTS:\n"," Gaussian Naive Bayes, scikit-learn library, decision boundary, Python code implementation, sklearn documentation, Naive Bayes algorithm, Naive Bayes formula derivation, Gaussian Naive Bayes classifier, Google search for sklearn, algorithm use cases\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The lecture examines the distinction between Gaussian and log-normal distributions, defining the latter through the Gaussian nature of the logarithm of the variable. Gaussian data exhibit symmetric bell-curve concentration (68 %, 95 %, 99.7 % within 1, 2, 3 σ), whereas log-normal data display right-skewed, heavy-tailed behaviour exemplified by personal income and product-review lengths. Correct preprocessing is therefore distribution-dependent: Gaussian features are standardised via (x–μ)/σ, while log-normal features are first log-transformed to approximate Gaussianity before identical centring and scaling. This unified scaling procedure equalises predictor variances, mitigates skew-induced leverage, and enhances downstream model accuracy and stability.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What percentage of data falls within one standard deviation in a Gaussian distribution?\n","A: Around 68 percent\n","Q: Why do we study different distributions like Gaussian and log-normal?\n","A: To choose correct scaling and boost model accuracy\n","Q: How is a log-normal distribution converted to standard normal?\n","A: Take log of each value, then apply (X−μ)/σ\n","Q: When is standard scaling applied after checking data distribution?\n","A: Before feeding data to the model\n","Q: Who follows Gaussian distribution according to the examples?\n","A: Heights of people and iris petal lengths\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, log-normal distribution, standard normal distribution, bell curve, empirical rule, 68-95-99.7 rule, logarithmic transformation, standard scaler, log normalization, mean and standard deviation, feature scaling for model accuracy\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This educational series delivers a complete deep-learning pipeline for potato-leaf disease detection, targeting early and late blight to curb yield loss. Commissioned by AtliQ Agriculture, the project guides learners through collecting healthy and diseased foliage images, augmenting and cleaning the dataset, and training a TensorFlow CNN. The trained model is exported to TensorFlow Lite for quantized, on-device inference within a React-Native mobile app, while TF Serving and FastAPI provide scalable, versioned back-end ML Ops. Deployment on Google Cloud Functions furnishes serverless, low-latency predictions, and an optional React.js web interface supports drag-and-drop diagnosis. Emphasizing reproducibility, the series outlines systematic data preprocessing, model optimization, and cloud-native serving, enabling farmers to photograph crops and receive immediate, accurate treatment recommendations. Mastery of the workflow—covering data engineering, CNN design, TF-Lite conversion, and GCP deployment—equips learners with a portfolio-grade project applicable to other crops and employable across data-science roles.\n","\n","TOPICS:\n"," ['Deep Learning', 'Mlops', 'Python Programming']\n","\n","Q&A:\n"," Q: What two diseases threaten potato farmers?\n","A: early blight and late blight\n","Q: Why must farmers distinguish between early and late blight?\n","A: treatments for early blight and late blight are little different\n","Q: How will farmers interact with the AI system?\n","A: take a picture of the plant and the mobile application will tell\n","Q: When should farmers use the mobile app?\n","A: go to their farm and just take a picture\n","Q: Who is responsible for building the end-to-end application?\n","A: you are a data scientist working for AtliQ Agriculture\n","\n","KEY CONCEPTS:\n"," end-to-end deep learning project, TF serving ML Ops, FastAPI backend server, Google Cloud Functions deployment, React Native mobile app, early blight fungus detection, late blight microorganism identification, convolutional neural network inference, agriculture domain AI solution, potato plant disease classification, AtliQ Agriculture mobile application\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The lecture presents a six-level taxonomy of autonomy in large-language-model applications, ranging from deterministic code (level 0) through single LLM calls (level 1), deterministic chains (level 2), and conditional routers (level 3) to increasingly autonomous agents. Crossing the agentic threshold, state-machine agents (level 4) incorporate cyclic control flow, persistent memory, tool use, and human-in-the-loop checkpoints, with LangGraph exemplifying this paradigm. Fully autonomous agents (level 5), capable of open-ended action selection, remain aspirational. The framework distinguishes non-agentic systems—rigid or conditionally branching—from agentic ones where the LLM dynamically orchestrates iterative, context-aware execution.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the autonomy level of hard-coded applications?\n","A: Zero autonomy, 100% deterministic.\n","Q: Why do single LLM calls often produce confused responses?\n","A: One shot tries to do everything, like asking one person to be expert at all.\n","Q: How do chains improve output quality over a single LLM call?\n","A: Multiple specialist LLMs handle distinct steps instead of one generalist.\n","Q: At the State-machine/agent level with loops and memory.\n","Q: Who controls the flow in chains and routers?\n","A: Humans define the fixed sequences, not the LLM.\n","\n","KEY CONCEPTS:\n"," LLM autonomy levels, Zero-autonomy hard-coded rules, Single LLM call architecture, Sequential AI chains, Router-based dynamic routing, State-machine agent loops, LangGraph state management, Human-in-the-loop approval, Multi-agent hierarchical delegation, Tool-calling API endpoints, Agent-driven control flow, Autonomous agent cycles\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," The transcript provides an expert-level guide to prompt engineering, tracing its evolution from basic techniques to multimodal (text, image, audio) and production-grade implementations. Central methods include multitask learning, which trains models on multiple objectives to create robust shared representations, and knowledge distillation, compressing large teacher models into smaller, accurate students. Optimal data pipelines require meticulous tokenisation, normalisation, and augmentation to deliver clean, unbiased inputs. Deployment is addressed through TensorFlow Serving and Flask-based REST APIs. Throughout, ethical imperatives of bias, fairness, and privacy are treated as design constraints, mandating representative, inclusive datasets to prevent discriminatory outcomes in high-stakes domains such as education, healthcare, and finance.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Generative AI']\n","\n","Q&A:\n"," Q: What types of prompts are covered in the advanced section?\n","A: Text-based, image-based, and audio-based prompts.\n","Q: Why is multitask learning useful when fine-tuning large language models?\n","A: It trains on multiple tasks simultaneously to learn robust, generalizable representations.\n","Q: How does model distillation improve deployment efficiency?\n","A: A smaller model mimics a larger one, becoming faster yet comparably capable.\n","Q: When should TensorFlow Serving be chosen for a prompt engineering model?\n","A: When serving machine-learning models in production environments.\n","Q: Who is responsible for ensuring ethical prompt engineering outcomes?\n","A: Developers must ensure diverse, inclusive data to avoid bias and unfair results.\n","\n","KEY CONCEPTS:\n"," multitask learning fine-tuning, knowledge distillation training, text tokenization normalization, TensorFlow Serving deployment, Flask API serving, ethical bias mitigation, audio-image prompt handling, pre-trained model fine-tuning, data augmentation pipeline, cross-entropy loss optimization, Adam optimizer tuning, T5 small distillation\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:JSON parsing failed. Using fallback regex extraction.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," The lecture illustrates eigenface-based face recognition through singular value decomposition, using action-hero portraits to show how unsupervised learning can separate identities. After cropping and aligning 40 grayscale 200×175-pixel images of Arnold Schwarzenegger and Sylvester Stallone, the 35,000-pixel vectors are mean-centred to form matrix B; economy SVD of B produces orthogonal eigenfaces whose dominant modes capture distinctive traits such as Terminator spectacles. Projecting every image onto the first three principal components yields well-separated point clouds for each actor, allowing nearest-centroid classification of new faces, including synthetic hybrids. Repeating the protocol with Taylor Swift demonstrates that skin-tone similarity can override perceived gender, exposing the limitations of purely correlation-driven, 2-D representations and underscoring the need for modern 3-D-aware algorithms.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," singular value decomposition, eigenfaces, principal component analysis, economy SVD, image vectorization, eigen action heroes, face space projection, three-dimensional face geometry, image classification via clustering, pixel-wise mean subtraction, 35k-dimensional face vectors, skin-tone correlation bias\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is an open-source orchestration framework that transforms large language models from isolated text generators into context-aware, action-oriented agents capable of interacting with external systems. By interposing a modular abstraction layer between LLMs and real-world APIs, databases, and services, it enables multi-step tasks such as flight reservations, hotel bookings, email dispatch, web-scraping, and querying private corporate data without altering underlying code when swapping proprietary or open-weight models. This architecture overcomes LLMs’ intrinsic isolation, converting static conversational intelligence into executable agency across heterogeneous resources while preserving security and scalability. The framework thus extends generative models beyond passive reasoning, positioning them as interactive systems that autonomously manipulate external environments and anticipate expanded enterprise use cases.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is LangChain?\n","A: LangChain is the most popular framework that helps build applications using LLMs.\n","Q: Why is LangChain needed?\n","A: LLMs alone cannot interact with the real world; LangChain bridges them to APIs and databases.\n","Q: How does LangChain help developers?\n","A: It lets you swap models without touching code when cash or needs change.\n","Q: When does the chat interface send the query to the LLM?\n","A: When the user presses enter.\n","Q: Who are the large language models mentioned?\n","A: ChatGPT 3.5, GPT-4, GPT-4 mini, and Hugging Face models.\n","\n","KEY CONCEPTS:\n"," large language models, LangChain framework, real-world API integration, LLM-application bridge, flight booking APIs, restaurant booking APIs, model swap capability, chat application interface, LLM reasoning limitations, external database interaction, email automation via LLM\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residual diagnostics assess time-series model adequacy by analysing the discrepancy between fitted and observed values. After fitting a Holt-Winters exponential-smoothing model to monthly AirPassenger data, the empirical residuals are inspected for zero mean, homoscedasticity and absence of autocorrelation. Autocorrelation and partial autocorrelation functions, histograms and the Ljung-Box test reveal significant serial correlation (p < 0.05), signalling unmodelled seasonality, while the sample mean bias of −0.02 passengers is negligible relative to the 100–600 range. These findings jointly indicate that although the systematic level offset is minimal, residual dependence persists, motivating iterative refinement of the seasonal structure to improve forecast accuracy and achieve white-noise residuals.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time-series analysis?\n","A: The difference between the fitted value ŷ and the actual value y.\n","Q: Why should residuals have zero mean?\n","A: Otherwise the forecast is biased, persistently over- or under-predicting.\n","Q: How is autocorrelation in residuals checked?\n","A: Plot ACF/PACF and run the Ljung-Box test on the residual series.\n","Q: When is the Holt-Winters model fitted in the demo?\n","A: After splitting the air-passenger data into training and test sets.\n","Q: Who presents the residual-analysis tutorial?\n","A: Eagle, a London-based data scientist.\n","\n","KEY CONCEPTS:\n"," residual analysis, time series forecasting, Holt-Winters exponential smoothing, fitted values vs. actual values, autocorrelation of residuals, partial autocorrelation function, Ljung-Box test, zero-mean residuals, seasonality detection, bias correction via mean shift, serial correlation detection\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This project demonstrates a conversational text-to-SQL agent that translates natural-language questions into safe SQLite queries via a ReAct architecture orchestrated in LangGraph. A Next.js TypeScript frontend, styled with Tailwind, collects user input and renders an interactive chat history; watsonx.ai hosts Mistral Large as the reasoning engine, while an in-memory SQLite database stores customer-order relations. LangChain community libraries wire the agent: a GetFromDB tool wraps an execute function that submits double-quoted SQL and returns rows or errors, and a system prompt enforces strict tool invocation and identifier quoting. React state manages message history as an array of HumanMessage, SystemMessage, and AIMessage objects; UI feedback is controlled through an isLoading flag. On start-up, a seed script creates and populates tables, ensuring data availability. The agent autonomously generates and executes multi-step queries, correctly answering aggregate and join questions, and demonstrates guard-railed, read-only access to the database. The reproducible codebase is publicly available.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Langraph', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the main goal of the video?\n","A: Build an AI agent that uses SQL knowledge to connect to databases.\n","Q: Why is LangGraph mentioned in the transcript?\n","A: It will be used to build a ReAct agent.\n","Q: How is the Next.js boilerplate created?\n","A: By running create-next-app at latest via the CLI.\n","Q: When does the speaker run npm run dev?\n","A: After finishing the code for the messages component.\n","Q: Who is creating the Home component in the transcript?\n","A: The speaker inside VS Code.\n","\n","KEY CONCEPTS:\n"," ReAct agent, LangGraph framework, Text2SQL agent, Next.js frontend, watsonx.ai models, SQLite in-memory database, create-next-app CLI, TypeScript boilerplate, Tailwind CSS styling, client-side component, VS Code development, npm run dev server\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering, a natural-language-processing discipline, formulates task-specific inputs that elicit accurate, coherent outputs from large pre-trained language models such as GPT, BERT, or Hugging Face Transformers. Core practices—prompt decomposition, constraint identification, and targeted fine-tuning—replace brittle rule- or keyword-based systems, enabling chatbots, translation, and content-generation applications to achieve greater contextual appropriateness and user engagement. By iteratively refining prompts and model parameters, practitioners mitigate ambiguous queries that otherwise risk biased or inaccurate responses, establishing robust, scalable pipelines. Mastery of these techniques equips developers to design, evaluate, and continuously improve prompt-driven solutions across diverse NLP tasks, advancing state-of-the-art performance while highlighting remaining limitations in model interpretability and data dependency.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A specialized NLP field building models that generate high-quality text outputs in response to prompts.\n","Q: Why is prompt engineering important?\n","A: It yields more accurate, coherent, contextually appropriate text than rule- or keyword-based approaches.\n","Q: How do prompt engineering models work?\n","A: They fine-tune pre-trained large language models like GPT, BERT, or Transformers for specific inputs.\n","Q: When may prompt engineering models struggle?\n","A: When prompts are complex, ambiguous, or when biased data yields inaccurate outputs.\n","Q: Who provides the pre-trained models used in prompt engineering?\n","A: OpenAI, Google, and Hugging Face.\n","\n","KEY CONCEPTS:\n"," prompt engineering, natural language processing, pre-trained large language models, fine-tuning, prompt analysis, contextually appropriate text generation, rule-based text generation, keyword-based text generation, chatbot response generation, language translation software, content generation models, bias in language models\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is an off-policy, value-based reinforcement-learning algorithm that optimizes behavior by iteratively estimating state-action value (Q) functions representing the expected total discounted reward. Using temporal-difference updates, the agent revises Q-values via the Bellman equation, combining immediate rewards with the maximum future Q-value discounted by γ. After each transition, a TD error—observed return minus current estimate—is scaled by learning-rate α and applied as a gradient-style correction, progressively refining the Q-table. Repeated episodes of stochastic action selection under a behavior policy drive exploration, while convergence yields a deterministic target policy that greedily maximizes value. The method operates in fully observable environments such as grid-world navigation, distinguishing itself from supervised and unsupervised learning paradigms through autonomous reward-driven value iteration.\n","\n","TOPICS:\n"," ['Reinforcement Learning']\n","\n","Q&A:\n"," Q: What are the three machine learning paradigms mentioned?\n","A: Supervised, unsupervised, and reinforcement learning.\n","Q: Why does Q-learning use the state-action value function instead of the state value function?\n","A: It quantifies how good it is to take a specific action in a state.\n","Q: How is the observed Q value calculated after an action?\n","A: Reward plus gamma times max future Q value for the next state.\n","Q: When does the agent use the behavior policy?\n","A: During exploration, when choosing actions by random chance.\n","Q: Who initializes the Q table values in the example?\n","A: The user, to arbitrary values or from a prior agent.\n","\n","KEY CONCEPTS:\n"," Q-learning algorithm, value-based reinforcement learning, state-action value function, optimal policy, behavior policy vs target policy, Bellman equation, discount factor gamma, exploration strategy, grid-world environment, fully observable environment, reward signal maximization, Q-table initialization\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," The transcript details the training of a logistic classifier, a linear model that maps input features to class predictions through matrix multiplication WX+b, with learnable parameters W and b. The learning process optimizes these weights to maximize the score of the correct class. Raw scores, or logits, are transformed into a probability distribution using the softmax function, ensuring outputs sum to 1 and emphasize the target class while suppressing others. This approach enables effective single-label classification by converting model outputs into interpretable probabilities, facilitating clear decision boundaries and robust performance in tasks such as image recognition.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A linear classifier that applies a matrix multiply to inputs to generate predictions.\n","Q: Why use the softmax function?\n","A: It turns scores into proper probabilities that sum to 1 for classification.\n","Q: How are scores converted into probabilities?\n","A: By applying the softmax function, making correct-class probability near 1.\n","Q: When are the weights and bias updated?\n","A: During training, while finding values good at performing predictions.\n","Q: Who denotes the inputs, weights, and bias?\n","A: The speaker denotes inputs by X, weights by W, and bias by b.\n","\n","KEY CONCEPTS:\n"," logistic classifier, linear classifier, giant matrix multiply, softmax function, logits, weights matrix, bias term, proper probabilities, score-to-probability conversion, single-label classification, linear function application\n","Saved row 29\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_instruction_full_output.xlsx\n","\n","Instruction-based (Groq) pipeline completed successfully.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_instruction_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/kimi-k2-instruct-0905/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"id":"BfMt8fgDslC6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763962868161,"user_tz":-330,"elapsed":118885,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"beeeeb6e-fddf-4938-e87a-184196641fe3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_instruction_full_output.xlsx\n","\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2112\n","  - BLEU: 0.0165\n","  - BERTScore F1: 0.8626\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.3829\n","  - Micro F1: 0.5237\n","  - Macro F1: 0.4883\n","  - Weighted F1: 0.5101\n","\n","Q&A Generation:\n","  - BLEU: 0.0278\n","  - Diversity: 0.8262\n","  - Answerability: 0.6267\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5800\n","  - Recall@10: 0.2320\n","  - F1@10: 0.3314\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/kimi-k2-instruct-0905/evaluation_final.json\n"]}]}]}