{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMCLZ4aRHIZZ2ZQ7beAAuz5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"aoEbdbj66ad6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763695626568,"user_tz":-330,"elapsed":19643,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"4b9c4841-04a0-4945-bc24-17f79b219414"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d27481bde8efa62766ef49c6ca662e6cb66441346705b875b0877959a6fb3916\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"EG0JzvTw8dn1","executionInfo":{"status":"ok","timestamp":1763695626593,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"775c7bcd-7906-402a-ea29-91c22e14190f"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED ONLY THIS\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25        # SAFE FOR FREE COLAB\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    s = text.find(\"{\")\n","    e = text.rfind(\"}\")\n","    if s == -1 or e == -1 or e <= s:\n","        return {}\n","    try:\n","        return json.loads(text[s:e+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (RATE LIMIT SAFE)\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"Groq failed after all retries\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. INSTRUCTION-BASED TASKS (PROMPTS UNCHANGED)\n","#####################################################################\n","\n","##########################\n","# 8.1 SUMMARISATION\n","##########################\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, 1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = groq_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip() or out[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(partial_summaries)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\").strip() or out2[:900]\n","\n","\n","##########################\n","# 8.2 TOPIC CLASSIFICATION\n","##########################\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...] }}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","\n","##########################\n","# 8.3 Q&A GENERATION\n","##########################\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Generate five question–answer pairs based on the transcript content.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples discussed in the transcript.\n","\n","GUIDELINES:\n","• Create EXACTLY five (5) question–answer pairs.\n","• Each pair should begin with a different question type:\n","  1. What – factual or definitional\n","  2. Why – reasoning or purpose\n","  3. How – process or mechanism\n","  4. When – timing or condition\n","  5. Who – person, system, or entity\n","• Each answer must be directly supported by information in the transcript.\n","• Keep answers concise (maximum 25 words).\n","• Avoid generic or meta questions.\n","• Ensure all questions are technically relevant and educational.\n","\n","OUTPUT FORMAT:\n","Return ONLY a one-line JSON object:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}, ...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    lines = []\n","    for qa in j.get(\"generated_questions\", []):\n","        q = qa.get(\"q\", \"\").strip()\n","        a = qa.get(\"a\", \"\").strip()\n","        if q: lines.append(f\"Q: {q}\")\n","        if a: lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines)\n","\n","\n","##########################\n","# 8.4 KEY CONCEPT EXTRACTION\n","##########################\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list core terminology, methods, or technical phrases.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases.\n","• Exclude generic words.\n","• Capitalise each concept.\n","• No duplicates.\n","\n","OUTPUT FORMAT:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    if not isinstance(concepts, list):\n","        concepts = []\n","\n","    return \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based (Groq) pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"dODog1Je8dvT","executionInfo":{"status":"error","timestamp":1763373346864,"user_tz":-330,"elapsed":420762,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"d935b8f3-d488-4636-fe15-27ef30f12a57"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning with human feedback is a powerful framework that integrates human input into the training process, guiding and accelerating learning. By utilizing algorithms such as Q-learning and proximal policy optimization, human feedback can be incorporated to enable more informed decision-making. This approach is exemplified in ChatGPT, where a rewards model provides human feedback to enhance response generation capabilities. The resulting framework enables more efficient and effective learning, making it a valuable tool for various applications, and demonstrating the potential of human-in-the-loop reinforcement learning to drive advancements in multiple fields.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is reinforcement learning with human feedback?\n","A: A framework integrating human feedback into a reinforcement learning algorithm\n","Q: Why is human feedback used in reinforcement learning?\n","A: To guide and accelerate the learning process\n","Q: How does the rewards model work in chat GPT?\n","A: It assesses and scores the quality of answers\n","Q: When is human feedback used in the training process?\n","A: During the iterative training process\n","Q: Who uses reinforcement learning through human feedback?\n","A: Chat GPT uses it via the rewards model\n","\n","KEY CONCEPTS:\n"," REINFORCEMENT LEARNING ALGORITHM, PROXIMAL POLICY OPTIMIZATION, HUMAN FEEDBACK MECHANISM, REWARD MODEL ARCHITECTURE, GRID WORLD ENVIRONMENT, DECISION-MAKING PROCESS, LEARNING THROUGH INTERACTION, REINFORCEMENT LEARNING FRAMEWORK, REWARDS MODEL TRAINING, FINE-TUNING PROCESS, REINFORCEMENT LEARNING APPLICATION\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," The tutorial covers machine learning, specifically Support Vector Machines (SVMs) and their utilization of kernels, including linear, polynomial, and Gaussian. It demonstrates the impact of kernels using CVX opt and quadratic programming solvers, minimizing equations subject to constraints. The process involves importing numpy and linear algebra functions to solve quadratic programming problems. SVMs can be used for various classification tasks, including linearly separable, nonlinearly separable, and soft margin classification, with kernels simplifying to dot products. The tutorial provides a detailed explanation of SVMs, including the fit method, support vectors, and intercept, laying the groundwork for further exploration of scikit-learn's Support Vector classifier parameters.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What library is used for educational purposes to visualize kernels?\n","A: CVX opt\n","Q: Why is CVX opt used in this tutorial?\n","A: To show kernel impact\n","Q: How does the quadratic programming solver work?\n","A: Minimizes equation with constraints\n","Q: When would you use lib svm?\n","A: For support Vector machine\n","Q: Who wrote the pattern recognition and machine learning book?\n","A: Christopher Bishop\n","\n","KEY CONCEPTS:\n"," Machine Learning Tutorial, Support Vector Machine, CVX Opt Solver, Kernel Application, Quadratic Programming, Nonlinear Visualization, Soft Margin Analysis, Lib SVM Library, Pattern Recognition Models, Quadratic Programming Solver, Convex Optimization Techniques\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a crucial aspect of working with large language models, involving the careful crafting of inputs to generate desired text outputs. Prompts can range from simple to complex and context-dependent, with seven distinct types, including question and statement prompts. The key features of prompts, such as length, language, and constraints, play a significant role in defining the expected output and its quality. By understanding these features and constraints, users can select the most suitable prompt and achieve accurate results, highlighting the importance of prompt engineering in optimizing language model performance.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are prompts in prompt engineering?\n","A: Inputs to prompt engineering models\n","Q: Why is understanding prompts important?\n","A: To generate required output\n","Q: How do key features of prompts impact output?\n","A: Affect complexity and quality\n","Q: When should constraints be included in prompts?\n","A: To set tone or style\n","Q: Who can benefit from understanding prompt types?\n","A: Users of large language models\n","\n","KEY CONCEPTS:\n"," PROMPT ENGINEERING MODELS, LARGE LANGUAGE MODELS, PROMPT TYPES, QUESTION PROMPTS, STATEMENT PROMPTS, MULTIPLE INPUT PROMPTS, PROBLEM PROMPTS, PROMPT FEATURES, PROMPT LENGTH, PROMPT LANGUAGE, CONTEXT CONSTRAINTS, DECONSTRUCTING PROMPTS\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence agents are autonomous problem solvers that make decisions and complete tasks using specific functions. The react agent pattern is a popular approach that mimics human thinking through a cycle of thinking, action, and observation, repeating until a solution is found. This pattern involves an agent thinking about a problem, taking action, and observing the result, then adjusting its approach as needed. By equipping a language model with tools such as API calls or Python functions, agents can be created to solve complex problems, enabling them to make autonomous decisions and complete tasks effectively.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'LangChain', 'Agentic AI']\n","\n","Q&A:\n"," Q: What are Artificial Intelligence (AI) agents?\n","A: Problem solvers that can think on their own\n","Q: Why do agents use tools?\n","A: To complete tasks\n","Q: How is the react agent pattern used to create AI agents?\n","A: By mimicking human thinking\n","Q: When does the observe step occur in the react pattern?\n","A: After taking action\n","Q: Who or what is the brain in the agent diagram?\n","A: The LLM's reasoning ability\n","\n","KEY CONCEPTS:\n"," ARTIFICIAL INTELLIGENCE AGENTS, REACT AGENT PATTERN, REASONING PLUS ACTING, THINK ACTION OBSERVATION LOOP, LANG CHAIN SYSTEM, TOOL EXECUTION CONTROL FLOW, AGENT PROBLEM SOLVING, AUTONOMOUS DECISION MAKING, MULTI-STEP PROBLEM SOLVING, API CALLS INTEGRATION, PYTHON FUNCTION EXECUTION, LANG GRAPH TECHNOLOGY\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The main topic is Langchain and its support for Lsmith, a tool for tracing projects. The reflection agent system is being traced to understand its functionality, particularly how it works with a generation agent to produce a refined viral tweet. Langchain's strong support for Lsmith enables easy tracing of projects by generating an API key and running the project, which records each operation. The results can be viewed in Lsmith, providing a high-level overview of the project's execution, demonstrating the collaboration between the generation and reflection agents in refining a tweet.\n","\n","TOPICS:\n"," ['LangChain', 'Generative AI', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What system will be traced in this section?\n","A: Reflection agent system\n","Q: Why is the system being traced?\n","A: To understand how it works\n","Q: How will the tracing be done?\n","A: Using a website\n","Q: When will the tracing be done?\n","A: In this section\n","Q: Who is performing the tracing?\n","A: The speaker\n","\n","KEY CONCEPTS:\n"," REFLECTION AGENT SYSTEM, VIRAL TWEET, SYSTEM INTEGRATION, AGENT BASED SYSTEM, REFINED OUTPUT, SYSTEM INTEROPERABILITY, FINAL DELIVERY SYSTEM, CHAIN ANALYSIS, SYSTEM ARCHITECTURE, INFORMATION REFLECTION, AGENT BASED MODELING\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," Working with Lang chains chat models involves installing and importing the necessary AI package, initializing a model, and setting up API communication. The process includes installing the python-dotenv package to access an .env file containing an API key, and using the 'invoke' keyword to make API calls. Interacting with a Large Language Model (LLM) involves passing prompts and conversation history to improve response accuracy. A demonstration of using an LLM to find the square root of 49 shows how to access the result using result.content. This approach enables effective communication with APIs, such as Open AI, and facilitates informed responses from the LLM.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What package needs to be installed for Lang chains chat models?\n","A: Lchain Das open aai\n","Q: Why is the latest model expensive?\n","A: It's the most advanced model\n","Q: How is the chat model imported in the file?\n","A: From longchain open AI\n","Q: When can a cheaper model be used?\n","A: If you're short on cash\n","Q: Who released the latest models?\n","A: Open AI\n","\n","KEY CONCEPTS:\n"," ARTIFICIAL INTELLIGENCE APIS, CHAT MODELS INSTALLATION, OPEN ARTIFICIAL INTELLIGENCE, LANG CHAIN DAS, PACKAGE INSTALLATION COMMAND, TERMINAL COMMAND EXECUTION, MODULE IMPORTATION PROCESS, CHAT OPEN ARTIFICIAL INTELLIGENCE CLASS, MODEL INITIALIZATION PROCESS, KEYWORD PARAMETER PASSING, MODEL SELECTION CRITERIA\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," Sorting lists in Python is demonstrated, showing how the sort method handles strings with varying cases and mixed data types. The method prioritizes uppercase letters first and places numbers before strings, with this behavior consistent when reversing the sort order. To achieve specific sorting results, ensuring uniform case in strings may be necessary. Python's sort method can handle mixed data types, but its default behavior must be considered when working with diverse lists, allowing for effective management of list sorting in various scenarios.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," PYTHON SORT METHOD, UPPERCASE LETTERS, LOWERCASE LETTERS, REVERSE ALPHABETICAL ORDER, ALPHABETICAL SORTING, LIST CONTAINING STRINGS, LIST CONTAINING NUMBERS, STRING MANIPULATION TECHNIQUES, CASE SENSITIVITY ISSUES, DATA TYPE HANDLING, LIST INSERTION OPERATION\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," Decision-making involves humans, Artificial Intelligence (AI), or a combination of both, with each having unique strengths. AI excels in certain cases, particularly those with low and high confidence scores, while humans outperform AI in uncertain cases. Augmented intelligence, combining human and AI decision-making, achieves the highest success rate for low and high confidence scores, but is affected by human cognitive bias. Presenting AI information in a way that preserves human judgment, such as optional display, can mitigate automation bias. By combining human and AI decision-making effectively, augmented intelligence can improve outcomes and move from subjective to quantifiable choices, forming a powerful team.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the purpose of a fraud detection system?\n","A: Generate alerts of potentially fraudulent transactions\n","Q: Why should humans and AI work together in decision-making?\n","A: To combine strengths and alleviate workload\n","Q: How does an AI system handle alerts in a fraud detection system?\n","A: By tracking success rate and confidence score\n","Q: When should a human analyst process an alert instead of AI?\n","A: When AI is unsure or has low confidence score\n","Q: Who should make decisions in tasks where AI is unsure?\n","A: Humans\n","\n","KEY CONCEPTS:\n"," ARTIFICIAL INTELLIGENCE SYSTEM, FRAUD DETECTION SYSTEM, HUMAN BIAS ANALYSIS, HOLISTIC CURVES ANALYSIS, CONFIDENCE SCORE EVALUATION, SUCCESS RATE TRACKING, PERFORMANCE CURVE ANALYSIS, FALSE POSITIVES REDUCTION, PREDICTION ACCURACY EVALUATION, DECISION MAKING PROCESS, STATISTICAL TASK ANALYSIS\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Dimitrius discusses building generative applications with Vertex Artificial Intelligence, introducing six new APIs to solve technical challenges. These APIs, including document understanding, embedding, and fact-checking, embed Google's expertise for seamless integration into workflows. They enable developers to focus on unique aspects of their projects, prototype, and combine them with other APIs to build solutions. The APIs aim to streamline the development process, allowing developers to create innovative applications with ease. By leveraging these APIs, developers can tap into Google's expertise and build robust generative applications, enhancing their overall development experience and productivity.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is Demitrius' role at Google?\n","A: Product manager within Cloud Ai\n","Q: Why are new Vertex AI APIs being launched?\n","A: To solve technical challenges in building gen apps\n","Q: How do the new Vertex AI APIs improve gen app development?\n","A: By providing tools to access Enterprise data\n","Q: When can developers expect to see improvements in existing services?\n","A: With the launch of new APIs\n","Q: Who is the target user for the new Vertex AI APIs?\n","A: Developers building generative applications\n","\n","KEY CONCEPTS:\n"," ARTIFICIAL INTELLIGENCE (AI) APPLICATIONS, GENERATIVE APPLICATIONS FOR ENTERPRISES, DOCUMENT UNDERSTANDING API, VECTOR SEARCH APPLICATIONS, RANKING API, GROUNDING GENERATION API, CHECK GROUNDING API, EMBEDDING API, VERTEX ARTIFICIAL INTELLIGENCE (AI) APIS, GENERIC OBJECT DETECTION MODELS, PLANET SCALE APPLICATIONS\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The singular value decomposition (SVD) of a matrix X involves unitary matrices U and V, which preserve angles and lengths of vectors and can be thought of as rotations in vector space. The SVD transforms a sphere of unit vectors into an ellipsoid, with orientation and elongation determined by the singular vectors and values of X. This transformation applies to both square and rectangular matrices, resulting in ellipsoids of varying dimensions, and is a key concept in linear algebra with various applications.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the Singular Value Decomposition (SVD) of a matrix X?\n","A: X = U Σ V^T\n","Q: Why are unitary matrices important in science and engineering?\n","A: They preserve angles and lengths of vectors\n","Q: How do unitary transformations affect vectors in a vector space?\n","A: They rotate vectors, preserving angles and lengths\n","Q: When is a matrix considered unitary?\n","A: When U^T U = U U^T = I\n","Q: Who uses unitary transformations in their work?\n","A: Scientists and engineers\n","\n","KEY CONCEPTS:\n"," UNITARY MATRICES, SINGULAR VALUE DECOMPOSITION, ECONOMY SIZE SVD, FOURIER TRANSFORM, COMPLEX CONJUGATE TRANSPOSE, VECTOR SPACE TRANSFORMATIONS, UNITARY TRANSFORMATIONS, INNER PRODUCT PRESERVATION, GEOMETRIC INTERPRETATION, ELLIPSOIDAL TRANSFORMATIONS, PRINCIPAL COMPONENT ANALYSIS\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Building generative Artificial Intelligence applications using Google Gemini Pro 1.5 is the main topic. This multimodel works with both text and images, enabling long context understanding and experimental features. The model has a context window of up to 1 million multimodal tokens, allowing for extensive capabilities and processing of large data amounts. To utilize this model, an API key can be created, enabling access to various models and features. The model can generate content, answer questions, and produce human-like responses from image and text inputs. Its capabilities are demonstrated through various examples, including PDF queries and image-based text generation, showcasing its potential applications in multiple areas.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the topic of the YouTube video?\n","A: Building AI app with Google Gemini Pro 1.5\n","Q: Why is Google Gemini Pro 1.5 useful?\n","A: It works with both text and images\n","Q: How will the speaker demonstrate Google Gemini Pro 1.5?\n","A: With a demo video and hands-on application\n","Q: When will the speaker show the demo video?\n","A: At the start of the video\n","Q: Who is the creator of the YouTube channel?\n","A: Krishn\n","\n","KEY CONCEPTS:\n"," GENERATIVE ARTIFICIAL INTELLIGENCE, GOOGLE GEMINI PRO, MULTI MODEL ARCHITECTURE, ARTIFICIAL INTELLIGENCE APPLICATION, END TO END PROJECTS, API KEY GENERATION, IMAGE PROCESSING TECHNIQUES, TEXT ANALYSIS ALGORITHMS, MULTIMODAL MACHINE LEARNING, CONTEXT UNDERSTANDING MODELS, EXPERIMENTAL FEATURE DEVELOPMENT\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Evaluating prompt engineering models involves using matrices such as perplexity, accuracy, and human evaluation to measure performance. The evaluation assesses the model's ability to generate accurate and meaningful responses. Techniques for debugging and improving models include analyzing generated responses for common errors and fine-tuning the models. Additionally, testing models on different data sets and tasks determines their ability to generalize, utilizing tools like visualization and cross-validation for comprehensive testing and evaluation, ultimately refining the models' performance and reliability.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What matrices are used to evaluate prompt engineering models?\n","A: Perplexity, accuracy, human evaluation\n","Q: Why is human evaluation used in prompt engineering models?\n","A: To rate response quality\n","Q: How do you evaluate a large language model?\n","A: Using functions like evaluate translation\n","Q: When should you test prompt engineering models on different data sets?\n","A: After initial evaluation\n","Q: Who rates the quality of responses in human evaluation?\n","A: Humans\n","\n","KEY CONCEPTS:\n"," PERPLEXITY MATRICES, LANGUAGE MODEL EVALUATION, PROMPT ENGINEERING MODELS, ACCURACY MEASUREMENT, HUMAN EVALUATION TECHNIQUES, CROSS VALIDATION METHODS, MODEL DEBUGGING TECHNIQUES, DATA SET ANALYSIS, RESPONSE GENERATION ALGORITHMS, MODEL GENERALIZATION TECHNIQUES, VISUALIZATION TOOL APPLICATIONS\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99346, Requested 2128. Please try again in 21m13.536s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99341, Requested 2128. Please try again in 21m9.216s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99329, Requested 2128. Please try again in 20m58.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99311, Requested 2030. Please try again in 19m18.624s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99305, Requested 2030. Please try again in 19m13.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99294, Requested 2030. Please try again in 19m3.936s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99276, Requested 2261. Please try again in 22m7.968s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","SUMMARY:\n"," Generative Artificial Intelligence (AI) creates new content based on patterns learned from existing data, utilizing large language models trained on vast amounts of internet data. This technology enables AI agents to take input, think, and act to complete tasks, leveraging tools and knowledge to make decisions. Agentic AI takes this a step further, involving one or more agents working autonomously to reach complex goals through multi-step reasoning and planning, allowing for the handling of sophisticated tasks such as flight booking and visa applications with increasing complexity.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Agentic AI']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99270, Requested 2261. Please try again in 22m2.784s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99258, Requested 2261. Please try again in 21m52.416s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99484, Requested 2340. Please try again in 26m15.936s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99478, Requested 2340. Please try again in 26m10.752s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99467, Requested 2340. Please try again in 26m1.248s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99449, Requested 2313. Please try again in 25m22.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99443, Requested 2313. Please try again in 25m17.184s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99432, Requested 2313. Please try again in 25m7.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99414, Requested 2215. Please try again in 23m27.456s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99408, Requested 2215. Please try again in 23m22.272s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99397, Requested 2215. Please try again in 23m12.767999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99379, Requested 2417. Please try again in 25m51.744s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," The main overall topic revolves around the discussion of advanced technical methods and their applications. Key ideas include the integration of complex processes and the utilization of specialized tools to achieve precise outcomes. The technical methods employed involve a combination of theoretical frameworks and practical approaches, ensuring a comprehensive understanding of the subject matter. Through the application of these methods, researchers can draw meaningful conclusions and make informed decisions. The overall discussion highlights the importance of technical precision and academic rigor in achieving accurate results, ultimately contributing to the advancement of knowledge in the field.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99373, Requested 2417. Please try again in 25m46.56s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99361, Requested 2417. Please try again in 25m36.192s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99604, Requested 2496. Please try again in 30m14.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99599, Requested 2496. Please try again in 30m10.08s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99587, Requested 2496. Please try again in 29m59.712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99569, Requested 2469. Please try again in 29m20.832s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99563, Requested 2469. Please try again in 29m15.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99552, Requested 2469. Please try again in 29m6.144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99534, Requested 2371. Please try again in 27m25.92s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99528, Requested 2371. Please try again in 27m20.736s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99517, Requested 2371. Please try again in 27m11.232s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99499, Requested 1458. Please try again in 13m46.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," The main overall topic revolves around the discussion of advanced technical methods and their applications. Key ideas include the integration of complex processes and the utilization of specialized tools to achieve precise outcomes. The technical methods employed involve a combination of theoretical frameworks and practical approaches, ensuring a comprehensive understanding of the subject matter. Through the application of these methods, researchers can draw meaningful conclusions and make informed decisions. The overall discussion highlights the importance of a multidisciplinary approach, incorporating various fields of study to foster innovation and progress. Ultimately, the topic emphasizes the significance of technical precision and academic rigor in driving advancements and achieving desired results.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99493, Requested 1458. Please try again in 13m41.664s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99482, Requested 1458. Please try again in 13m32.16s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99716, Requested 1542. Please try again in 18m6.912s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99711, Requested 1542. Please try again in 18m2.591999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99699, Requested 1542. Please try again in 17m52.223999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99681, Requested 1510. Please try again in 17m9.024s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99675, Requested 1510. Please try again in 17m3.839999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99664, Requested 1510. Please try again in 16m54.336s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99646, Requested 1412. Please try again in 15m14.112s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99640, Requested 1412. Please try again in 15m8.928s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2224681143.py\u001b[0m in \u001b[0;36mgroq_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             resp = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99640, Requested 1412. Please try again in 15m8.928s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2224681143.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;31m# 10. RUN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nInstruction-based (Groq) pipeline completed successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2224681143.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mqa_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mconcepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_concepts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error row {idx}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2224681143.py\u001b[0m in \u001b[0;36mgenerate_concepts\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;31m\\\u001b[0m\u001b[0;34m\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroq_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.22\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0mconcepts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"key_concepts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2224681143.py\u001b[0m in \u001b[0;36mgroq_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Groq call failed ({attempt}/{retries}): {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Groq failed after all retries\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED ONLY THIS\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_instruction_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25        # SAFE FOR FREE COLAB\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = f\"{cur} {s}\".strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    if not text:\n","        return {}\n","    s = text.find(\"{\")\n","    e = text.rfind(\"}\")\n","    if s == -1 or e == -1 or e <= s:\n","        return {}\n","    try:\n","        return json.loads(text[s:e+1])\n","    except:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL (RATE LIMIT SAFE)\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content.strip()\n","\n","        except Exception as e:\n","            err_msg = str(e).lower()\n","            if \"cloudflare\" in err_msg or \"503\" in err_msg or \"500\" in err_msg:\n","                  logger.warning(\n","                      f\"[Cloudflare Error] ({attempt}/{retries}) – Internal server error from Groq. Retrying...\"\n","                  )\n","            elif \"429\" in err_msg:\n","                  logger.warning(\n","                      f\"[Rate Limit] ({attempt}/{retries}) – Groq rate limit hit. Retrying...\"\n","                  )\n","            else:\n","                  logger.warning(\n","                      f\"[Groq Error] ({attempt}/{retries}) – {str(e)[:200]} ...\"\n","                  )\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"Groq failed after all retries\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. INSTRUCTION-BASED TASKS (PROMPTS UNCHANGED)\n","#####################################################################\n","\n","##########################\n","# 8.1 SUMMARISATION\n","##########################\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, 1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","\n","        prompt = f\"\"\"\n","INSTRUCTION:\n","Summarise the following educational transcript chunk in clear, academic English.\n","\n","TASK OBJECTIVE:\n","Produce a concise and coherent summary that captures the key ideas, definitions, technical processes, and any conclusions.\n","\n","REQUIREMENTS:\n","• Length: 80–120 words (4–6 sentences) for this chunk.\n","• Start with the main topic or focus of the chunk.\n","• Include important concepts, methods, or examples.\n","• Maintain logical flow and preserve domain-specific terminology.\n","• Do NOT include any reasoning steps or chain-of-thought.\n","• Do NOT add headings, bullet points, or commentary.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","\n","        out = groq_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        chunk_summary = j.get(\"generated_summary\", \"\").strip() or out[:600]\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(partial_summaries)\n","\n","    final_prompt = f\"\"\"\n","INSTRUCTION:\n","Combine the draft summaries into one final summary for the full transcript.\n","\n","TASK OBJECTIVE:\n","Produce a single coherent summary that captures the overall topic, key ideas, technical methods, and any final conclusions.\n","\n","REQUIREMENTS:\n","• Length: 120–160 words.\n","• Start with the main overall topic.\n","• Integrate the main concepts and processes from all chunks.\n","• Maintain logical flow, academic tone, and technical precision.\n","• Do NOT include reasoning steps or chain-of-thought.\n","• Do NOT add headings or bullet points.\n","\n","OUTPUT FORMAT:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Chunk Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\").strip() or out2[:900]\n","\n","\n","##########################\n","# 8.2 TOPIC CLASSIFICATION\n","##########################\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","    topics_list = \", \".join(VALID_TOPICS)\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Classify the educational transcript into one or more relevant topics from the list below.\n","\n","AVAILABLE TOPICS:\n","{topics_list}\n","\n","TASK OBJECTIVE:\n","Identify which topics best describe the main technical and conceptual content of the transcript.\n","\n","GUIDELINES:\n","• Choose up to THREE relevant topics.\n","• Use the summary as a high-level guide.\n","• Only use topics from the AVAILABLE TOPICS list.\n","• If nothing fits, use \"Other\".\n","\n","OUTPUT FORMAT:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...] }}\n","\n","Summary (hint):\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","Transcript Chunk:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","\n","##########################\n","# 8.3 Q&A GENERATION\n","##########################\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Generate five question–answer pairs based on the transcript content.\n","\n","TASK OBJECTIVE:\n","Form comprehension questions that test understanding of key ideas, reasoning, and examples discussed in the transcript.\n","\n","GUIDELINES:\n","• Create EXACTLY five (5) question–answer pairs.\n","• Each pair should begin with a different question type:\n","  1. What – factual or definitional\n","  2. Why – reasoning or purpose\n","  3. How – process or mechanism\n","  4. When – timing or condition\n","  5. Who – person, system, or entity\n","• Each answer must be directly supported by information in the transcript.\n","• Keep answers concise (maximum 25 words).\n","• Avoid generic or meta questions.\n","• Ensure all questions are technically relevant and educational.\n","\n","OUTPUT FORMAT:\n","Return ONLY a one-line JSON object:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}, ...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","\n","    lines = []\n","    for qa in j.get(\"generated_questions\", []):\n","        q = qa.get(\"q\", \"\").strip()\n","        a = qa.get(\"a\", \"\").strip()\n","        if q: lines.append(f\"Q: {q}\")\n","        if a: lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines)\n","\n","\n","##########################\n","# 8.4 KEY CONCEPT EXTRACTION\n","##########################\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","INSTRUCTION:\n","Extract the key technical concepts and terms that represent the main ideas in the transcript.\n","\n","TASK OBJECTIVE:\n","Identify and list core terminology, methods, or technical phrases.\n","\n","GUIDELINES:\n","• Extract 10–12 distinct concepts.\n","• Prefer multi-word technical phrases.\n","• Exclude generic words.\n","• No duplicates.\n","\n","OUTPUT FORMAT:\n","{{\"key_concepts\":[\"Concept 1\",\"Concept 2\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","\n","    out = groq_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","\n","    if not isinstance(concepts, list):\n","        concepts = []\n","\n","    return \", \".join([c.strip() for c in concepts if c.strip()])\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx}\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nInstruction-based (Groq) pipeline completed successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6BWzBxBe_KOW","executionInfo":{"status":"ok","timestamp":1763697836820,"user_tz":-330,"elapsed":2171425,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"cca26a11-ba43-43dc-e187-3b7cbd62c265"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile\n","Groq key loaded ✓\n","Resuming: 17 rows already processed.\n","Skipping row 0\n","Skipping row 1\n","Skipping row 2\n","Skipping row 3\n","Skipping row 4\n","Skipping row 5\n","Skipping row 6\n","Skipping row 7\n","Skipping row 8\n","Skipping row 9\n","Skipping row 10\n","Skipping row 11\n","Skipping row 12\n","Skipping row 13\n","Skipping row 14\n","Skipping row 15\n","Skipping row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," The main topic is Meta Llama 3, an open-source LLM model released by Meta, offering 8 billion and 70 billion pre-trained and instruction-tuned versions. This model excels at language nuances and complex tasks, outperforming other models, including paid LLM models. It has been trained on a large dataset and supports an 8K context length. The model's performance is notable, with competitive results, and is available on Meta, Hugging Face, and Kaggle. Users can access the model by downloading it and following the provided instructions for installation and local inference, with additional examples and source recipes available on GitHub.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the speaker's name?\n","A: Krishak\n","Q: Why is the speaker introducing himself?\n","A: To welcome viewers\n","Q: How does the speaker greet viewers?\n","A: Saying hello\n","Q: When is the speaker recording?\n","A: 2 a.m.\n","Q: Who is speaking?\n","A: Krishak\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The topic revolves around writing Python code for a decision boundary using the scikit-learn library, focusing on the Naive Bayes algorithm, specifically Gaussian Naive Bayes. The scikit-learn library will be utilized extensively, with its documentation accessed through Google. The goal is to enable independent coding of the decision boundary by the end of the next few lessons, leveraging the library's functions and the Gaussian Naive Bayes algorithm to achieve this objective, ultimately enhancing proficiency in writing Python code for decision boundaries.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What library is used in the Python code for the decision boundary?\n","A: scikit-learn\n","Q: Why is Google used in this lesson?\n","A: To access library documentation\n","Q: How will the student learn to write the code?\n","A: Through the next video or two\n","Q: When will the student be able to write the code themselves?\n","A: By the end of the next video or two\n","Q: Who will be able to write the code after the lesson?\n","A: The student\n","\n","KEY CONCEPTS:\n"," DECISION BOUNDARY, PYTHON CODE, PYTHON LIBRARY, SCIKIT-LEARN, NAIVE BAYES, GAUSSIAN NAIVE BAYES, ALGORITHM DERIVATION, USE CASES, CLASSIFIER IMPLEMENTATION, DOCUMENTATION SEARCH, GOOGLE SEARCH RESULTS\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The discussion focuses on log normal distribution, a type of data distribution where the log of a random variable is normally distributed, following a Gaussian distribution with a mean and standard deviation. This distribution is observed in real-world phenomena such as income of people and product reviews. Understanding log normal distribution is crucial for data analysis and modeling as it enables standard scaling, thereby increasing model accuracy and allowing for more precise predictions and insights to be drawn from the data.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is a Gaussian distribution?\n","A: Normal distribution\n","Q: Why are we learning various distributions?\n","A: To understand data patterns\n","Q: How is log normal distribution denoted?\n","A: If log of X is normally distributed\n","Q: When does a distribution follow a log normal distribution?\n","A: If log of X is normally distributed\n","Q: Who uses log normal distribution?\n","A: Data analysts and statisticians\n","\n","KEY CONCEPTS:\n"," GAUSSIAN DISTRIBUTION, NORMAL DISTRIBUTION, LOG NORMAL DISTRIBUTION, STANDARD NORMAL DISTRIBUTION, EMPIRICAL FORMULA, BELL CURVE, SYMMETRICAL DISTRIBUTION, LOG NORMALIZATION, STANDARD SCALING, REGRESSION ALGORITHM, CLASSIFICATION ALGORITHM\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," The main topic is an end-to-end deep learning project in the agriculture domain, focusing on detecting potato diseases using convolutional neural networks. The project involves data collection, model building, and deployment to Google Cloud, utilizing technologies such as TensorFlow, TF Serving, and React Native. The application, developed by AtliQ Agriculture, enables farmers to identify diseases early and prevent economic losses. The project covers technical methods including data pre-processing, image classification, and model deployment on edge devices and Google Cloud, using TensorFlow Lite and Google Cloud Functions. This project can be customized and is beneficial for machine learning engineers and data scientists, demonstrating expertise in deep learning and image classification.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What domain is the deep learning project series focused on?\n","A: Agriculture\n","Q: Why is early detection of diseases important for farmers?\n","A: Prevent economic loss\n","Q: How will the mobile application detect diseases in potato plants?\n","A: Deep learning and CNN\n","Q: When will the farmer know if the potato plant is healthy or diseased?\n","A: After taking a picture\n","Q: Who is building the mobile application for farmers?\n","A: AtliQ Agriculture\n","\n","KEY CONCEPTS:\n"," Deep Learning Project, Machine Learning Ops, TF Serving, Fast API, Google Cloud Functions, React Native, Artificial Intelligence Company, Convolutional Neural Network, End-To-End Application, Model Building, Data Collection\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The levels of autonomy in Large Language Model applications vary, with code having zero autonomy and agents having maximum autonomy. LLM calls have limited autonomy, whereas chains and routers offer more autonomy by breaking tasks into smaller steps and making decisions based on user input. State machines, or agents, combine routers with loops, enabling decision-making, conversation memory, and learning from mistakes, thus achieving higher autonomy. This range of autonomy enables LLMs to perform tasks with varying degrees of complexity and independence, from simple code execution to more complex agent-based interactions.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the level of autonomy in code-based applications?\n","A: Zero autonomy\n","Q: Why do chains have a disadvantage in llm applications?\n","A: Fixed sequences, like a rigid assembly line\n","Q: How do routers handle user input in llm applications?\n","A: By deciding which steps to take next\n","Q: When are state machines considered agent-executed in llm applications?\n","A: When the control flow is controlled by the llm\n","Q: Who or what is responsible for decision-making in a state machine?\n","A: The llm itself\n","\n","KEY CONCEPTS:\n"," LEVELS OF AUTONOMY, LLM APPLICATIONS, CODE AUTONOMY, SINGLE LLM CALL, CHAIN AUTONOMY, ROUTER AUTONOMY, STATE MACHINE AUTONOMY, AGENT EXECUTED AUTONOMY, HUMAN DRIVEN APPROACH, AGENT DRIVEN APPROACH, MULTI-AGENT SYSTEMS, ADVANCED MEMORY MANAGEMENT\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is the main overall topic, involving the handling of various prompt types, including text, image, and audio-based prompts. Advanced techniques like multitask learning and distillation are utilized to fine-tune pre-trained large language models. The process also entails data pre-processing and cleaning, comprising tokenization and normalization. For deployment, frameworks such as TensorFlow Serving or Flask can be employed. Furthermore, ethical considerations, including fairness and privacy, are essential to consider in prompt engineering, ensuring responsible model development and use.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is prompt engineering\n","A: Field of study on designing inputs for AI models\n","Q: Why is multitasking learning used\n","A: To learn robust representations for generalization\n","Q: How is distillation used in training\n","A: Training smaller model to mimic larger model's behavior\n","Q: When is tokenization used\n","A: During data pre-processing to break down text\n","Q: Who can use prompt engineering models\n","A: Developers and users of AI-powered applications\n","\n","KEY CONCEPTS:\n"," ADVANCED TECHNIQUES FOR FINE-TUNING, MULTITASK LEARNING APPROACH, DISTILLATION TECHNIQUE, DATA PRE-PROCESSING METHODS, TOKENIZATION PROCESS, NORMALIZATION TECHNIQUE, DEPLOYING MACHINE LEARNING MODELS, MULTINOMIAL LOGISTIC REGRESSION, CROSS ENTROPY LOSS FUNCTION, TRANSFER LEARNING TECHNIQUE, SELF-SUPERVISED LEARNING TECHNIQUE\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The singular value decomposition is applied to sets of images of celebrities to find eigenfaces, a method for image classification. Images are converted to greyscale, then the average face is computed and subtracted from each image. The SVD is applied to the resulting matrix, yielding eigenfaces from the U matrix columns. Projections into the first three eigenfaces create a three-dimensional representation of each image, demonstrating good separation between individuals. Experiments with various pairs, including Arnold Schwarzenegger, Sylvester Stallone, and Taylor Swift, showcase the effectiveness of eigenfaces in distinguishing between faces, highlighting its potential for image classification tasks.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is the size of the matrix B used for SVD computation?\n","A: 35,000 by 40\n","Q: Why is the separation between Arnold and Stallone not perfect?\n","A: Overlap in distributions\n","Q: How are the images projected into the first three eigenfaces?\n","A: Image vector times library transpose\n","Q: When is the average face computed and used to subtract from all images?\n","A: After loading all images\n","Q: Who are the two action heroes used as examples in the eigenfaces demonstration?\n","A: Arnold Schwarzenegger and Sylvester Stallone\n","\n","KEY CONCEPTS:\n"," EIGENVALUE DECOMPOSITION, SINGULAR VALUE DECOMPOSITION, PRINCIPAL COMPONENT ANALYSIS, EIGENFACES, IMAGE CLASSIFICATION, FACE RECOGNITION, NEURAL NETWORK ARCHITECTURES, DEEP LEARNING ALGORITHMS, DIMENSIONALITY REDUCTION, FEATURE EXTRACTION, IMAGE COMPRESSION\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," Lang chain is a framework that bridges the gap between large language models and the real world, enabling applications to leverage their reasoning abilities while interacting with external systems. It allows for seamless integration with various APIs, databases, and services, and supports multiple language models without requiring code modifications. By expanding AI capabilities, Lang chain enables access to real-world services, such as booking and browsing, and allows AI to interact with private databases, send emails, and scrape websites. This technology enhances AI's ability to act in the real world, making it a powerful tool for AI development, with various potential applications to be explored.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is Lang chain?\n","A: Framework bridging LLMS and real world\n","Q: Why can't LLMS interact with the real world?\n","A: Limited to trained data\n","Q: How does Lang chain help build applications?\n","A: Acts as bridge between LLMS and APIs\n","Q: When can Lang chain be used?\n","A: Building applications with LLMS\n","Q: Who can use Lang chain?\n","A: Developers building AI applications\n","\n","KEY CONCEPTS:\n"," Large Language Models, LLM Model, Chat Application Interface, Real World APIs, Artificial Intelligence Framework, Lang Chain Framework, Language Model Training, Real World Interaction, API Database Integration, Machine Learning Applications, Natural Language Processing\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residual analysis is a crucial tool for improving forecasting methods in time series data, involving the examination of differences between fitted and actual values to detect trends and inconsistencies. Key aspects include checking for autocorrelation and ensuring the mean of residuals is zero. Techniques like the Ljung-Box test and histograms are used to analyze residuals, with libraries such as statsmodels in Python facilitating model diagnosis. Residual analysis reveals issues like autocorrelation and bias, guiding model improvement. By identifying these issues, residuals help build accurate forecasting models, with analysis results indicating whether a model is relatively unbiased or requires refitting to achieve better performance.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: Differences between fitted and actual values\n","Q: Why is residual analysis important in forecasting methods?\n","A: To diagnose performance and improve models\n","Q: How can residual analysis be used to improve forecasting methods?\n","A: To detect trends and inconsistencies\n","Q: When should residuals have no autocorrelation or partial autocorrelation?\n","A: Always, to ensure no missed information\n","Q: Who can use residual analysis to improve their forecasting methods?\n","A: Data scientists and forecasters\n","\n","KEY CONCEPTS:\n"," RESIDUAL ANALYSIS, TIME SERIES FORECASTING, PARTIAL AUTO CORRELATION, AUTO CORRELATION FUNCTION, YOUNG BOX TEST, EXPONENTIAL SMOOTHING MODEL, HOLT WINTERS MODEL, RESIDUAL DIAGNOSTICS, MODEL PERFORMANCE EVALUATION, FORECASTING METHODS, STATISTICAL SIGNIFICANCE TESTING, SERIAL CORRELATION ANALYSIS\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," Building an Artificial Intelligence agent that interacts with databases is the main focus, utilizing LangGraph, Next.js, and models on Watsonx.ai. The project involves creating a Next.js boilerplate, modifying the pages.tsx file, and implementing a joke-telling feature using SQL and NoSQL databases. The process utilizes LangGraph and LangChain libraries, and involves setting up a database, configuring environment variables, and creating state variables to manage input and message history. A large language model is used to generate SQL queries from natural language, and a TXS SQL agent is built to execute SQL queries against a database. The application is tested in the browser, successfully generating SQL queries and providing answers to complex questions, demonstrating the creation of a text to SQL agent with guardrails to limit database control.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Python Programming']\n","\n","Q&A:\n"," Q: What type of database will be used in the project?\n","A: SQLite\n","Q: Why is LangGraph used in the project?\n","A: To build a ReAct agent\n","Q: How will the Next.js application be started?\n","A: By running npm run dev\n","Q: When should you move into the project directory?\n","A: Before starting the application\n","Q: Who will be using the Next.js CLI to set up the project?\n","A: The developer\n","\n","KEY CONCEPTS:\n"," ARTIFICIAL INTELLIGENCE AGENT, LANGUAGE MODELS TRAINING, SQL KNOWLEDGE CONNECTION, LANGGRAPH REACT AGENT, NEXT JS FRONTEND APPLICATION, WATSONX AI MODELS, IN MEMORY DATABASE MANAGEMENT, SQLITE DATABASE SYSTEM, TYPESCRIPT PROGRAMMING LANGUAGE, TAILWIND CSS FRAMEWORK, CLIENT SIDE COMPONENT RENDERING\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering, a field within natural language processing, involves building models to generate high-quality text outputs in response to prompts, utilizing pre-trained large language models fine-tuned for specific tasks. It aims to produce accurate and coherent text outputs, essential for applications like chatbots and language translation. Despite its benefits, prompt engineering models may face challenges with complex prompts or biased outputs. This field provides a foundation for understanding the basics, fine-tuning techniques, and limitations of prompt engineering, ultimately enhancing its applications and addressing potential drawbacks.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: Field within NLP focusing on building models for high-quality text outputs.\n","Q: Why is prompt engineering important?\n","A: Generates accurate, coherent, and contextually appropriate text outputs.\n","Q: How are prompt engineering models based?\n","A: Pre-trained large language models fine-tuned for specific tasks.\n","Q: When can prompt engineering models struggle?\n","A: With complex and ambiguous prompts.\n","Q: Who can benefit from this course on prompt engineering?\n","A: Those interested in NLP and chatbots.\n","\n","KEY CONCEPTS:\n"," NATURAL LANGUAGE PROCESSING, PROMPT ENGINEERING MODELS, PRE-TRAINED LARGE LANGUAGE MODELS, FINE-TUNED LANGUAGE MODELS, RULE BASED APPROACHES, KEYWORD BASED APPROACHES, CONTEXTUALLY APPROPRIATE RESPONSES, LANGUAGE TRANSLATION SOFTWARE, CHATBOT DEVELOPMENT, PROMPT ANALYSIS TECHNIQUES, ADVANCED FINE-TUNING TECHNIQUES\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning involves machines learning to map situations to actions to maximize a numerical reward signal. Q-learning, a value-based method, determines a value function using a state-action value function, or Q-value, and updates it through a Q-table based on the Bellman equation and temporal difference learning. The Q-value is updated using a gradient update rule with a learning rate alpha, combining the current Q-value with the temporal difference error. This process is repeated over multiple episodes, allowing an agent to select actions based on the highest Q-value once stable. As an off-policy algorithm, Q-learning decouples the behavior policy from the target policy, enabling optimal Q-table learning to maximize total reward.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is Q-learning?\n","A: Value-based reinforcement learning method\n","Q: Why do we use value-based methods?\n","A: To maximize total reward\n","Q: How does Q-learning work?\n","A: By learning state-action value function\n","Q: When is the state-action value function used?\n","A: To quantify total reward\n","Q: Who uses the Q-table?\n","A: The agent\n","\n","KEY CONCEPTS:\n"," REINFORCEMENT LEARNING, Q-LEARNING ALGORITHM, VALUE BASED METHODS, POLICY BASED METHODS, STATE VALUE FUNCTIONS, STATE ACTION VALUE FUNCTIONS, Q-VALUE CALCULATION, BELLMAN EQUATION, DISCOUNT FACTOR, OPTIMAL POLICY, EXPLORATION POLICY\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," Logistic classification is a process that involves training a linear classifier to generate predictions from input data. This process applies a linear function, represented by a matrix multiply of input, weight, and bias, to produce scores. The goal of logistic classification is to find optimal weights and bias values through training. The scores are then converted into probabilities using a softmax function, which transforms them into proper probabilities that sum to 1. This enables the model to predict the correct class label for each input, such as image pixels, and is a crucial step in image classification tasks, ultimately allowing for accurate predictions to be made.\n","\n","TOPICS:\n"," ['Machine Learning', 'Statistics', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: Linear classifier\n","Q: Why do we use a softmax function?\n","A: To turn scores into probabilities\n","Q: How do we turn scores into probabilities?\n","A: Using a softmax function\n","Q: When do we use the softmax function?\n","A: After generating scores\n","Q: Who denotes the inputs in the logistic classifier?\n","A: X\n","\n","KEY CONCEPTS:\n"," LINEAR CLASSIFIER, LINEAR FUNCTION, GIANT MATRIX MULTIPLY, INPUT VECTOR, OUTPUT CLASS, WEIGHTS MATRIX, BIASED TERM, MACHINE LEARNING MODEL, SOFTMAX FUNCTION, PROPER PROBABILITIES, LOGISTIC REGRESSION, LINEAR CLASSIFICATION TASK\n","Saved row 29\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_instruction_full_output.xlsx\n","\n","Instruction-based (Groq) pipeline completed successfully.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_instruction_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ioibhO1Smyj4","executionInfo":{"status":"ok","timestamp":1763699100853,"user_tz":-330,"elapsed":104789,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"2c2f87c5-60a2-4417-dce2-67f3a28d1072"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_instruction_full_output.xlsx\n","\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2901\n","  - BLEU: 0.0587\n","  - BERTScore F1: 0.8863\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.8667\n","  - Jaccard Index: 0.3411\n","  - Micro F1: 0.4598\n","  - Macro F1: 0.4189\n","  - Weighted F1: 0.4275\n","\n","Q&A Generation:\n","  - BLEU: 0.0297\n","  - Diversity: 0.7584\n","  - Answerability: 0.6867\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.6267\n","  - Recall@10: 0.2507\n","  - F1@10: 0.3581\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Instruction Prompting/llama-3.3-70b-versatile/evaluation_final.json\n"]}]}]}