{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbfHMukRRnBBALihtUanxi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bee2a45fadad45fb9ad656e438c64d0b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63ed0dd623f542a3a1e9ede40f00a80e","IPY_MODEL_19f6a49b22f1466983e9972ecb8ead47","IPY_MODEL_1e6990df5e7540428e6e461315a5c579"],"layout":"IPY_MODEL_cefb94ce58bc431181484722e435331a"}},"63ed0dd623f542a3a1e9ede40f00a80e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e567f70c4154999bf861a26e00f8c27","placeholder":"​","style":"IPY_MODEL_f5d1e8e423174b0680031286484298cf","value":"tokenizer_config.json: 100%"}},"19f6a49b22f1466983e9972ecb8ead47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7759c2469044b1c9fbdae6e82f1f06f","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e322d1677596412eb9fc3bdb19e51d3c","value":25}},"1e6990df5e7540428e6e461315a5c579":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc26b48e0e8a45b7a6c81e2efdd99eac","placeholder":"​","style":"IPY_MODEL_61c63d1c372c4c93b42f65504afd562f","value":" 25.0/25.0 [00:00&lt;00:00, 369B/s]"}},"cefb94ce58bc431181484722e435331a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e567f70c4154999bf861a26e00f8c27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5d1e8e423174b0680031286484298cf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7759c2469044b1c9fbdae6e82f1f06f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e322d1677596412eb9fc3bdb19e51d3c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc26b48e0e8a45b7a6c81e2efdd99eac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61c63d1c372c4c93b42f65504afd562f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b058d5b30d0b45f1b645a09442989848":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_568178db0fe2495a8e8f0dfe3cc700d5","IPY_MODEL_0ef9cac58da742509d7d304f4475fc00","IPY_MODEL_be1b0bddf60540589bab8b412d8b4ae1"],"layout":"IPY_MODEL_de76db72f7e4417ab4b5ba95381b6a1e"}},"568178db0fe2495a8e8f0dfe3cc700d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3b517d2d7474bcd9f8e23730d6fe341","placeholder":"​","style":"IPY_MODEL_7bc85f9f4fd84dc48536534482a955d9","value":"config.json: 100%"}},"0ef9cac58da742509d7d304f4475fc00":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f99e8b4d70841ae8cb201c433610ebc","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e886e0f75e634510b99cd37960a3c6b4","value":482}},"be1b0bddf60540589bab8b412d8b4ae1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce5e1dafa5ba4068be26ee4034b95d9e","placeholder":"​","style":"IPY_MODEL_ed73bc5d29ed4ff9af624954ed128df5","value":" 482/482 [00:00&lt;00:00, 4.57kB/s]"}},"de76db72f7e4417ab4b5ba95381b6a1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3b517d2d7474bcd9f8e23730d6fe341":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bc85f9f4fd84dc48536534482a955d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f99e8b4d70841ae8cb201c433610ebc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e886e0f75e634510b99cd37960a3c6b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce5e1dafa5ba4068be26ee4034b95d9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed73bc5d29ed4ff9af624954ed128df5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"912012f02d554daabda0d0ce47230c38":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fce22f3f5e541bab9f89ce5789ccc87","IPY_MODEL_4c849ec38c0241978545438c50bca6a0","IPY_MODEL_639c63fd6ca24b09a79a123476cc9b39"],"layout":"IPY_MODEL_52a3b4966543485f8fb8bfebf9504fad"}},"6fce22f3f5e541bab9f89ce5789ccc87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0cb990fb904449ab997b74945d4ddf3","placeholder":"​","style":"IPY_MODEL_15c2b26dfb264debaa96f73ded50426a","value":"vocab.json: 100%"}},"4c849ec38c0241978545438c50bca6a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8397d6c73b345f3bae5522392a5fb0a","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3378901d9acf428ebf51042bcc12f786","value":898823}},"639c63fd6ca24b09a79a123476cc9b39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_262fe5b2fba8430998e8bd2d3c207308","placeholder":"​","style":"IPY_MODEL_180c270ebd3f4087b2dbaef780b74441","value":" 899k/899k [00:00&lt;00:00, 6.00MB/s]"}},"52a3b4966543485f8fb8bfebf9504fad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0cb990fb904449ab997b74945d4ddf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15c2b26dfb264debaa96f73ded50426a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8397d6c73b345f3bae5522392a5fb0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3378901d9acf428ebf51042bcc12f786":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"262fe5b2fba8430998e8bd2d3c207308":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"180c270ebd3f4087b2dbaef780b74441":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0818eeb7cef41308075fa630b280cc9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_81a17813426c403dac53a91609c50de5","IPY_MODEL_b8bb5ac8d8894be8a1093ba3148ae44b","IPY_MODEL_f1bf5c58796f41c1aa034d10e2d7b917"],"layout":"IPY_MODEL_641aaedd6e5d4cc2a311646e3eff98cc"}},"81a17813426c403dac53a91609c50de5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f0ae351837c4f27b8d912364b306011","placeholder":"​","style":"IPY_MODEL_7024adf426404efc984e629ad5540ed7","value":"merges.txt: 100%"}},"b8bb5ac8d8894be8a1093ba3148ae44b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb56026f8fcc4143b01c507cc7f32e2e","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4023c361c6dd47539e600ff13345e5e0","value":456318}},"f1bf5c58796f41c1aa034d10e2d7b917":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ddcde5fcd77d4d648e0b6ec926c6cd99","placeholder":"​","style":"IPY_MODEL_45a2201cb8a1408b97fbc6f6cc8ce8bd","value":" 456k/456k [00:00&lt;00:00, 3.98MB/s]"}},"641aaedd6e5d4cc2a311646e3eff98cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f0ae351837c4f27b8d912364b306011":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7024adf426404efc984e629ad5540ed7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb56026f8fcc4143b01c507cc7f32e2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4023c361c6dd47539e600ff13345e5e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ddcde5fcd77d4d648e0b6ec926c6cd99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45a2201cb8a1408b97fbc6f6cc8ce8bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"867023b48067420b8514a15d905c6d94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54d8462f4c7f422e98d80b9598ecd1d3","IPY_MODEL_85498066d9504f1ea59f76a664e5df0e","IPY_MODEL_858a6c64b78b4879844aedaf7cdcfbb5"],"layout":"IPY_MODEL_4563dacd0902441d94bb383da1d798d7"}},"54d8462f4c7f422e98d80b9598ecd1d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f318b865a63341fe87309d517cc84c26","placeholder":"​","style":"IPY_MODEL_cc0acbf6cf4e4a22bbf848fd23c0d509","value":"tokenizer.json: 100%"}},"85498066d9504f1ea59f76a664e5df0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2da0473ec63c49df8ea2c53949e99cff","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_939883940da641d681c6a9b05bc7ded8","value":1355863}},"858a6c64b78b4879844aedaf7cdcfbb5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a6268c42e134c82b35afb424f2e7603","placeholder":"​","style":"IPY_MODEL_fd0f773d5a07416db8fdc6f28a4f8962","value":" 1.36M/1.36M [00:00&lt;00:00, 11.4MB/s]"}},"4563dacd0902441d94bb383da1d798d7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f318b865a63341fe87309d517cc84c26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc0acbf6cf4e4a22bbf848fd23c0d509":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2da0473ec63c49df8ea2c53949e99cff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"939883940da641d681c6a9b05bc7ded8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a6268c42e134c82b35afb424f2e7603":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd0f773d5a07416db8fdc6f28a4f8962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d45b51a2dbd540849f256935304ea5b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3188ce04b57f47b684686e5e7befe32c","IPY_MODEL_b2c57a3c8a3a4281a2e2b98ec23de3ff","IPY_MODEL_0d70364384914c9eb531885a93ff2819"],"layout":"IPY_MODEL_2e51ffedbd4b400dbe69f105ea7d61a1"}},"3188ce04b57f47b684686e5e7befe32c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e30e5baa60854b1c881e8242b091a4fb","placeholder":"​","style":"IPY_MODEL_0d1aeb38d5974e788185e7541f7aa0e5","value":"model.safetensors: 100%"}},"b2c57a3c8a3a4281a2e2b98ec23de3ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a3197df89e44af3be9312efe2427016","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d40b44cfc14b4f95a3f203170fd047eb","value":1421700479}},"0d70364384914c9eb531885a93ff2819":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67f527264a3941bcb3847d5b90591e44","placeholder":"​","style":"IPY_MODEL_49bc16cc91a14b8482c7e16a852a8c35","value":" 1.42G/1.42G [00:26&lt;00:00, 45.3MB/s]"}},"2e51ffedbd4b400dbe69f105ea7d61a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e30e5baa60854b1c881e8242b091a4fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d1aeb38d5974e788185e7541f7aa0e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a3197df89e44af3be9312efe2427016":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d40b44cfc14b4f95a3f203170fd047eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67f527264a3941bcb3847d5b90591e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49bc16cc91a14b8482c7e16a852a8c35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Aysfb-l4WJ2b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764043225810,"user_tz":-330,"elapsed":28550,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"3ac90865-4fa6-4a53-fb16-e4208fe8d9ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=f1891f9189b0d04ee0568aeec2426f0f9f3631017315ff2829e812f5a82bbc3a\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"4g9gOHMnKmNs","executionInfo":{"status":"ok","timestamp":1764043225823,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"aa8f3295-b0c4-4965-a4b0-0e20f638a8a9"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["# ================================================================\n","# Few-Shot Prompting Pipeline – Groq\n","# ================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","# ================================================================\n","# 1. FEW-SHOT EXAMPLES\n","# ================================================================\n","\n","FEWSHOT_SUMMARIES = [\n","    {\"input\": \"Explains attention in transformers and its role in capturing long-range dependencies.\",\n","     \"output\": \"The lecture introduces attention in transformers, showing how query, key, and value vectors enable models to weigh relevant tokens. It contrasts this with RNN limitations and demonstrates gains on translation and summarisation.\"},\n","    {\"input\": \"CNN architecture for image classification.\",\n","     \"output\": \"This tutorial covers convolutional, pooling, and fully connected layers, explaining hierarchical feature extraction and typical training steps for vision classification tasks.\"},\n","    {\"input\": \"Reinforcement learning agents learn by reward feedback.\",\n","     \"output\": \"The session formalises RL with policies, rewards, and value estimation. It compares Q-learning and policy gradients, discusses exploration–exploitation, and highlights robotics and gaming use cases.\"},\n","    {\"input\": \"Prompt engineering improves LLM outputs.\",\n","     \"output\": \"Zero-shot, few-shot, and chain-of-thought prompts are compared. The talk emphasises instruction clarity, role specification, and constraint setting to improve reliability and reasoning.\"},\n","    {\"input\": \"MLOps pipelines for reliable deployment.\",\n","     \"output\": \"The talk explains CI/CD for models, experiment tracking, model registries, and monitoring, with tools such as MLflow and Kubeflow for production-grade ML.\"}\n","]\n","\n","FEWSHOT_TOPICS = [\n","    {\"input\": \"Explaining self-attention and BERT internals.\", \"output\": [\"Natural Language Processing\"]},\n","    {\"input\": \"Building CNNs with pooling for object recognition.\", \"output\": [\"Deep Learning\"]},\n","    {\"input\": \"Learning with rewards via Q-learning.\", \"output\": [\"Reinforcement Learning\"]},\n","    {\"input\": \"Designing prompts to improve LLM reasoning.\", \"output\": [\"Prompt Engineering\"]},\n","    {\"input\": \"Automating ML deployment with pipelines and monitoring.\", \"output\": [\"Mlops\"]},\n","    {\"input\": \"Creating data visualisations and feature analysis.\", \"output\": [\"Data Science\"]},\n","    {\"input\": \"Explaining model fine-tuning for generative image models.\", \"output\": [\"Generative AI\"]},\n","    {\"input\": \"Discussing NLP and ML synergy for LLMs.\", \"output\": [\"Natural Language Processing\", \"Machine Learning\"]},\n","]\n","\n","FEWSHOT_QA = [\n","    {\"q\": \"What does attention allow models to do?\",\n","     \"a\": \"It lets models focus on the most relevant tokens in a sequence.\"},\n","    {\"q\": \"Why are convolutions useful in vision?\",\n","     \"a\": \"They extract local spatial features for image classification.\"},\n","    {\"q\": \"How do agents learn in reinforcement learning?\",\n","     \"a\": \"They learn by maximising cumulative rewards through trial and error.\"},\n","    {\"q\": \"When is few-shot prompting effective?\",\n","     \"a\": \"When limited task-specific data exists but examples guide behaviour.\"},\n","    {\"q\": \"Who typically maintains ML pipelines in production?\",\n","     \"a\": \"Machine learning engineers and DevOps teams.\"}\n","]\n","\n","FEWSHOT_CONCEPTS = [\n","    [\"Self-Attention Mechanism\", \"Query-Key-Value\", \"Positional Encoding\"],\n","    [\"Convolutional Layer\", \"Pooling Operation\", \"Feature Map\"],\n","    [\"Reward Function\", \"Policy Gradient\", \"Q-Learning\"],\n","    [\"Few-Shot Prompting\", \"Chain-of-Thought Reasoning\", \"Instruction Tuning\"],\n","    [\"CI/CD Pipeline\", \"Model Registry\", \"Experiment Tracking\"]\n","]\n","\n","# ================================================================\n","# 2. PATHS & API\n","# ================================================================\n","\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_fewshot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key4.txt\"\n","\n","def load_key(path):\n","    with open(path) as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","# ================================================================\n","# 3. GLOBAL CONFIG\n","# ================================================================\n","\n","MODEL_NAME = \"openai/gpt-oss-20b\"\n","GLOBAL_MIN_GAP = 15\n","LAST_TS = 0.0\n","MAX_CHARS = 2600\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","# ================================================================\n","# 4. LOGGING\n","# ================================================================\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","\n","# ================================================================\n","# 5. CLEANING & CHUNKING\n","# ================================================================\n","\n","def deep_clean(t):\n","    t = str(t)\n","    t = re.sub(r\"https?://\\S+\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","def chunk_text(text, max_chars=MAX_CHARS):\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean]\n","    sents = re.split(r\"(?<=[.!?])\\s+\", clean)\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) < max_chars:\n","            cur += \" \" + s\n","        else:\n","            chunks.append(cur.strip())\n","            cur = s\n","    if cur.strip(): chunks.append(cur.strip())\n","    return chunks\n","\n","# ================================================================\n","# 6. JSON EXTRACTION\n","# ================================================================\n","\n","def extract_json(txt):\n","    try:\n","        s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","        if s == -1 or e == -1:\n","            return {}\n","        return json.loads(txt[s:e+1])\n","    except:\n","        return {}\n","\n","# ================================================================\n","# 7. GROQ CALL (RELIABLE)\n","# ================================================================\n","\n","def groq_call(prompt, temperature=0.2, retries=3):\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        time.sleep(GLOBAL_MIN_GAP - (now - LAST_TS))\n","\n","    for attempt in range(retries):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content\n","        except Exception as e:\n","            print(f\"Retry {attempt+1}/{retries}: {e}\")\n","            time.sleep(4)\n","\n","    return \"\"\n","\n","# ================================================================\n","# 8. FEW-SHOT TASKS\n","# ================================================================\n","\n","# ------ SUMMARY ------\n","def generate_summary(transcript):\n","    chunks = chunk_text(transcript)\n","    partial = []\n","\n","    fewshot = \"\\n\\n\".join([f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_SUMMARIES])\n","\n","    for c in chunks:\n","        prompt = f\"\"\"\n","Learn from examples:\n","{fewshot}\n","\n","Now summarise the transcript chunk.\n","Return ONLY JSON:\n","{{\"generated_summary\":\"...\"}}\n","\n","CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\n","\"\"\"\n","        out = groq_call(prompt, 0.15)\n","        j = extract_json(out)\n","        partial.append(j.get(\"generated_summary\", \"\"))\n","\n","    combined = \" \".join(partial)\n","\n","    final_prompt = f\"\"\"\n","Combine the drafts into a 120–160 word summary.\n","Return ONLY JSON: {{\"generated_summary\":\"...\"}}\n","\n","DRAFTS:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\n","\"\"\"\n","    out2 = groq_call(final_prompt, 0.15)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\")\n","\n","# ------ TOPICS ------\n","def classify_topic(transcript, summary):\n","    text = summary + \" \" + transcript[:2000]\n","\n","    examples = \"\\n\".join(\n","        [f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_TOPICS]\n","    )\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Pick up to 3 topics from:\n","{', '.join(VALID_TOPICS)}\n","\n","Return JSON: {{\"predicted_topics\":[\"...\"]}}\n","\n","TEXT:\n","\\\"\\\"\\\"{text}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","# ------ Q&A ------\n","def generate_qa(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([f\"Q:{x['q']}\\nA:{x['a']}\" for x in FEWSHOT_QA])\n","\n","    prompt = f\"\"\"\n","Learn QA from examples:\n","{examples}\n","\n","Return JSON: {{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    for qa in qas:\n","        lines.append(f\"Q: {qa.get('q','')}\")\n","        lines.append(f\"A: {qa.get('a','')}\")\n","    return \"\\n\".join(lines)\n","\n","# ------ CONCEPTS ------\n","def generate_concepts(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([\", \".join(lst) for lst in FEWSHOT_CONCEPTS])\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Extract 10–12 technical concepts.\n","Return JSON: {{\"key_concepts\":[\"...\"]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.15)\n","    j = extract_json(out)\n","    return \", \".join(j.get(\"key_concepts\", []))\n","\n","# ================================================================\n","# 9. MAIN PIPELINE\n","# ================================================================\n","\n","def run_pipeline():\n","    df = pd.read_excel(INPUT_FILE)\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        processed = set(old[\"row_index\"])\n","        results = old.to_dict(orient=\"records\")\n","        print(f\"Resuming: {len(processed)} rows already completed.\")\n","    else:\n","        processed = set()\n","        results = []\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            continue\n","\n","        title = str(row[\"title\"])\n","        transcript = str(row[\"transcript\"])\n","\n","        print(\"\\nProcessing:\", title)\n","\n","        summary = generate_summary(transcript)\n","        topics = classify_topic(transcript, summary)\n","        qa = generate_qa(transcript)\n","        concepts = generate_concepts(transcript)\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","\n","    return pd.DataFrame(results)\n","\n","# ================================================================\n","# 10. RUN\n","# ================================================================\n","\n","df_out = run_pipeline()\n","print(\"Few-Shot pipeline completed successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCR9VY4pKmSh","executionInfo":{"status":"ok","timestamp":1764046394688,"user_tz":-330,"elapsed":3152292,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"f613beb0-f290-4c41-c810-3fa54d00a235"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Processing: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","The video opens with a simple grid‑world scenario in which a character named Frank must navigate to a reward spot. It demonstrates how reinforcement learning (RL) can be guided by human feedback (RLHF) to accelerate learning and improve decision quality. The presenter interjects quizzes that test viewers’ understanding of RL algorithms and the role of human input. The discussion then shifts to the training of ChatGPT, explaining that a reward model, built from human‑rated responses, assigns scores to candidate outputs. These scores feed into Proximal Policy Optimization (PPO), which fine‑tunes the language model to favor higher‑reward responses. The talk concludes by emphasizing that integrating human judgments into RL not only speeds up convergence but also yields more reliable, user‑aligned behavior in large language models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Generative AI', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","The video opens with an overview of a hands‑on tutorial that uses CVXopt to explore kernel functions in support‑vector machines. The presenter cites Matthew Blondell’s GitHub repository, a related blog post, and Christopher Bishop’s textbook as key resources. Viewers learn how different kernels shape the SVM decision boundary, especially in soft‑margin settings, and see a step‑by‑step demonstration of the quadratic‑programming solver in CVXopt. The speaker notes that CVXopt is primarily an educational tool; in production one would typically rely on libraries such as libsvm. A concise quadratic‑programming example is shown, with a link to a more detailed MIT tutorial. The talk briefly explains linear versus non‑linear kernels, walks through sample code, and teases an upcoming lesson on scikit‑learn’s SVC parameters, multi‑class handling, and other modern SVM topics. Viewers are invited to ask questions, and the presenter thanks the audience for their support.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Artificial Intelligence', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is CVX opt used for in this tutorial?\n","A: It is used to solve the quadratic programming problem that underlies the support vector machine, allowing the learner to see how a kernel is injected and how it changes the SVM formulation.\n","Q: Why might someone prefer libsvm over CVX opt for training an SVM?\n","A: libsvm is a specialized, highly optimized library for SVM training, whereas CVX opt is a general convex‑optimization tool that is mainly educational and not typically used in production.\n","Q: What role do kernels play in a support vector machine?\n","A: Kernels map the input data into a higher‑dimensional space where it becomes easier to separate the classes with a linear boundary, enabling the SVM to capture nonlinear relationships.\n","Q: How does the tutorial visualize the impact of kernels?\n","A: It uses plots to show how the kernel transforms the data and affects the soft‑margin decision boundary, making the effect of the kernel visible.\n","Q: What is the purpose of the quadratic programming solver mentioned?\n","A: The solver minimizes the objective ½xᵀPx + qᵀx subject to constraints Gx ≤ h and Ax = b, which is the mathematical formulation of the SVM training problem.\n","Q: Which book is referenced for further reading on pattern recognition and machine learning?\n","A: Christopher Bishop’s \"Pattern Recognition and Machine Learning\" is cited as a reference.\n","Q: Who originally wrote the code used in the tutorial?\n","A: The code was taken from Matthew Blondell’s GitHub repository.\n","\n","KEY CONCEPTS:\n","\n","Support Vector Machine, Kernel Methods, CVXopt (convex optimization library), Quadratic Programming, Soft Margin, Nonlinear Classification, Feature Mapping, Quadratic Objective Function, Linear Constraints, Lagrange Multipliers, LibSVM, Visualization of Decision Boundary\n","\n","============================================\n","\n","Processing: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","The video provides a concise introduction to prompt engineering, explaining that prompts are the inputs given to large language models (LLMs) that establish the context and constraints for the desired output. It enumerates seven common prompt styles—question, statement, multi‑input, constraint‑based, and others—highlighting key attributes such as length, language, context, and explicit constraints like tone, style, or word count. Through illustrative examples, the speaker demonstrates how specifying precise constraints (e.g., a one‑word answer or a 500‑word essay) leads to more accurate and predictable responses. The tutorial also covers the process of deconstructing prompts to uncover underlying expectations and constraints, and it previews how to build a prompt‑engineering framework using pre‑trained LLMs. Overall, the video equips viewers with practical techniques for crafting effective prompts that guide LLM behavior.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #3 - Agents & Tools - Intro\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","The transcript explains AI agents as autonomous problem‑solvers that can decide their own steps, contrasting them with chains and routers that follow preset instructions. It introduces tools—specific functions like calculators or search engines—that agents use to complete tasks. The core concept highlighted is the React agent pattern (Reasoning + Acting), which mimics human thinking: the LLM first thinks about the problem, then decides whether to act (use a tool) or answer directly, provides arguments for the tool, executes it via LangChain, observes the result, and repeats the cycle until a satisfactory answer is reached. The speaker outlines how this loop works, visualizes the process, and previews coding examples and a discussion of LangGraph’s role in addressing limitations of the basic React pattern.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Agentic AI', 'LangChain', 'Langraph']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","The speaker outlines how to trace a reflection‑agent system to refine a viral tweet. They walk through setting up LangChain with LangSmith: creating an account, generating an API key, and adding it to an .env file so trace data streams automatically. After running a sample project, the speaker showcases the LangSmith UI, pointing out runs and detailed traces that capture each workflow step. The example workflow features a generation agent that drafts a tweet, a reflect agent that critiques it, and the generation agent revising the tweet iteratively based on feedback. This loop continues until a polished, viral tweet emerges. The speaker also previews a more advanced reflection agent, the reflexion agent, hinting at future discussions in the next session.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Agentic AI', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main goal of tracing the reflection agent system in this section?\n","A: To understand exactly what is happening and how both systems work together to deliver the final refined viral tweet.\n","Q: What will the speaker do next to continue the demonstration?\n","A: They will go to a particular website, smith.chain.\n","Q: What is the final output that the system aims to produce?\n","A: A refined viral tweet.\n","Q: Who is the speaker addressing during this explanation?\n","A: They are addressing the audience informally, saying \"guys.\"\n","\n","KEY CONCEPTS:\n","\n","Reflection Agent, System Tracing, Viral Tweet Generation, Content Refinement, Web Integration, SmithChain Platform, System Integration, Debugging, Deployment, Optimization, Social Media Automation, Process Monitoring\n","\n","============================================\n","\n","Processing: LangChain Crash Course #7 - Chat Models - Setup\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","The tutorial walks through installing LangChain’s OpenAI chat model in VS Code, starting with the `langchain-openai` package and importing `ChatOpenAI`. The speaker initializes the GPT‑4o model, noting its higher cost and suggesting cheaper options like GPT‑3. Using LangChain’s magic keyword “invoke,” they demonstrate how to call the OpenAI API, handle an API‑key error by creating a `.env` file, and load the key with `python‑dotenv`. The session covers extracting the useful “content” field from the response, checking token usage, and troubleshooting low‑balance errors, including how to top up the account. Finally, the speaker explains how to provide the model with full conversation history to improve context, illustrated with a prompt that asks for the square root of 49. The key takeaway is that LangChain offers a simple, flexible interface for interacting with OpenAI’s API and managing responses.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Python Programming', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What library is used for working with OpenAI chat models in this section?\n","A: LangChain, specifically the LangChain OpenAI package.\n","Q: How do you install the LangChain OpenAI package?\n","A: Run the command `pip install langchain-openai` in your terminal.\n","Q: Which class from the LangChain OpenAI module is imported to create a chat model?\n","A: The `ChatOpenAI` class.\n","Q: How do you initialize a chat model with a specific OpenAI model name?\n","A: Instantiate `ChatOpenAI` with the `model_name` parameter, e.g., `ChatOpenAI(model_name=\"gpt-4o\")`.\n","Q: Why might the GPT‑4o model be more expensive than older models?\n","A: Because it is a newer, more advanced model, which typically incurs higher usage costs.\n","Q: What alternative model is suggested for users with budget constraints?\n","A: The GPT‑3 model, which is less expensive than GPT‑4o.\n","\n","KEY CONCEPTS:\n","\n","LangChain, ChatOpenAI, OpenAI API, pip package installation, Python import syntax, Model initialization, Model selection (GPT‑4, GPT‑3), Keyword arguments, Terminal (VS Code), Cost considerations for large language models, Package dependency management, Class instantiation\n","\n","============================================\n","\n","Processing: Python Training Course - Python Sort List\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","The video provides a concise walkthrough of Python’s built‑in list sorting behavior. It starts by showing that when a list of strings is sorted, all uppercase letters come before lowercase letters because of their ASCII values; reversing the sort simply flips the order within each case group. The presenter then demonstrates a mixed list containing both strings and numbers, illustrating that Python’s default sort places all numeric values before any string values, and that a reverse sort will move the numeric element to the end of the list. Finally, the speaker recommends normalizing the case of strings—by converting them to all lower‑or upper‑case—when a predictable, case‑insensitive ordering is needed. This simple preprocessing step ensures consistent results across different datasets. These insights help developers avoid unexpected ordering bugs when handling heterogeneous data.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What does Python's sort method do when sorting a list of strings with mixed uppercase and lowercase letters?\n","A: It places all strings that start with uppercase letters first, sorted alphabetically, followed by strings that start with lowercase letters, also sorted alphabetically.\n","Q: How can you avoid the default case‑sensitive sorting behavior when sorting a list of strings?\n","A: Convert all strings to the same case (e.g., all lowercase or all uppercase) before sorting.\n","Q: What happens when you sort a list that contains both numbers and strings?\n","A: Python places all numbers at the beginning of the sorted list, followed by the strings, because numbers are considered smaller than strings in the sort order.\n","Q: How does reversing a sorted list affect the order of numbers and strings?\n","A: Reversing a sorted list puts the numbers at the end and the strings at the beginning, maintaining the reverse alphabetical order within each group.\n","Q: Why might you want to insert a number into a specific position in a list before sorting?\n","A: Inserting a number at a specific index can demonstrate how the sort method reorders elements, showing that numbers will move to the front of the list after sorting.\n","\n","KEY CONCEPTS:\n","\n","Python list data structure, list.sort() method, case-sensitive sorting (uppercase vs lowercase), alphabetical ordering, reverse sorting (reverse=True), mixed-type list sorting (strings and numbers), insertion into list (list.insert(index, value)), list indexing, string comparison, numeric comparison, handling of mixed types in Python 3, sorting stability\n","\n","============================================\n","\n","Processing: \n","Humans vs. AI: Who should make the decision?\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","The speaker discusses how the decision to let AI or humans handle a task hinges on the AI’s confidence level. Using fraud‑detection alerts as an example, AI excels on high‑confidence cases while humans are more reliable on low‑confidence, ambiguous ones. The optimal strategy blends AI predictions with human review, assigning alerts to the most suitable decision maker based on confidence curves. This augmented‑intelligence approach can outperform either component alone. However, human cognitive biases—particularly automation bias—can erode the benefit. The speaker contrasts forced display (AI recommendation shown automatically) with optional display (shown only on request), noting that optional display reduces bias by letting analysts form independent judgments first. They also point out that presenting an AI’s accuracy percentage often decreases human reliance on the recommendation, as people distrust explicit uncertainty. By quantifying outcomes, the most effective decision maker—human, AI, or hybrid—can be identified, and the presentation can be tailored to mitigate bias. Together, humans and AI can significantly improve decision quality.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Decision-making between AI and human, Fraud detection system, Alert generation, False positives, AI workload alleviation, Confidence score, Success rate curve, AI performance curve, Human performance curve, AI uncertainty handling, Human bias, Contextual reasoning\n","\n","============================================\n","\n","Processing: \n","Build generative apps faster with Vertex AI\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","Dimitrius, a product manager at Google Cloud AI, outlines six new Vertex AI APIs designed to streamline enterprise generative‑AI development. The suite includes a Document Understanding API that parses complex documents to boost retrieval and answer quality; an enhanced Embedding API for high‑performance embeddings; Vector Search with hybrid search for scalable, cost‑efficient retrieval; a Ranking API that re‑ranks results to surface the most relevant answers; a Grounded Generation API that produces citation‑rich responses using Gemini; and a Check Grounding API that fact‑checks statements against evidence. He emphasizes quality, Google’s deep expertise, and the stateless, easily integrable primitives, noting support in popular frameworks such as LangChain and LlamaIndex.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Natural Language Processing', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Unitary Transformations\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","\n","\n","TOPIC CLASSIFICATION:\n","\n","['Data Science', 'Machine Learning', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","Krishn presents a hands‑on tutorial on building generative‑AI apps with Google Gemini Pro 1.5. He starts with a quick demo of the model’s multimodal abilities, querying a 402‑page Apollo 11 transcript for jokes, quotes, emojis, and even a timestamp for a specific moment. A simple drawing prompt shows the model can interpret abstract visuals, while a comparison of context lengths highlights Gemini 1.5’s 1 million‑token capacity versus earlier 32k‑token Gemini 1.0 and GPT‑4 Turbo’s 128k. The session walks through obtaining a free API key, installing the Python client, and setting up a Jupyter notebook. Krishn demonstrates text‑and‑image queries, streaming responses, and image‑to‑text generation with Gemini Pro Vision. He also discusses common errors, prompt‑engineering tricks, and end‑to‑end projects like PDF‑query RAG, concluding with excitement for future developments and encouraging viewers to explore Gemini’s potential.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Prompt Engineering', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main focus of the video?\n","A: The video focuses on building a generative AI powered application using Google Gemini Pro 1.5.\n","Q: What type of model is Google Gemini Pro 1.5?\n","A: Google Gemini Pro 1.5 is a multimodal model that can work with both text and images.\n","Q: What will the presenter show first in the video?\n","A: The presenter will first show a one‑minute demo video that Google has produced to showcase Gemini Pro 1.5’s capabilities.\n","Q: What will the presenter cover after the demo video?\n","A: After the demo, the presenter will demonstrate a hands‑on application, run code, play with images and text, and discuss how to create and use the API key.\n","Q: What experimental feature is mentioned in the transcript?\n","A: The transcript mentions an experimental feature for long context understanding in the newest Gemini model.\n","Q: Who is the presenter of the video?\n","A: The presenter is Krishn, who welcomes viewers to his YouTube channel.\n","Q: What kind of projects has the presenter created before?\n","A: The presenter has created a number of end‑to‑end projects related to Gemini Pro.\n","Q: What is the length of the demo video that will be shown?\n","A: The demo video is approximately one minute long.\n","\n","KEY CONCEPTS:\n","\n","Generative AI, Google Gemini Pro 1.5, Multimodal capabilities (text & images), API key usage, Long context understanding, Experimental feature, Demo video, Hands‑on application, Code execution, End‑to‑end projects, Content playlist, Application building\n","\n","============================================\n","\n","Processing: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","The video provides a practical guide to evaluating and debugging prompt‑engineering models. It begins by outlining the core metrics—perplexity, accuracy, and human evaluation—that quantify model performance. The presenter walks through a concrete example, calculating perplexity and accuracy on a small dataset to illustrate how these metrics are derived and interpreted. Debugging is addressed by inspecting common error patterns, such as over‑fitting or hallucinations, and by systematically reviewing model outputs. Emphasis is placed on testing across diverse data sources and employing cross‑validation to assess generalisation. The speaker stresses that evaluation is an iterative, ongoing process, not a one‑time check, and that thorough validation should precede any move into advanced product‑engineering topics. By following these steps, practitioners can build more reliable, robust prompt‑engineering pipelines.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Generative AI vs AI agents vs Agentic AI\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","The speaker explains the progression from simple generative AI (LLMs like GPT‑4) to AI agents and finally to agentic AI systems. Generative AI can produce text, images, or videos but is limited by a knowledge cutoff. An AI agent extends this by accessing tools (e.g., travel or weather APIs) and memory, enabling it to perform narrow tasks such as booking a flight. Agentic AI takes it further, allowing one or more autonomous agents to coordinate, plan multi‑step tasks, and use other agents (e.g., a visa‑checking agent) to achieve complex goals. The talk highlights examples like booking a flight with weather constraints, integrating visa checks, and building such systems with frameworks like N8N, Agno, or LangGraph. It emphasizes that generative AI is a core component of agentic systems and that the complexity of tasks handled increases as we move from generative AI to agents to agentic AI.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Covariance in Statistics\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","Covariance is a statistical measure that describes how two variables change together. In the talk, the speaker defines it as the average product of the deviations of each variable from its mean, and writes the formula Cov(X,Y)=∑(xi−x̄)(yi−ȳ)/n. He explains that a positive covariance means that when one variable rises, the other tends to rise as well, whereas a negative covariance indicates that one variable tends to fall when the other rises. Using simple numerical examples, he walks through the calculation step by step, highlighting that covariance only tells us the direction of the relationship, not how strong it is. He notes that the Pearson correlation coefficient, which normalizes covariance, will be covered next to address this limitation. This sets the stage for a deeper exploration of correlation in the next session.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: 3. Objective || End to End AI Tutorial\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","The video explains how to define the objective in reinforcement learning, contrasting episodic and continuous tasks. Using a manager‑employee analogy, it illustrates goal setting and then states that RL’s objective is to learn an optimal policy that maximizes cumulative reward. Episodic tasks have a finite horizon, while continuous tasks run indefinitely, requiring different handling of discount factors. The speaker demonstrates reward design with concrete examples: in Tic‑Tac‑Toe a positive reward for a win, negative for a loss, and zero for a draw; in continuous trading rewards can be profit or risk‑adjusted metrics like the Sharpe ratio. By carefully shaping the reward signal, practitioners can guide the agent toward behaviors that align with business goals or safety constraints. The key point is to parameterize a reward function that reflects the desired outcome, enabling the agent to maximize expected cumulative reward over time. Upcoming videos will cover the learning algorithms that achieve this objective.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Python Training - Python Dictionary Basics\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","The transcript provides a concise yet thorough overview of Python dictionaries, the language’s primary key‑value data structure. Dictionaries are defined with curly braces, containing immutable keys and associated values separated by commas and colons. The speaker walks through common built‑in methods—.items(), .keys(), .values()—and demonstrates how to construct a dictionary from two parallel lists using zip() and dict(). Practical examples show how to access, modify, and delete entries, check a dictionary’s length, and convert its keys or values into lists. Additional insights cover dictionary comprehensions for concise creation, handling nested dictionaries, and the importance of hash‑based lookup speed. The discussion emphasizes dictionaries’ role in mapping real‑world data, such as stock prices, and their central place in data‑science pipelines, especially when interfaced with pandas for efficient data manipulation.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Fight Insider Threats with AI-infused SIEM\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","Security professionals can cut breach containment time by an average of 108 days by harnessing AI and automation, according to IBM’s 2023 Cost of a Data Breach report. A key focus is User Behavior Analytics (UBA) powered by machine learning, which detects insider threats—incidents that cost an average of $4 million. IBM Security’s UBA learns normal user patterns over a seven‑day window and flags anomalies. When paired with Q Radar, a Security Information Management (SIM) platform, the solution delivers a unified dashboard that prioritizes high‑risk employees, displays alerts, timelines, and IOC details. Q Radar automates the investigation workflow, maps findings to MITRE tactics, provides natural‑language insights, and lets analysts give feedback to reinforce the model. Together, the system slashes investigation time from hours or days to minutes, freeing teams to focus on proactive defense rather than reactive alert handling.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Mlops']\n","\n","GENERATED Q&A:\n","\n","Q: What benefit does AI and automation bring to breach containment times according to IBM's 2023 report?\n","A: It reduces the average time to identify and contain a data breach by 108 days.\n","Q: What is the average cost of an insider threat to an organization as mentioned in the text?\n","A: $4.\n","Q: What does UBA stand for in the context of security?\n","A: User Behavior Analytics.\n","Q: According to the text, what is a major concern for organizations of all sizes?\n","A: Insider threats.\n","Q: What type of data was used in IBM's cost of a data breach report 2023?\n","A: A survey of over 500 organizations.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence (AI), Automation, User Behavior Analytics (UBA), Machine Learning, Data Breach Detection, Insider Threat Detection, Threat Containment, Security Posture, Threat Intelligence, Anomaly Detection, Incident Response, Risk Assessment\n","\n","============================================\n","\n","Processing: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","Krishak opens his midnight‑time livestream, welcoming viewers to his channel and announcing Meta’s fresh Llama 3, an open‑source large‑language model released just hours earlier. Llama 3 comes in 8 B, 70 B, and 80 B variants, trained on 50 trillion tokens—seven times more data than Llama 2—and supports an 8 K context window, double the previous capacity. It tops benchmarks such as MM‑GP, QA, GSM‑K, and math, outperforming many paid LLMs, while instruction‑tuned versions cut false refusals, boost alignment, and enhance answer diversity, excelling at reasoning, code generation, translation, and dialogue. Meta has already integrated the model into its AI assistant for coding and problem solving, underscoring its practical value. The release marks a major leap for open‑source AI, offering competitive performance and scalability. Viewers learn how to access Llama 3 via Meta’s site, Hugging Face, Kaggle, or GitHub, with code snippets for local inference. A follow‑up video will dive deeper into usage.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Artificial Intelligence', 'Generative AI']\n","\n","GENERATED Q&A:\n","\n","Q: Who is speaking in the text?\n","A: The speaker is Krishak.\n","Q: What platform is the speaker welcoming viewers to?\n","A: The speaker welcomes viewers to his YouTube channel.\n","Q: At what time does the speaker mention the current time?\n","A: The speaker says it is 2 a.m.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: Getting Started With sklearn\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","In this video, the presenter guides viewers through constructing a decision boundary using Python’s scikit‑learn library, specifically the Gaussian Naive Bayes classifier. The session begins with a quick online search to locate the relevant documentation and algorithmic background, ensuring viewers understand the mathematical derivation and practical applications of Naive Bayes. The speaker then walks through the entire coding process, from importing the necessary modules to fitting the model on a sample dataset and visualizing the resulting boundary. Emphasis is placed on interpreting the probabilistic assumptions that underpin the algorithm and on troubleshooting common pitfalls. By the end of the tutorial, viewers will have a working implementation they can adapt to their own data, and the speaker hints at deeper dives into feature engineering and model evaluation in future videos.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Data Science', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What Python library is the lesson primarily focused on?\n","A: The lesson focuses on the scikit-learn library, commonly abbreviated as sklearn.\n","Q: Which classification algorithm is being used in the example code?\n","A: The example uses the Naive Bayes algorithm, specifically the Gaussian Naive Bayes variant.\n","Q: How does the instructor recommend learning how to use scikit-learn functions?\n","A: The instructor suggests searching Google for the library name and the specific algorithm, then reviewing the official documentation pages that explain the functions.\n","Q: What is the main purpose of the Python code discussed in the text?\n","A: The code is designed to generate a decision boundary for a classification task.\n","Q: Why does the instructor mention walking through the steps of writing the code?\n","A: They want to explain each step in detail so that learners can understand and replicate the process themselves.\n","\n","KEY CONCEPTS:\n","\n","scikit-learn library, Naive Bayes algorithm, Gaussian Naive Bayes, Decision boundary, Python code implementation, Google search for documentation, Algorithm derivation, Use cases, Classifier, Library functions, Documentation usage, Model training\n","\n","============================================\n","\n","Processing: \n","Log Normal Distribution in Statistics\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","In this concise lecture, the speaker first revisits the Gaussian (normal) distribution, underscoring its familiar bell‑curve shape and the empirical rule that roughly 68 %, 95 %, and 99.7 % of observations lie within one, two, and three standard deviations, respectively. He then introduces the log‑normal distribution, defined as a variable whose logarithm follows a normal law, and contrasts it with the Gaussian case using everyday examples such as human height (Gaussian) versus income or product review lengths (log‑normal). The talk explains why recognizing these patterns is essential for data preprocessing: Gaussian data can be standardized directly, whereas log‑normal data must first be log‑transformed to approximate normality before scaling. Proper standard scaling aligns feature ranges, boosting model performance. The speaker concludes by highlighting the practical gains in machine‑learning pipelines and teasing deeper statistical concepts in future videos.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","An end‑to‑end deep‑learning pipeline for detecting potato blight is presented, spanning data capture, model training, and deployment. Images of healthy and diseased leaves are collected, cleaned, and augmented with TensorFlow datasets to boost diversity. A convolutional neural network is trained, exported, and served via TensorFlow Serving, optionally wrapped in a FastAPI backend. For mobile, the model is quantized to TensorFlow Lite and hosted on Google Cloud Functions; a React Native app captures photos and queries the cloud function for real‑time predictions. The same architecture powers a React‑JS web interface for image uploads. The series, led by AtliQ Agriculture, includes 7–8 videos covering each stage, plus learning resources on Python and deep‑learning fundamentals. Viewers are encouraged to adapt the code to other crops and share their results, enhancing both skill and résumé.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Mlops', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main goal of the deep learning project described in the video?\n","A: To build a mobile application that allows farmers to identify potato plant diseases—early blight or late blight—by taking a picture, thereby reducing economic losses.\n","Q: Which two potato diseases are the focus of this project?\n","A: Early blight, caused by a fungus, and late blight, caused by a specific microorganism.\n","Q: What technologies are mentioned for building the backend and deploying the model?\n","A: FastAPI for the backend server, TensorFlow Serving for ML Ops, and deployment on Google Cloud Platform (GCP) with Cloud Functions.\n","Q: How will the mobile application interact with the backend?\n","A: The mobile app, built with React Native, will call Google Cloud Functions that in turn invoke the deployed model to classify the plant image.\n","Q: Why is accurate disease identification important for farmers?\n","A: Because early detection and correct treatment can prevent waste and significant economic loss.\n","Q: What role does convolutional neural networks (CNNs) play in this project?\n","A: CNNs are used to analyze the plant images and classify them as healthy, early blight, or late blight.\n","Q: Who is responsible for maintaining the ML pipelines in production according to the text?\n","A: Machine learning engineers and DevOps teams are typically responsible for maintaining ML pipelines in production.\n","\n","KEY CONCEPTS:\n","\n","Data Collection, Model Building, ML Ops, TensorFlow Serving, FastAPI, Google Cloud Platform, Cloud Functions, React Native, Convolutional Neural Network, Supervised Machine Learning, Disease Classification, Mobile Application Backend\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The transcript outlines a progressive hierarchy of LLM application autonomy, beginning with deterministic code, moving through single LLM calls, fixed multi‑step chains, LLM‑driven routers, and culminating in state‑machine agents that loop, incorporate human approvals, and refine iteratively. Using content‑creation scenarios—LinkedIn posts, tweets, blog articles—the speaker shows how chains distribute tasks to specialized sub‑models, routers choose the appropriate chain, and agents orchestrate dynamic control flow, memory, and learning. Each level’s advantages and limitations are discussed, emphasizing that only agents truly exercise autonomous decision‑making. The talk also highlights emerging frameworks such as LangGraph, Baby AGI, and AutoGPT that enable building these advanced agents. The speaker also notes that deterministic code offers predictability but limited flexibility, single LLM calls are simple yet brittle, chains provide modularity but fixed logic, routers add adaptability by selecting chains, and agents bring full autonomy with state management and learning. These insights help developers choose the right level for their application.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Agentic AI', 'Langraph', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","The transcript provides a comprehensive overview of advanced prompt engineering techniques for large language models. It begins by detailing how to craft and manage prompts across multiple modalities—text, images, and audio—highlighting the importance of clear, context‑rich prompt design. The discussion then moves to model fine‑tuning, emphasizing multitask learning strategies and knowledge distillation to create efficient, task‑specific variants of pre‑trained LLMs. Best practices for data preprocessing are covered, including tokenization, normalization, and augmentation methods that improve model robustness. Deployment considerations are explored, with examples of TensorFlow Serving, Flask‑based REST APIs, and containerized solutions for scalable production. Ethical issues such as bias mitigation, fairness, and privacy safeguards are underscored. Finally, the transcript suggests hands‑on exercises that let participants practice prompt crafting, fine‑tuning pipelines, and end‑to‑end deployment workflows, ensuring a solid grasp of both theory and practical implementation.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Machine Learning', 'Mlops']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: SVD: Eigen Action Heros [Matlab]\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","The lecture demonstrates how Singular Value Decomposition (SVD) can extract eigenfaces from a small dataset of action‑hero portraits. After aligning and greyscaling 20 images of each actor, the instructor builds a data matrix, computes the mean face, subtracts it, and performs an economy SVD to obtain principal components (eigenfaces). The first nine eigenfaces are visualized, revealing features such as hair and glasses. Each image is projected onto the first three principal components, producing a 3‑D representation where Arnold Schwarzenegger and Sylvester Stallone form distinct clusters, though with some overlap. The instructor then tests classification on unseen images and repeats the experiment with Taylor Swift versus Stallone, noting stronger separation. A comparison of Arnold versus Taylor highlights how superficial pixel correlations can dominate classification, illustrating the limits of naive eigenface methods. The talk concludes by referencing Facebook’s historical progress in face recognition, which moved beyond simple 2‑D eigenfaces to incorporate 3‑D geometry for improved accuracy.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Data Science', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangChain Crash Course #3 - What is LangChain?\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","LangChain is a widely adopted framework that connects large language models (LLMs) to real‑world APIs, enabling applications to perform practical tasks such as booking flights, hotels, and restaurants. By adding a middleware layer that manages external interactions, LangChain mitigates inherent LLM limitations and allows developers to swap underlying models—like switching from GPT‑4 to a free Hugging Face model—without modifying the application code. This flexibility extends AI beyond pure reasoning, empowering it to execute actions such as querying private company databases, sending emails, browsing the web, and scraping websites, thereby demonstrating its tangible real‑world utility. In the presentation, the speaker offers a concise example of these capabilities and hints at more advanced use cases that will be explored in depth throughout the course, highlighting LangChain’s role as a bridge between advanced language models and everyday business workflows.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is LangChain and why is it useful?\n","A: LangChain is a framework that acts as a bridge between large language models (LLMs) and real‑world APIs, databases, and other services, enabling developers to build applications that combine the reasoning power of LLMs with the ability to interact with external systems.\n","Q: What limitation of large language models is highlighted in the text?\n","A: LLMs can understand and generate text about tasks like travel planning, but they cannot directly interact with the real world or perform actions such as booking flights or hotels.\n","Q: How does LangChain help with switching between different LLMs?\n","A: LangChain abstracts the LLM layer so that developers can swap out models (e.g., from GPT‑4 to a free Hugging Face model) without changing the application code that uses the framework.\n","Q: What types of external services can LangChain connect to?\n","A: LangChain can connect to various APIs, such as flight booking, restaurant booking, and other web services, allowing the LLM to retrieve or send data to those services.\n","Q: Why might a developer choose to use LangChain instead of building integrations manually?\n","A: Using LangChain simplifies the process of integrating LLMs with real‑world services, reducing boilerplate code and making it easier to maintain and extend applications that rely on both AI reasoning and external data.\n","Q: What example scenario is used to illustrate LangChain’s capabilities?\n","A: The text describes a user wanting to plan a vacation to Paris, including booking flights, hotels, and restaurants, and shows how LangChain can enable an LLM to handle such requests by interfacing with relevant APIs.\n","Q: What role do LLMs play in applications that use LangChain?\n","A: LLMs serve as the 'brains' or reasoning engine, providing natural language understanding and generation, while LangChain handles the execution of real‑world actions based on the LLM’s instructions.\n","\n","KEY CONCEPTS:\n","\n","Large Language Models, LangChain framework, API integration, Real-world interaction, Model abstraction, Chat interface, LLM reasoning, Training data, External databases, Email sending, Hugging Face models, GPT-4\n","\n","============================================\n","\n","Processing: How To Use Residuals For Time Series Forecasting\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","The video explains residuals in time‑series forecasting, distinguishing them from forecast errors. Residuals are the differences between a model’s fitted values on training data and the actual observations, while errors refer to forecast‑on‑test differences. Good residuals should have zero mean and no autocorrelation; otherwise the model has missed structure. Using a Holt‑Winters exponential‑smoothing model on the Air Passengers dataset, the speaker shows how to compute fitted values, derive residuals, and plot their autocorrelation (ACF) and partial autocorrelation (PACF). A Ljung‑Box test is applied to detect remaining serial correlation. The discussion also covers visual diagnostics: a histogram to check for bias (mean close to zero, slight negative bias noted) and a p‑value threshold of 5 % for rejecting the null of no autocorrelation. These residual analyses guide model refinement, revealing weaknesses and enabling iterative improvement of forecasts.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Statistics', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","The tutorial walks through creating a Next.js + TypeScript app that hosts a text‑to‑SQL agent powered by a Watson X.ai LLM. After bootstrapping the project with Tailwind, the author builds a Home component with a header, input box, and message list, wiring React state (messages, inputMessage, loading) to a LangChain ReAct agent. The agent is configured with a GetFromDB tool that executes SQL against an in‑memory SQLite database seeded with customer and order tables. A system prompt instructs the LLM to generate queries, quote identifiers, and invoke the tool. The demo shows the model telling a joke, counting customers, and performing a join to find the top customer (“Lucas Bill”). Safety guardrails prevent unrestricted access, and the code is hosted on GitHub for reference.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'LangChain', 'Langraph']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main goal of the AI agent described in the video?\n","A: To build an AI agent that can talk to a database by leveraging SQL knowledge from large language models.\n","Q: Which framework is used to build the ReAct agent in this project?\n","A: LangGraph.\n","Q: What frontend framework is used to create the user interface?\n","A: Next.js.\n","Q: Which cloud service hosts the language models used in the agent?\n","A: watsonx.ai.\n","Q: What database is used for in-memory storage in the example?\n","A: SQLite.\n","Q: What command is used to create the Next.js project boilerplate?\n","A: create-next-app@latest.\n","Q: Which styling library is employed to avoid writing custom CSS?\n","A: Tailwind.\n","Q: In Next.js, how can components be executed?\n","A: Components can run either client‑side or server‑side.\n","Q: What is the purpose of the input box in the UI?\n","A: To allow the user to type a question that will be sent to the large language model.\n","Q: How do you start the Next.js application locally?\n","A: By running npm run dev.\n","\n","KEY CONCEPTS:\n","\n","AI agent, Database connectivity, SQL knowledge in large language models, LangGraph framework, ReAct agent architecture, Next.js framework, Tailwind CSS utility framework, SQLite in‑memory database, VS Code IDE, create‑next‑app CLI, TypeScript, Client‑side vs server‑side rendering\n","\n","============================================\n","\n","Processing: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","Prompt engineering is a specialized branch of natural language processing that fine‑tunes large language models—such as GPT, BERT, and Hugging Face Transformers—to generate text that is accurate, coherent, and contextually appropriate for a wide range of applications, including chatbots, machine translation, and automated content creation. By crafting carefully designed prompts, practitioners can achieve higher output quality than traditional rule‑based systems, while also navigating challenges like ambiguous user inputs and model bias. The course offers a thorough introduction, beginning with the fundamentals of prompt analysis and the key features that influence model behavior. It then explores practical constraints, such as token limits and inference latency, before outlining a curriculum that blends theory with hands‑on projects. Learners will gain both foundational knowledge and advanced techniques, equipping them to design effective prompts for real‑world NLP tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What is prompt engineering?\n","A: Prompt engineering is a specialized field within natural language processing that focuses on building models that generate high‑quality text outputs in response to prompts, using pre‑trained large language models fine‑tuned for specific tasks.\n","Q: Why is prompt engineering important?\n","A: It allows the generation of more accurate, coherent, and contextually appropriate text compared to traditional rule‑based or keyword‑based approaches, thereby improving user experience in chatbots, translation, and content generation.\n","Q: What are the key benefits of prompt engineering?\n","A: The main benefits include higher accuracy, better coherence, contextual relevance, and increased user engagement.\n","Q: What limitations can prompt engineering models face?\n","A: They may struggle with complex or ambiguous prompts and can produce biased or inaccurate outputs due to the underlying data or model architecture.\n","Q: Which large language models are commonly used in prompt engineering?\n","A: Commonly used models include OpenAI GPT, Google BERT, and Hugging Face Transformers.\n","Q: What topics will the course cover?\n","A: The course covers the basics of prompt engineering, prompt analysis, deconstructing prompts, identifying key features and constraints, benefits and limitations, and advanced techniques for fine‑tuning pre‑trained large language models.\n","\n","KEY CONCEPTS:\n","\n","Prompt Engineering, Natural Language Processing (NLP), Large Language Models (LLMs), Fine-tuning, Pre-trained Models, Prompt Analysis, Deconstructing Prompts, Bias in Models, Accuracy and Coherence of Outputs, Contextual Appropriateness, Rule-based vs Keyword-based Approaches, Applications (Chatbots, Language Translation, Content Generation)\n","\n","============================================\n","\n","Processing: Q-learning - Explained!\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","The episode introduces Q‑learning, a value‑based reinforcement learning algorithm that learns a Q‑table mapping state–action pairs to expected cumulative rewards. It begins by reviewing the three machine‑learning paradigms, distinguishing value‑based from policy‑based methods, and defining state‑value (V) and state‑action value (Q) functions. A grid‑world example shows an agent exploring, receiving rewards, transitioning between states, and updating Q‑values via the Bellman equation, with the discount factor weighting future rewards. The transcript walks through a Q‑learning update: for state S1 taking action right, the observed Q (reward + discounted max Q of the next state) is 0.85 against a table value of 1.0, yielding a temporal‑difference error of –1.85; with learning rate α = 0.1 the entry becomes 0.815. A second example with state S2 illustrates the same process. Repeated episodes refine the Q‑table, producing an optimal policy that differs from the exploratory behavior policy, underscoring Q‑learning’s off‑policy nature.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, CI/CD Pipeline\n","\n","============================================\n","\n","Processing: Training Your Logistic Classifier\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","The logistic (linear) classifier described in the chunk transforms raw input data—such as image pixels—into a vectorized representation X. This vector is multiplied by a weight matrix W and augmented with a bias term b to produce raw class scores. During training, the objective is to adjust W and b so that the scores best discriminate between classes. To turn these raw scores into interpretable probabilities, a softmax function is applied, normalizing the outputs so they sum to one and amplifying the probability of the correct class while diminishing others. This probabilistic framework is particularly suited for single‑label classification problems, where each input is assigned to exactly one category. The model’s simplicity and efficiency make it a foundational tool in many machine learning pipelines.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What type of classifier is a logistic classifier?\n","A: It is a linear classifier that applies a linear function to inputs to generate predictions.\n","Q: How does a logistic classifier compute its predictions?\n","A: It performs a matrix multiplication of the input vector X with a weight matrix W and adds a bias term b.\n","Q: What symbols are used to denote the inputs, weights, and bias in the logistic classifier description?\n","A: Inputs are denoted by X, weights by W, and bias by b.\n","Q: What is the goal of training a logistic classifier?\n","A: To find values for the weights and bias that perform accurate predictions.\n","Q: How are scores converted into probabilities in logistic regression?\n","A: By applying the softmax function, which turns any scores into proper probabilities that sum to 1.\n","Q: What property must the probabilities produced by the softmax function satisfy?\n","A: They must sum to 1 and be close to 1 for the correct class and close to 0 for all other classes.\n","Q: What are scores in the context of logistic regression also called?\n","A: Logits.\n","\n","KEY CONCEPTS:\n","\n","logistic classifier, linear classifier, matrix multiplication, input vector X, weight matrix W, bias term b, training, classification, softmax function, probabilities, logits, linear function\n","\n","============================================\n","Few-Shot pipeline completed successfully!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGTJs_1fqiC7","executionInfo":{"status":"ok","timestamp":1764051534040,"user_tz":-330,"elapsed":5830,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"aaf9c6cb-2a22-4348-8c05-091faaba1c4c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gpt-oss-20b/gpt-oss-20b_fewshot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gpt-oss-20b/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["bee2a45fadad45fb9ad656e438c64d0b","63ed0dd623f542a3a1e9ede40f00a80e","19f6a49b22f1466983e9972ecb8ead47","1e6990df5e7540428e6e461315a5c579","cefb94ce58bc431181484722e435331a","0e567f70c4154999bf861a26e00f8c27","f5d1e8e423174b0680031286484298cf","f7759c2469044b1c9fbdae6e82f1f06f","e322d1677596412eb9fc3bdb19e51d3c","dc26b48e0e8a45b7a6c81e2efdd99eac","61c63d1c372c4c93b42f65504afd562f","b058d5b30d0b45f1b645a09442989848","568178db0fe2495a8e8f0dfe3cc700d5","0ef9cac58da742509d7d304f4475fc00","be1b0bddf60540589bab8b412d8b4ae1","de76db72f7e4417ab4b5ba95381b6a1e","c3b517d2d7474bcd9f8e23730d6fe341","7bc85f9f4fd84dc48536534482a955d9","6f99e8b4d70841ae8cb201c433610ebc","e886e0f75e634510b99cd37960a3c6b4","ce5e1dafa5ba4068be26ee4034b95d9e","ed73bc5d29ed4ff9af624954ed128df5","912012f02d554daabda0d0ce47230c38","6fce22f3f5e541bab9f89ce5789ccc87","4c849ec38c0241978545438c50bca6a0","639c63fd6ca24b09a79a123476cc9b39","52a3b4966543485f8fb8bfebf9504fad","b0cb990fb904449ab997b74945d4ddf3","15c2b26dfb264debaa96f73ded50426a","f8397d6c73b345f3bae5522392a5fb0a","3378901d9acf428ebf51042bcc12f786","262fe5b2fba8430998e8bd2d3c207308","180c270ebd3f4087b2dbaef780b74441","f0818eeb7cef41308075fa630b280cc9","81a17813426c403dac53a91609c50de5","b8bb5ac8d8894be8a1093ba3148ae44b","f1bf5c58796f41c1aa034d10e2d7b917","641aaedd6e5d4cc2a311646e3eff98cc","3f0ae351837c4f27b8d912364b306011","7024adf426404efc984e629ad5540ed7","cb56026f8fcc4143b01c507cc7f32e2e","4023c361c6dd47539e600ff13345e5e0","ddcde5fcd77d4d648e0b6ec926c6cd99","45a2201cb8a1408b97fbc6f6cc8ce8bd","867023b48067420b8514a15d905c6d94","54d8462f4c7f422e98d80b9598ecd1d3","85498066d9504f1ea59f76a664e5df0e","858a6c64b78b4879844aedaf7cdcfbb5","4563dacd0902441d94bb383da1d798d7","f318b865a63341fe87309d517cc84c26","cc0acbf6cf4e4a22bbf848fd23c0d509","2da0473ec63c49df8ea2c53949e99cff","939883940da641d681c6a9b05bc7ded8","5a6268c42e134c82b35afb424f2e7603","fd0f773d5a07416db8fdc6f28a4f8962","d45b51a2dbd540849f256935304ea5b9","3188ce04b57f47b684686e5e7befe32c","b2c57a3c8a3a4281a2e2b98ec23de3ff","0d70364384914c9eb531885a93ff2819","2e51ffedbd4b400dbe69f105ea7d61a1","e30e5baa60854b1c881e8242b091a4fb","0d1aeb38d5974e788185e7541f7aa0e5","7a3197df89e44af3be9312efe2427016","d40b44cfc14b4f95a3f203170fd047eb","67f527264a3941bcb3847d5b90591e44","49bc16cc91a14b8482c7e16a852a8c35"]},"id":"YJASQHidKmXp","executionInfo":{"status":"ok","timestamp":1764051732221,"user_tz":-330,"elapsed":198156,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"3b0abe32-982a-41de-c5a3-e73de91bc962"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gpt-oss-20b/gpt-oss-20b_fewshot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee2a45fadad45fb9ad656e438c64d0b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b058d5b30d0b45f1b645a09442989848"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912012f02d554daabda0d0ce47230c38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0818eeb7cef41308075fa630b280cc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"867023b48067420b8514a15d905c6d94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d45b51a2dbd540849f256935304ea5b9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2592\n","  - BLEU: 0.0445\n","  - BERTScore F1: 0.8719\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.4008\n","  - Micro F1: 0.5329\n","  - Macro F1: 0.5077\n","  - Weighted F1: 0.5119\n","\n","Q&A Generation:\n","  - BLEU: 0.0419\n","  - Diversity: 0.8456\n","  - Answerability: 0.7293\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.2800\n","  - Recall@10: 0.1120\n","  - F1@10: 0.1600\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gpt-oss-20b/evaluation_final.json\n"]}]}]}