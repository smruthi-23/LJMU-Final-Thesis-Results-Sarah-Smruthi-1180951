{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPsNykvNFRPTHzS5GTHtQMi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8f1b555091844f8d8e7fdc068ae0a36e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9427731536fb4ca3bdfe595bb06fe517","IPY_MODEL_d19340a241e34ebdb8e08b13a06085a8","IPY_MODEL_4a05019a7c8c4ba88ebfc6a9c28e2596"],"layout":"IPY_MODEL_1f6a66c44c6544eeb2c2d0b08cafb6bc"}},"9427731536fb4ca3bdfe595bb06fe517":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2e07fce58a04e4a9138cce40b89e5f6","placeholder":"​","style":"IPY_MODEL_7fce9d7ceec644dd8220175a03e0f623","value":"tokenizer_config.json: 100%"}},"d19340a241e34ebdb8e08b13a06085a8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b27f72969f3840d89b15fda3b90952e5","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c33f4438b7354adeb9ee3916562c4059","value":25}},"4a05019a7c8c4ba88ebfc6a9c28e2596":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_29a7db99df0f4f12bef2d54213479b60","placeholder":"​","style":"IPY_MODEL_d8cb6ef71be149eabf5f50b72c84713a","value":" 25.0/25.0 [00:00&lt;00:00, 2.18kB/s]"}},"1f6a66c44c6544eeb2c2d0b08cafb6bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2e07fce58a04e4a9138cce40b89e5f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fce9d7ceec644dd8220175a03e0f623":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b27f72969f3840d89b15fda3b90952e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c33f4438b7354adeb9ee3916562c4059":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"29a7db99df0f4f12bef2d54213479b60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8cb6ef71be149eabf5f50b72c84713a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f41f604c99d4c50bc8982b05b3dfa6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d61e502cd70145d09dca13c35a80ba13","IPY_MODEL_5d1df31f080b4a7ab2b2f3f2ea205993","IPY_MODEL_cc4f29f400ea46f29a791558e1996451"],"layout":"IPY_MODEL_2fa94a971ae14b32aed17eb3e9d26809"}},"d61e502cd70145d09dca13c35a80ba13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fa5ca7ddb04423d830a51e9317421ad","placeholder":"​","style":"IPY_MODEL_74d3b5d0f051429ab48f1e71f9764949","value":"config.json: 100%"}},"5d1df31f080b4a7ab2b2f3f2ea205993":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7022d47309e46bf8c6c12a37737b546","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d863d387c44408586680798a7c0a816","value":482}},"cc4f29f400ea46f29a791558e1996451":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b355f67ad1dc4508bcf8377aa22b47fc","placeholder":"​","style":"IPY_MODEL_1572287093844b7abb6639e3b4cc8ca8","value":" 482/482 [00:00&lt;00:00, 39.6kB/s]"}},"2fa94a971ae14b32aed17eb3e9d26809":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fa5ca7ddb04423d830a51e9317421ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74d3b5d0f051429ab48f1e71f9764949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7022d47309e46bf8c6c12a37737b546":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d863d387c44408586680798a7c0a816":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b355f67ad1dc4508bcf8377aa22b47fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1572287093844b7abb6639e3b4cc8ca8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f75cda58d60e4da2bce5369b1f4410c2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6457ea9309b84a318fade008e941da74","IPY_MODEL_6832b7b97b6d45138bfd0b3807dea0f2","IPY_MODEL_26e76cf5ae7b44ae81aa22760ae474f3"],"layout":"IPY_MODEL_3eeacb5e1e654576ad739c6dab7dbfef"}},"6457ea9309b84a318fade008e941da74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b25499a18dc4a649ee57e561fcb36c9","placeholder":"​","style":"IPY_MODEL_0043289b593a42ca96bb422a0ec13d0a","value":"vocab.json: 100%"}},"6832b7b97b6d45138bfd0b3807dea0f2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfd7c6f52c5648368bd3f01857123835","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_443eee1573f04f9c8caba9605a596bf9","value":898823}},"26e76cf5ae7b44ae81aa22760ae474f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c73c707b81b41d888270a001beebf81","placeholder":"​","style":"IPY_MODEL_b19ea463b3614bb48dbef2d2f81db78b","value":" 899k/899k [00:00&lt;00:00, 7.24MB/s]"}},"3eeacb5e1e654576ad739c6dab7dbfef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b25499a18dc4a649ee57e561fcb36c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0043289b593a42ca96bb422a0ec13d0a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bfd7c6f52c5648368bd3f01857123835":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"443eee1573f04f9c8caba9605a596bf9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0c73c707b81b41d888270a001beebf81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b19ea463b3614bb48dbef2d2f81db78b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7062746eff364d05aaf38aa76cfc3838":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07b9f74912794dfdb85cec3e913e7c28","IPY_MODEL_c282cb34a87240afb794319105563170","IPY_MODEL_27527911d98d4c859db17d12cd03b2e1"],"layout":"IPY_MODEL_558e6ea1e0a54c9fb3bb231b30881399"}},"07b9f74912794dfdb85cec3e913e7c28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_796feb01cda7460193250a090d746343","placeholder":"​","style":"IPY_MODEL_d110e2ce0030455190ffc3756239bc7a","value":"merges.txt: 100%"}},"c282cb34a87240afb794319105563170":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be93dd90db18443aa04c7c2f644e1a6b","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb43621f62fa44c887f0fc72528bd2b6","value":456318}},"27527911d98d4c859db17d12cd03b2e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_589346a359d146e68123664a35b616d6","placeholder":"​","style":"IPY_MODEL_3c6c3fbd41614d1394ea116bb5de9286","value":" 456k/456k [00:00&lt;00:00, 5.88MB/s]"}},"558e6ea1e0a54c9fb3bb231b30881399":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"796feb01cda7460193250a090d746343":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d110e2ce0030455190ffc3756239bc7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be93dd90db18443aa04c7c2f644e1a6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb43621f62fa44c887f0fc72528bd2b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"589346a359d146e68123664a35b616d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c6c3fbd41614d1394ea116bb5de9286":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eebef5298c1a48f6a2b98b2f6b9eff16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_37ebc667ecaf4426898d24e2d5cb99ee","IPY_MODEL_6a2ab4147f904a988222fe981c34ad8f","IPY_MODEL_93de464cbec24db0a76c86915c8e78f8"],"layout":"IPY_MODEL_2766849a2edf4844861de4831763e2d8"}},"37ebc667ecaf4426898d24e2d5cb99ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d52b61c3022941faafc6dcc16f0ae44d","placeholder":"​","style":"IPY_MODEL_f3bc74af9b5a4186ad6cef047e20d214","value":"tokenizer.json: 100%"}},"6a2ab4147f904a988222fe981c34ad8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d66b4ded505d4695a8342ce74d399c65","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3458430ccd024462bd6356947a82f73d","value":1355863}},"93de464cbec24db0a76c86915c8e78f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6345d8fe7deb4acaa1ee9e5a642b85db","placeholder":"​","style":"IPY_MODEL_e77e07a344564895a5bf19712891d696","value":" 1.36M/1.36M [00:00&lt;00:00, 16.6MB/s]"}},"2766849a2edf4844861de4831763e2d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d52b61c3022941faafc6dcc16f0ae44d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3bc74af9b5a4186ad6cef047e20d214":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d66b4ded505d4695a8342ce74d399c65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3458430ccd024462bd6356947a82f73d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6345d8fe7deb4acaa1ee9e5a642b85db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e77e07a344564895a5bf19712891d696":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1b526b425474185a4045b3d97a85b82":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ed712b4576246c0b4ab19a03aa8003c","IPY_MODEL_edf1aa4421964e0195557bda21c7f6cf","IPY_MODEL_37955539a57f42ec86b2ba82b5c26b0e"],"layout":"IPY_MODEL_391297e458e14c599be8fcdffb5a282a"}},"1ed712b4576246c0b4ab19a03aa8003c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b8ffd58d18643c8953b6fba7e23d19c","placeholder":"​","style":"IPY_MODEL_6736a8a817124216ad64fc5a33ef6ec1","value":"model.safetensors: 100%"}},"edf1aa4421964e0195557bda21c7f6cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1372fa367c54cd296d9b3939fa426bd","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28aa95c3e9454b8e86336f1ff7fef6b0","value":1421700479}},"37955539a57f42ec86b2ba82b5c26b0e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f800d4fc44f34cf9ac6691376b681d3f","placeholder":"​","style":"IPY_MODEL_9a81c3852e2e455e95ade6f3c331f31c","value":" 1.42G/1.42G [00:29&lt;00:00, 170MB/s]"}},"391297e458e14c599be8fcdffb5a282a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b8ffd58d18643c8953b6fba7e23d19c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6736a8a817124216ad64fc5a33ef6ec1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1372fa367c54cd296d9b3939fa426bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28aa95c3e9454b8e86336f1ff7fef6b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f800d4fc44f34cf9ac6691376b681d3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a81c3852e2e455e95ade6f3c331f31c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Aysfb-l4WJ2b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764052110697,"user_tz":-330,"elapsed":26330,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"7676f2d9-a78c-48aa-df8b-60867b028296"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=4b3e5de3192634b8dd9a72e67231089b5616d4b47e294349eb46236abc5493bc\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"4g9gOHMnKmNs","executionInfo":{"status":"ok","timestamp":1764052110738,"user_tz":-330,"elapsed":31,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"bf7bbc60-060e-4301-a534-4a0eeffc5e52"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["# ================================================================\n","# Few-Shot Prompting Pipeline – Groq\n","# ================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","# ================================================================\n","# 1. FEW-SHOT EXAMPLES\n","# ================================================================\n","\n","FEWSHOT_SUMMARIES = [\n","    {\"input\": \"Explains attention in transformers and its role in capturing long-range dependencies.\",\n","     \"output\": \"The lecture introduces attention in transformers, showing how query, key, and value vectors enable models to weigh relevant tokens. It contrasts this with RNN limitations and demonstrates gains on translation and summarisation.\"},\n","    {\"input\": \"CNN architecture for image classification.\",\n","     \"output\": \"This tutorial covers convolutional, pooling, and fully connected layers, explaining hierarchical feature extraction and typical training steps for vision classification tasks.\"},\n","    {\"input\": \"Reinforcement learning agents learn by reward feedback.\",\n","     \"output\": \"The session formalises RL with policies, rewards, and value estimation. It compares Q-learning and policy gradients, discusses exploration–exploitation, and highlights robotics and gaming use cases.\"},\n","    {\"input\": \"Prompt engineering improves LLM outputs.\",\n","     \"output\": \"Zero-shot, few-shot, and chain-of-thought prompts are compared. The talk emphasises instruction clarity, role specification, and constraint setting to improve reliability and reasoning.\"},\n","    {\"input\": \"MLOps pipelines for reliable deployment.\",\n","     \"output\": \"The talk explains CI/CD for models, experiment tracking, model registries, and monitoring, with tools such as MLflow and Kubeflow for production-grade ML.\"}\n","]\n","\n","FEWSHOT_TOPICS = [\n","    {\"input\": \"Explaining self-attention and BERT internals.\", \"output\": [\"Natural Language Processing\"]},\n","    {\"input\": \"Building CNNs with pooling for object recognition.\", \"output\": [\"Deep Learning\"]},\n","    {\"input\": \"Learning with rewards via Q-learning.\", \"output\": [\"Reinforcement Learning\"]},\n","    {\"input\": \"Designing prompts to improve LLM reasoning.\", \"output\": [\"Prompt Engineering\"]},\n","    {\"input\": \"Automating ML deployment with pipelines and monitoring.\", \"output\": [\"Mlops\"]},\n","    {\"input\": \"Creating data visualisations and feature analysis.\", \"output\": [\"Data Science\"]},\n","    {\"input\": \"Explaining model fine-tuning for generative image models.\", \"output\": [\"Generative AI\"]},\n","    {\"input\": \"Discussing NLP and ML synergy for LLMs.\", \"output\": [\"Natural Language Processing\", \"Machine Learning\"]},\n","]\n","\n","FEWSHOT_QA = [\n","    {\"q\": \"What does attention allow models to do?\",\n","     \"a\": \"It lets models focus on the most relevant tokens in a sequence.\"},\n","    {\"q\": \"Why are convolutions useful in vision?\",\n","     \"a\": \"They extract local spatial features for image classification.\"},\n","    {\"q\": \"How do agents learn in reinforcement learning?\",\n","     \"a\": \"They learn by maximising cumulative rewards through trial and error.\"},\n","    {\"q\": \"When is few-shot prompting effective?\",\n","     \"a\": \"When limited task-specific data exists but examples guide behaviour.\"},\n","    {\"q\": \"Who typically maintains ML pipelines in production?\",\n","     \"a\": \"Machine learning engineers and DevOps teams.\"}\n","]\n","\n","FEWSHOT_CONCEPTS = [\n","    [\"Self-Attention Mechanism\", \"Query-Key-Value\", \"Positional Encoding\"],\n","    [\"Convolutional Layer\", \"Pooling Operation\", \"Feature Map\"],\n","    [\"Reward Function\", \"Policy Gradient\", \"Q-Learning\"],\n","    [\"Few-Shot Prompting\", \"Chain-of-Thought Reasoning\", \"Instruction Tuning\"],\n","    [\"CI/CD Pipeline\", \"Model Registry\", \"Experiment Tracking\"]\n","]\n","\n","# ================================================================\n","# 2. PATHS & API\n","# ================================================================\n","\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/kimi-k2-instruct-0905/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"kimi-k2-instruct-0905_fewshot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path):\n","    with open(path) as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","# ================================================================\n","# 3. GLOBAL CONFIG\n","# ================================================================\n","\n","MODEL_NAME = \"moonshotai/kimi-k2-instruct-0905\"\n","GLOBAL_MIN_GAP = 15\n","LAST_TS = 0.0\n","MAX_CHARS = 2600\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","# ================================================================\n","# 4. LOGGING\n","# ================================================================\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","\n","# ================================================================\n","# 5. CLEANING & CHUNKING\n","# ================================================================\n","\n","def deep_clean(t):\n","    t = str(t)\n","    t = re.sub(r\"https?://\\S+\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","def chunk_text(text, max_chars=MAX_CHARS):\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean]\n","    sents = re.split(r\"(?<=[.!?])\\s+\", clean)\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) < max_chars:\n","            cur += \" \" + s\n","        else:\n","            chunks.append(cur.strip())\n","            cur = s\n","    if cur.strip(): chunks.append(cur.strip())\n","    return chunks\n","\n","# ================================================================\n","# 6. JSON EXTRACTION\n","# ================================================================\n","\n","def extract_json(txt):\n","    try:\n","        s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","        if s == -1 or e == -1:\n","            return {}\n","        return json.loads(txt[s:e+1])\n","    except:\n","        return {}\n","\n","# ================================================================\n","# 7. GROQ CALL (RELIABLE)\n","# ================================================================\n","\n","def groq_call(prompt, temperature=0.2, retries=3):\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        time.sleep(GLOBAL_MIN_GAP - (now - LAST_TS))\n","\n","    for attempt in range(retries):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content\n","        except Exception as e:\n","            print(f\"Retry {attempt+1}/{retries}: {e}\")\n","            time.sleep(4)\n","\n","    return \"\"\n","\n","# ================================================================\n","# 8. FEW-SHOT TASKS\n","# ================================================================\n","\n","# ------ SUMMARY ------\n","def generate_summary(transcript):\n","    chunks = chunk_text(transcript)\n","    partial = []\n","\n","    fewshot = \"\\n\\n\".join([f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_SUMMARIES])\n","\n","    for c in chunks:\n","        prompt = f\"\"\"\n","Learn from examples:\n","{fewshot}\n","\n","Now summarise the transcript chunk.\n","Return ONLY JSON:\n","{{\"generated_summary\":\"...\"}}\n","\n","CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\n","\"\"\"\n","        out = groq_call(prompt, 0.15)\n","        j = extract_json(out)\n","        partial.append(j.get(\"generated_summary\", \"\"))\n","\n","    combined = \" \".join(partial)\n","\n","    final_prompt = f\"\"\"\n","Combine the drafts into a 120–160 word summary.\n","Return ONLY JSON: {{\"generated_summary\":\"...\"}}\n","\n","DRAFTS:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\n","\"\"\"\n","    out2 = groq_call(final_prompt, 0.15)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\")\n","\n","# ------ TOPICS ------\n","def classify_topic(transcript, summary):\n","    text = summary + \" \" + transcript[:2000]\n","\n","    examples = \"\\n\".join(\n","        [f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_TOPICS]\n","    )\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Pick up to 3 topics from:\n","{', '.join(VALID_TOPICS)}\n","\n","Return JSON: {{\"predicted_topics\":[\"...\"]}}\n","\n","TEXT:\n","\\\"\\\"\\\"{text}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","# ------ Q&A ------\n","def generate_qa(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([f\"Q:{x['q']}\\nA:{x['a']}\" for x in FEWSHOT_QA])\n","\n","    prompt = f\"\"\"\n","Learn QA from examples:\n","{examples}\n","\n","Return JSON: {{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    for qa in qas:\n","        lines.append(f\"Q: {qa.get('q','')}\")\n","        lines.append(f\"A: {qa.get('a','')}\")\n","    return \"\\n\".join(lines)\n","\n","# ------ CONCEPTS ------\n","def generate_concepts(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([\", \".join(lst) for lst in FEWSHOT_CONCEPTS])\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Extract 10–12 technical concepts.\n","Return JSON: {{\"key_concepts\":[\"...\"]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.15)\n","    j = extract_json(out)\n","    return \", \".join(j.get(\"key_concepts\", []))\n","\n","# ================================================================\n","# 9. MAIN PIPELINE\n","# ================================================================\n","\n","def run_pipeline():\n","    df = pd.read_excel(INPUT_FILE)\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        processed = set(old[\"row_index\"])\n","        results = old.to_dict(orient=\"records\")\n","        print(f\"Resuming: {len(processed)} rows already completed.\")\n","    else:\n","        processed = set()\n","        results = []\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            continue\n","\n","        title = str(row[\"title\"])\n","        transcript = str(row[\"transcript\"])\n","\n","        print(\"\\nProcessing:\", title)\n","\n","        summary = generate_summary(transcript)\n","        topics = classify_topic(transcript, summary)\n","        qa = generate_qa(transcript)\n","        concepts = generate_concepts(transcript)\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","\n","    return pd.DataFrame(results)\n","\n","# ================================================================\n","# 10. RUN\n","# ================================================================\n","\n","df_out = run_pipeline()\n","print(\"Few-Shot pipeline completed successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCR9VY4pKmSh","executionInfo":{"status":"ok","timestamp":1764055264810,"user_tz":-330,"elapsed":3138777,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"2c72d066-7b7b-4f7c-8245-2a25e382c927"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Processing: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","The lecture first shows how transformers overcome RNN limits by using self-attention: query, key and value vectors let the model weigh every token, boosting translation and summarisation. It then introduces reinforcement learning from human feedback (RLHF) with a grid-world agent, Frank, to prove that human guidance speeds learning. Finally, it describes ChatGPT’s two-step RLHF pipeline: (1) train a reward model from human answer rankings, and (2) fine-tune the LLM via proximal policy optimisation, iterating until outputs satisfy human preferences.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Reinforcement Learning', 'Generative AI']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilise training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same set of weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed models?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","The tutorial uses CVXopt to illustrate how kernels shape SVMs, visualising non-linear boundaries and soft margins for education, not production. It walks through a minimal quadratic-programming example, checks results with NumPy/SciPy, and references Matthew Blondel’s GitHub, Bishop’s PRML, and a 4-page MIT solver guide. From scratch, it implements linear, polynomial and RBF kernels, hard/soft margins via C, solves for α, extracts support vectors and predicts with sign-projection. Toy datasets demonstrate how kernels lift overlapping data into separable spaces, shown with 2-D contours. Linear kernels reduce to dot products, whereas non-linear ones need explicit kernel matrices. The next tutorial will cover scikit-learn hyper-parameters, multi-class strategies and practical tips.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What is CVXOPT used for in this tutorial?\n","A: It’s used to solve the quadratic-programming formulation of an SVM so you can see exactly where and how the kernel is injected.\n","Q: Why does the speaker say you probably won’t use CVXOPT in production?\n","A: Because for real-world SVMs you would almost always rely on optimized libraries like LIBSVM instead.\n","Q: What practical insight does the code provide beyond a standard sklearn SVM?\n","A: It lets you visualize the non-linear decision boundary and the soft-margin effect that the kernel produces.\n","Q: Where did the example code originate?\n","A: It was taken from Matthew Blondel’s GitHub, with concepts also referenced from Christopher Bishop’s PRML book.\n","\n","KEY CONCEPTS:\n","\n","Support Vector Machine, Kernel Trick, CVXOPT, Quadratic Programming, Soft Margin, Non-linear Classification, LIBSVM, Pattern Recognition and Machine Learning, Visualization of Decision Boundary, Hyperparameter Impact\n","\n","============================================\n","\n","Processing: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","The lecture shows how transformers use self-attention to capture long-range dependencies, outperforming RNNs in translation and summarisation. It defines prompts as the inputs that seed large-language-model generation and demonstrates that their length, language, context and constraints control output quality. Seven prompt types—question, statement, multi-input, constrained, etc.—are introduced, with live demos contrasting vague queries (“capital of France”) with refined ones (“one-word answer”). Effective prompts specify both the desired result (“what”) and the required format or tone (“how”). The session ends by teaching prompt deconstruction: extracting objectives, requirements and constraints such as word count or SEO optimisation, preparing students to build prompt-engineering workflows next time.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #3 - Agents & Tools - Intro\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","The lecture first explains diffusion models: a forward noising stage and a learned reverse process that together produce crisp images, offering more stable training and clearer likelihood objectives than VAEs or GANs, with demos on class-conditional synthesis and inpainting. It then shifts to AI agents, systems that autonomously choose their next steps instead of following fixed chains. Tools—search, calculator, etc.—are the callable functions agents use. The ReAct pattern structures this interaction: an LLM repeats think → act → observe until the goal is met, with LangChain running tools and feeding results back. Adding tools to an LLM thus creates an agent; upcoming code will build a basic ReAct agent in LangChain and show how LangGraph overcomes its limitations.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Agentic AI', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip pathways, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention for vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","The tutorial demonstrates a live LangSmith trace of a LangChain reflection-agent pipeline refining a tweet. After adding the API key, each node (generate, reflect) streams execution data, capturing six iterative exchanges in 46 s. Viewers see how critique feedback boosts virality through emojis and hashtags, learn the difference between traces and runs, and get a preview of the upcoming Reflexion agent.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Prompt Engineering', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of tracing the reflection agent system?\n","A: To understand exactly what is happening where so we can see how both systems work together to deliver the final refined viral tweet.\n","Q: Where does the speaker go to trace the system?\n","A: To the website smith.chain.\n","\n","KEY CONCEPTS:\n","\n","Reflection Agent, Tracing Agent Execution, System Integration, Viral Tweet Generation, Smith.chain Platform\n","\n","============================================\n","\n","Processing: LangChain Crash Course #7 - Chat Models - Setup\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","The tutorial demonstrates installing LangChain’s ChatOpenAI, instantiating GPT-4o (or cheaper GPT-3.5), and calling it with the invoke method. It covers handling a missing-API-key error by storing OPENAI_API_KEY in a .env file and loading it with python-dotenv. After showing how to parse the response to extract content, it addresses low-balance issues and previews feeding full conversation history so the model retains context for coherent replies.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Prompt Engineering', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What package must be installed to use LangChain’s OpenAI chat model?\n","A: langchain-openai\n","Q: Which class is imported to instantiate the chat model?\n","A: ChatOpenAI\n","Q: What keyword argument specifies the model version when initializing ChatOpenAI?\n","A: model_name (or model)\n","Q: Which GPT model does the speaker choose for the demo?\n","A: gpt-4o\n","Q: Why might someone pick GPT-3.5-turbo instead of GPT-4o?\n","A: To reduce cost while still using a capable model.\n","\n","KEY CONCEPTS:\n","\n","Chat Model, OpenAI API, LangChain, Package Installation, Import Statement, Model Initialization, GPT-4o, GPT-3, Keyword Parameter, Module Import\n","\n","============================================\n","\n","Processing: Python Training Course - Python Sort List\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","The clip illustrates Python’s list.sort() behaviour: capitalised strings are grouped and sorted before lowercase ones, while mixed-type lists place numbers ahead of strings; in both cases reverse=True is respected.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What does Python’s default list.sort() do when the list contains both strings and numbers?\n","A: It places all numbers before all strings, then sorts each group in its own order.\n","Q: Why do uppercase words appear before lowercase ones after sorting?\n","A: Because ASCII/Unicode ordering gives uppercase letters lower code points than lowercase letters.\n","Q: How can you force a case-insensitive alphabetical sort of strings?\n","A: Convert every element to the same case (e.g., lower()) before or during the sort.\n","Q: Where does reverse=True place numbers when the list mixes strings and numbers?\n","A: At the end of the list, because the numeric group is still treated as coming before the string group.\n","\n","KEY CONCEPTS:\n","\n","Python list sorting, Case-sensitive ordering, Mixed-type list handling, ASCII-based collation, Reverse sort, String comparison rules, Type precedence in sorting, Capital vs lowercase ordering, In-place sort method, Python sort stability\n","\n","============================================\n","\n","Processing: \n","Humans vs. AI: Who should make the decision?\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","The lecture champions a hybrid fraud-detection workflow: AI handles high-confidence alerts, humans tackle uncertain ones, and joint review governs mid-confidence cases, outperforming either alone. Success hinges on interface choices—forced AI advice invites automation bias, optional transparent cues curb it. Paradoxically, exposing accuracy scores can erode trust. Decision authority should be assigned by expected effectiveness, with careful design fostering augmented intelligence and superior joint outcomes.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What do the flat portions of the human performance curve indicate in hybrid AI–human decision systems?\n","A: They show that humans maintain more consistent accuracy than AI when confidence is middling, letting them outperform on ambiguous or rare cases.\n","Q: Why does an AI’s success rate drop near the 50 % confidence mark?\n","A: The model has learned that these are the statistically uncertain regions where its features are weak, so its predictions become little better than random.\n","Q: How does the confidence-score axis guide the hand-off between AI and human reviewers in fraud detection?\n","A: Alerts whose confidence is very high or very low are routed to the AI, while the intermediate band—where the AI curve dips—is escalated to humans.\n","Q: What practical problem does the 90 % false-positive rate create for analysts?\n","A: It overloads them with noise, wasting cognitive effort and delaying review of the few genuine fraud cases.\n","Q: In production, who typically monitors and retrains the fraud-detection model to keep the performance curves accurate?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Confidence Score, Success Rate Curve, False-Positive Rate, Human-in-the-Loop, AI Performance Curve, Threshold Tuning, Uncertainty Quantification, Contextual Reasoning, Rare-Event Detection, Workload Allocation, Hybrid Decision System, Alert Prioritization\n","\n","============================================\n","\n","Processing: \n","Build generative apps faster with Vertex AI\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","The lecture shows how transformers’ self-attention mechanism, using query, key, and value vectors, overcomes RNN limits by weighing every token, boosting translation and summarisation quality. It also introduces six new Vertex AI APIs: document-understanding for complex layouts, upgraded text embeddings, hybrid vector search, relevance ranking, grounded generation with citations, and automatic fact-checking. Google stresses built-in quality, planet-scale infrastructure, and framework-agnostic integration so developers can concentrate on unique application logic.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Generative AI', 'Mlops']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip pathways, preventing vanishing gradients.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By applying the same filter weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML systems?\n","A: ML engineers and data scientists jointly track input distribution shifts and trigger retraining pipelines.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Unitary Transformations\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","The lecture contrasts prompt-engineering strategies—zero-shot, few-shot and chain-of-thought—to improve LLM reliability through clear instructions, roles and constraints. It then visualises SVD: orthogonal matrices U and V rotate space without distortion, while Σ scales axes. Multiplying the unit sphere by X yields an ellipsoid whose principal directions and stretches are given by the singular vectors and values, offering an intuitive, coordinate-free picture of how any matrix reshapes space.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Data Science', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilise training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same set of weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","The video showcases Google Gemini Pro 1.5’s 1-million-token multimodal context by feeding it a 330k-token Apollo 11 PDF, sketching Neil’s first step, and retrieving exact quotes and timestamps. After a 1-min official demo, the presenter walks through getting a free API key at ai.google.com, installing the Python SDK, and securely configuring genai in a notebook. Code examples show unified text-and-vision calls: streaming answers to “what is life?” and “what is ML?”, plus generating a blog post from an image prompt. Latency is noted as token-generation overhead that will shrink in production. Viewers are urged to test the new single-model, multimodal workflow in their own projects.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Python Programming', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is Google Gemini Pro 1.5?\n","A: A multimodal generative-AI model that works with both text and images.\n","Q: Why does the speaker show Google’s 1-minute demo first?\n","A: To give viewers a quick overview of what Gemini Pro 1.5 can do.\n","Q: How will viewers get hands-on experience in the video?\n","A: By seeing code run that uses the model with both images and text.\n","Q: What key setup step is covered after the demo?\n","A: How to create and use the Gemini Pro API key.\n","Q: What does ‘multimodal’ mean in the context of Gemini Pro 1.5?\n","A: The model can process and generate both text and image data.\n","\n","KEY CONCEPTS:\n","\n","Google Gemini Pro 1.5, Generative AI, Multimodal Model, API Key, Long Context Understanding, End-to-End Projects, Hands-On Application, Text and Image Processing, Demo Video, YouTube Playlist, Experimental Feature, 1-Minute Demo\n","\n","============================================\n","\n","Processing: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","The lecture describes how transformers overcome RNN limits through self-attention, using query, key, and value vectors to weigh relevant tokens, yielding gains in translation and summarisation. It then details evaluation of prompt-engineered LLMs via perplexity, accuracy, and human judgment, demonstrates computing these on a small translation set, and shows how error-pattern analysis, cross-validation, and visualisation refine models iteratively to improve generalisation.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Generative AI vs AI agents vs Agentic AI\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","The session demystifies diffusion models by showing how they learn to undo a noising process via forward and reverse Markov chains, and how classifier-free guidance steers high-fidelity image generation. It then maps the autonomy spectrum: generative AI (static LLM answers), AI agents (single LLM plus tools that finish narrow tasks like booking one flight), and agentic AI (multi-agent loops that plan and execute complex goals such as an entire trip, checking visas, weather and budget). Throughout, the talk stresses tool use, memory, and human oversight, and names practical orchestration frameworks including N8N, LangGraph and Agno.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Agentic AI', 'Langraph']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Covariance in Statistics\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","The lecture first describes diffusion models, which iteratively denoise data to synthesise high-quality images. It covers the forward and reverse Markov processes, the variational lower-bound objective, and practical training techniques such as noise scheduling and classifier-free guidance, illustrating results on ImageNet and text-to-image tasks. The second part introduces covariance as a statistic that quantifies the direction of the linear relationship between two random variables. Starting from variance, it shows how covariance generalises to paired data like house size and price, explains how its sign reveals whether variables increase together or move oppositely, and uses scatter-plot geometry to clarify the intuition. The segment concludes by noting that covariance indicates direction but not strength, setting the stage for Pearson correlation.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention for vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: Machine-learning engineers and site-reliability engineers jointly track input distribution shifts.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: 3. Objective || End to End AI Tutorial\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning seeks an optimal policy that maximises cumulative reward through trial-and-error feedback. Tasks fall into episodic (e.g., tic-tac-toe with +1/−1/0 outcomes) or continuing (e.g., trading with profit or risk-adjusted returns) categories. Algorithms are grouped into value-based, policy-based, and hybrid approaches that will be explored to achieve this objective.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections critical in deep vision Transformers?\n","A: They stabilize training by providing gradient highways and allowing layers to learn small perturbations on the identity mapping.\n","Q: How do vision Transformers handle variable image sizes?\n","A: They split images into fixed-size patches and add learnable positional encodings that interpolate to any resolution.\n","Q: When should you prefer convolutional backbones over ViTs?\n","A: \n","Q: Who typically tunes the hyper-parameters of a ViT during pre-training?\n","A: Research scientists and ML engineers using large-scale distributed experiments and automated tuning frameworks.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Python Training - Python Dictionary Basics\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","The tutorial first shows how diffusion models iteratively denoise data to create sharp images, covering the forward/reverse Markov processes, variational lower-bound objective, and practical training hacks like noise scheduling and classifier-free guidance. It then introduces Python dictionaries as curly-bracket key-value stores with immutable keys, demonstrating core methods (items, keys, values), construction from zipped lists, square-bracket access/updates, deletion with del, length checks, and conversion back to lists, preparing learners for data-science work with pandas.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections critical in deep networks?\n","A: They provide direct gradient paths that mitigate vanishing gradients and enable stable training of very deep architectures.\n","Q: How does policy-gradient reinforcement learning update an agent’s policy?\n","A: It increases the log-probability of actions that led to higher-than-expected returns and decreases it for worse actions.\n","Q: When is zero-shot prompting preferable to fine-tuning?\n","A: When no labeled data are available and the pre-trained model already encodes sufficient semantic knowledge for the task.\n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: Data scientists and ML engineers who set up alerting pipelines and retraining triggers.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Fight Insider Threats with AI-infused SIEM\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","AI-powered User Behaviour Analytics (UBA) can shorten breach containment by 108 days, IBM’s 2023 report shows. The session spotlights IBM QRadar UBA, which builds machine-learning baselines of user and peer activity inside the SIEM. After a seven-day learning phase, analysts receive risk-ranked dashboards, watch-lists and timeline views that map anomalies to MITRE ATT&CK tactics and enrich IOCs via AI correlation. Human feedback continuously refines the model, cutting insider-threat investigation from hours to minutes and freeing SOC teams for proactive defence.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Artificial Intelligence', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: How much faster can AI reduce breach containment time?\n","A: By 108 days on average compared with organisations that don’t extensively use AI and automation.\n","Q: What does UBA stand for?\n","A: User Behaviour Analytics.\n","Q: Which IBM report provided the 108-day statistic?\n","A: The Cost of a Data Breach Report 2023.\n","Q: What was the average cost of an Insider threat according to the report?\n","A: Four million dollars.\n","Q: What type of threats does UBA with AI aim to detect?\n","A: Insider threats.\n","\n","KEY CONCEPTS:\n","\n","User Behavior Analytics (UBA), Insider Threat Detection, AI-driven Incident Response, Machine Learning for Security, Data Breach Containment, Threat Detection Automation, Behavioral Anomaly Detection, Security Posture Improvement, AI vs Manual Response Time, Cost of Insider Threats, IBM Cost of a Data Breach Report, AI-enabled Security Operations\n","\n","============================================\n","\n","Processing: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","Krishak’s 2 a.m. video announces Meta’s open-source Llama 3 in 8 B and 70 B pre-trained and instruction-tuned forms, trained on 50 T tokens with 8 k context and leading open benchmarks. It slightly trails on MML/math yet beats Gemini Pro on HumanEval/GSM. Access is gated: accept Meta’s safety rules, request weights, then download approved checkpoints from Hugging Face, Kaggle or GitHub and run local inference with the supplied snippets; extra recipes are on GitHub.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Generative AI']\n","\n","GENERATED Q&A:\n","\n","Q: What time is it in the video?\n","A: 2 a.m.\n","Q: Who is introducing themselves?\n","A: Krishak\n","Q: Where is Krishak welcoming viewers?\n","A: To his YouTube channel\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: Getting Started With sklearn\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","The tutorial demonstrates how to build a Naive Bayes classifier in Python using scikit-learn. It directs learners to the official documentation to locate Gaussian Naive Bayes, the specific variant employed to generate the previously shown decision boundary.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What Python library is introduced for building the classifier?\n","A: scikit-learn (sklearn).\n","Q: Which specific Naive Bayes variant is chosen for the implementation?\n","A: Gaussian Naive Bayes.\n","Q: Why does the instructor suggest starting with Google?\n","A: To quickly access the library’s documentation and find the right function.\n","Q: What will the next videos enable students to do?\n","A: Write the decision-boundary code themselves.\n","\n","KEY CONCEPTS:\n","\n","scikit-learn, Naive Bayes, Gaussian Naive Bayes, decision boundary, Python library, documentation lookup, algorithm implementation, classifier, derivation, use cases\n","\n","============================================\n","\n","Processing: \n","Log Normal Distribution in Statistics\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","The lecture explains that log-normal distributions emerge when the logarithm of the data is normally distributed, producing the fat tails visible in income, product-review length, and similar variables. To standardize such features for modeling, apply a log-transform followed by a z-score, converting them to a common Gaussian scale and improving downstream predictive accuracy.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip pathways, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","A 7-part series teaches an end-to-end deep-learning pipeline for agriculture: collect potato-leaf images, train a CNN to spot early/late blight, serve the model with TF Serving or FastAPI on GCP, and consume it from a React-Native mobile app. After dataset cleaning and augmentation with tf.data, the model is exported, quantized to TF-Lite, and deployed via Google Cloud Functions. Viewers can skip videos 14–20 for brevity; prerequisites are basic Python and DL knowledge. The walkthrough highlights résumé value and suggests repurposing the classifier for tomatoes as practice.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Mlops', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main goal of the end-to-end deep-learning project described in the video?\n","A: To build a mobile app that lets farmers photograph potato plants and immediately detect early or late blight so they can apply the correct treatment and avoid economic loss.\n","Q: Why is accurate disease classification critical for potato farmers?\n","A: Because early blight (fungal) and late blight (microbial) require different treatments; mis-classification would waste resources and still lose the crop.\n","Q: Which ML-Ops tools are mentioned for serving the trained model in production?\n","A: TensorFlow Serving for model serving, FastAPI for the backend REST API, Google Cloud Functions for serverless inference, and React Native for the mobile client.\n","Q: How many videos will the complete tutorial series contain?\n","A: Seven to eight videos covering data collection, model building, ML-Ops, backend, cloud deployment, and mobile integration.\n","Q: Who is the intended end-user of the finished application?\n","A: Potato farmers who can take a simple photo with their phone and receive an instant disease diagnosis.\n","\n","KEY CONCEPTS:\n","\n","Convolutional Neural Network, Transfer Learning, Image Classification, Data Augmentation, TF Serving, FastAPI, Google Cloud Platform, Cloud Functions, React Native, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The lecture contrasts transformers’ self-attention mechanism with RNNs, highlighting gains in translation and summarisation, then outlines six ascending autonomy levels for LLM applications. Level 0 is rigid, deterministic code; level 1 issues single LLM calls; level 2 chains fixed prompts; level 3 adds routing choices; levels 4–5 introduce loops, memory, tools and human-in-the-loop checks. Fully autonomous agents (level 6) remain experimental. LangGraph is introduced as the framework to orchestrate these stateful, iterative agentic workflows beyond simple pipelines.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Agentic AI', 'Langraph']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilise training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","The lecture covers diffusion models, showing how iterative denoising converts Gaussian noise into high-quality images, matching GANs without mode collapse, and compares score-based and DDPM formulations while explaining the re-weighted ELBO objective. It then advances to expert prompt engineering, demonstrating multimodal inputs (text, image, audio) on a dog-breed task with ResNet-50 and VGG-16, and details fine-tuning via multitask learning and distillation, exemplified by T5-small. Rigorous preprocessing, TensorFlow Serving or Flask deployment, and ethical safeguards on bias, fairness and privacy are underscored, followed by hands-on exercises to consolidate skills.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Prompt Engineering', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilise training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across all spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: SVD: Eigen Action Heros [Matlab]\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","The lecture first reviews diffusion models: they create data by learning to reverse a noising Markov chain, optimise a reparameterised variational lower-bound, and deliver stable training and high-fidelity samples, contrasting favourably with VAEs and GANs. It then demonstrates face clustering with eigenfaces: 40 grayscale 200×175 action-hero photos are mean-centred and decomposed via economy SVD; the first three principal components cleanly separate Arnold and Stallone clouds, correctly placing test images. Repeating the protocol with Taylor Swift reveals that light skin/hair correlations can overwhelm identity, exposing the limits of simple pixel-based classifiers.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections critical in deep vision Transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip pathways, mitigating vanishing-gradient issues.\n","Q: How do convolutional kernels achieve translation equivariance?\n","A: By sharing the same set of weights across every spatial location, forcing the detector to respond to features regardless of position.\n","Q: When should you prefer convolution over self-attention for vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangChain Crash Course #3 - What is LangChain?\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","LangChain turns passive LLMs into active agents by linking them to external tools. While a raw model can only describe booking travel, a LangChain-powered agent can query databases, call booking APIs, and send emails. The framework keeps reasoning pluggable across models, enabling secure interaction with private data, web services, and mail systems, and shifting LLMs from generating text to executing real-world tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Agentic AI', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What problem does LangChain solve?\n","A: It bridges the gap between LLMs and the real world so the AI can call APIs, query databases, or send emails instead of only generating text.\n","Q: Why can’t an LLM alone book a flight?\n","A: Because LLMs are reasoning engines that lack built-in tools to interact with external services like booking APIs.\n","Q: What does LangChain let you swap without touching application code?\n","A: The underlying LLM—e.g., replace GPT-4 with a free Hugging-Face model.\n","Q: When would you pick LangChain for a project?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Large Language Model (LLM), LangChain Framework, API Integration, Model Interchangeability, Real-World Interaction, Application Framework, Chat Interface, Reasoning Ability, Booking APIs, Hugging Face Models, GPT-4, Travel Planning Application\n","\n","============================================\n","\n","Processing: How To Use Residuals For Time Series Forecasting\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","The lecture first surveys diffusion models, showing how iterative denoising converts Gaussian noise into realistic images, compares score-based and DDPM formulations, and highlights a re-weighted ELBO objective that yields GAN-level quality without mode collapse. It then turns to time-series diagnostics, demonstrating residual analysis on AirPassengers data: after fitting a Holt-Winters model in Python, residuals are tested for white-noise properties via ACF/PACF plots and the Ljung-Box test, uncovering significant yearly autocorrelation (p < 0.05) despite a near-zero mean (–0.02). These findings guide targeted model refinement before refitting.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Time Series', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections critical in deep vision Transformers?\n","A: They stabilize training by providing gradient highways and allowing layers to learn small perturbations on the identity mapping.\n","Q: How do vision Transformers handle variable image sizes?\n","A: They split images into fixed-size patches and add learnable positional encodings that interpolate to any resolution.\n","Q: When should patch-size be reduced in a vision Transformer?\n","A: When finer-grained spatial detail is needed for small objects or high-resolution images.\n","Q: Who typically tunes the patch-size and embedding dimensions?\n","A: Machine-learning researchers and engineers during model design and hyper-parameter optimization.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","This demo builds a Text2SQL ReAct agent with LangGraph on watsonx.ai, letting users query an in-memory SQLite database through a Next.js chat UI. It scaffolds a TypeScript/Tailwind frontend, wires a header, message list and input bar, and adds loading states. Server-side, it installs LangChain/LangGraph, stores watsonx credentials in .env, and creates actions.ts that deserializes chat history, configures Mistral Large, and returns agent answers. A local SQLite module seeds Customer and Order tables, exposes an execute function, and wraps it in a GetFromDB tool attached to the agent. A system prompt directs the LLM to generate SQL, always call the tool, and quote identifiers. The agent answers natural-language questions, tells SQL jokes, and reports insights like top-ordering customers, demonstrating ReAct text-to-SQL with guardrails.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Langraph', 'Natural Language Processing', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What lets an LLM-based agent query databases directly?\n","A: Its pre-trained SQL knowledge plus a ReAct loop that turns natural-language questions into executable SQL.\n","Q: Why use LangGraph when building a Text2SQL agent?\n","A: \n","Q: How does the tutorial keep the demo lightweight yet realistic?\n","A: \n","Q: When is Next.js set to client-side rendering in this project?\n","A: \n","Q: Who is expected to extend or productionise this minimal Text2SQL prototype?\n","A: \n","\n","KEY CONCEPTS:\n","\n","ReAct agent, LangGraph, Next.js, TypeScript, Tailwind CSS, SQLite, Text2SQL, Client-side component, Server-side rendering, watsonx.ai, create-next-app, VS Code\n","\n","============================================\n","\n","Processing: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","Prompt engineering is the art of designing inputs that guide large language models to produce accurate, coherent, and context-aware responses. The lecture contrasts this data-driven method with rigid rule-based systems, showing how it boosts chatbots, translation, and content creation while warning of pitfalls like ambiguity and embedded bias. The course will teach prompt analysis, weigh benefits against limitations, and explore advanced fine-tuning techniques to maximize model utility.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What is prompt engineering?\n","A: A specialized NLP field that designs inputs (prompts) to make large pre-trained language models produce accurate, coherent, context-appropriate text.\n","Q: Why is prompt engineering important?\n","A: It yields higher-quality outputs than rule- or keyword-based methods, improving user experience in chatbots, translation, and content generation.\n","Q: What base models are typically used in prompt engineering?\n","A: Large pre-trained language models such as OpenAI GPT, Google Bard, and Hugging Face Transformers.\n","Q: What limitations can prompt-engineered models face?\n","A: They may struggle with complex or ambiguous prompts and can produce biased or inaccurate text inherited from training data or model architecture.\n","Q: What will learners cover first in this course?\n","A: Fundamentals of prompt engineering—prompt analysis, benefits, limitations, and core structure—before advancing to fine-tuning techniques.\n","\n","KEY CONCEPTS:\n","\n","Prompt Engineering, Large Language Models (LLMs), Fine-Tuning, Pre-trained Models, Contextual Appropriateness, Prompt Analysis, Natural Language Processing (NLP), Text Generation, Bias in Language Models, Ambiguous Prompts, Rule-Based vs. Keyword-Based Approaches, User Experience & Engagement\n","\n","============================================\n","\n","Processing: Q-learning - Explained!\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","The lecture explains how transformers use query, key, and value vectors to focus on relevant tokens, outperforming RNNs in translation and summarisation. It then introduces Q-learning, a value-based reinforcement-learning method, contrasting it with supervised and unsupervised learning. Core concepts—states, actions, and the Q-function—are defined, and a grid-world example shows how the Bellman equation updates a Q-table toward an optimal policy. Two updates are detailed: compute the temporal-difference error between observed and expected reward, then adjust the Q-value with a learning-rate-weighted step. Repeating this process across many episodes, while following an exploratory behaviour policy, gradually stabilises Q-values, enabling an off-policy algorithm to converge on the optimal target policy.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Reinforcement Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does the self-attention mechanism in Transformers compute?\n","A: It computes a weighted average of all token representations, where weights are learned based on token-to-token relevance.\n","Q: Why are residual connections essential in deep vision transformers?\n","A: They stabilize training by allowing gradients to flow directly through skip connections, preventing degradation.\n","Q: How do convolutional kernels capture translational equivariance?\n","A: By sharing the same weights across spatial locations, ensuring identical features are detected regardless of position.\n","Q: When should you prefer convolution over self-attention in vision?\n","A: \n","Q: Who is responsible for monitoring data drift in deployed ML services?\n","A: \n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Training Your Logistic Classifier\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","The lecture presents logistic (softmax) classification: a linear model that maps image pixels through matrix multiplication to per-class scores, converts these scores into probabilities via softmax, and learns weights W and bias b so the probability assigned to the true class approaches 1.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What kind of classifier is a logistic classifier?\n","A: A linear classifier that applies a matrix multiply to the inputs.\n","Q: What turns raw classification scores into probabilities?\n","A: The softmax function, which outputs values that sum to 1.\n","Q: Which parameters does training a logistic model optimise?\n","A: The weight matrix W and the bias vector b.\n","Q: Why are logits converted to probabilities?\n","A: To ensure the predicted class has probability near 1 while all others are near 0.\n","\n","KEY CONCEPTS:\n","\n","Logistic classifier, Linear classifier, Linear function, Matrix multiply, Weights, Bias, Softmax function, Logits, Scores, Probabilities, Training, Predictions\n","\n","============================================\n","Few-Shot pipeline completed successfully!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OGTJs_1fqiC7","executionInfo":{"status":"ok","timestamp":1764055674209,"user_tz":-330,"elapsed":2877,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"ea100dca-a81d-46f6-9aa4-3842749d02e4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_fewshot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/kimi-k2-instruct-0905/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["8f1b555091844f8d8e7fdc068ae0a36e","9427731536fb4ca3bdfe595bb06fe517","d19340a241e34ebdb8e08b13a06085a8","4a05019a7c8c4ba88ebfc6a9c28e2596","1f6a66c44c6544eeb2c2d0b08cafb6bc","d2e07fce58a04e4a9138cce40b89e5f6","7fce9d7ceec644dd8220175a03e0f623","b27f72969f3840d89b15fda3b90952e5","c33f4438b7354adeb9ee3916562c4059","29a7db99df0f4f12bef2d54213479b60","d8cb6ef71be149eabf5f50b72c84713a","7f41f604c99d4c50bc8982b05b3dfa6e","d61e502cd70145d09dca13c35a80ba13","5d1df31f080b4a7ab2b2f3f2ea205993","cc4f29f400ea46f29a791558e1996451","2fa94a971ae14b32aed17eb3e9d26809","2fa5ca7ddb04423d830a51e9317421ad","74d3b5d0f051429ab48f1e71f9764949","f7022d47309e46bf8c6c12a37737b546","6d863d387c44408586680798a7c0a816","b355f67ad1dc4508bcf8377aa22b47fc","1572287093844b7abb6639e3b4cc8ca8","f75cda58d60e4da2bce5369b1f4410c2","6457ea9309b84a318fade008e941da74","6832b7b97b6d45138bfd0b3807dea0f2","26e76cf5ae7b44ae81aa22760ae474f3","3eeacb5e1e654576ad739c6dab7dbfef","1b25499a18dc4a649ee57e561fcb36c9","0043289b593a42ca96bb422a0ec13d0a","bfd7c6f52c5648368bd3f01857123835","443eee1573f04f9c8caba9605a596bf9","0c73c707b81b41d888270a001beebf81","b19ea463b3614bb48dbef2d2f81db78b","7062746eff364d05aaf38aa76cfc3838","07b9f74912794dfdb85cec3e913e7c28","c282cb34a87240afb794319105563170","27527911d98d4c859db17d12cd03b2e1","558e6ea1e0a54c9fb3bb231b30881399","796feb01cda7460193250a090d746343","d110e2ce0030455190ffc3756239bc7a","be93dd90db18443aa04c7c2f644e1a6b","eb43621f62fa44c887f0fc72528bd2b6","589346a359d146e68123664a35b616d6","3c6c3fbd41614d1394ea116bb5de9286","eebef5298c1a48f6a2b98b2f6b9eff16","37ebc667ecaf4426898d24e2d5cb99ee","6a2ab4147f904a988222fe981c34ad8f","93de464cbec24db0a76c86915c8e78f8","2766849a2edf4844861de4831763e2d8","d52b61c3022941faafc6dcc16f0ae44d","f3bc74af9b5a4186ad6cef047e20d214","d66b4ded505d4695a8342ce74d399c65","3458430ccd024462bd6356947a82f73d","6345d8fe7deb4acaa1ee9e5a642b85db","e77e07a344564895a5bf19712891d696","f1b526b425474185a4045b3d97a85b82","1ed712b4576246c0b4ab19a03aa8003c","edf1aa4421964e0195557bda21c7f6cf","37955539a57f42ec86b2ba82b5c26b0e","391297e458e14c599be8fcdffb5a282a","4b8ffd58d18643c8953b6fba7e23d19c","6736a8a817124216ad64fc5a33ef6ec1","f1372fa367c54cd296d9b3939fa426bd","28aa95c3e9454b8e86336f1ff7fef6b0","f800d4fc44f34cf9ac6691376b681d3f","9a81c3852e2e455e95ade6f3c331f31c"]},"id":"YJASQHidKmXp","executionInfo":{"status":"ok","timestamp":1764055841480,"user_tz":-330,"elapsed":167266,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"a82c9183-4f62-486a-cd84-500461497a02"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_fewshot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f1b555091844f8d8e7fdc068ae0a36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f41f604c99d4c50bc8982b05b3dfa6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f75cda58d60e4da2bce5369b1f4410c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7062746eff364d05aaf38aa76cfc3838"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebef5298c1a48f6a2b98b2f6b9eff16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1b526b425474185a4045b3d97a85b82"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.1949\n","  - BLEU: 0.0111\n","  - BERTScore F1: 0.8567\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3936\n","  - Micro F1: 0.5135\n","  - Macro F1: 0.4637\n","  - Weighted F1: 0.4845\n","\n","Q&A Generation:\n","  - BLEU: 0.0199\n","  - Diversity: 0.8616\n","  - Answerability: 0.2033\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.2933\n","  - Recall@10: 0.1173\n","  - F1@10: 0.1676\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/kimi-k2-instruct-0905/evaluation_final.json\n"]}]}]}