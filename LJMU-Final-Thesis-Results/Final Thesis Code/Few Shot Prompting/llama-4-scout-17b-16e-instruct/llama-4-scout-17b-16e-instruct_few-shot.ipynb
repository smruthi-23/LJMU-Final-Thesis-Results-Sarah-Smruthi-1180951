{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMsBZPH5nDE5o3Gd38V8zo1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"21575f6ffe974bc19f295857c7aa83f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c596ba5b49644572a2eb3eb14123f205","IPY_MODEL_97866925ac564ee0b5879f7031632d61","IPY_MODEL_6f812fe91cb140afad78f59db9b3f858"],"layout":"IPY_MODEL_83d6059bb5e6463f9c5007039f5de096"}},"c596ba5b49644572a2eb3eb14123f205":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5add3af666548118f7f3043c02346ce","placeholder":"​","style":"IPY_MODEL_41c5a25b97014475bbcd3f94912b49fc","value":"tokenizer_config.json: 100%"}},"97866925ac564ee0b5879f7031632d61":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c6ee339ab3b428a907241ad8693115a","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_245d7512281945dfa57eda7ed76f39fa","value":25}},"6f812fe91cb140afad78f59db9b3f858":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bc2e471439745078c3259a0223a8f82","placeholder":"​","style":"IPY_MODEL_4c0807ea643f46d28e1b27e0538ba3b2","value":" 25.0/25.0 [00:00&lt;00:00, 2.14kB/s]"}},"83d6059bb5e6463f9c5007039f5de096":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5add3af666548118f7f3043c02346ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41c5a25b97014475bbcd3f94912b49fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c6ee339ab3b428a907241ad8693115a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"245d7512281945dfa57eda7ed76f39fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8bc2e471439745078c3259a0223a8f82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c0807ea643f46d28e1b27e0538ba3b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b339cdb3ec94bd6a3b911fb4edbc423":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ee25c7d6f07d457cb5a9561a874bdc95","IPY_MODEL_ef086e5e000b45aab6642791d11f4876","IPY_MODEL_a9e51286422f41789112c6e8083d14f5"],"layout":"IPY_MODEL_233b59b616e248fbaef3437db20e1a52"}},"ee25c7d6f07d457cb5a9561a874bdc95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b15265996bc4a0f9d8c2fd2ff92e1d3","placeholder":"​","style":"IPY_MODEL_47880022d6a5448cb4efbfd38a2381f9","value":"config.json: 100%"}},"ef086e5e000b45aab6642791d11f4876":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_15b3e2eb40824fa399538a74185ff423","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f650e57c6ae43dc9037fc5b1f39fad6","value":482}},"a9e51286422f41789112c6e8083d14f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33bc08b403ee493caeada675f72b6bb6","placeholder":"​","style":"IPY_MODEL_390147775e484300b60e1db4056b467c","value":" 482/482 [00:00&lt;00:00, 45.3kB/s]"}},"233b59b616e248fbaef3437db20e1a52":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b15265996bc4a0f9d8c2fd2ff92e1d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47880022d6a5448cb4efbfd38a2381f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"15b3e2eb40824fa399538a74185ff423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f650e57c6ae43dc9037fc5b1f39fad6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"33bc08b403ee493caeada675f72b6bb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"390147775e484300b60e1db4056b467c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2020e2a416064215a45d1370f94f5cb9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e54a58c5f26547d9a06aca605efe3397","IPY_MODEL_86d8eaf8dc9d479cae9d8891dba9badc","IPY_MODEL_52a8607f302c410593f818d8f619bc04"],"layout":"IPY_MODEL_4a939179204046a88d5de6e0391d8409"}},"e54a58c5f26547d9a06aca605efe3397":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cf710409d554821a94869187a4718af","placeholder":"​","style":"IPY_MODEL_d47123827d6c46b7a52a2bd5eacf228f","value":"vocab.json: 100%"}},"86d8eaf8dc9d479cae9d8891dba9badc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1eb33c74b06646038ec301bc5e8e91f3","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84e9a63cb62f4497a5cc4d523f96f955","value":898823}},"52a8607f302c410593f818d8f619bc04":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9ee0463ee34641799c31283af913e591","placeholder":"​","style":"IPY_MODEL_3fb32825e1b24d5c9e375be87c4c008c","value":" 899k/899k [00:00&lt;00:00, 17.9MB/s]"}},"4a939179204046a88d5de6e0391d8409":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cf710409d554821a94869187a4718af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d47123827d6c46b7a52a2bd5eacf228f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1eb33c74b06646038ec301bc5e8e91f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84e9a63cb62f4497a5cc4d523f96f955":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9ee0463ee34641799c31283af913e591":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fb32825e1b24d5c9e375be87c4c008c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c950b9dcf9cf4238a74a8f14d115556a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02ca8d6470554e858dafb882a3678864","IPY_MODEL_32224240970a439c85c26ba05f68cc65","IPY_MODEL_c4233e5810914865bafa12d714eecbf3"],"layout":"IPY_MODEL_3f4e3cf2eddb4fc8a74d5ebbc3b48f07"}},"02ca8d6470554e858dafb882a3678864":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d950e67e9f0c4b9cb96c8272d8ab5cd2","placeholder":"​","style":"IPY_MODEL_94a4f76cd97247fcb2dbcb4e43402de2","value":"merges.txt: 100%"}},"32224240970a439c85c26ba05f68cc65":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a5613c63a73430488d86fb6c03a65c4","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5d038eb631c54c88a503e68a035729ee","value":456318}},"c4233e5810914865bafa12d714eecbf3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d17cecfd84ba4849bced000d244874d8","placeholder":"​","style":"IPY_MODEL_70081880ecc04d2093aea531b6280a7a","value":" 456k/456k [00:00&lt;00:00, 3.67MB/s]"}},"3f4e3cf2eddb4fc8a74d5ebbc3b48f07":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d950e67e9f0c4b9cb96c8272d8ab5cd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94a4f76cd97247fcb2dbcb4e43402de2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a5613c63a73430488d86fb6c03a65c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d038eb631c54c88a503e68a035729ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d17cecfd84ba4849bced000d244874d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70081880ecc04d2093aea531b6280a7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf34a57586b74de5b5833da349c9baf5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c18f84955c8d454f9180f35c1eb9e47a","IPY_MODEL_d09200041b1d455682e63bf251d1f57f","IPY_MODEL_23b9fd1a370f444bb17d247bab4e8ed1"],"layout":"IPY_MODEL_eae3f74c42694d738b70a11779ea935d"}},"c18f84955c8d454f9180f35c1eb9e47a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbb98475e688422aa935188ee94bf7f8","placeholder":"​","style":"IPY_MODEL_176d00e993c140d3a18f828efe7212e2","value":"tokenizer.json: 100%"}},"d09200041b1d455682e63bf251d1f57f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb75418bb35443e19961db3178fc64e9","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d1244adbd4524e8485c2bd22b25df976","value":1355863}},"23b9fd1a370f444bb17d247bab4e8ed1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76c985fc93e849b7bf1f26fc775d6df0","placeholder":"​","style":"IPY_MODEL_de19cca44c0c4adc8bda4a4e79ac12b4","value":" 1.36M/1.36M [00:00&lt;00:00, 7.28MB/s]"}},"eae3f74c42694d738b70a11779ea935d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbb98475e688422aa935188ee94bf7f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"176d00e993c140d3a18f828efe7212e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb75418bb35443e19961db3178fc64e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1244adbd4524e8485c2bd22b25df976":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"76c985fc93e849b7bf1f26fc775d6df0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de19cca44c0c4adc8bda4a4e79ac12b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c878d71e88284fd0bbec5a9aa37f566a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9408f986cd64f7da2f3ab33aad4cb00","IPY_MODEL_a2ec7e43812c48ddaa419de27f526b6a","IPY_MODEL_38f4d6b7788640d290f432d8e3c68159"],"layout":"IPY_MODEL_846b3920b8424e6192e02a1d40c0e208"}},"f9408f986cd64f7da2f3ab33aad4cb00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbec21b3c5a04fb0b9020373c3681c68","placeholder":"​","style":"IPY_MODEL_4f3e45fd59d14277b281c1f19e61389f","value":"model.safetensors: 100%"}},"a2ec7e43812c48ddaa419de27f526b6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_457cfbcc05b4476bbd6be2a047d99414","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4763212ed13240689bd890d0a4325a53","value":1421700479}},"38f4d6b7788640d290f432d8e3c68159":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b55d99625e2544529c52002abf5f2615","placeholder":"​","style":"IPY_MODEL_9ca5b2cc81b64da0994561361f9a4996","value":" 1.42G/1.42G [00:16&lt;00:00, 108MB/s]"}},"846b3920b8424e6192e02a1d40c0e208":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbec21b3c5a04fb0b9020373c3681c68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f3e45fd59d14277b281c1f19e61389f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"457cfbcc05b4476bbd6be2a047d99414":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4763212ed13240689bd890d0a4325a53":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b55d99625e2544529c52002abf5f2615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ca5b2cc81b64da0994561361f9a4996":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"eM6nskD0nxmu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764038616384,"user_tz":-330,"elapsed":21312,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"129c937f-e662-4976-dd6c-eb13fc4b4075"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=1cc15aee34b3aca5ee6a02da11410f7f5dbab63326f3a4914210bac3bed37d2d\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"usIQlE72421l","executionInfo":{"status":"ok","timestamp":1764038616407,"user_tz":-330,"elapsed":24,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"14334f20-b54c-41a4-a26d-16f623f96869"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["# ================================================================\n","# Few-Shot Prompting Pipeline – Groq\n","# ================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","# ================================================================\n","# 1. FEW-SHOT EXAMPLES\n","# ================================================================\n","\n","FEWSHOT_SUMMARIES = [\n","    {\"input\": \"Explains attention in transformers and its role in capturing long-range dependencies.\",\n","     \"output\": \"The lecture introduces attention in transformers, showing how query, key, and value vectors enable models to weigh relevant tokens. It contrasts this with RNN limitations and demonstrates gains on translation and summarisation.\"},\n","    {\"input\": \"CNN architecture for image classification.\",\n","     \"output\": \"This tutorial covers convolutional, pooling, and fully connected layers, explaining hierarchical feature extraction and typical training steps for vision classification tasks.\"},\n","    {\"input\": \"Reinforcement learning agents learn by reward feedback.\",\n","     \"output\": \"The session formalises RL with policies, rewards, and value estimation. It compares Q-learning and policy gradients, discusses exploration–exploitation, and highlights robotics and gaming use cases.\"},\n","    {\"input\": \"Prompt engineering improves LLM outputs.\",\n","     \"output\": \"Zero-shot, few-shot, and chain-of-thought prompts are compared. The talk emphasises instruction clarity, role specification, and constraint setting to improve reliability and reasoning.\"},\n","    {\"input\": \"MLOps pipelines for reliable deployment.\",\n","     \"output\": \"The talk explains CI/CD for models, experiment tracking, model registries, and monitoring, with tools such as MLflow and Kubeflow for production-grade ML.\"}\n","]\n","\n","FEWSHOT_TOPICS = [\n","    {\"input\": \"Explaining self-attention and BERT internals.\", \"output\": [\"Natural Language Processing\"]},\n","    {\"input\": \"Building CNNs with pooling for object recognition.\", \"output\": [\"Deep Learning\"]},\n","    {\"input\": \"Learning with rewards via Q-learning.\", \"output\": [\"Reinforcement Learning\"]},\n","    {\"input\": \"Designing prompts to improve LLM reasoning.\", \"output\": [\"Prompt Engineering\"]},\n","    {\"input\": \"Automating ML deployment with pipelines and monitoring.\", \"output\": [\"Mlops\"]},\n","    {\"input\": \"Creating data visualisations and feature analysis.\", \"output\": [\"Data Science\"]},\n","    {\"input\": \"Explaining model fine-tuning for generative image models.\", \"output\": [\"Generative AI\"]},\n","    {\"input\": \"Discussing NLP and ML synergy for LLMs.\", \"output\": [\"Natural Language Processing\", \"Machine Learning\"]},\n","]\n","\n","FEWSHOT_QA = [\n","    {\"q\": \"What does attention allow models to do?\",\n","     \"a\": \"It lets models focus on the most relevant tokens in a sequence.\"},\n","    {\"q\": \"Why are convolutions useful in vision?\",\n","     \"a\": \"They extract local spatial features for image classification.\"},\n","    {\"q\": \"How do agents learn in reinforcement learning?\",\n","     \"a\": \"They learn by maximising cumulative rewards through trial and error.\"},\n","    {\"q\": \"When is few-shot prompting effective?\",\n","     \"a\": \"When limited task-specific data exists but examples guide behaviour.\"},\n","    {\"q\": \"Who typically maintains ML pipelines in production?\",\n","     \"a\": \"Machine learning engineers and DevOps teams.\"}\n","]\n","\n","FEWSHOT_CONCEPTS = [\n","    [\"Self-Attention Mechanism\", \"Query-Key-Value\", \"Positional Encoding\"],\n","    [\"Convolutional Layer\", \"Pooling Operation\", \"Feature Map\"],\n","    [\"Reward Function\", \"Policy Gradient\", \"Q-Learning\"],\n","    [\"Few-Shot Prompting\", \"Chain-of-Thought Reasoning\", \"Instruction Tuning\"],\n","    [\"CI/CD Pipeline\", \"Model Registry\", \"Experiment Tracking\"]\n","]\n","\n","# ================================================================\n","# 2. PATHS & API\n","# ================================================================\n","\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-4-scout-17b-16e-instruct/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-4-scout-17b-16e-instruct_fewshot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path):\n","    with open(path) as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","# ================================================================\n","# 3. GLOBAL CONFIG\n","# ================================================================\n","\n","MODEL_NAME = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n","GLOBAL_MIN_GAP = 15\n","LAST_TS = 0.0\n","MAX_CHARS = 2600\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","# ================================================================\n","# 4. LOGGING\n","# ================================================================\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","\n","# ================================================================\n","# 5. CLEANING & CHUNKING\n","# ================================================================\n","\n","def deep_clean(t):\n","    t = str(t)\n","    t = re.sub(r\"https?://\\S+\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","def chunk_text(text, max_chars=MAX_CHARS):\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean]\n","    sents = re.split(r\"(?<=[.!?])\\s+\", clean)\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) < max_chars:\n","            cur += \" \" + s\n","        else:\n","            chunks.append(cur.strip())\n","            cur = s\n","    if cur.strip(): chunks.append(cur.strip())\n","    return chunks\n","\n","# ================================================================\n","# 6. JSON EXTRACTION\n","# ================================================================\n","\n","def extract_json(txt):\n","    try:\n","        s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","        if s == -1 or e == -1:\n","            return {}\n","        return json.loads(txt[s:e+1])\n","    except:\n","        return {}\n","\n","# ================================================================\n","# 7. GROQ CALL (RELIABLE)\n","# ================================================================\n","\n","def groq_call(prompt, temperature=0.2, retries=3):\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        time.sleep(GLOBAL_MIN_GAP - (now - LAST_TS))\n","\n","    for attempt in range(retries):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content\n","        except Exception as e:\n","            print(f\"Retry {attempt+1}/{retries}: {e}\")\n","            time.sleep(4)\n","\n","    return \"\"\n","\n","# ================================================================\n","# 8. FEW-SHOT TASKS\n","# ================================================================\n","\n","# ------ SUMMARY ------\n","def generate_summary(transcript):\n","    chunks = chunk_text(transcript)\n","    partial = []\n","\n","    fewshot = \"\\n\\n\".join([f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_SUMMARIES])\n","\n","    for c in chunks:\n","        prompt = f\"\"\"\n","Learn from examples:\n","{fewshot}\n","\n","Now summarise the transcript chunk.\n","Return ONLY JSON:\n","{{\"generated_summary\":\"...\"}}\n","\n","CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\n","\"\"\"\n","        out = groq_call(prompt, 0.15)\n","        j = extract_json(out)\n","        partial.append(j.get(\"generated_summary\", \"\"))\n","\n","    combined = \" \".join(partial)\n","\n","    final_prompt = f\"\"\"\n","Combine the drafts into a 120–160 word summary.\n","Return ONLY JSON: {{\"generated_summary\":\"...\"}}\n","\n","DRAFTS:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\n","\"\"\"\n","    out2 = groq_call(final_prompt, 0.15)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\")\n","\n","# ------ TOPICS ------\n","def classify_topic(transcript, summary):\n","    text = summary + \" \" + transcript[:2000]\n","\n","    examples = \"\\n\".join(\n","        [f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_TOPICS]\n","    )\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Pick up to 3 topics from:\n","{', '.join(VALID_TOPICS)}\n","\n","Return JSON: {{\"predicted_topics\":[\"...\"]}}\n","\n","TEXT:\n","\\\"\\\"\\\"{text}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","# ------ Q&A ------\n","def generate_qa(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([f\"Q:{x['q']}\\nA:{x['a']}\" for x in FEWSHOT_QA])\n","\n","    prompt = f\"\"\"\n","Learn QA from examples:\n","{examples}\n","\n","Return JSON: {{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    for qa in qas:\n","        lines.append(f\"Q: {qa.get('q','')}\")\n","        lines.append(f\"A: {qa.get('a','')}\")\n","    return \"\\n\".join(lines)\n","\n","# ------ CONCEPTS ------\n","def generate_concepts(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([\", \".join(lst) for lst in FEWSHOT_CONCEPTS])\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Extract 10–12 technical concepts.\n","Return JSON: {{\"key_concepts\":[\"...\"]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.15)\n","    j = extract_json(out)\n","    return \", \".join(j.get(\"key_concepts\", []))\n","\n","# ================================================================\n","# 9. MAIN PIPELINE\n","# ================================================================\n","\n","def run_pipeline():\n","    df = pd.read_excel(INPUT_FILE)\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        processed = set(old[\"row_index\"])\n","        results = old.to_dict(orient=\"records\")\n","        print(f\"Resuming: {len(processed)} rows already completed.\")\n","    else:\n","        processed = set()\n","        results = []\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            continue\n","\n","        title = str(row[\"title\"])\n","        transcript = str(row[\"transcript\"])\n","\n","        print(\"\\nProcessing:\", title)\n","\n","        summary = generate_summary(transcript)\n","        topics = classify_topic(transcript, summary)\n","        qa = generate_qa(transcript)\n","        concepts = generate_concepts(transcript)\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","\n","    return pd.DataFrame(results)\n","\n","# ================================================================\n","# 10. RUN\n","# ================================================================\n","\n","df_out = run_pipeline()\n","print(\"Few-Shot pipeline completed successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qR4kaIDx427L","executionInfo":{"status":"ok","timestamp":1764041795422,"user_tz":-330,"elapsed":3132851,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"85183a5e-07da-481c-b4ed-1f6347c2c59e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Processing: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning with human feedback integrates human input into training, guiding and accelerating learning. It uses algorithms like Q-learning, DQ learning, or proximal policy optimization. In ChatGPT, human feedback is provided via a rewards model that assesses answer quality. This feedback is then used with proximal policy optimization to fine-tune the model, significantly enhancing its response generation capabilities. By leveraging human feedback, the model improves its performance and produces more accurate and relevant responses.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of a validation set in machine learning?\n","A: It helps to evaluate the performance of a model during the training process and prevent overfitting.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","This tutorial covers working with CVXopt and kernels in Support Vector Machines (SVMs) for educational purposes. It explains the basics of SVMs, including hard and soft margin classification, and demonstrates how different kernels (linear, polynomial) affect data separation. The speaker provides resources for learning about solving quadratic programming problems and walks through code examples using numpy and linalg. The tutorial includes visualizations to illustrate the impact of kernels on non-linearly separable data. The speaker reviews SVM code, focusing on kernel usage and prediction, and plans to cover SVM parameters, multi-class classification, and current practices in a future tutorial.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What is CVX opt used for in this machine learning tutorial?\n","A: It is used to directly see the impact of a kernel on a support vector machine and for educational purposes.\n","\n","KEY CONCEPTS:\n","\n","Support Vector Machine (SVM), Kernel, CVX Opt, Quadratic Programming, Convex Optimization, Nonlinear Visualization, Soft Margin, LIBSVM, PyITLearn, Machine Learning, Tutorial\n","\n","============================================\n","\n","Processing: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","The lecture covers the basics of prompt engineering for large language models, including definition and types of prompts, key features such as length, language, and constraints. It also explains how to deconstruct prompts to understand their components and requirements. Examples illustrate how different prompts impact output, highlighting the importance of clear instructions and constraints for generating accurate and efficient text.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #3 - Agents & Tools - Intro\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","The lecture introduces AI agents, autonomous problem solvers that can make decisions. It covers the REACT agent pattern, which combines reasoning and acting to mimic human thinking. This pattern involves a cycle of thinking, taking action, observing the result, and repeating until a solution is found. Agents can be equipped with special abilities using tools like API calls or Python functions. The REACT pattern is illustrated with a diagram and will be further explored through coding examples using Lang chain, providing a practical understanding of AI agents and their applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of a validation set in machine learning?\n","A: It helps to evaluate the performance of a model during the training process and prevent overfitting.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","The speaker introduces tracing a reflection agent system to understand its workings in delivering refined viral tweets. Using the website smith.chain as an example, the tutorial guides through setting up a LangChain project with LSmith for tracing and monitoring. A reflection agent is showcased, generating and refining a tweet through multiple iterations. This process illustrates the power of reflection agents in handling complex tasks, providing insight into their iterative refinement process. By tracing the agent's steps, the tutorial aims to demystify how reflection agents contribute to creating effective viral content.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'LangChain', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Convolutional Layer, Reward Function, Few-Shot Prompting, CI/CD Pipeline, Query-Key-Value, Positional Encoding, Pooling Operation, Policy Gradient, Chain-of-Thought Reasoning, Q-Learning, Model Registry\n","\n","============================================\n","\n","Processing: LangChain Crash Course #7 - Chat Models - Setup\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","The tutorial covers using Lang chain chat models with Open AI's GPT 4.0 model to interact with the OpenAI API. The speaker initializes the model, noting GPT 4.0 is the latest but more expensive option, and suggests GPT 3 as a cheaper alternative. The tutorial demonstrates making API calls, storing responses, and handling errors due to missing API keys. It also explains configuring environment variables using an EnV file and the python-dotenv package. The instructor shows how to retrieve and extract API responses, handle potential errors, and troubleshoot issues. The discussion explores using prompts with LLMs, starting with simple queries and planning to expand to sending conversation histories to improve response accuracy by providing context.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'LangChain', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of installing the Lang chain chat model related to Open AI?\n","A: To work with Lang chains chat models using Open AI APIs.\n","Q: What is the command to install the L chain Open AI package?\n","A: The command is shown in the documentation, it starts with 'L chain Das open aai'.\n","Q: Why did the installation fail initially?\n","A: Because of a percentage sign in the command.\n","Q: What is the module and class imported for the chat model?\n","A: The module is 'langchain.openai' and the class is 'ChatOpenAI'.\n","Q: What is the model used in the example and why?\n","A: The model used is 'gbt 40' because it is one of the latest models released by Open AI, although it can be expensive.\n","\n","KEY CONCEPTS:\n","\n","Lang Chains, Chat Models, Open AI APIs, Chat Open AI, L Chain, Open AAI, Model Installation, Package Management, Importing Modules, Class Initialization, Model Selection, GPT 4.0, GPT 3.0\n","\n","============================================\n","\n","Processing: Python Training Course - Python Sort List\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","The video explains how Python's sort method works for lists with strings and numbers. When sorting lists with both uppercase and lowercase strings, it sorts them separately in alphabetical order. For lists containing both strings and numbers, the sort method prioritizes numbers first, followed by strings, and sorts them accordingly.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: How does Python sort lists containing strings with different cases?\n","A: Python sorts lists containing strings with different cases by putting uppercase letters first, then lowercase letters, and sorting them alphabetically within each group.\n","Q: What happens when you sort a list containing both strings and numbers in Python?\n","A: When you sort a list containing both strings and numbers in Python, it puts the numbers first and then the strings.\n","Q: Why might you need to ensure all strings in a list are the same case before sorting?\n","A: You might need to ensure all strings in a list are the same case before sorting if you want to sort them in a particular way and avoid Python's default behavior of sorting uppercase letters before lowercase letters.\n","\n","KEY CONCEPTS:\n","\n","List Sorting, String Sorting, Case Sensitivity, Alphabetical Order, Reverse Alphabetical Order, List Manipulation, Type Mixing, Type Handling, Sort Method, List Indexing, Insertion, Programming Examples\n","\n","============================================\n","\n","Processing: \n","Humans vs. AI: Who should make the decision?\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","The talk explores the intersection of human and AI decision-making, using a fraud detection example to illustrate their respective strengths. AI excels at high-confidence predictions, while humans outperform AI at lower confidence levels, particularly in complex or rare cases. Augmented intelligence, combining human decision-making with AI assistance, can outperform both human and AI-only approaches. However, presentation and cognitive bias play a crucial role in human-AI collaboration. The talk highlights the importance of balancing human and AI input to minimize bias and leverage their strengths, ultimately improving decision-making outcomes. Effective collaboration depends on factors like the display of AI recommendations and accuracy percentages, which can influence human trust and reliance on AI outputs.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: Who should make decisions, humans or AI?\n","A: A combination of both, depending on the situation and confidence level of the AI.\n","Q: What is a common challenge in fraud detection systems?\n","A: Dealing with a high volume of false positives, which can overwhelm financial analysts.\n","Q: How can AI help with fraud detection?\n","A: By automating the review of alerts and handling cases with high confidence scores.\n","Q: When do humans outperform AI in decision-making?\n","A: When the AI is unsure or has a low confidence score, especially in complex or statistically rare cases.\n","Q: Why do humans perform better than AI at low confidence levels?\n","A: Because humans can bring in additional information and context to make more informed decisions.\n","Q: What is a key advantage of AI over humans in decision-making?\n","A: AI's consistency and focus, which allow it to perform well when it is certain of itself.\n","\n","KEY CONCEPTS:\n","\n","Fraud Detection System, Machine Learning Model, Confidence Score, Success Rate, Performance Curve, Human Bias, Artificial Intelligence, Decision Making, False Positives, Predictive Analytics, Human-AI Collaboration, Uncertainty Estimation\n","\n","============================================\n","\n","Processing: \n","Build generative apps faster with Vertex AI\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","Vertex AI has launched six new APIs to simplify the development of generative applications. The new APIs include the Document Understanding API, an improved Embedding API, Vector Search with hybrid search capabilities, the Ranking API, the Grounded Generation API, and the Check Grounding API. These APIs are designed to tackle common technical challenges, boost accuracy, and streamline the development process. By providing these tools, developers can focus on creating unique use cases rather than addressing technical complexities. The APIs aim to improve the efficiency and effectiveness of generative application development, enabling developers to build more sophisticated and accurate models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Unitary Transformations\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses Singular Value Decomposition (SVD) of a matrix X, focusing on unitary matrices U and V that preserve angles and lengths of vectors, akin to rotations in vector space. The SVD is represented as X = U Σ V^T, where U and V are unitary and Σ is a diagonal matrix of singular values. Geometrically, SVD maps a sphere to an ellipsoid, with singular values and vectors indicating the ellipsoid's elongation and orientation. This decomposition provides insight into the transformation of vectors under matrix multiplication, highlighting the roles of rotation, scaling, and reflection.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","The video tutorial introduces building generative AI applications using Google Gemini Pro 1.5, a multi-modal model that can work with both text and images. The model boasts a large context window of up to 1 million multimodal tokens and has demonstrated capabilities in processing and understanding large amounts of data. The speaker showcases the model's features, including its ability to handle large inputs such as PDFs, text, and images, and highlights its potential applications. They walk through the process of creating an API key, installing required packages, and configuring the Gen AI API. The speaker experiments with asking the model questions and demonstrates its capabilities in generating text and handling various types of questions. The model can generate descriptive text about image contents and has improved features over previous versions, making it a promising development in the field of large language models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is Google Gemini Pro 1.5?\n","A: It's a multi-model that can work with both text and images.\n","Q: What kind of applications can be built using Google Gemini Pro 1.5?\n","A: Generative AI powered applications.\n","Q: What is the experimental feature in Gemini 1.5 mentioned in the text?\n","A: Long context understanding.\n","Q: What will the speaker demonstrate in the video?\n","A: A demo video provided by Google and hands-on applications using code with both images and text.\n","Q: Who is the speaker in the video?\n","A: Krishn.\n","\n","KEY CONCEPTS:\n","\n","Generative AI, Google Gemini Pro, Multimodel, API Key, Long Context Understanding, Experimental Feature\n","\n","============================================\n","\n","Processing: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","Evaluating and testing prompt engineering models is crucial to ensure their performance. Metrics such as perplexity, accuracy, and human evaluation are used to measure model performance. Techniques like analyzing generated responses, fine-tuning, and testing on different datasets help debug and improve models. Ongoing evaluation and testing are essential to ensure models continue to perform well. This process involves assessing model performance, identifying areas for improvement, and refining the model to achieve optimal results. Effective evaluation and testing lead to more reliable and efficient prompt engineering models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Generative AI vs AI agents vs Agentic AI\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses the distinctions between Generative AI, AI Agents, and Agentic AI. Generative AI, such as LLMs, creates new content based on learned patterns. AI Agents are autonomous programs that use tools and knowledge to complete tasks. Agentic AI systems comprise one or more AI agents that work autonomously to achieve complex goals through multi-step reasoning, planning, and coordination. The complexity of tasks increases from Generative AI to AI Agents to Agentic AI, with Agentic AI being capable of performing the most complex tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: \n","Covariance in Statistics\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","The lecture covers covariance, a key concept in data analysis and preprocessing, which measures the relationship between two random variables. It explains the covariance equation and its implications, highlighting how it indicates a positive or negative relationship between variables, such as house size and price. However, covariance has a limitation - it does not quantify the strength of the relationship. This is addressed in subsequent topics, including the Pearson correlation coefficient. Understanding covariance is essential for data analysis, as it provides insights into how variables interact with each other.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of a validation set in machine learning?\n","A: It helps to evaluate the performance of a model during the training process and prevent overfitting.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: 3. Objective || End to End AI Tutorial\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","The objective of reinforcement learning problems is to learn an optimal policy that maximizes a numerical reward signal. Reinforcement learning involves two types of tasks: episodic and continuous. Episodic tasks, such as Tic-Tac-Toe, have a clear start and end, with the goal of maximizing cumulative rewards within an episode. Continuous tasks, like stock market trading, involve ongoing decision-making to optimize long-term rewards. A crucial aspect of reinforcement learning is defining a reward function that accurately reflects the desired goals and preferences. This reward function guides the learning process, enabling the agent to make informed decisions and adapt to the environment. By maximizing the reward signal, the agent learns an optimal policy to achieve its objectives.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Python Training - Python Dictionary Basics\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","This tutorial covers Python dictionaries, a data structure composed of key-value pairs. It explains how to create dictionaries, access and modify values, and delete entries. The tutorial also explores dictionary functions like `.items()`, `.keys()`, and `.values()`. Additionally, it shows how to convert lists to dictionaries using the `dict()` and `zip()` functions, providing a comprehensive overview of working with dictionaries in Python.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: \n","Fight Insider Threats with AI-infused SIEM\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","The talk explores using AI and machine learning for User Behavior Analytics (UBA) to rapidly detect and respond to insider threats. Citing IBM's 2023 Cost of a Data Breach report, it highlights AI's potential to enhance security posture and reduce breach containment time. UBA analyzes user behavior to identify anomalies and potential threats, and when integrated with a Security Information and Event Management (SIEM) solution like IBM's QRadar, enables security professionals to more effectively detect and respond to insider threats. A demo showcases UBA's capabilities, including prioritizing employees by risk, providing alert and offense views, and visualizing risks over time. QRadar's automation and AI capabilities accelerate investigations, provide actionable insights, and allow security analysts to focus on proactive defense efforts.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: How can AI help improve an organization's security posture?\n","A: It can help stay ahead of emerging threats and reduce the time to identify and contain data breaches.\n","Q: What was the average cost of an Insider threat for an organization?\n","A: $4 million.\n","Q: How can User Behavior Analytics (UBA) with AI and machine learning help security teams?\n","A: It can help detect and respond to Insider threats quickly and precisely.\n","Q: What was the average reduction in days to identify and contain a data breach for organizations that extensively used AI and automation?\n","A: 108 days.\n","Q: What type of threats are a major concern for organizations of all sizes?\n","A: Insider threats.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","Krishak introduces himself and welcomes viewers to his YouTube channel, discussing Meta's open-source LLM model, Llama 3. The model is available in 8 billion and 70 billion parameter variants, boasting state-of-the-art performance in language understanding and complex tasks. Llama 3 was trained on over 50 trillion tokens of data and supports an 8K context length. Its performance metrics are competitive with paid LLM models, excelling in tasks like reasoning and code generation. The model is available for download on Meta's website, Hugging Face, and Kaggle, with instructions provided for accessing and using it. The speaker provides additional instructions on accessing and running Llama 3 models, including downloading weights and installation steps, with resources available on GitHub for inference and examples.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Machine Learning', 'Generative AI']\n","\n","GENERATED Q&A:\n","\n","Q: What is the speaker doing in the provided text?\n","A: The speaker is introducing themselves on their YouTube channel.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Getting Started With sklearn\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","The speaker outlines the steps to write Python code for a decision boundary using scikit-learn's Naive Bayes algorithm. They utilize Google to find relevant documentation and examples. The process involves leveraging the scikit-learn library to implement the Naive Bayes algorithm for creating a decision boundary. By searching for examples and documentation, the speaker is able to write the necessary Python code.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What Python library is used for machine learning in this example?\n","A: The library used is scikit-learn, often abbreviated as sk-learn.\n","Q: How does the speaker find information about the scikit-learn library?\n","A: The speaker uses Google to search for the library's documentation.\n","Q: What type of Naive Bayes algorithm is used in this example?\n","A: The algorithm used is Gaussian Naive Bayes.\n","Q: Why does the speaker initially search for information on Naive Bayes?\n","A: To figure out how to use some of the functions in the scikit-learn library.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: \n","Log Normal Distribution in Statistics\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","The lecture covers Gaussian and log-normal distributions, their characteristics, differences, and applications. Understanding these distributions is vital for data preprocessing, particularly in scaling and normalizing data to enhance model accuracy. The Gaussian distribution is exemplified by human height, while the log-normal distribution is illustrated through income and product reviews. The talk also explains how to convert log-normal distributions to standard normal distributions for consistent scaling, emphasizing the importance of proper data transformation in improving model performance.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","This video series presents an end-to-end deep learning project to detect potato plant diseases, specifically early and late blight, using a mobile app and convolutional neural networks. The goal is to prevent economic losses for farmers. The project involves building an image classification model using CNN and data augmentation, exporting and serving the model with TF serving and FastAPI, and creating a ReactJS website and mobile app. The tech stack includes TensorFlow, TF serving, Fast API, and GCP for deployment. Prerequisites include basic Python and deep learning knowledge. The speaker outlines the project workflow, encourages engagement, and suggests that completing the series can enhance resumes for machine learning or data science job applications. The project allows for customization and practice, covering model optimization using quantization and deployment using TensorFlow Lite and Google Cloud.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Machine Learning', 'Mlops']\n","\n","GENERATED Q&A:\n","\n","Q: What is the problem that farmers face in potato cultivation?\n","A: Farmers face economic losses due to diseases such as early blight and late blight in potato plants.\n","Q: What is the goal of the mobile application to be built by AtliQ Agriculture?\n","A: The mobile application aims to help farmers detect diseases in potato plants by taking a picture of the plant and providing a diagnosis.\n","Q: What type of machine learning model will be used in the mobile application?\n","A: A deep learning model using a convolutional neural network (CNN) will be used for disease detection.\n","Q: What is the role of Google Cloud functions in the end-to-end application?\n","A: Google Cloud functions will be called by the mobile app to perform tasks, likely related to disease detection and diagnosis.\n","Q: What is the purpose of using Fast API in the project?\n","A: Fast API will be used to build the backend server for the application.\n","\n","KEY CONCEPTS:\n","\n","Deep Learning, Convolutional Neural Network (CNN), Machine Learning Ops (MLOps), TF Serving, Fast API, Google Cloud Platform (GCP), Cloud Functions, React Native, Data Collection, Model Building, Model Deployment, End-to-End Application\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The lecture explores levels of autonomy in LLM applications, spanning from zero autonomy with hardcoded rules to maximum autonomy with autonomous agents. It covers various concepts, including single LLM calls, chains, routers, state machines, and autonomous agents. These technologies enable complex task handling, decision-making, and learning from mistakes. LangGraph plays a significant role in state machines, and its potential applications extend to content creation and other areas, showcasing the versatility and potential of autonomous LLM applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Machine Learning', 'Langraph']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of a validation set in machine learning?\n","A: It helps to evaluate the performance of a model during the training process and prevent overfitting.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","This lecture covers advanced topics in prompt engineering, including handling various types of prompts, fine-tuning pre-trained language models, and best practices for data preprocessing. It also discusses deploying models in production using frameworks like TensorFlow Serving and Flask. Additionally, the lecture considers the ethical implications of prompt engineering, such as bias, fairness, and privacy, providing a comprehensive overview of the field.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: SVD: Eigen Action Heros [Matlab]\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","The lecture covers Singular Value Decomposition (SVD) and eigenfaces, using them to cluster images of Arnold Schwarzenegger and Sylvester Stallone. It demonstrates loading and aligning images, computing average faces, and performing principal component analysis. The technique is applied to classify test images, correctly identifying Terminator and Harry Potter-Stallone. Further tests cluster images of Taylor Swift with Stallone and Schwarzenegger, showing clear separation. The lecture concludes by noting the limitations of simple image classification methods and mentioning Facebook's use of 3D geometry for more accurate face classification.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Deep Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of a validation set in machine learning?\n","A: It helps to evaluate the performance of a model during the training process and prevent overfitting.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangChain Crash Course #3 - What is LangChain?\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","LangChain is a framework that connects large language models (LLMs) to the real world, enabling applications to tap into their reasoning capabilities and interact with external APIs, databases, and services. This facilitates building and switching between different LLMs. With LangChain, AI systems can access private databases, send emails, browse websites, and perform various tasks, allowing them to act on customer queries and take action in the real world.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'LangChain', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main limitation of large language models?\n","A: They cannot interact with the real world.\n","Q: What is Lang chain's role in building applications with large language models?\n","A: It acts as a bridge between large language models and the real world, enabling communication with APIs, databases, and more.\n","Q: What problem does Lang chain solve?\n","A: It allows applications to have the reasoning ability of large language models while also interacting with the real world.\n","Q: What is a benefit of using Lang chain?\n","A: It enables easy switching of large language models without modifying code.\n","Q: What kind of APIs can Lang chain access?\n","A: APIs for flight and restaurant booking, among others.\n","\n","KEY CONCEPTS:\n","\n","Large Language Models (LLMs), Chat Application, API Integration, LangChain Framework, Reasoning Ability, Real-World Interaction, Model Interchangeability, Application Development, Bridging LLMs and Real World, Modular Design\n","\n","============================================\n","\n","Processing: How To Use Residuals For Time Series Forecasting\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses residuals in time series analysis to improve forecasting methods. Residuals are analyzed to evaluate model performance, with two key aspects being the absence of autocorrelation and a mean of zero. Techniques such as autocorrelation and partial autocorrelation plots, and the Ljung-Box test are used to detect trends and inconsistencies in a model. An example using a Holt-Winters model on the Air Passenger dataset illustrates these concepts. The goal of residual analysis is to identify areas where the model struggles, such as autocorrelation and bias, and improve it in the next iteration. This is achieved by diagnosing issues using plots like the Ljung-Box test and histograms, and interpreting the results to refine the model.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Statistics', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of a validation set in machine learning?\n","A: It helps evaluate and fine-tune a model's performance during training.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","This video tutorial demonstrates building an AI agent that interacts with a database using SQL knowledge. The agent is created using LangGraph, Next.js, and models on watsonx.ai, with an in-memory SQLite database. The tutorial covers setting up a Next.js project, designing a user interface, and implementing a ReAct agent with LangChain to enable a Large Language Model (LLM) to tell jokes. It also discusses setting up a database using SQLite 3, creating a Text2SQL agent, and implementing guardrails to prevent unlimited database access. The agent can generate SQL queries from natural language prompts and answer complex questions by interacting with the database.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'LangChain', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What type of agent is being built in the video?\n","A: A ReAct agent that can use SQL knowledge to connect to databases.\n","Q: What tools are being used to build the agent?\n","A: LangGraph, Next.js, and models running on watsonx.ai.\n","Q: What database is being used in the example?\n","A: An in-memory database using SQLite.\n","Q: What is the purpose of the input box in the frontend application?\n","A: To type a message to the large language model.\n","Q: What is the role of the Home component in the Next.js application?\n","A: It renders the page with a header, input box, and placeholder messages.\n","\n","KEY CONCEPTS:\n","\n","LangGraph, ReAct agent, Next.js, watsonx.ai, SQLite, VS Code, TypeScript, Tailwind, Create-next-app, Client-side component, Server-side rendering, Large language model, SQL\n","\n","============================================\n","\n","Processing: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","The lecture introduces prompt engineering, a field within natural language processing that focuses on building models generating high-quality text outputs from prompts. It leverages pre-trained large language models to produce accurate, coherent, and contextually appropriate responses.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is prompt engineering?\n","A: Prompt engineering is a specialized field within natural language processing that focuses on building models that can generate high-quality text outputs in response to prompts or input.\n","Q: Why is prompt engineering important?\n","A: It allows us to generate text outputs that are more accurate, coherent, and contextually appropriate than traditional rule-based or keyword-based approaches, which significantly impacts user experience and engagement.\n","Q: What are the benefits of prompt engineering?\n","A: It enables the generation of accurate, coherent, and contextually appropriate text outputs, making it especially important for applications such as chatbots, language translation, and content generation.\n","Q: What are the limitations of prompt engineering?\n","A: Prompt engineering models may struggle with complex and ambiguous prompts, or they may generate outputs that are biased and inaccurate due to underlying data or model architecture.\n","Q: What will you learn in this course on prompt engineering?\n","A: You will learn the fundamentals of prompt engineering, including prompt analysis, benefits and limitations, and advanced techniques for fine-tuning pre-trained large language models.\n","\n","KEY CONCEPTS:\n","\n","Natural Language Processing (NLP), Prompt Engineering, Large Language Models, Pre-trained Models, Fine-tuning, Text Generation, Chatbots, Language Translation, Content Generation, Conversational AI, Language Model Architecture, Model Training\n","\n","============================================\n","\n","Processing: Q-learning - Explained!\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","This lecture introduces Q-learning, a value-based reinforcement learning method that aims to maximize a numerical reward signal by learning the state-action value function (Q-value). Q-learning is an off-policy algorithm that uses a Q-table to store Q-values, which quantify the goodness of taking an action in a given state. The algorithm iterates through episodes, updating Q-values based on temporal differences, to learn an optimal policy. A grid world example illustrates how Q-learning works, using the Bellman equation to calculate observed Q-values and a discount factor to value current versus future rewards. Q-learning decouples data collection from the target policy, allowing for effective learning and decision-making. The target policy guides actions, while a behavior policy is used for exploration, enabling the algorithm to learn and make decisions effectively.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of regularization in machine learning?\n","A: It helps prevent overfitting by adding a penalty term to the loss function.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Training Your Logistic Classifier\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","The lecture introduces logistic classifiers, a type of linear classifier that applies a linear function to input data, such as image pixels, to generate predictions. It uses matrix multiplication with weights (W) and bias (b) to produce scores, which are then converted into probabilities using a softmax function, enabling classification into a single label.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Logistic Classifier, Linear Classifier, Matrix Multiply, Weights, Bias, Machine Learning, Softmax Function, Probabilities, Logits, Classification, Linear Function\n","\n","============================================\n","Few-Shot pipeline completed successfully!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYRWF60wJTQc","executionInfo":{"status":"ok","timestamp":1764042823378,"user_tz":-330,"elapsed":6662,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"f3dafde3-9e9f-4661-e494-1c3a67fa46d0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_fewshot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["21575f6ffe974bc19f295857c7aa83f3","c596ba5b49644572a2eb3eb14123f205","97866925ac564ee0b5879f7031632d61","6f812fe91cb140afad78f59db9b3f858","83d6059bb5e6463f9c5007039f5de096","f5add3af666548118f7f3043c02346ce","41c5a25b97014475bbcd3f94912b49fc","3c6ee339ab3b428a907241ad8693115a","245d7512281945dfa57eda7ed76f39fa","8bc2e471439745078c3259a0223a8f82","4c0807ea643f46d28e1b27e0538ba3b2","0b339cdb3ec94bd6a3b911fb4edbc423","ee25c7d6f07d457cb5a9561a874bdc95","ef086e5e000b45aab6642791d11f4876","a9e51286422f41789112c6e8083d14f5","233b59b616e248fbaef3437db20e1a52","1b15265996bc4a0f9d8c2fd2ff92e1d3","47880022d6a5448cb4efbfd38a2381f9","15b3e2eb40824fa399538a74185ff423","2f650e57c6ae43dc9037fc5b1f39fad6","33bc08b403ee493caeada675f72b6bb6","390147775e484300b60e1db4056b467c","2020e2a416064215a45d1370f94f5cb9","e54a58c5f26547d9a06aca605efe3397","86d8eaf8dc9d479cae9d8891dba9badc","52a8607f302c410593f818d8f619bc04","4a939179204046a88d5de6e0391d8409","4cf710409d554821a94869187a4718af","d47123827d6c46b7a52a2bd5eacf228f","1eb33c74b06646038ec301bc5e8e91f3","84e9a63cb62f4497a5cc4d523f96f955","9ee0463ee34641799c31283af913e591","3fb32825e1b24d5c9e375be87c4c008c","c950b9dcf9cf4238a74a8f14d115556a","02ca8d6470554e858dafb882a3678864","32224240970a439c85c26ba05f68cc65","c4233e5810914865bafa12d714eecbf3","3f4e3cf2eddb4fc8a74d5ebbc3b48f07","d950e67e9f0c4b9cb96c8272d8ab5cd2","94a4f76cd97247fcb2dbcb4e43402de2","1a5613c63a73430488d86fb6c03a65c4","5d038eb631c54c88a503e68a035729ee","d17cecfd84ba4849bced000d244874d8","70081880ecc04d2093aea531b6280a7a","cf34a57586b74de5b5833da349c9baf5","c18f84955c8d454f9180f35c1eb9e47a","d09200041b1d455682e63bf251d1f57f","23b9fd1a370f444bb17d247bab4e8ed1","eae3f74c42694d738b70a11779ea935d","cbb98475e688422aa935188ee94bf7f8","176d00e993c140d3a18f828efe7212e2","bb75418bb35443e19961db3178fc64e9","d1244adbd4524e8485c2bd22b25df976","76c985fc93e849b7bf1f26fc775d6df0","de19cca44c0c4adc8bda4a4e79ac12b4","c878d71e88284fd0bbec5a9aa37f566a","f9408f986cd64f7da2f3ab33aad4cb00","a2ec7e43812c48ddaa419de27f526b6a","38f4d6b7788640d290f432d8e3c68159","846b3920b8424e6192e02a1d40c0e208","bbec21b3c5a04fb0b9020373c3681c68","4f3e45fd59d14277b281c1f19e61389f","457cfbcc05b4476bbd6be2a047d99414","4763212ed13240689bd890d0a4325a53","b55d99625e2544529c52002abf5f2615","9ca5b2cc81b64da0994561361f9a4996"]},"id":"wN_s-OIH43CC","executionInfo":{"status":"ok","timestamp":1764042979330,"user_tz":-330,"elapsed":155949,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"8b24cefa-cfd4-4faa-8e5d-1c3824fe4e75"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_fewshot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21575f6ffe974bc19f295857c7aa83f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b339cdb3ec94bd6a3b911fb4edbc423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2020e2a416064215a45d1370f94f5cb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c950b9dcf9cf4238a74a8f14d115556a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf34a57586b74de5b5833da349c9baf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c878d71e88284fd0bbec5a9aa37f566a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2810\n","  - BLEU: 0.0563\n","  - BERTScore F1: 0.8851\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.8667\n","  - Jaccard Index: 0.3762\n","  - Micro F1: 0.4978\n","  - Macro F1: 0.5052\n","  - Weighted F1: 0.4935\n","\n","Q&A Generation:\n","  - BLEU: 0.0540\n","  - Diversity: 0.8793\n","  - Answerability: 0.2906\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.2500\n","  - Recall@10: 0.1000\n","  - F1@10: 0.1429\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\n"]}]}]}