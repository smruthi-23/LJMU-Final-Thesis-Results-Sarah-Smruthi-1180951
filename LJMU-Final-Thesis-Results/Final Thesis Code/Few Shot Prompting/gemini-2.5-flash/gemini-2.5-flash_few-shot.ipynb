{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32163,"status":"ok","timestamp":1763997907276,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"},"user_tz":-330},"id":"ChHg1hyNPAZC","outputId":"8536b8e6-a2ec-47da-c7b6-d4fb90e45dea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=861fb75dff27ccecbb3500a42919a2ef0b72eb7d00352e97740f4ae87550bfe1\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1763997907306,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"},"user_tz":-330},"id":"Xg-U9mCEs_Ti","outputId":"a4c44ef8-ab36-48ce-f597-d47c1443cd76"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}],"source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"id":"nEVwPEErs_bK","outputId":"3742246c-ed8b-4b5c-eb5d-5eab6e4bf58a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Processing row 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","\n","SUMMARY:\n"," This session introduces Reinforcement Learning with Human Feedback (RLHF), a powerful methodology leveraging human input to guide and accelerate the training of reinforcement learning agents. RLHF is crucial for aligning agent behavior with human preferences, a concept effectively demonstrated through a grid world example. The discussion extends to RLHF's critical application within advanced systems like ChatGPT. Here, human evaluators rank various generated responses, and these rankings are used to train a sophisticated reward model. Subsequently, this reward model is integrated with Proximal Policy Optimization (PPO) to fine-tune the underlying Large Language Model (LLM). This intricate process aims to empower the LLM to consistently produce high-quality, contextually relevant, and human-aligned responses, thereby optimizing its performance and user experience.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the primary goal of a loss function?\n","A: It quantifies the error between predicted and actual values, guiding model optimization.\n","Q: How does transfer learning benefit model training?\n","A: It reuses pre-trained models on new, related tasks, reducing training time and data needs.\n","Q: What is the role of a validation set?\n","A: It evaluates model performance during training and helps tune hyperparameters without bias.\n","Q: When would you use a recurrent neural network (RNN)?\n","A: For sequential data tasks like natural language processing or time series prediction.\n","Q: What is the 'curse of dimensionality'?\n","A: It's the phenomenon where data becomes sparse and distances become less meaningful in high-dimensional spaces.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","Processing row 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","\n","SUMMARY:\n"," This session comprehensively details the implementation of Support Vector Machines (SVMs), emphasizing the crucial role of kernels in achieving nonlinear classification. It demonstrates how kernels like linear, polynomial, and Gaussian are injected into the SVM's formal structure, enabling the separation of complex datasets. The session distinguishes between hard and soft margin SVMs, explaining the C parameter's function and the underlying quadratic programming (QP) solver used to identify support vectors and define decision boundaries. CVXopt is utilized for educational visualization of these concepts, though its practical utility is contrasted with tools like scikit-learn. Practical implementation involves Python libraries such as NumPy, with a reference to an MIT tutorial for deeper QP understanding. The talk showcases SVM performance across various datasets, concluding with visualizations of kernel transformations. Future discussions will cover scikit-learn's SVC parameters and multi-class SVM extensions.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the primary focus of this machine learning tutorial?\n","A: It focuses on working with CVXopt and applying kernels to Support Vector Machines.\n","Q: Why is CVXopt used in this specific tutorial?\n","A: It's used to directly visualize the impact of a kernel and how it modifies the Support Vector Machine.\n","Q: Is CVXopt a commonly recommended tool for implementing SVMs?\n","A: No, it's generally not used, with libsvm being the preferred choice for custom SVM implementations.\n","Q: What aspects of SVMs does this tutorial aim to visualize?\n","A: It aims to visualize the effect of kernels, nonlinear SVMs, and the soft margin.\n","Q: What type of mathematical solver does CVXopt employ?\n","A: It employs a quadratic programming solver to minimize an equation subject to given constraints.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, Kernel, Quadratic Programming, Soft Margin, Self-Attention Mechanism, Query-Key-Value, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Few-Shot Prompting, CI/CD Pipeline\n","\n","Processing row 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","\n","SUMMARY:\n"," This session introduces prompts as foundational inputs for Large Language Models (LLMs), explaining their critical role in providing context and constraints for text generation. It explores various prompt types, including questions, statements, and those with multiple inputs or specific limitations. Key features like desired length, specific language, and explicit constraints such as tone, style, or word count are discussed, underscoring the necessity of clearly defining 'what you expect' and 'how you want it done.' Examples illustrate how precise prompting, like specifying output format or length, significantly improves accuracy. The talk also covers the process of deconstructing prompts to identify their core components and constraints, preparing participants for more advanced prompt engineering techniques.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the primary goal of a loss function?\n","A: To quantify the difference between predicted and actual values, guiding model optimisation.\n","Q: How does backpropagation work?\n","A: It calculates gradients of the loss with respect to model parameters, enabling weight updates.\n","Q: What is the role of a validation set?\n","A: To evaluate model performance during training and tune hyperparameters without overfitting to the test set.\n","Q: When would you use a recurrent neural network (RNN)?\n","A: For sequential data tasks like natural language processing or time series prediction.\n","Q: What is the purpose of regularisation techniques like L1 or L2?\n","A: To prevent overfitting by adding a penalty to the loss function based on the magnitude of model weights.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, CI/CD Pipeline, Experiment Tracking\n","\n","Processing row 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","\n","SUMMARY:\n"," This session introduces AI agents as sophisticated, autonomous problem-solvers designed to independently determine their operational steps. These agents are equipped to leverage various 'tools,' such as calculators or search engines, enabling them to efficiently complete complex tasks. A core mechanism detailed is the ReAct pattern, an acronym for Reasoning + Acting. This pattern is crucial as it mimics human thought processes through a structured and iterative 'Think, Action, Action Input, Observe' loop. The ReAct mechanism significantly empowers Large Language Models (LLMs) to engage in sophisticated reasoning, intelligently select and utilize appropriate tools with precise inputs, and accurately interpret the resulting outputs. By following this systematic and iterative process, AI agents can effectively approach and solve a wide array of intricate problems. The discussion further sets the stage for the practical implementation of these powerful AI agent concepts using the LangChain framework, facilitating their real-world application and development.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the primary goal of a Generative Adversarial Network (GAN)?\n","A: To generate new data instances that resemble the training data distribution.\n","Q: How does backpropagation work in neural networks?\n","A: It calculates gradients of the loss function with respect to weights, allowing for parameter updates.\n","Q: What is the main advantage of using transfer learning?\n","A: It leverages pre-trained models on large datasets to improve performance on new, smaller datasets.\n","Q: When would you use a recurrent neural network (RNN)?\n","A: For tasks involving sequential data like natural language processing or time series prediction.\n","Q: What is the role of a loss function in machine learning?\n","A: It quantifies the error between a model's predictions and the actual target values.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Few-Shot Prompting, Instruction Tuning, CI/CD Pipeline, Experiment Tracking\n","\n","Processing row 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","\n","SUMMARY:\n"," The section details the tracing of a reflection agent system, explaining its internal workings and how its components collaborate to refine a viral tweet. LangSmith is highlighted as the crucial tool for observing and debugging these LangChain applications, with instructions on setting up environment variables to capture and stream every operation. The focus is on \"reflection agents,\" illustrated by an iterative tweet generation process. A generation agent first creates initial content, which a separate reflect agent then critically evaluates. This reflect agent provides detailed feedback, guiding the generation agent in subsequent refinements. This generate-critique loop repeats multiple times, allowing for progressive enhancement and leading to a highly polished final output. This methodology effectively demonstrates how reflection agents enable sophisticated deep thinking and iterative improvement, crucial for tackling complex tasks and achieving superior results.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the purpose of tracing the reflection agent system?\n","A: To understand exactly what is happening and how both systems are working together.\n","Q: What is the final product these systems aim to deliver?\n","A: Our final refined viral tweet.\n","Q: What action is taken to begin tracing the system?\n","A: Going to a particular website.\n","Q: Which specific website is mentioned for tracing?\n","A: smith.chain.\n","Q: What is the main topic of this section?\n","A: Tracing the reflection agent system that was built.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","Processing row 5: LangChain Crash Course #7 - Chat Models - Setup\n","\n","SUMMARY:\n"," This session provides a comprehensive introduction to working with LangChain's OpenAI chat models, starting with the installation of the `langchain-openai` package and initializing the `ChatOpenAI` class. It guides users through model selection, contrasting advanced options like `gpt-4o` with more cost-effective alternatives such as `gpt-3`. The tutorial demonstrates making API calls using the `llm.invoke()` method and offers solutions for common issues, including resolving the `API key missing` error by configuring environment variables in a `.env` file and loading them with `python-dotenv`. It also covers troubleshooting billing issues and extracting specific content from responses. While the current focus is on basic single-turn prompting, the upcoming section will explore providing LLMs with full conversation history for context-aware, multi-turn interactions.\n","\n","TOPICS:\n"," ['LangChain', 'Prompt Engineering', 'Generative AI']\n","\n","Q&A:\n"," Q: What are we starting to work with in this section?\n","A: LangChain's chat models.\n","Q: Which specific chat model related to OpenAI is being installed?\n","A: ChatOpenAI.\n","Q: What package needs to be installed for LangChain's OpenAI chat models?\n","A: `langchain-openai`.\n","Q: Which specific model is chosen for the example?\n","A: GPT-4o.\n","Q: Why is GPT-4o chosen for the example?\n","A: It is the latest model released by OpenAI.\n","\n","KEY CONCEPTS:\n"," Large Language Models (LLMs), Chat Models, API (Application Programming Interface), LLM Frameworks, Model Initialization, Model Selection, API Costs, Package Installation, Module Import, Model Versioning\n","\n","Processing row 6: Python Training Course - Python Sort List\n","\n","SUMMARY:\n"," This tutorial delves into the functionality of Python's built-in `sort()` method, a powerful tool for ordering elements within lists. A crucial aspect highlighted is its case-sensitive nature when dealing with strings. Specifically, the method prioritizes uppercase letters over their lowercase counterparts, meaning 'A' will precede 'a' in a sorted list. To achieve a truly consistent alphabetical order, the tutorial recommends applying case normalization techniques before sorting, ensuring that case distinctions do not interfere with the desired sequence. Furthermore, the `sort()` method exhibits a distinct behavior when processing mixed-type lists, consistently placing numerical values before string elements. The tutorial also illustrates the practical application and impact of the `reverse` option, which allows users to easily invert the default sorting order, arranging elements from largest to smallest or Z to A, depending on the data type. This comprehensive overview provides essential insights into mastering list sorting in Python.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: How does Python's `sort` method order strings with mixed uppercase and lowercase starting letters?\n","A: It sorts words starting with uppercase letters alphabetically first, then words starting with lowercase letters alphabetically.\n","Q: What is the order when reversing a list of strings that were sorted with mixed-case letters?\n","A: It places lowercase letters in reverse alphabetical order, followed by uppercase letters in reverse alphabetical order.\n","Q: How does Python's `sort` method handle lists containing both strings and numbers?\n","A: It puts the numbers first, followed by the strings.\n","Q: Where do numbers appear when a mixed list of strings and numbers is sorted in reverse order?\n","A: Numbers appear at the end of the list.\n","Q: What is recommended for sorting strings with mixed cases if a specific order is desired?\n","A: Make sure all strings are either all lowercase or all uppercase before sorting.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, CI/CD Pipeline, Experiment Tracking\n","\n","Processing row 7: \n","Humans vs. AI: Who should make the decision?\n","\n","SUMMARY:\n"," The session explores optimizing human-AI decision-making, using fraud detection as an example. It highlights AI's superiority in high-confidence, consistent scenarios, while humans excel in ambiguous or rare cases by incorporating additional context. This suggests augmented intelligence, combining both, often outperforms either alone. A significant challenge is human cognitive bias, particularly concerning how AI recommendations are presented. Forced display, showing AI suggestions automatically, can lead to automation bias. In contrast, optional display, where AI input is requested, allows for initial human judgment, mitigating this. Interestingly, displaying AI accuracy percentages can reduce human trust and uptake of recommendations. The discussion emphasizes quantifying the most effective decision-maker—human, AI, or augmented—and the critical need to minimize human cognitive bias when integrating AI recommendations to achieve improved outcomes through powerful human-AI collaboration.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is a common challenge for financial analysts reviewing fraud alerts?\n","A: They are often overwhelmed by thousands of alerts daily, with 90 percent of them being false positives.\n","Q: How does an AI's success rate typically correlate with its confidence score?\n","A: AI shows a high success rate at very low and very high confidence scores, but a lower success rate when it is unsure.\n","Q: When might a human perform better than an AI in decision-making?\n","A: A human is likely to do a better job than an AI when the AI is unsure, particularly around a 50 percent confidence level.\n","Q: What allows humans to outperform AI in complex or rare cases where AI is uncertain?\n","A: Humans can bring in additional information and context to make a better prediction.\n","Q: What are the advantages of AI when it is highly confident in a prediction?\n","A: When certain, AI is highly performant, consistent, and doesn't get distracted, often outperforming humans.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Fraud Detection System, False Positives, Performance Curve, Success Rate, Confidence Score, Human-AI Collaboration, Complex Cases, Contextual Information, Decision Making, Workload Alleviation, Human Bias\n","\n","Processing row 8: \n","Build generative apps faster with Vertex AI\n","\n","SUMMARY:\n"," Google has unveiled new Vertex AI APIs specifically designed to significantly accelerate and improve generative AI application development, with a particular focus on achieving accurate and consistent responses through robust grounding. The suite introduces six crucial APIs. The Document Understanding API is engineered for processing complex documents, while an enhanced Embedding API delivers superior performance for various AI tasks. Vector Search now features innovative hybrid capabilities, and a dedicated Ranking API works to improve the relevance of search results. To provide reliable, cited answers, the Grounded Generation API leverages Gemini's power, further supported by the Check Grounding API for efficient fact-checking of generated statements. These APIs are distinguished by their inherent high quality, integrating Google's extensive know-how derived from planet-scale applications, and are built for seamless integration into developer workflows, notably through popular frameworks like LangChain.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'LangChain']\n","\n","Q&A:\n"," Q: What is a key benefit of self-attention in sequence models?\n","A: It enables parallel processing of all tokens, improving efficiency over sequential models.\n","Q: What role does the learning rate play in gradient descent?\n","A: It determines the step size for updating model parameters during optimization.\n","Q: How does overfitting manifest in a machine learning model?\n","A: The model performs well on training data but poorly on new, unseen data.\n","Q: What is the primary function of word embeddings?\n","A: To represent words as dense vectors, capturing semantic meaning and relationships.\n","Q: When should accuracy be avoided as a classification metric?\n","A: When the dataset has a significant class imbalance.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, CI/CD Pipeline, Experiment Tracking\n","\n","Processing row 9: Unitary Transformations\n","\n","SUMMARY:\n"," This session thoroughly examines the Singular Value Decomposition (SVD), with a particular focus on the crucial roles played by the unitary matrices U and V. It establishes that unitary transformations are operations designed to preserve both vector lengths and angles, effectively rotating the vector space without any distortion, a characteristic shared with processes like the Fourier transform. Geometrically, the SVD provides a clear visualization: a matrix X maps an initial unit sphere, residing in its row space, directly into an ellipsoid within its column space. Within this transformation, the singular values are instrumental, dictating the precise lengths of the ellipsoid's principal axes, while the singular vectors define the ellipsoid's overall orientation in space. This decomposition offers a profoundly powerful, data-driven method for transforming coordinates into a much simpler system, one inherently aligned with the principal directions of the underlying data.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What is the core innovation of the Transformer architecture?\n","A: It uses self-attention to process sequences in parallel, overcoming RNN limitations.\n","Q: What is the purpose of gradient descent in machine learning?\n","A: To minimise a model's loss function by iteratively adjusting its parameters.\n","Q: How can overfitting be mitigated in a neural network?\n","A: Techniques like regularisation, dropout, and early stopping can help.\n","Q: What defines a supervised learning task?\n","A: It involves learning from labelled datasets to make predictions on new data.\n","Q: What are embeddings used for in natural language processing?\n","A: They represent words or tokens as dense vectors, capturing semantic relationships.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline\n","\n","Processing row 10: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","\n","SUMMARY:\n"," This tutorial introduces Google Gemini 1.5 Pro, a powerful generative AI model featuring advanced multimodal capabilities for processing both text and image inputs. A standout feature is its experimental 1-million token context window, significantly surpassing previous models like Gemini 1.0 Pro and GPT-4 Turbo. Demonstrations showcase its ability to analyze extensive documents, such as a 402-page Apollo 11 transcript, accurately extracting details, quotes, and identifying scenes from drawings with precise time codes. The session guides users through obtaining a free API key from ai.google.com/app/apikey, installing the `google-generativeai` library, and configuring the API. It highlights the transition to a single 1.5 Pro model, simplifying workflows by eliminating the need for separate vision-specific models. While initial response latency is noted, streaming responses are available, and dedicated API access is expected to improve speed, enhancing model interaction and Google's competitive position.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the main topic of the video?\n","A: The video is about building generative AI powered applications using Google Gemini Pro 1.5.\n","Q: Who is the speaker in this video?\n","A: The speaker is Krish Naik.\n","Q: What kind of model is Google Gemini Pro 1.5 described as?\n","A: It is described as a multimodal model.\n","Q: What does it mean for Gemini Pro 1.5 to be a multimodal model?\n","A: It means it will be able to work with both text and images.\n","Q: What practical aspects will be covered in the video after the initial demo?\n","A: The video will cover hands-on applications, including running code with images and text, and how to create and use the API key.\n","\n","KEY CONCEPTS:\n"," Generative AI, Google Gemini Pro 1.5, Multimodal AI, Long Context Understanding, Large Language Model (LLM), Transformer Architecture, API Key, AI Application Development, Text and Image Processing, Prompting, Context Window\n","\n","Processing row 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","\n","SUMMARY:\n"," This section comprehensively addresses the critical methodologies for evaluating and testing prompt engineering models, a cornerstone for their effective development. It meticulously details the essential metrics employed to gauge model performance, which include quantitative measures like perplexity and accuracy, alongside qualitative assessments derived from human evaluation. Beyond mere measurement, the discussion extends to practical techniques vital for refining and optimizing these sophisticated models. These encompass systematic debugging processes to identify and rectify underlying issues, precise fine-tuning strategies to enhance specific functionalities, and rigorous testing protocols. The latter involves deploying models on diverse datasets and utilizing advanced methods such as cross-validation. This multi-faceted approach is paramount for ensuring that prompt engineering models not only achieve robust generalization across varied contexts but also demonstrate sustained high performance and reliability in real-world applications.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is a key feature of Recurrent Neural Networks?\n","A: They process sequential data by maintaining an internal state.\n","Q: What is the main goal of gradient descent?\n","A: To minimize a model's loss function by iteratively adjusting parameters.\n","Q: When does a model overfit?\n","A: When it performs well on training data but poorly on unseen data.\n","Q: What is the benefit of transfer learning?\n","A: It leverages knowledge from a pre-trained model for a new, related task.\n","Q: What do word embeddings represent?\n","A: They are dense vector representations of words capturing semantic meaning.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, Model Registry, Experiment Tracking\n","\n","Processing row 12: Generative AI vs AI agents vs Agentic AI\n","\n","SUMMARY:\n"," The discussion distinguishes between Generative AI, AI Agents, and Agentic AI, illustrating a clear progression in their capabilities. Generative AI, based on Large Language Models (LLMs), primarily creates new content and answers questions, constrained by its knowledge cutoff. AI Agents represent an evolution, enhancing LLMs with tools (APIs) and memory, enabling them to perform actions, make autonomous decisions, and complete narrow tasks, such as booking flights with live data. The highest level of complexity is Agentic AI, where one or more AI agents work autonomously on intricate, multi-step goals. This demands multi-step reasoning, planning, and coordination, exemplified by a travel agent AI interacting with an immigration agent AI. Generative AI remains a core component within these advanced systems, which can be built using frameworks like LangGraph and N8N.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Langraph']\n","\n","Q&A:\n"," Q: What is the main benefit of self-attention?\n","A: It allows models to weigh the importance of different input tokens for each output.\n","Q: Why were LSTMs developed?\n","A: To overcome vanishing gradients in RNNs and capture long-term dependencies.\n","Q: What defines supervised learning?\n","A: Learning a function from labeled training data to predict outputs.\n","Q: When is clustering used?\n","A: To group similar data points together without prior labels.\n","Q: What does the learning rate control in optimization?\n","A: The step size taken towards the minimum of the loss function.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, CI/CD Pipeline, Model Registry\n","\n","Processing row 13: \n","Covariance in Statistics\n","\n","SUMMARY:\n"," The session commenced with an introduction to covariance, detailing its formula and explaining its fundamental role in quantifying the linear relationship between two random variables. It was elucidated that a positive covariance indicates a direct relationship, meaning that as one variable increases, the other tends to increase proportionally, exemplified by house size and price. Conversely, a negative covariance suggests an inverse relationship, where an increase in one variable typically corresponds to a decrease in the other. A significant limitation of covariance was then discussed: while it effectively reveals the direction of this linear association, it inherently fails to measure the *strength* or magnitude of this relationship. This critical shortcoming sets the stage for the subsequent introduction of Pearson correlation, which is designed precisely to provide a standardized measure of the strength of linear association.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science']\n","\n","Q&A:\n"," Q: What is the core innovation of the Transformer architecture?\n","A: It uses self-attention to process sequences in parallel, overcoming RNN limitations.\n","Q: What is the purpose of gradient descent in machine learning?\n","A: To minimise a model's loss function by iteratively adjusting its parameters.\n","Q: How can overfitting be mitigated in a neural network?\n","A: Techniques like regularisation, dropout, and early stopping can help.\n","Q: What defines a supervised learning task?\n","A: It involves learning from labelled datasets to make predictions on new data.\n","Q: What are embeddings used for in natural language processing?\n","A: They represent words or tokens as dense vectors, capturing semantic relationships.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Experiment Tracking\n","\n","Processing row 14: 3. Objective || End to End AI Tutorial\n","\n","SUMMARY:\n"," This video clarifies how to define the objective in Reinforcement Learning (RL) problems. The fundamental RL objective is to learn an optimal policy that maximizes a numerical reward signal, specifically the cumulative reward accumulated over time. Agents achieve this by engaging in trial and error, interacting with their environment, and continuously updating their policy based on the feedback provided by these rewards. The discussion distinguishes between two primary types of RL tasks: episodic and continuous. Episodic tasks, like Tic-Tac-Toe, involve finite interactions with distinct win, loss, or draw rewards. Conversely, continuous tasks, such as stock market trading, necessitate defining reward functions based on ongoing metrics like profit or risk-adjusted measures such as the Sharpe ratio. A crucial emphasis is placed on accurately parameterizing the RL objective by carefully defining a reward function that precisely reflects the agent's intended goals.\n","\n","TOPICS:\n"," ['Reinforcement Learning']\n","\n","Q&A:\n"," Q: What is the core innovation of the Transformer architecture?\n","A: It uses self-attention to process sequences in parallel, overcoming RNN limitations.\n","Q: What is the purpose of gradient descent in machine learning?\n","A: It's an optimization algorithm used to minimize the loss function of a model.\n","Q: What is overfitting in machine learning?\n","A: It occurs when a model learns the training data too well, performing poorly on unseen data.\n","Q: What defines supervised learning?\n","A: It involves training a model on labeled datasets, where inputs are mapped to known outputs.\n","Q: What are embeddings used for in NLP?\n","A: They represent words or phrases as dense vectors, capturing semantic relationships.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, CI/CD Pipeline, Experiment Tracking\n","\n","Processing row 15: Python Training - Python Dictionary Basics\n","\n","SUMMARY:\n"," This tutorial offers a comprehensive introduction to Python dictionaries, defining them as key-value pairs, or items, enclosed in curly brackets. A fundamental rule is that dictionary keys must be immutable. The session illustrates how to create dictionaries directly or from zipped lists using the `dict()` constructor. It covers essential operations such as accessing, modifying, and deleting items, alongside crucial methods like `items()`, `keys()`, `values()`, and `len()`. The tutorial emphasizes the utility of dictionaries for efficiently mapping data, highlighting their significant importance in data science, especially when integrated with libraries like Pandas, making them a core tool for data manipulation and analysis.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is the primary goal of a loss function?\n","A: It quantifies the error between predicted and actual values, guiding model optimization.\n","Q: How does transfer learning benefit model training?\n","A: It reuses pre-trained models on new, related tasks, reducing training time and data needs.\n","Q: What is the role of a validation set?\n","A: It evaluates model performance during training and helps tune hyperparameters without bias.\n","Q: When would you use a recurrent neural network (RNN)?\n","A: For sequential data tasks like natural language processing or time series prediction.\n","Q: What is the 'curse of dimensionality'?\n","A: It's the phenomenon where data becomes sparse and distances become less meaningful in high-dimensional spaces.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Instruction Tuning, CI/CD Pipeline, Experiment Tracking\n","\n","Processing row 16: \n","Fight Insider Threats with AI-infused SIEM\n","\n","SUMMARY:\n"," This session explores the pivotal role of AI in strengthening cybersecurity posture, emphasizing its potential to significantly reduce data breach identification and containment times, a finding supported by IBM's 2023 report. A primary focus is User Behavior Analytics (UBA), which leverages AI and machine learning to enable rapid and precise detection and response to insider threats. These threats are presented as a major and financially costly concern for organizations. The talk explains how UBA, when integrated with a SIEM like IBM QRadar, uses machine learning and rules to learn normal user patterns, identify behavioral anomalies, and effectively prioritize risks. The session demonstrates practical applications, including dashboards for monitoring employee behavior, investigating offenses with MITRE ATT&CK mappings, and showcasing how AI accelerates security investigations from hours to mere minutes, thereby enabling a robust, proactive defense against sophisticated cyber risks.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What significant benefit can AI provide in managing data breaches?\n","A: It can reduce the average time to identify and contain a data breach by 108 fewer days.\n","Q: Which report identified faster containment as a key finding for extensive AI and automation use?\n","A: IBM's Cost of a Data Breach Report 2023.\n","Q: What technology, combined with AI and machine learning, can help detect and respond to Insider threats?\n","A: User Behavior Analytics (UBA).\n","Q: What is a major concern for organizations of all sizes, as mentioned in the transcript?\n","A: Insider threats.\n","Q: According to the cost of a data breach report, what was the average cost of an Insider threat for an organization?\n","A: $4.\n","\n","KEY CONCEPTS:\n"," AI in Cybersecurity, Machine Learning, User Behavior Analytics (UBA), Insider Threats, Data Breach, Threat Detection, Threat Containment, Security Posture, Automation, Emerging Threats\n","\n","Processing row 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","\n","SUMMARY:\n"," Krishak introduces Meta's Llama 3, a new open-source LLM available in 8B and 70B parameter versions, integrated into Meta AI. Llama 3 demonstrates state-of-the-art performance, excelling in language nuances, contextual understanding, and complex tasks like translation, reasoning, and code generation. It boasts an 8K context length, trained on 50 trillion tokens (7x Llama 2), with refined post-training processes reducing false refusals and boosting response diversity. Benchmarks show Llama 3 outperforms other open-source models and competes strongly with proprietary LLMs like GPT-4 and Claude 3 Sonnet, though it may lag on specific mathematical tasks against Gemini Pro 1. Meta emphasizes a comprehensive responsible AI approach, including Meta Llama Guard. Users can access Llama 3 via Meta, Hugging Face, or Kaggle, with detailed instructions provided for downloading model weights and setting up local inference.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Generative AI', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the speaker's name?\n","A: The speaker's name is Krishak.\n","Q: What are listeners being welcomed to?\n","A: Listeners are being welcomed to the speaker's YouTube channel.\n","Q: What time is it currently?\n","A: It is currently 2 a.m.\n","Q: Who introduces themselves in the transcript?\n","A: Krishak introduces themselves.\n","Q: What type of online platform is mentioned?\n","A: A YouTube channel is mentioned.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Positional Encoding, Convolutional Layer, Pooling Operation, Reward Function, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","Processing row 18: Getting Started With sklearn\n","\n","SUMMARY:\n"," This comprehensive session focuses on the practical implementation of a decision boundary, a fundamental concept in machine learning for classifying data points. Utilizing `scikit-learn`, a widely-used Python library, participants are guided through the application of the `Gaussian Naive Bayes` algorithm. This particular classifier is chosen for its simplicity and efficiency, making it an excellent starting point for understanding probabilistic classification.\n","\n","The core methodology emphasizes a hands-on, code-first approach. Rather than delving into extensive theoretical explanations upfront, the session prioritizes active coding and direct engagement with the `scikit-learn` documentation. This strategy empowers users to independently navigate and leverage official resources, fostering self-sufficiency and practical problem-solving skills crucial for real-world data science tasks. By focusing on immediate application, learners gain tangible experience in fitting models, making predictions, and visualizing the resulting decision boundaries, thereby solidifying their understanding through direct implementation.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What Python library will be used frequently in this lesson?\n","A: The scikit-learn library, often abbreviated as sk-learn, will be used frequently.\n","Q: How will the speaker use Google in this lesson?\n","A: Google will be used to help navigate the documentation of the scikit-learn library to understand its functions.\n","Q: What specific algorithm was used to create the decision boundary?\n","A: The Gaussian Naive Bayes algorithm was used to create the decision boundary.\n","Q: What is the immediate goal for the audience regarding the code?\n","A: The immediate goal is for the audience to be able to run the code.\n","Q: What will the audience be able to do by the end of the next video or two?\n","A: By the end of the next video or two, the audience will be able to write the Python code for the decision boundary themselves.\n","\n","KEY CONCEPTS:\n"," Self-Attention Mechanism, Query-Key-Value, Convolutional Layer, Pooling Operation, Reward Function, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, CI/CD Pipeline, Experiment Tracking, Algorithm, Decision Boundary\n","\n","Processing row 19: \n","Log Normal Distribution in Statistics\n"]},{"ename":"ReadTimeout","evalue":"HTTPConnectionPool(host='localhost', port=45517): Read timed out. (read timeout=600.0)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mReadTimeout\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2454128403.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;31m# ================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFew-Shot pipeline completed successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2454128403.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nProcessing row {idx}: {title}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mqa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2454128403.py\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0mTRANSCRIPT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;31m\\\u001b[0m\u001b[0;34m\"\\\"\\\"{c}\\\"\\\"\\\"\"\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgemini_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mpartial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generated_summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2454128403.py\u001b[0m in \u001b[0;36mgemini_call\u001b[0;34m(prompt, temperature)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGLOBAL_MIN_GAP\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnow\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLAST_TS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"temperature\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mLAST_TS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    711\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_InvalidHeader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mReadTimeout\u001b[0m: HTTPConnectionPool(host='localhost', port=45517): Read timed out. (read timeout=600.0)"]}],"source":["# ================================================================\n","# Few-Shot Prompting Pipeline – Gemini-2.5-Flash\n","# ================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","import google.generativeai as genai\n","\n","# ================================================================\n","# 1. FEW-SHOT EXAMPLES\n","# ================================================================\n","\n","FEWSHOT_SUMMARIES = [\n","    {\"input\": \"Explains attention in transformers and its role in capturing long-range dependencies.\",\n","     \"output\": \"The lecture introduces attention in transformers, showing how query, key, and value vectors enable models to weigh relevant tokens. It contrasts this with RNN limitations and demonstrates gains on translation and summarisation.\"},\n","    {\"input\": \"CNN architecture for image classification.\",\n","     \"output\": \"This tutorial covers convolutional, pooling, and fully connected layers, explaining hierarchical feature extraction and typical training steps for vision classification tasks.\"},\n","    {\"input\": \"Reinforcement learning agents learn by reward feedback.\",\n","     \"output\": \"The session formalises RL with policies, rewards, and value estimation. It compares Q-learning and policy gradients, discusses exploration–exploitation, and highlights robotics and gaming use cases.\"},\n","    {\"input\": \"Prompt engineering improves LLM outputs.\",\n","     \"output\": \"Zero-shot, few-shot, and chain-of-thought prompts are compared. The talk emphasises instruction clarity, role specification, and constraint setting to improve reliability and reasoning.\"},\n","    {\"input\": \"MLOps pipelines for reliable deployment.\",\n","     \"output\": \"The talk explains CI/CD for models, experiment tracking, model registries, and monitoring, with tools such as MLflow and Kubeflow for production-grade ML.\"}\n","]\n","\n","FEWSHOT_TOPICS = [\n","    {\"input\": \"Explaining self-attention and BERT internals.\", \"output\": [\"Natural Language Processing\"]},\n","    {\"input\": \"Building CNNs with pooling for object recognition.\", \"output\": [\"Deep Learning\"]},\n","    {\"input\": \"Learning with rewards via Q-learning.\", \"output\": [\"Reinforcement Learning\"]},\n","    {\"input\": \"Designing prompts to improve LLM reasoning.\", \"output\": [\"Prompt Engineering\"]},\n","    {\"input\": \"Automating ML deployment with pipelines and monitoring.\", \"output\": [\"Mlops\"]},\n","    {\"input\": \"Creating data visualisations and feature analysis.\", \"output\": [\"Data Science\"]},\n","    {\"input\": \"Explaining model fine-tuning for generative image models.\", \"output\": [\"Generative AI\"]},\n","    {\"input\": \"Discussing NLP and ML synergy for LLMs.\", \"output\": [\"Natural Language Processing\", \"Machine Learning\"]},\n","]\n","\n","FEWSHOT_QA = [\n","    {\"q\": \"What does attention allow models to do?\",\n","     \"a\": \"It lets models focus on the most relevant tokens in a sequence.\"},\n","    {\"q\": \"Why are convolutions useful in vision?\",\n","     \"a\": \"They extract local spatial features for image classification.\"},\n","    {\"q\": \"How do agents learn in reinforcement learning?\",\n","     \"a\": \"They learn by maximising cumulative rewards through trial and error.\"},\n","    {\"q\": \"When is few-shot prompting effective?\",\n","     \"a\": \"When limited task-specific data exists but examples guide behaviour.\"},\n","    {\"q\": \"Who typically maintains ML pipelines in production?\",\n","     \"a\": \"Machine learning engineers and DevOps teams.\"}\n","]\n","\n","FEWSHOT_CONCEPTS = [\n","    [\"Self-Attention Mechanism\", \"Query-Key-Value\", \"Positional Encoding\"],\n","    [\"Convolutional Layer\", \"Pooling Operation\", \"Feature Map\"],\n","    [\"Reward Function\", \"Policy Gradient\", \"Q-Learning\"],\n","    [\"Few-Shot Prompting\", \"Chain-of-Thought Reasoning\", \"Instruction Tuning\"],\n","    [\"CI/CD Pipeline\", \"Model Registry\", \"Experiment Tracking\"]\n","]\n","\n","# ================================================================\n","# 2. PATHS & API\n","# ================================================================\n","\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gemini-2.5-flash/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-flash_fewshot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key3.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","# ================================================================\n","# 3. GLOBAL CONFIG\n","# ================================================================\n","\n","MODEL_NAME = \"gemini-2.5-flash\"\n","GLOBAL_MIN_GAP = 70\n","LAST_TS = 0.0\n","MAX_CHARS = 2600\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","# ================================================================\n","# 4. LOGGING\n","# ================================================================\n","\n","logs = Path(\"/content/logs\")\n","logs.mkdir(exist_ok=True)\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","\n","# ================================================================\n","# 5. UTILITIES\n","# ================================================================\n","\n","def deep_clean(t):\n","    t = str(t)\n","    t = re.sub(r\"https?://\\S+\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","def chunk_text(text, max_chars=MAX_CHARS):\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean]\n","    sents = re.split(r\"(?<=[.!?])\\s+\", clean)\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) < max_chars:\n","            cur += \" \" + s\n","        else:\n","            chunks.append(cur.strip())\n","            cur = s\n","    if cur.strip(): chunks.append(cur.strip())\n","    return chunks\n","\n","def extract_json(txt):\n","    try:\n","        s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","        if s == -1 or e == -1:\n","            return {}\n","        return json.loads(txt[s:e+1])\n","    except:\n","        return {}\n","\n","# ================================================================\n","# 6. GEMINI CALL\n","# ================================================================\n","\n","def gemini_call(prompt, temperature=0.2):\n","    global LAST_TS\n","    now = time.time()\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        time.sleep(GLOBAL_MIN_GAP - (now - LAST_TS))\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    out = model.generate_content(prompt, generation_config={\"temperature\": temperature})\n","    LAST_TS = time.time()\n","    return getattr(out, \"text\", \"\")\n","\n","# ================================================================\n","# 7. FEW-SHOT TASKS\n","# ================================================================\n","\n","# ------- SUMMARISATION -------\n","def generate_summary(transcript):\n","    chunks = chunk_text(transcript)\n","    partial = []\n","    examples = \"\\n\\n\".join([f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_SUMMARIES])\n","\n","    for c in chunks:\n","        prompt = f\"\"\"\n","You are a summarisation expert. Learn from examples:\n","\n","{examples}\n","\n","Now summarise the new chunk.\n","Return JSON: {{\"generated_summary\":\"...\"}}\n","\n","TRANSCRIPT:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = gemini_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        partial.append(j.get(\"generated_summary\", out[:500]))\n","\n","    combined = \" \".join(partial)\n","\n","    prompt2 = f\"\"\"\n","Combine these into a final 120-160 word summary.\n","Return JSON: {{\"generated_summary\":\"...\"}}\n","\n","Chunks:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","    out2 = gemini_call(prompt2, temperature=0.15)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", out2[:800])\n","\n","# ------- TOPIC CLASSIFICATION -------\n","def classify_topic(transcript, summary):\n","    \"\"\"\n","    Few-shot topic classification using BOTH the model summary\n","    and a truncated portion of the transcript for better global context.\n","    \"\"\"\n","\n","    # Combine summary + first 2000 characters of transcript\n","    # This keeps tokens safe but captures RLHF / PPO / reward model cues etc.\n","    text = (str(summary) + \" \" + str(transcript)[:2000]).strip()\n","\n","    # Build few-shot examples block\n","    examples = \"\\n\".join(\n","        [f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_TOPICS]\n","    )\n","\n","    prompt = f\"\"\"\n","Learn topic classification from examples:\n","\n","{examples}\n","\n","Available topics: {', '.join(VALID_TOPICS)}\n","\n","Task:\n","Given the TEXT below, choose up to THREE topics that best describe\n","the main technical focus. Only use topics from the list above.\n","\n","Return JSON ONLY in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","TEXT:\n","\\\"\\\"\\\"{text}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.1)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    # Normalise type\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        # keep only labels that are exactly in VALID_TOPICS (case-insensitive)\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    # ensure unique order-preserving, max 3, fallback Other\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","# ------- Q&A -------\n","def generate_qa(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([f\"Q:{x['q']}\\nA:{x['a']}\" for x in FEWSHOT_QA])\n","\n","    prompt = f\"\"\"\n","Learn Q&A format:\n","\n","{examples}\n","\n","Now produce EXACTLY five Q/A pairs.\n","Return JSON: {{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}]}}\n","\n","TRANSCRIPT:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\"\"\"\n","    out = gemini_call(prompt, temperature=0.1)\n","    j = extract_json(out)\n","    lines = []\n","    for qa in j.get(\"generated_questions\", []):\n","        lines.append(f\"Q: {qa.get('q','')}\\nA: {qa.get('a','')}\")\n","    return \"\\n\".join(lines)\n","\n","# ------- KEY CONCEPT EXTRACTION -------\n","def generate_concepts(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([\", \".join(lst) for lst in FEWSHOT_CONCEPTS])\n","\n","    prompt = f\"\"\"\n","Learn from these concept lists:\n","\n","{examples}\n","\n","Extract 10-12 concepts.\n","Return JSON: {{\"key_concepts\":[\"...\"]}}\n","\n","TRANSCRIPT:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\"\"\"\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    return \", \".join(j.get(\"key_concepts\", []))\n","\n","# ================================================================\n","# 8. MAIN PIPELINE\n","# ================================================================\n","\n","def run_pipeline():\n","    df = pd.read_excel(INPUT_FILE)\n","\n","    # Resume logic\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing_df = pd.read_excel(FINAL_OUTPUT_FILE)\n","        processed_indices = set(existing_df[\"row_index\"].tolist())\n","        print(f\"Resuming... {len(processed_indices)} rows already completed.\")\n","    else:\n","        existing_df = pd.DataFrame()\n","        processed_indices = set()\n","\n","    for idx, row in df.iterrows():\n","\n","        # Skip already completed rows\n","        if idx in processed_indices:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row['title'])\n","        transcript = str(row['transcript'])\n","        print(f\"\\nProcessing row {idx}: {title}\")\n","\n","        summary = generate_summary(transcript)\n","        topics = classify_topic(transcript, summary)\n","        qa = generate_qa(transcript)\n","        concepts = generate_concepts(transcript)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa,\n","            \"key_concepts\": concepts\n","        }\n","\n","        # Append new record to existing output\n","        updated_df = pd.concat([existing_df, pd.DataFrame([rec])], ignore_index=True)\n","\n","        # Save updated output\n","        updated_df.to_excel(FINAL_OUTPUT_FILE, index=False)\n","\n","        # Update in-memory df so resume continues properly\n","        existing_df = updated_df.copy()\n","\n","    print(\"\\nDONE. Final output saved to:\", FINAL_OUTPUT_FILE)\n","    return existing_df\n","\n","# ================================================================\n","# 9. RUN\n","# ================================================================\n","\n","df_out = run_pipeline()\n","print(\"\\nFew-Shot pipeline completed successfully!\")\n"]},{"cell_type":"code","source":["# ================================================================\n","# Few-Shot Prompting Pipeline – Gemini-2.5-Flash\n","# ================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","import google.generativeai as genai\n","\n","# ================================================================\n","# 1. FEW-SHOT EXAMPLES\n","# ================================================================\n","\n","FEWSHOT_SUMMARIES = [\n","    {\"input\": \"Explains attention in transformers and its role in capturing long-range dependencies.\",\n","     \"output\": \"The lecture introduces attention in transformers, showing how query, key, and value vectors enable models to weigh relevant tokens. It contrasts this with RNN limitations and demonstrates gains on translation and summarisation.\"},\n","    {\"input\": \"CNN architecture for image classification.\",\n","     \"output\": \"This tutorial covers convolutional, pooling, and fully connected layers, explaining hierarchical feature extraction and typical training steps for vision classification tasks.\"},\n","    {\"input\": \"Reinforcement learning agents learn by reward feedback.\",\n","     \"output\": \"The session formalises RL with policies, rewards, and value estimation. It compares Q-learning and policy gradients, discusses exploration–exploitation, and highlights robotics and gaming use cases.\"},\n","    {\"input\": \"Prompt engineering improves LLM outputs.\",\n","     \"output\": \"Zero-shot, few-shot, and chain-of-thought prompts are compared. The talk emphasises instruction clarity, role specification, and constraint setting to improve reliability and reasoning.\"},\n","    {\"input\": \"MLOps pipelines for reliable deployment.\",\n","     \"output\": \"The talk explains CI/CD for models, experiment tracking, model registries, and monitoring, with tools such as MLflow and Kubeflow for production-grade ML.\"}\n","]\n","\n","FEWSHOT_TOPICS = [\n","    {\"input\": \"Explaining self-attention and BERT internals.\", \"output\": [\"Natural Language Processing\"]},\n","    {\"input\": \"Building CNNs with pooling for object recognition.\", \"output\": [\"Deep Learning\"]},\n","    {\"input\": \"Learning with rewards via Q-learning.\", \"output\": [\"Reinforcement Learning\"]},\n","    {\"input\": \"Designing prompts to improve LLM reasoning.\", \"output\": [\"Prompt Engineering\"]},\n","    {\"input\": \"Automating ML deployment with pipelines and monitoring.\", \"output\": [\"Mlops\"]},\n","    {\"input\": \"Creating data visualisations and feature analysis.\", \"output\": [\"Data Science\"]},\n","    {\"input\": \"Explaining model fine-tuning for generative image models.\", \"output\": [\"Generative AI\"]},\n","    {\"input\": \"Discussing NLP and ML synergy for LLMs.\", \"output\": [\"Natural Language Processing\", \"Machine Learning\"]},\n","]\n","\n","FEWSHOT_QA = [\n","    {\"q\": \"What does attention allow models to do?\",\n","     \"a\": \"It lets models focus on the most relevant tokens in a sequence.\"},\n","    {\"q\": \"Why are convolutions useful in vision?\",\n","     \"a\": \"They extract local spatial features for image classification.\"},\n","    {\"q\": \"How do agents learn in reinforcement learning?\",\n","     \"a\": \"They learn by maximising cumulative rewards through trial and error.\"},\n","    {\"q\": \"When is few-shot prompting effective?\",\n","     \"a\": \"When limited task-specific data exists but examples guide behaviour.\"},\n","    {\"q\": \"Who typically maintains ML pipelines in production?\",\n","     \"a\": \"Machine learning engineers and DevOps teams.\"}\n","]\n","\n","FEWSHOT_CONCEPTS = [\n","    [\"Self-Attention Mechanism\", \"Query-Key-Value\", \"Positional Encoding\"],\n","    [\"Convolutional Layer\", \"Pooling Operation\", \"Feature Map\"],\n","    [\"Reward Function\", \"Policy Gradient\", \"Q-Learning\"],\n","    [\"Few-Shot Prompting\", \"Chain-of-Thought Reasoning\", \"Instruction Tuning\"],\n","    [\"CI/CD Pipeline\", \"Model Registry\", \"Experiment Tracking\"]\n","]\n","\n","# ================================================================\n","# 2. PATHS & API\n","# ================================================================\n","\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gemini-2.5-flash/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-flash_fewshot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key3.txt\"\n","\n","def load_key(path):\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","# ================================================================\n","# 3. GLOBAL CONFIG\n","# ================================================================\n","\n","MODEL_NAME = \"gemini-2.5-flash\"\n","GLOBAL_MIN_GAP = 70\n","LAST_TS = 0.0\n","MAX_CHARS = 2600\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","# ================================================================\n","# 4. LOGGING\n","# ================================================================\n","\n","logs = Path(\"/content/logs\")\n","logs.mkdir(exist_ok=True)\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","\n","# ================================================================\n","# 5. UTILITIES\n","# ================================================================\n","\n","def deep_clean(t):\n","    t = str(t)\n","    t = re.sub(r\"https?://\\S+\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","def chunk_text(text, max_chars=MAX_CHARS):\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean]\n","    sents = re.split(r\"(?<=[.!?])\\s+\", clean)\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) < max_chars:\n","            cur += \" \" + s\n","        else:\n","            chunks.append(cur.strip())\n","            cur = s\n","    if cur.strip(): chunks.append(cur.strip())\n","    return chunks\n","\n","def extract_json(txt):\n","    try:\n","        s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","        if s == -1 or e == -1:\n","            return {}\n","        return json.loads(txt[s:e+1])\n","    except:\n","        return {}\n","\n","# ================================================================\n","# 6. GEMINI CALL\n","# ================================================================\n","\n","def gemini_call(prompt, temperature=0.2):\n","    global LAST_TS\n","    now = time.time()\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        time.sleep(GLOBAL_MIN_GAP - (now - LAST_TS))\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    out = model.generate_content(prompt, generation_config={\"temperature\": temperature})\n","    LAST_TS = time.time()\n","    return getattr(out, \"text\", \"\")\n","\n","# ================================================================\n","# 7. FEW-SHOT TASKS\n","# ================================================================\n","\n","# ------- SUMMARISATION -------\n","def generate_summary(transcript):\n","    chunks = chunk_text(transcript)\n","    partial = []\n","    examples = \"\\n\\n\".join([f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_SUMMARIES])\n","\n","    for c in chunks:\n","        prompt = f\"\"\"\n","You are a summarisation expert. Learn from examples:\n","\n","{examples}\n","\n","Now summarise the new chunk.\n","Return JSON: {{\"generated_summary\":\"...\"}}\n","\n","TRANSCRIPT:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = gemini_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        partial.append(j.get(\"generated_summary\", out[:500]))\n","\n","    combined = \" \".join(partial)\n","\n","    prompt2 = f\"\"\"\n","Combine these into a final 120-160 word summary.\n","Return JSON: {{\"generated_summary\":\"...\"}}\n","\n","Chunks:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","    out2 = gemini_call(prompt2, temperature=0.15)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", out2[:800])\n","\n","# ------- TOPIC CLASSIFICATION -------\n","def classify_topic(transcript, summary):\n","    \"\"\"\n","    Few-shot topic classification using BOTH the model summary\n","    and a truncated portion of the transcript for better global context.\n","    \"\"\"\n","\n","    # Combine summary + first 2000 characters of transcript\n","    # This keeps tokens safe but captures RLHF / PPO / reward model cues etc.\n","    text = (str(summary) + \" \" + str(transcript)[:2000]).strip()\n","\n","    # Build few-shot examples block\n","    examples = \"\\n\".join(\n","        [f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_TOPICS]\n","    )\n","\n","    prompt = f\"\"\"\n","Learn topic classification from examples:\n","\n","{examples}\n","\n","Available topics: {', '.join(VALID_TOPICS)}\n","\n","Task:\n","Given the TEXT below, choose up to THREE topics that best describe\n","the main technical focus. Only use topics from the list above.\n","\n","Return JSON ONLY in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","TEXT:\n","\\\"\\\"\\\"{text}\\\"\\\"\\\"\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.1)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    # Normalise type\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        # keep only labels that are exactly in VALID_TOPICS (case-insensitive)\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    # ensure unique order-preserving, max 3, fallback Other\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","# ------- Q&A -------\n","def generate_qa(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([f\"Q:{x['q']}\\nA:{x['a']}\" for x in FEWSHOT_QA])\n","\n","    prompt = f\"\"\"\n","Learn Q&A format:\n","\n","{examples}\n","\n","Now produce EXACTLY five Q/A pairs.\n","Return JSON: {{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}]}}\n","\n","TRANSCRIPT:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\"\"\"\n","    out = gemini_call(prompt, temperature=0.1)\n","    j = extract_json(out)\n","    lines = []\n","    for qa in j.get(\"generated_questions\", []):\n","        lines.append(f\"Q: {qa.get('q','')}\\nA: {qa.get('a','')}\")\n","    return \"\\n\".join(lines)\n","\n","# ------- KEY CONCEPT EXTRACTION -------\n","def generate_concepts(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([\", \".join(lst) for lst in FEWSHOT_CONCEPTS])\n","\n","    prompt = f\"\"\"\n","Learn from these concept lists:\n","\n","{examples}\n","\n","Extract 10-12 concepts.\n","Return JSON: {{\"key_concepts\":[\"...\"]}}\n","\n","TRANSCRIPT:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\"\"\"\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    return \", \".join(j.get(\"key_concepts\", []))\n","\n","# ================================================================\n","# 8. MAIN PIPELINE\n","# ================================================================\n","\n","def run_pipeline():\n","    df = pd.read_excel(INPUT_FILE)\n","\n","    # Resume logic\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing_df = pd.read_excel(FINAL_OUTPUT_FILE)\n","        processed_indices = set(existing_df[\"row_index\"].tolist())\n","        print(f\"Resuming... {len(processed_indices)} rows already completed.\")\n","    else:\n","        existing_df = pd.DataFrame()\n","        processed_indices = set()\n","\n","    for idx, row in df.iterrows():\n","\n","        # Skip already completed rows\n","        if idx in processed_indices:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row['title'])\n","        transcript = str(row['transcript'])\n","        print(f\"\\nProcessing row {idx}: {title}\")\n","\n","        summary = generate_summary(transcript)\n","        topics = classify_topic(transcript, summary)\n","        qa = generate_qa(transcript)\n","        concepts = generate_concepts(transcript)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa,\n","            \"key_concepts\": concepts\n","        }\n","\n","        # Append new record to existing output\n","        updated_df = pd.concat([existing_df, pd.DataFrame([rec])], ignore_index=True)\n","\n","        # Save updated output\n","        updated_df.to_excel(FINAL_OUTPUT_FILE, index=False)\n","\n","        # Update in-memory df so resume continues properly\n","        existing_df = updated_df.copy()\n","\n","    print(\"\\nDONE. Final output saved to:\", FINAL_OUTPUT_FILE)\n","    return existing_df\n","\n","# ================================================================\n","# 9. RUN\n","# ================================================================\n","\n","df_out = run_pipeline()\n","print(\"\\nFew-Shot pipeline completed successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":885},"id":"YKmtT08IBnI2","executionInfo":{"status":"ok","timestamp":1763812566441,"user_tz":-330,"elapsed":5838465,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"549e5828-9059-43c2-bd4c-506c964ff1c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Resuming... 19 rows already completed.\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","Skipping row 7 (already processed)\n","Skipping row 8 (already processed)\n","Skipping row 9 (already processed)\n","Skipping row 10 (already processed)\n","Skipping row 11 (already processed)\n","Skipping row 12 (already processed)\n","Skipping row 13 (already processed)\n","Skipping row 14 (already processed)\n","Skipping row 15 (already processed)\n","Skipping row 16 (already processed)\n","Skipping row 17 (already processed)\n","Skipping row 18 (already processed)\n","\n","Processing row 19: \n","Log Normal Distribution in Statistics\n","\n","Processing row 20: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","\n","Processing row 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","\n","Processing row 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","\n","Processing row 23: SVD: Eigen Action Heros [Matlab]\n","\n","Processing row 24: LangChain Crash Course #3 - What is LangChain?\n","\n","Processing row 25: How To Use Residuals For Time Series Forecasting\n","\n","Processing row 26: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","\n","Processing row 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","\n","Processing row 28: Q-learning - Explained!\n","\n","Processing row 29: Training Your Logistic Classifier\n","\n","DONE. Final output saved to: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gemini-2.5-flash/gemini-2.5-flash_fewshot_full_output.xlsx\n","\n","Few-Shot pipeline completed successfully!\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8ftCcGvtNWz","colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["a68bf72644c44ca28d3146453ac9b99b","d3dd079770b9405ea7348748449bcf74","000dc529261c4a8c9df80a2d59f50c0e","bb80f684cdf947b0a77a23f9d908f59d","a120d03d3bc34ed087719c039c23f7f1","03047f06e852466fad95e045eb7fe1b1","5ddbd094182e40b3b0d7882412cf29eb","ee74e871cf014352a62dd2e425faa08c","484f6f79aa594dadae43a444abac24e1","3d6506c76b254a7f8b873fe403a00419","1bf5cc24341f4f94b0d23301f8ae6658","67441e441f784ada852ec61103474d15","8e66858d80ba4c959af72e2924ee88a2","d301be6104e44746816e88976a82632f","febaec8e084c4e019ffef54a61372c69","764464f62abc4366ad6466622343cb8a","54c24a5ae58944d5aff2df18238aab22","65d05f69791146cd92a09b28ac1a03a6","198b233b4e2343d68d2b6cc6a563465b","442a66dd9bb4478a8474edce5d6ea189","361eacbee4054f9db453531fe47cc956","d2638fccebb84d11bcec96bcf94270ac","d6c82610b8904dd6b4eed92cab7b385b","8d0cb16dd6294c39a25787e021db6b12","392f0ab4db774c7884bf69c0127d5249","37a1dee1649d40d880c697be3a73d0e2","ac9177da1de94df1888ce8d7a6a7e3c3","6d3ba91120ab4e07b8ab7e4b87036ba2","bfeae46d3ea843269dfc67e781b54326","c8c5d36586ff44e6b459318143d50e7a","28a47f83dbe541b4aff71da04b761503","d6dbb359468349acbbcf03789a2bb2ee","175d6ef63a3148d89c0cb08d52e80e62","b79ebc1eba294d3d992c24e8050e338c","b979450a9d514e058c749eb4e2c96aff","45442d92ba9f4a1b9e0860d6509756b4","e09f5e0db6974bda9f3d669cd6c558d6","3eaf5d6105fd4f26bad731e832d4c738","c4cd3f3f1d36483ba2f915fe793c4eba","96c28656361b4d77aa13364c50682170","d3057524250d47d590ea99f48af53aa0","1100cef373ce4bee912d1387f23307e6","ac50380205fa4e76b1b87c5915cc4729","f659aac719e64c1580482a99995c1c43","af96f2eae02245688c1bfe0588e5ef41","6a1be69cc0164032850437b7009fb0d4","88a3befef5c848b98eb8a5427d9de798","3a8f6a6f576a4d0784275e490429cff9","5a019a48dbcd4bd882ccb1bf56cf6654","e330e708c6f74850bacd1f710bbee5c6","b9d203c6f2b142c48470c45cbf9204d5","c78cb9658c2941159828f4c633bf4a73","100fd533f4d24e29a69d2bdcf632043d","7952bd2a11944bfd9366853d867a35dc","059c03c60dc441fa9434c6d871ab0fd8","b2fba3f5c17c4b9f8840c72324e8911a","517c726866764d20ac4b870b45a09f40","356ca190112e40a78780d504fb9379f6","1570abf4ea8b4e42a5d27f5b037376da","e98ca02852b04f57a04e9bfdde5f57e2","4baee7f2f6904341b4345e9fb8ee6749","a7203ae33f714662b8a4f145ca8b7600","d6ebacc56c8344a0bea2226e9c9f61cc","1c7af5c544c64e3ba42c98888937056b","b23a53872c5b4644a18ee000a437fee8","3d97c8edf5c14f459a2da8c804d180ae"]},"executionInfo":{"status":"ok","timestamp":1763812946849,"user_tz":-330,"elapsed":206794,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"5a9f7625-6e61-4992-913f-d3d4cc0b827e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gemini-2.5-flash/gemini-2.5-flash_fewshot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a68bf72644c44ca28d3146453ac9b99b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67441e441f784ada852ec61103474d15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c82610b8904dd6b4eed92cab7b385b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b79ebc1eba294d3d992c24e8050e338c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af96f2eae02245688c1bfe0588e5ef41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2fba3f5c17c4b9f8840c72324e8911a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3043\n","  - BLEU: 0.0760\n","  - BERTScore F1: 0.8884\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.4097\n","  - Micro F1: 0.5423\n","  - Macro F1: 0.5030\n","  - Weighted F1: 0.5147\n","\n","Q&A Generation:\n","  - BLEU: 0.0395\n","  - Diversity: 0.7152\n","  - Answerability: 0.2867\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.2333\n","  - Recall@10: 0.0933\n","  - F1@10: 0.1333\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gemini-2.5-flash/evaluation_final.json\n"]}],"source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gemini-2.5-flash/gemini-2.5-flash_fewshot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/gemini-2.5-flash/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJLCeVoj5PwhgJfe8e4SMy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a68bf72644c44ca28d3146453ac9b99b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d3dd079770b9405ea7348748449bcf74","IPY_MODEL_000dc529261c4a8c9df80a2d59f50c0e","IPY_MODEL_bb80f684cdf947b0a77a23f9d908f59d"],"layout":"IPY_MODEL_a120d03d3bc34ed087719c039c23f7f1"}},"d3dd079770b9405ea7348748449bcf74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03047f06e852466fad95e045eb7fe1b1","placeholder":"​","style":"IPY_MODEL_5ddbd094182e40b3b0d7882412cf29eb","value":"tokenizer_config.json: 100%"}},"000dc529261c4a8c9df80a2d59f50c0e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee74e871cf014352a62dd2e425faa08c","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_484f6f79aa594dadae43a444abac24e1","value":25}},"bb80f684cdf947b0a77a23f9d908f59d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d6506c76b254a7f8b873fe403a00419","placeholder":"​","style":"IPY_MODEL_1bf5cc24341f4f94b0d23301f8ae6658","value":" 25.0/25.0 [00:00&lt;00:00, 852B/s]"}},"a120d03d3bc34ed087719c039c23f7f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"03047f06e852466fad95e045eb7fe1b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ddbd094182e40b3b0d7882412cf29eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee74e871cf014352a62dd2e425faa08c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"484f6f79aa594dadae43a444abac24e1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d6506c76b254a7f8b873fe403a00419":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bf5cc24341f4f94b0d23301f8ae6658":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"67441e441f784ada852ec61103474d15":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8e66858d80ba4c959af72e2924ee88a2","IPY_MODEL_d301be6104e44746816e88976a82632f","IPY_MODEL_febaec8e084c4e019ffef54a61372c69"],"layout":"IPY_MODEL_764464f62abc4366ad6466622343cb8a"}},"8e66858d80ba4c959af72e2924ee88a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54c24a5ae58944d5aff2df18238aab22","placeholder":"​","style":"IPY_MODEL_65d05f69791146cd92a09b28ac1a03a6","value":"config.json: 100%"}},"d301be6104e44746816e88976a82632f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_198b233b4e2343d68d2b6cc6a563465b","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_442a66dd9bb4478a8474edce5d6ea189","value":482}},"febaec8e084c4e019ffef54a61372c69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_361eacbee4054f9db453531fe47cc956","placeholder":"​","style":"IPY_MODEL_d2638fccebb84d11bcec96bcf94270ac","value":" 482/482 [00:00&lt;00:00, 11.8kB/s]"}},"764464f62abc4366ad6466622343cb8a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54c24a5ae58944d5aff2df18238aab22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65d05f69791146cd92a09b28ac1a03a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"198b233b4e2343d68d2b6cc6a563465b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"442a66dd9bb4478a8474edce5d6ea189":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"361eacbee4054f9db453531fe47cc956":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2638fccebb84d11bcec96bcf94270ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6c82610b8904dd6b4eed92cab7b385b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8d0cb16dd6294c39a25787e021db6b12","IPY_MODEL_392f0ab4db774c7884bf69c0127d5249","IPY_MODEL_37a1dee1649d40d880c697be3a73d0e2"],"layout":"IPY_MODEL_ac9177da1de94df1888ce8d7a6a7e3c3"}},"8d0cb16dd6294c39a25787e021db6b12":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6d3ba91120ab4e07b8ab7e4b87036ba2","placeholder":"​","style":"IPY_MODEL_bfeae46d3ea843269dfc67e781b54326","value":"vocab.json: 100%"}},"392f0ab4db774c7884bf69c0127d5249":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8c5d36586ff44e6b459318143d50e7a","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28a47f83dbe541b4aff71da04b761503","value":898823}},"37a1dee1649d40d880c697be3a73d0e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6dbb359468349acbbcf03789a2bb2ee","placeholder":"​","style":"IPY_MODEL_175d6ef63a3148d89c0cb08d52e80e62","value":" 899k/899k [00:00&lt;00:00, 5.26MB/s]"}},"ac9177da1de94df1888ce8d7a6a7e3c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6d3ba91120ab4e07b8ab7e4b87036ba2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfeae46d3ea843269dfc67e781b54326":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8c5d36586ff44e6b459318143d50e7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28a47f83dbe541b4aff71da04b761503":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6dbb359468349acbbcf03789a2bb2ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"175d6ef63a3148d89c0cb08d52e80e62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b79ebc1eba294d3d992c24e8050e338c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b979450a9d514e058c749eb4e2c96aff","IPY_MODEL_45442d92ba9f4a1b9e0860d6509756b4","IPY_MODEL_e09f5e0db6974bda9f3d669cd6c558d6"],"layout":"IPY_MODEL_3eaf5d6105fd4f26bad731e832d4c738"}},"b979450a9d514e058c749eb4e2c96aff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4cd3f3f1d36483ba2f915fe793c4eba","placeholder":"​","style":"IPY_MODEL_96c28656361b4d77aa13364c50682170","value":"merges.txt: 100%"}},"45442d92ba9f4a1b9e0860d6509756b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3057524250d47d590ea99f48af53aa0","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1100cef373ce4bee912d1387f23307e6","value":456318}},"e09f5e0db6974bda9f3d669cd6c558d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac50380205fa4e76b1b87c5915cc4729","placeholder":"​","style":"IPY_MODEL_f659aac719e64c1580482a99995c1c43","value":" 456k/456k [00:00&lt;00:00, 2.80MB/s]"}},"3eaf5d6105fd4f26bad731e832d4c738":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4cd3f3f1d36483ba2f915fe793c4eba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96c28656361b4d77aa13364c50682170":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d3057524250d47d590ea99f48af53aa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1100cef373ce4bee912d1387f23307e6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ac50380205fa4e76b1b87c5915cc4729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f659aac719e64c1580482a99995c1c43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af96f2eae02245688c1bfe0588e5ef41":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a1be69cc0164032850437b7009fb0d4","IPY_MODEL_88a3befef5c848b98eb8a5427d9de798","IPY_MODEL_3a8f6a6f576a4d0784275e490429cff9"],"layout":"IPY_MODEL_5a019a48dbcd4bd882ccb1bf56cf6654"}},"6a1be69cc0164032850437b7009fb0d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e330e708c6f74850bacd1f710bbee5c6","placeholder":"​","style":"IPY_MODEL_b9d203c6f2b142c48470c45cbf9204d5","value":"tokenizer.json: 100%"}},"88a3befef5c848b98eb8a5427d9de798":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c78cb9658c2941159828f4c633bf4a73","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_100fd533f4d24e29a69d2bdcf632043d","value":1355863}},"3a8f6a6f576a4d0784275e490429cff9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7952bd2a11944bfd9366853d867a35dc","placeholder":"​","style":"IPY_MODEL_059c03c60dc441fa9434c6d871ab0fd8","value":" 1.36M/1.36M [00:00&lt;00:00, 5.54MB/s]"}},"5a019a48dbcd4bd882ccb1bf56cf6654":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e330e708c6f74850bacd1f710bbee5c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9d203c6f2b142c48470c45cbf9204d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c78cb9658c2941159828f4c633bf4a73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"100fd533f4d24e29a69d2bdcf632043d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7952bd2a11944bfd9366853d867a35dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"059c03c60dc441fa9434c6d871ab0fd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2fba3f5c17c4b9f8840c72324e8911a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_517c726866764d20ac4b870b45a09f40","IPY_MODEL_356ca190112e40a78780d504fb9379f6","IPY_MODEL_1570abf4ea8b4e42a5d27f5b037376da"],"layout":"IPY_MODEL_e98ca02852b04f57a04e9bfdde5f57e2"}},"517c726866764d20ac4b870b45a09f40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4baee7f2f6904341b4345e9fb8ee6749","placeholder":"​","style":"IPY_MODEL_a7203ae33f714662b8a4f145ca8b7600","value":"model.safetensors: 100%"}},"356ca190112e40a78780d504fb9379f6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6ebacc56c8344a0bea2226e9c9f61cc","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c7af5c544c64e3ba42c98888937056b","value":1421700479}},"1570abf4ea8b4e42a5d27f5b037376da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b23a53872c5b4644a18ee000a437fee8","placeholder":"​","style":"IPY_MODEL_3d97c8edf5c14f459a2da8c804d180ae","value":" 1.42G/1.42G [00:26&lt;00:00, 96.8MB/s]"}},"e98ca02852b04f57a04e9bfdde5f57e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4baee7f2f6904341b4345e9fb8ee6749":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7203ae33f714662b8a4f145ca8b7600":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6ebacc56c8344a0bea2226e9c9f61cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c7af5c544c64e3ba42c98888937056b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b23a53872c5b4644a18ee000a437fee8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d97c8edf5c14f459a2da8c804d180ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}