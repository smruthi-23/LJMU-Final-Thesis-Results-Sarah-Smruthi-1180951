{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDQbYmv6Tl9QSCB2QgJUJA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aa2b2604650f4dbf82d8c9a93580b2eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e99a58bbbca941fd87c26aad3d03f4d0","IPY_MODEL_bc377ca73ea044ddb625439bc8f83eb4","IPY_MODEL_dbd67e1bb6e2444a80e4a942406667de"],"layout":"IPY_MODEL_0c289896234448eeb0b72dbfe2ef4138"}},"e99a58bbbca941fd87c26aad3d03f4d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2023c27a020d461fb5e5b2577c7cbcce","placeholder":"​","style":"IPY_MODEL_c6bfc9b413f34442aeaa97b653a6a076","value":"tokenizer_config.json: 100%"}},"bc377ca73ea044ddb625439bc8f83eb4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6c778a54b394fd89dd4d08d59683946","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ea656fda17044e608506da304dbd842f","value":25}},"dbd67e1bb6e2444a80e4a942406667de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86aba703ec234aada4e413e13089663b","placeholder":"​","style":"IPY_MODEL_eadcb89defa94802aaee4999d3787d43","value":" 25.0/25.0 [00:00&lt;00:00, 1.40kB/s]"}},"0c289896234448eeb0b72dbfe2ef4138":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2023c27a020d461fb5e5b2577c7cbcce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6bfc9b413f34442aeaa97b653a6a076":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6c778a54b394fd89dd4d08d59683946":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea656fda17044e608506da304dbd842f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"86aba703ec234aada4e413e13089663b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eadcb89defa94802aaee4999d3787d43":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a23ad9af34b74f4c9b07f90286799885":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e30776cb081d4f08abe9aee9fa1369ac","IPY_MODEL_8be7b902230746809d6f7239aacd44b9","IPY_MODEL_02c86744310641c7b48a6d4e60348b8e"],"layout":"IPY_MODEL_86c4e0d3fafd42f984715deb3c07c729"}},"e30776cb081d4f08abe9aee9fa1369ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a40c81949c1410e881dd28d60e10f17","placeholder":"​","style":"IPY_MODEL_ae8e7aef400f4ef289299090dda52b17","value":"config.json: 100%"}},"8be7b902230746809d6f7239aacd44b9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c35ed830f7d244438a52933af74b8b1b","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6c516c3792e428aa3795363ab8d66d5","value":482}},"02c86744310641c7b48a6d4e60348b8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84efcfbbac854f40801c2d0b28bb19cb","placeholder":"​","style":"IPY_MODEL_6685483b3ad744a18ee8bb824858a633","value":" 482/482 [00:00&lt;00:00, 27.8kB/s]"}},"86c4e0d3fafd42f984715deb3c07c729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a40c81949c1410e881dd28d60e10f17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae8e7aef400f4ef289299090dda52b17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c35ed830f7d244438a52933af74b8b1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6c516c3792e428aa3795363ab8d66d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84efcfbbac854f40801c2d0b28bb19cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6685483b3ad744a18ee8bb824858a633":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"044675e6622e4051b78d13a471512592":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db18d86e90e448af8d5e5de9c1eaac8b","IPY_MODEL_9ce62ea377734130a252c78f05154139","IPY_MODEL_dde539958ef9446cb366a81b3b1734e4"],"layout":"IPY_MODEL_9e51445189604364a170f3328a91766a"}},"db18d86e90e448af8d5e5de9c1eaac8b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7bd4ae0762e74255b6c60ea5b9d8457e","placeholder":"​","style":"IPY_MODEL_141c40d0e0ff4686994f0f7e5dbc6769","value":"vocab.json: 100%"}},"9ce62ea377734130a252c78f05154139":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75939cfbd04f47bcb17a5ae934e37bfe","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_658669fc1c74421c87c622a67f06cd17","value":898823}},"dde539958ef9446cb366a81b3b1734e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08156e823f314338a9d4d93457b90702","placeholder":"​","style":"IPY_MODEL_949b545a4b5948b08c324d3b7c953552","value":" 899k/899k [00:00&lt;00:00, 6.32MB/s]"}},"9e51445189604364a170f3328a91766a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7bd4ae0762e74255b6c60ea5b9d8457e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"141c40d0e0ff4686994f0f7e5dbc6769":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75939cfbd04f47bcb17a5ae934e37bfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"658669fc1c74421c87c622a67f06cd17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"08156e823f314338a9d4d93457b90702":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"949b545a4b5948b08c324d3b7c953552":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e10d03f52324dc8b3fea34fcdffb44e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_952f5f0985e741b8b2f694a200183248","IPY_MODEL_58b02450c8c34e5ab56097a222db2555","IPY_MODEL_a2483b4e2d3543968eb2be1721d3c5ce"],"layout":"IPY_MODEL_bbfc1fb0af2e4f70813eaaec0adc152d"}},"952f5f0985e741b8b2f694a200183248":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f29ca3fc85b445af98ebbb86a1aa9e6c","placeholder":"​","style":"IPY_MODEL_e1451054a0934eb483ddd48cdc34fe79","value":"merges.txt: 100%"}},"58b02450c8c34e5ab56097a222db2555":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_672192d0eed244df9906adcce7d8adf1","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a48816c1f9546ce8ce393850bb0b39d","value":456318}},"a2483b4e2d3543968eb2be1721d3c5ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cfc4be70210448f9b8923f093446cb8b","placeholder":"​","style":"IPY_MODEL_d724062527654d19bb54c5a40c7b979b","value":" 456k/456k [00:00&lt;00:00, 24.6MB/s]"}},"bbfc1fb0af2e4f70813eaaec0adc152d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f29ca3fc85b445af98ebbb86a1aa9e6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1451054a0934eb483ddd48cdc34fe79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"672192d0eed244df9906adcce7d8adf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a48816c1f9546ce8ce393850bb0b39d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cfc4be70210448f9b8923f093446cb8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d724062527654d19bb54c5a40c7b979b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"addc51d83cf641c786694eae9caae328":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_71648ad79eec48dabc12576d9a58f920","IPY_MODEL_05538824abd04898a5cac3df50c36d4b","IPY_MODEL_204b8f33565d4af69da9c5985d70966e"],"layout":"IPY_MODEL_f119de49d35b48ef8e96babaac89b755"}},"71648ad79eec48dabc12576d9a58f920":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67ac2340424b4b61950193d3ae76440a","placeholder":"​","style":"IPY_MODEL_700ceea2b78e458b9949f9aa110c0032","value":"tokenizer.json: 100%"}},"05538824abd04898a5cac3df50c36d4b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb657af8bfe5460cba8f1e57af913d59","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_956bc6dc873e478a87903bc2b2088ca6","value":1355863}},"204b8f33565d4af69da9c5985d70966e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c319a59ee6f74d54a9e4f57e9251c196","placeholder":"​","style":"IPY_MODEL_313f078eb4c7448f9a592b481760f191","value":" 1.36M/1.36M [00:00&lt;00:00, 42.3MB/s]"}},"f119de49d35b48ef8e96babaac89b755":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ac2340424b4b61950193d3ae76440a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"700ceea2b78e458b9949f9aa110c0032":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb657af8bfe5460cba8f1e57af913d59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"956bc6dc873e478a87903bc2b2088ca6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c319a59ee6f74d54a9e4f57e9251c196":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"313f078eb4c7448f9a592b481760f191":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"313ef6e387a8438388e52741db990f87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbc04eb8b1194ac5ae6236bf6ed51856","IPY_MODEL_4deaad1c49e24a98976605c00bf9a543","IPY_MODEL_980e0e38f6064da4a6e3cec7ee5d296e"],"layout":"IPY_MODEL_72ff4663cbbf48e49952f04b11ce15ec"}},"fbc04eb8b1194ac5ae6236bf6ed51856":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6e2952b03fb4c2582e44fb626aeaba5","placeholder":"​","style":"IPY_MODEL_81dba03ce0fd499caefba0a50bedc15a","value":"model.safetensors: 100%"}},"4deaad1c49e24a98976605c00bf9a543":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f85c0609530b4fe58d14a36ce4f0e439","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d936323bb5f94c38a6e4ad72eacc78f2","value":1421700479}},"980e0e38f6064da4a6e3cec7ee5d296e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6ab3f7c6ae3440ca19b90fbe717305a","placeholder":"​","style":"IPY_MODEL_ef115c5fe48143f1b198464a841c3faf","value":" 1.42G/1.42G [00:21&lt;00:00, 105MB/s]"}},"72ff4663cbbf48e49952f04b11ce15ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6e2952b03fb4c2582e44fb626aeaba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81dba03ce0fd499caefba0a50bedc15a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f85c0609530b4fe58d14a36ce4f0e439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d936323bb5f94c38a6e4ad72eacc78f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d6ab3f7c6ae3440ca19b90fbe717305a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef115c5fe48143f1b198464a841c3faf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"RLXTp_IqnxHL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763993391023,"user_tz":-330,"elapsed":22317,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"c05a432e-3288-4208-c77e-893023739363"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=d152f31a73a2f93aa1fe54d69276a25be4bbb2b440952c1fa4ea9cf001cf8ba8\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"CN0oYqqQLEjw","executionInfo":{"status":"ok","timestamp":1763993391084,"user_tz":-330,"elapsed":54,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"ea0f8bb8-fcf3-4cc8-8c5f-5149259816bb"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["# ================================================================\n","# Few-Shot Prompting Pipeline – Groq\n","# ================================================================\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","# ================================================================\n","# 1. FEW-SHOT EXAMPLES\n","# ================================================================\n","\n","FEWSHOT_SUMMARIES = [\n","    {\"input\": \"Explains attention in transformers and its role in capturing long-range dependencies.\",\n","     \"output\": \"The lecture introduces attention in transformers, showing how query, key, and value vectors enable models to weigh relevant tokens. It contrasts this with RNN limitations and demonstrates gains on translation and summarisation.\"},\n","    {\"input\": \"CNN architecture for image classification.\",\n","     \"output\": \"This tutorial covers convolutional, pooling, and fully connected layers, explaining hierarchical feature extraction and typical training steps for vision classification tasks.\"},\n","    {\"input\": \"Reinforcement learning agents learn by reward feedback.\",\n","     \"output\": \"The session formalises RL with policies, rewards, and value estimation. It compares Q-learning and policy gradients, discusses exploration–exploitation, and highlights robotics and gaming use cases.\"},\n","    {\"input\": \"Prompt engineering improves LLM outputs.\",\n","     \"output\": \"Zero-shot, few-shot, and chain-of-thought prompts are compared. The talk emphasises instruction clarity, role specification, and constraint setting to improve reliability and reasoning.\"},\n","    {\"input\": \"MLOps pipelines for reliable deployment.\",\n","     \"output\": \"The talk explains CI/CD for models, experiment tracking, model registries, and monitoring, with tools such as MLflow and Kubeflow for production-grade ML.\"}\n","]\n","\n","FEWSHOT_TOPICS = [\n","    {\"input\": \"Explaining self-attention and BERT internals.\", \"output\": [\"Natural Language Processing\"]},\n","    {\"input\": \"Building CNNs with pooling for object recognition.\", \"output\": [\"Deep Learning\"]},\n","    {\"input\": \"Learning with rewards via Q-learning.\", \"output\": [\"Reinforcement Learning\"]},\n","    {\"input\": \"Designing prompts to improve LLM reasoning.\", \"output\": [\"Prompt Engineering\"]},\n","    {\"input\": \"Automating ML deployment with pipelines and monitoring.\", \"output\": [\"Mlops\"]},\n","    {\"input\": \"Creating data visualisations and feature analysis.\", \"output\": [\"Data Science\"]},\n","    {\"input\": \"Explaining model fine-tuning for generative image models.\", \"output\": [\"Generative AI\"]},\n","    {\"input\": \"Discussing NLP and ML synergy for LLMs.\", \"output\": [\"Natural Language Processing\", \"Machine Learning\"]},\n","]\n","\n","FEWSHOT_QA = [\n","    {\"q\": \"What does attention allow models to do?\",\n","     \"a\": \"It lets models focus on the most relevant tokens in a sequence.\"},\n","    {\"q\": \"Why are convolutions useful in vision?\",\n","     \"a\": \"They extract local spatial features for image classification.\"},\n","    {\"q\": \"How do agents learn in reinforcement learning?\",\n","     \"a\": \"They learn by maximising cumulative rewards through trial and error.\"},\n","    {\"q\": \"When is few-shot prompting effective?\",\n","     \"a\": \"When limited task-specific data exists but examples guide behaviour.\"},\n","    {\"q\": \"Who typically maintains ML pipelines in production?\",\n","     \"a\": \"Machine learning engineers and DevOps teams.\"}\n","]\n","\n","FEWSHOT_CONCEPTS = [\n","    [\"Self-Attention Mechanism\", \"Query-Key-Value\", \"Positional Encoding\"],\n","    [\"Convolutional Layer\", \"Pooling Operation\", \"Feature Map\"],\n","    [\"Reward Function\", \"Policy Gradient\", \"Q-Learning\"],\n","    [\"Few-Shot Prompting\", \"Chain-of-Thought Reasoning\", \"Instruction Tuning\"],\n","    [\"CI/CD Pipeline\", \"Model Registry\", \"Experiment Tracking\"]\n","]\n","\n","# ================================================================\n","# 2. PATHS & API\n","# ================================================================\n","\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-3.1-8b-instant/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.1-8b-instant_fewshot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key4.txt\"\n","\n","def load_key(path):\n","    with open(path) as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","# ================================================================\n","# 3. GLOBAL CONFIG\n","# ================================================================\n","\n","MODEL_NAME = \"llama-3.1-8b-instant\"\n","GLOBAL_MIN_GAP = 15\n","LAST_TS = 0.0\n","MAX_CHARS = 2600\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","# ================================================================\n","# 4. LOGGING\n","# ================================================================\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger()\n","\n","# ================================================================\n","# 5. CLEANING & CHUNKING\n","# ================================================================\n","\n","def deep_clean(t):\n","    t = str(t)\n","    t = re.sub(r\"https?://\\S+\", \" \", t)\n","    t = re.sub(r\"\\s+\", \" \", t)\n","    return t.strip()\n","\n","def chunk_text(text, max_chars=MAX_CHARS):\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean]\n","    sents = re.split(r\"(?<=[.!?])\\s+\", clean)\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) < max_chars:\n","            cur += \" \" + s\n","        else:\n","            chunks.append(cur.strip())\n","            cur = s\n","    if cur.strip(): chunks.append(cur.strip())\n","    return chunks\n","\n","# ================================================================\n","# 6. JSON EXTRACTION\n","# ================================================================\n","\n","def extract_json(txt):\n","    try:\n","        s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","        if s == -1 or e == -1:\n","            return {}\n","        return json.loads(txt[s:e+1])\n","    except:\n","        return {}\n","\n","# ================================================================\n","# 7. GROQ CALL (RELIABLE)\n","# ================================================================\n","\n","def groq_call(prompt, temperature=0.2, retries=3):\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        time.sleep(GLOBAL_MIN_GAP - (now - LAST_TS))\n","\n","    for attempt in range(retries):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048\n","            )\n","            LAST_TS = time.time()\n","            return resp.choices[0].message.content\n","        except Exception as e:\n","            print(f\"Retry {attempt+1}/{retries}: {e}\")\n","            time.sleep(4)\n","\n","    return \"\"\n","\n","# ================================================================\n","# 8. FEW-SHOT TASKS\n","# ================================================================\n","\n","# ------ SUMMARY ------\n","def generate_summary(transcript):\n","    chunks = chunk_text(transcript)\n","    partial = []\n","\n","    fewshot = \"\\n\\n\".join([f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_SUMMARIES])\n","\n","    for c in chunks:\n","        prompt = f\"\"\"\n","Learn from examples:\n","{fewshot}\n","\n","Now summarise the transcript chunk.\n","Return ONLY JSON:\n","{{\"generated_summary\":\"...\"}}\n","\n","CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\n","\"\"\"\n","        out = groq_call(prompt, 0.15)\n","        j = extract_json(out)\n","        partial.append(j.get(\"generated_summary\", \"\"))\n","\n","    combined = \" \".join(partial)\n","\n","    final_prompt = f\"\"\"\n","Combine the drafts into a 120–160 word summary.\n","Return ONLY JSON: {{\"generated_summary\":\"...\"}}\n","\n","DRAFTS:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\n","\"\"\"\n","    out2 = groq_call(final_prompt, 0.15)\n","    j2 = extract_json(out2)\n","    return j2.get(\"generated_summary\", \"\")\n","\n","# ------ TOPICS ------\n","def classify_topic(transcript, summary):\n","    text = summary + \" \" + transcript[:2000]\n","\n","    examples = \"\\n\".join(\n","        [f\"INPUT: {x['input']}\\nOUTPUT: {x['output']}\" for x in FEWSHOT_TOPICS]\n","    )\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Pick up to 3 topics from:\n","{', '.join(VALID_TOPICS)}\n","\n","Return JSON: {{\"predicted_topics\":[\"...\"]}}\n","\n","TEXT:\n","\\\"\\\"\\\"{text}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","    if isinstance(topics, str):\n","        topics = [topics]\n","\n","    cleaned = []\n","    for t in topics:\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","\n","    return list(dict.fromkeys(cleaned))[:3] or [\"Other\"]\n","\n","# ------ Q&A ------\n","def generate_qa(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([f\"Q:{x['q']}\\nA:{x['a']}\" for x in FEWSHOT_QA])\n","\n","    prompt = f\"\"\"\n","Learn QA from examples:\n","{examples}\n","\n","Return JSON: {{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}}]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.1)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    for qa in qas:\n","        lines.append(f\"Q: {qa.get('q','')}\")\n","        lines.append(f\"A: {qa.get('a','')}\")\n","    return \"\\n\".join(lines)\n","\n","# ------ CONCEPTS ------\n","def generate_concepts(transcript):\n","    first = chunk_text(transcript)[0]\n","    examples = \"\\n\".join([\", \".join(lst) for lst in FEWSHOT_CONCEPTS])\n","\n","    prompt = f\"\"\"\n","Learn from examples:\n","{examples}\n","\n","Extract 10–12 technical concepts.\n","Return JSON: {{\"key_concepts\":[\"...\"]}}\n","\n","Text:\n","\\\"\\\"\\\"{first}\\\"\\\"\\\"\n","\"\"\"\n","    out = groq_call(prompt, 0.15)\n","    j = extract_json(out)\n","    return \", \".join(j.get(\"key_concepts\", []))\n","\n","# ================================================================\n","# 9. MAIN PIPELINE\n","# ================================================================\n","\n","def run_pipeline():\n","    df = pd.read_excel(INPUT_FILE)\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        processed = set(old[\"row_index\"])\n","        results = old.to_dict(orient=\"records\")\n","        print(f\"Resuming: {len(processed)} rows already completed.\")\n","    else:\n","        processed = set()\n","        results = []\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            continue\n","\n","        title = str(row[\"title\"])\n","        transcript = str(row[\"transcript\"])\n","\n","        print(\"\\nProcessing:\", title)\n","\n","        summary = generate_summary(transcript)\n","        topics = classify_topic(transcript, summary)\n","        qa = generate_qa(transcript)\n","        concepts = generate_concepts(transcript)\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa,\n","            \"key_concepts\": concepts\n","        }\n","\n","        results.append(rec)\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","\n","    return pd.DataFrame(results)\n","\n","# ================================================================\n","# 10. RUN\n","# ================================================================\n","\n","df_out = run_pipeline()\n","print(\"Few-Shot pipeline completed successfully!\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-FMEBQEXLEqa","executionInfo":{"status":"ok","timestamp":1763996503327,"user_tz":-330,"elapsed":3112238,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"ae8eaf42-310d-40c8-b29a-deee501c248c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","\n","Processing: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses reinforcement learning through human feedback, using a grid world example where an algorithm, Frank, learns to navigate to a reward spot. Human feedback is introduced as a mentor to guide Frank's learning, accelerating the process and allowing for more human-favored responses. The rewards model is trained to assess the quality of chat GPT's answers and is used with proximal policy optimization to fine-tune chat GPT, enhancing its capabilities for generating high-quality responses.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with examples guiding behaviour.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: What does a machine learning engineer typically do in production?\n","A: They maintain ML pipelines.\n","Q: When are convolutions useful in NLP?\n","A: They extract local spatial features for text classification.\n","Q: Who typically works with DevOps teams in production?\n","A: Machine learning engineers.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","This tutorial covers example code for working with CVX opt and kernels in a Support Vector Machine (SVM), including visualization and impact analysis. It explains the initialization method, fitment, and prediction methods, and demonstrates the use of different kernels for classification tasks. The lecture also shows how to generate linearly separable and nonlinearly separable data, and how to use SVMs to classify these data sets. It includes code examples and visualizations to illustrate the concepts, and plans to cover SVM parameters in the next tutorial.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What is the purpose of CVX opt in machine learning?\n","A: It's useful for directly seeing the impact of a kernel and where it's being injected and modified in a support Vector machine.\n","Q: What is the main difference between CVX opt and lib svm?\n","A: CVX opt is not commonly used, while lib svm is a more widely used library for support Vector machines.\n","Q: What is the quadratic programming solver used in CVX opt?\n","A: It's a solver that minimizes 1/2 x^T P x + q^T x subject to constraints G x <= h and a x = b.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","The lecture introduces the concept of prompts in prompt engineering, explaining that they are inputs given to large language models to generate text outputs. It discusses different types of prompts, including question, statement, and multi-input prompts, and explores key features such as length, language, context, and constraints. The importance of defining the 'what' and 'how' of a prompt is emphasized, and examples are provided to illustrate how prompts can be used to generate accurate and efficient outputs. The process of deconstructing a prompt is also discussed to better understand its key features and constraints.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with examples guiding behaviour.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are convolutions useful in NLP?\n","A: They extract local spatial features for text classification.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What does a convolution allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: How do models learn in few-shot prompting?\n","A: They learn by maximising cumulative rewards through trial and error.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #3 - Agents & Tools - Intro\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","This lecture introduces AI agents as problem solvers that can make autonomous decisions. It explains the concept of tools, which are specific functions that agents use to complete tasks. The React Agent Pattern is discussed, which mimics human thinking by combining reasoning and acting. The goal is to build a basic React Agent using Lang Chain and then explore its drawbacks and the role of Lang Graph in solving the problems associated with it.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Prompt Engineering', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with example guidance.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are convolutions useful in natural language processing?\n","A: They extract local spatial features for text classification.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What does a convolution allow models to do in vision?\n","A: It lets models focus on the most relevant local spatial features in an image.\n","Q: How do models learn in few-shot prompting?\n","A: They learn by following example guidance with limited task-specific data.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","The speaker plans to analyze the reflection agent system to understand its functionality and how it produces a refined viral tweet. They demonstrate using LangChain with Llama to generate a viral tweet, creating an API key, setting environment variables, and running the project. The speaker then uses Llama to generate a tweet and employs a reflection agent to critique and refine it. They explain how the reflection agent works, enabling deep thinking and generating a final output. Additionally, they mention exploring another type of reflection agent, the Reflexion agent, in the next section.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning\n","\n","============================================\n","\n","Processing: LangChain Crash Course #7 - Chat Models - Setup\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","This tutorial covers using LangChain's chat models with the Open AI API. It explains how to install the required package, import the chat model, and initialize the model using the GPT-4 model. The speaker demonstrates how to use the chat model to interact with Open AI's APIs, store and print responses, and extract relevant content. They also cover accessing the API using an API key, loading the key from an EnV file, and topping up the Open AI balance if necessary. Additionally, the speaker shows how to pass a conversation history to an LLM to improve its responses by providing context and awareness of previous interactions.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What is the purpose of installing the LLM model in a project?\n","A: To enable the use of the model for tasks such as chat and language processing.\n","Q: What is the name of the latest model used in the example?\n","A: GBT 40\n","Q: Why might using the latest model be expensive?\n","A: Because they are the most advanced models.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Python Training Course - Python Sort List\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","The video demonstrates Python's sort method handling lists with strings and numbers, prioritizing numbers over strings and maintaining alphabetical order within each type. It also explores inserting a number into a list of strings and numbers, and how the sort method handles this combination.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does the sort method do in Python when sorting a list of strings?\n","A: It puts the words that have a capital letter first and then sorts them alphabetically, followed by the ones with the lowercase first letter alphabetically.\n","Q: What happens when you try to sort a list that contains both strings and numbers in Python?\n","A: The numbers are put first and the strings, it's just how it works.\n","Q: How does the sort method handle lists that contain both strings and numbers in Python?\n","A: It puts the numbers first and the strings.\n","Q: What is the effect of using the reverse method on a list that has been sorted in Python?\n","A: It reverses the order of the list, so the numbers are at the end and the strings are at the start.\n","Q: What is the purpose of the insert method in Python when working with lists?\n","A: It inserts a specified value at a specified position in the list.\n","Q: What happens when you try to insert a number into a list that contains strings in Python?\n","A: The number is inserted into the list, and the list can handle both strings and numbers.\n","\n","KEY CONCEPTS:\n","\n","Python, List, Strings, Sorting, Uppercase, Lowercase, Alphabetical Order, Reverse Order, Insertion, Indexing\n","\n","============================================\n","\n","Processing: \n","Humans vs. AI: Who should make the decision?\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","The speaker discusses the benefits of combining human and artificial intelligence (AI) decision-making, known as augmented intelligence. AI can alleviate workload in fraud detection systems, but its performance is limited when unsure. Humans perform better when AI is unsure, bringing in additional information and context. However, humans may favor AI suggestions over their own judgment due to automation bias. To mitigate this, optional AI recommendations can be displayed, allowing humans to consider cases independently. Transparency about AI accuracy can actually decrease human trust, highlighting the need for careful presentation of information to maximize the benefits of augmented intelligence.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does a typical AI performance curve look like?\n","A: It has very low confidence scores correlated to a high success rate, and very high confidence scores also correlated to a high success rate.\n","Q: Why do human performance curves tend to be flatter than AI performance curves?\n","A: Because humans are often not quite as accurate as a very confident AI algorithm, but can do a better job when the AI is unsure.\n","Q: What happens when an AI is certain of itself?\n","A: It's highly performant and beats out humans who can lose consistency and focus and attention.\n","Q: What happens when an AI is unsure?\n","A: Humans can outperform an AI prediction by bringing in additional information and context.\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: \n","Build generative apps faster with Vertex AI\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","Demitrius Case, a Google Cloud AI product manager, discusses six new Vertex AI APIs designed to help developers build generative applications for Enterprises. These APIs address technical challenges such as document understanding, embedding, and fact-checking, with a focus on quality and ease of integration. The goal is to make it easier for developers to build reliable and accurate applications, leveraging Google's knowledge and expertise in AI.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Unitary Transformations\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses the singular value decomposition (SVD) of a matrix X, focusing on unitary matrices U and V. It explains how they preserve vector angles and lengths, and can be thought of as rotations of the vector space. The SVD is also given a geometric interpretation, where matrix X maps a sphere of unit length vectors to an ellipsoid in the column space of X, with singular values determining the ellipsoid's elongation and left singular vectors determining its orientation. The SVD is a powerful tool for understanding data geometry and simplifying complex data by rotating it into a new coordinate system.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with examples guiding behaviour.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: What does a machine learning engineer typically do in production?\n","A: They maintain ML pipelines.\n","Q: When are convolutions useful in NLP?\n","A: They extract local spatial features for text classification.\n","Q: Who typically works with DevOps teams in production?\n","A: Machine learning engineers.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","This video introduces Google Gemini Pro 1.5, a multi-model that can work with text and images, and demonstrates building a generative AI-powered application using it. The speaker showcases its capabilities, including processing large PDFs, extracting comedic moments, and generating responses to questions. They explain how to create an API key, use the Google Generative AI library, and configure the Gen AI model. The speaker plans to use this model for end-to-end projects and shares the code in upcoming videos. The model can handle large amounts of text, generate text from images, and write short engaging blog posts. The speaker discusses prompt feedback options, token size limitations, and the potential for future developments and competition in the LLM space.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What is the purpose of a demo video in a tutorial?\n","A: It gives an idea of what the application or feature can do.\n","Q: What is a multi-model?\n","A: It is a model that can work with both text and images.\n","Q: What is the name of the newest model mentioned in the text?\n","A: Gemini\n","\n","KEY CONCEPTS:\n","\n","Generative AI, Google Gemini Pro, Multi-model, Text and Images, API Key, Long Context Understanding, Experimental Feature, Model, Application, End-to-End Projects, Hands-On Application, Code\n","\n","============================================\n","\n","Processing: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","The transcript discusses evaluating and testing prompt engineering models, including metrics such as perplexity, accuracy, and human evaluation. It provides an example of evaluating a large language model using accuracy and perplexity checks, emphasizing the importance of debugging and improving models by analyzing generated responses and testing on different data sets or tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with example guidance.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are convolutions useful in NLP?\n","A: They extract local spatial features for text classification.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What does a convolution allow models to do in vision?\n","A: It lets models focus on the most relevant local spatial features in an image.\n","Q: How do models learn in few-shot prompting?\n","A: They learn by following example guidance with limited task-specific data.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Generative AI vs AI agents vs Agentic AI\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","The speaker explains the difference between generative AI, AI agents, and agentic AI. Generative AI creates new content based on patterns, while AI agents complete tasks with input and action. Agentic AI involves autonomous AI agents working together to achieve goals, handling complex tasks like multi-step goals, planning, and decision making.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Generative AI', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: \n","Covariance in Statistics\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","The lecture explains the concept of covariance, a measure of the relationship between two random variables. It discusses how covariance can be used to quantify the relationship between variables and its limitations, including its inability to indicate the strength of the relationship. The lecture introduces the Pearson correlation coefficient as a technique to overcome these limitations, providing a more comprehensive understanding of the relationship between variables.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: 3. Objective || End to End AI Tutorial\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","This video explains the objective of a reinforcement learning problem, which is to learn an optimal policy that maximizes a numerical reward signal within a time constraint. The agent interacts with an environment, makes decisions based on observations, and learns to take actions that give maximum reward or cumulative reward over time. The video discusses how to define objectives in real-time using examples like Tic-Tac-Toe and Stock Market trading, and how to parameterize the agent's objective using a reward function that reflects the trading goal and preferences.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Python Training - Python Dictionary Basics\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","A dictionary in Python is a data type consisting of key-value pairs. Each key must be immutable, such as a string or number, and the value can be any data type. Dictionaries are useful for mapping one item to another and have various functions associated with them. They can be created using the dict function and manipulated using methods such as accessing, changing, and deleting key-value pairs.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with example guidance.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are convolutions useful in NLP?\n","A: They extract local spatial features for text classification.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What does a convolution allow models to do in vision?\n","A: It lets models focus on the most relevant local spatial features in an image.\n","Q: How do models learn in few-shot prompting?\n","A: They learn by following example guidance with limited task-specific data.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: \n","Fight Insider Threats with AI-infused SIEM\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","This lecture explores how user behavior analytics with AI and machine learning can help detect and respond to Insider threats quickly and precisely. It introduces UBA, its role in identifying and containing Insider threats, and how it integrates with SIEM solutions for actionable insights. The talk demonstrates how UBA streamlines processes, enhances skills, and provides natural language insights and visualizations. Human feedback is also highlighted as crucial for improving analysis and future responses of UBA, potentially reducing the average time to identify and contain a data breach from 108 days.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What does AI have the potential to do for Security Professionals?\n","A: It can help them stay ahead of emerging threats and improve their organization's security posture.\n","Q: What was a key finding for organizations that extensively used AI and automation versus those that didn't?\n","A: Faster containment of data breaches.\n","Q: What can user Behavior analytics (UBA) with AI and machine learning help detect and respond to?\n","A: Insider threats.\n","Q: What is a major concern for organizations of all sizes?\n","A: Insider threats.\n","Q: What was the average cost of an Insider threat for an organization?\n","A: $4 million.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","Krishak welcomes viewers to their YouTube channel at 2 a.m. and discusses Meta's Llama 3, an open-source large language model surpassing its predecessor in performance metrics. Trained on 50 trillion tokens, Llama 3 supports 8K context length and shows improved accuracy in benchmarks, outperforming paid and open-source models. The speaker also discusses responsible AI development, including determining use cases and improving systems. Llama 3 is available on Meta, Hugging Face, and Kaggle, with instructions provided on how to access and download the model for local inference.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What is the current time mentioned in the given text?\n","A: 2 a.m.\n","Q: What is the name of the person in the given text?\n","A: Krishak\n","Q: What is the purpose of the YouTube channel mentioned in the given text?\n","A: Welcoming viewers\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Getting Started With sklearn\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","The speaker explains the process of writing Python code for a decision boundary using scikit-learn's Naive Bayes algorithm, guiding the viewer through finding documentation and examples on Google for the library and algorithm.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: \n","Log Normal Distribution in Statistics\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses statistical distributions, including Gaussian and log normal distributions, explaining how data often follows a bell curve and providing examples of income and product reviews that follow log normal distributions. The speaker emphasizes the importance of understanding these distributions to increase model accuracy by scaling down data values to the same scale. Techniques such as standard scaling and log normalization are introduced, and viewers are encouraged to subscribe to the channel for more statistical concepts.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","This video series is an end-to-end deep learning project in agriculture, focusing on potato disease detection using a mobile application. The project aims to identify early blight and late blight in potato plants using deep learning and convolutional neural networks. The lecture covers data collection, pre-processing, and model building using TensorFlow, CNN, and data augmentation. It also touches on ML Ops concepts, website and mobile app development, and deploying a TensorFlow Lite model to Google Cloud. The speaker provides prerequisites, recommendations, and suggests working on end-to-end projects to enhance skills as machine learning engineers or data scientists.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","\n","Processing: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses the levels of autonomy in LLM applications, from zero autonomy (code) to maximum autonomy, highlighting their limitations. It introduces chains, routers, and state machines, explaining how they can be used to create more intelligent and autonomous AI agents. State machines combine previous levels of autonomy with loops and the ability to refine tasks, making them more complex and intelligent than chains and routers. The lecture also touches on the difference between human-driven and agent-executed systems, with state machines being considered more intelligent and autonomous.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with example guidance.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are convolutions useful in natural language processing?\n","A: They extract local spatial features for text classification.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What does a convolution allow models to do in vision?\n","A: It lets models focus on the most relevant local spatial features in an image.\n","Q: How do models learn in few-shot prompting?\n","A: They learn by following example guidance with limited task-specific data.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","This lecture covers advanced topics in prompt engineering, including handling different types of prompts and fine-tuning pre-trained large language models. It discusses best practices for data preprocessing and cleaning, deploying models in production, and the ethical considerations of prompt engineering. The lecture also provides practical exercises for advanced prompt engineering models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with examples guiding behaviour.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are convolutions useful in natural language processing?\n","A: They extract local spatial features for text classification.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What does a convolution allow models to do in vision?\n","A: It lets models focus on the most relevant tokens in an image.\n","Q: How do models learn in few-shot prompting?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are attention mechanisms useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: Who typically uses reinforcement learning?\n","A: Agents learn by maximising cumulative rewards through trial and error.\n","Q: What does a reinforcement learning agent do?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are attention mechanisms useful in natural language processing?\n","A: They extract local spatial features for text classification.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: SVD: Eigen Action Heros [Matlab]\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","The lecture discusses eigenfaces for image classification, using a dataset of Arnold Schwarzenegger and Sylvester Stallone images. The speaker computes the average face, subtracts it from each image, and then applies Singular Value Decomposition (SVD) to create eigenfaces. These eigenfaces are linear combinations of the training images. The speaker projects the images into the first three eigenfaces, separating them into clusters. The process is repeated with Taylor Swift and Stallone images, showing improved separation. The lecture concludes by discussing the limitations of eigenfaces and how Facebook improved face classification accuracy using 3D geometry.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: LangChain Crash Course #3 - What is LangChain?\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","Lang chain is a framework that enables applications to leverage large language models (LLMs) while interacting with external APIs, databases, and more. It allows seamless switching between different LLMs and access to various APIs for tasks like booking flights and restaurants. Lang chain can also access private databases, send emails, browse the web, and scrape websites, giving it the ability to act in the real world.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering', 'Mlops']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What is the main limitation of large language models?\n","A: They are smart and can talk about a topic but cannot actually interact with the real world.\n","Q: What is the purpose of Lang chain?\n","A: It acts as a bridge between the llms and the real world, allowing the AI to interact with APIs, databases, and other external systems.\n","Q: What can Lang chain do?\n","A: It can access a lot of APIs, such as flight and restaurant booking APIs, and can send emails, among other things.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking, LLM (Large Language Model), APIs (Application Programming Interfaces), Lang Chain\n","\n","============================================\n","\n","Processing: How To Use Residuals For Time Series Forecasting\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","This video discusses residual analysis in time series forecasting, covering what residuals are, how to perform residual analysis, and how to use Python to implement this analysis. It explains the difference between residuals and errors, and how to use residual analysis to detect trends or inconsistencies in a model. The speaker also discusses diagnosing issues with a forecasting model by analyzing residuals, checking for autocorrelation and bias, and emphasizes the importance of understanding residuals to improve the forecasting model.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Data Science', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","This video teaches how to build an AI agent that interacts with a database using SQL knowledge. It uses LangGraph, Next.js, and WatsonX AI to create a ReAct agent and a frontend application. The agent connects to an in-memory database using SQLite and allows users to submit questions to a large language model. The speaker explains how to implement a ReAct agent in VS Code, set up a WatsonX AI project, and create a frontend application for interacting with a large language model. They also cover setting up a database using SQLite 3, creating SQL queries, and preparing for the large language model to access the database schema. The speaker demonstrates how to build a TXS SQL agent, use a large language model to generate SQL queries, and view the generated queries in VS Code.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","This course introduces prompt engineering, a field in NLP that focuses on generating high-quality text outputs in response to prompts. It covers the basics of prompt analysis, benefits, and limitations, and provides an overview of the core structure and content of prompt engineering models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What does attention allow models to do?\n","A: It lets models focus on the most relevant tokens in a sequence.\n","Q: Why are convolutions useful in vision?\n","A: They extract local spatial features for image classification.\n","Q: How do agents learn in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When is few-shot prompting effective?\n","A: When limited task-specific data exists but examples guide behaviour.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Q-learning - Explained!\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","This lecture introduces Q-learning, a value-based reinforcement learning method, explaining how it determines a value function to maximize total rewards. Q-learning uses a grid world example to illustrate its goal of learning an optimal policy to maximize rewards. The method calculates the temporal difference error by comparing observed and expected Q-values, updating the Q-table based on a gradient update rule with a learning rate. Q-learning is an off-policy reinforcement learning method where an agent learns by updating a Q-table based on the temporal difference error, eventually using Q-values to determine the policy for achieving a target reward.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What does few-shot prompting allow models to do?\n","A: It lets models learn from limited task-specific data with examples guiding behaviour.\n","Q: Why are agents useful in reinforcement learning?\n","A: They learn by maximising cumulative rewards through trial and error.\n","Q: When are convolutions useful in NLP?\n","A: They extract local spatial features for text classification.\n","Q: Who typically maintains ML pipelines in production?\n","A: Machine learning engineers and DevOps teams.\n","Q: What does a convolution allow models to do in vision?\n","A: It lets models focus on the most relevant local spatial features in an image.\n","Q: How do models learn in few-shot prompting?\n","A: They learn by maximising cumulative rewards through trial and error with examples guiding behaviour.\n","\n","KEY CONCEPTS:\n","\n","Self-Attention Mechanism, Query-Key-Value, Positional Encoding, Convolutional Layer, Pooling Operation, Feature Map, Reward Function, Policy Gradient, Q-Learning, Few-Shot Prompting, Chain-of-Thought Reasoning, Instruction Tuning, CI/CD Pipeline, Model Registry, Experiment Tracking\n","\n","============================================\n","\n","Processing: Training Your Logistic Classifier\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","This lecture introduces the logistic classifier, a linear classifier that applies a linear function to input data to generate predictions. The model's weights and bias are trained to perform predictions, and the scores are turned into probabilities using a softmax function.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Machine Learning', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Logistic Classifier, Linear Classifier, Matrix Multiply, Machine Learning, Softmax Function, Logits, Probabilities, Classification, Training Model, Weights and Bias\n","\n","============================================\n","Few-Shot pipeline completed successfully!\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zF7mTeF8aEKf","executionInfo":{"status":"ok","timestamp":1763996885711,"user_tz":-330,"elapsed":5823,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"d950d7b1-6f0e-4192-c759-324550ed7198"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_fewshot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-3.1-8b-instant/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["aa2b2604650f4dbf82d8c9a93580b2eb","e99a58bbbca941fd87c26aad3d03f4d0","bc377ca73ea044ddb625439bc8f83eb4","dbd67e1bb6e2444a80e4a942406667de","0c289896234448eeb0b72dbfe2ef4138","2023c27a020d461fb5e5b2577c7cbcce","c6bfc9b413f34442aeaa97b653a6a076","f6c778a54b394fd89dd4d08d59683946","ea656fda17044e608506da304dbd842f","86aba703ec234aada4e413e13089663b","eadcb89defa94802aaee4999d3787d43","a23ad9af34b74f4c9b07f90286799885","e30776cb081d4f08abe9aee9fa1369ac","8be7b902230746809d6f7239aacd44b9","02c86744310641c7b48a6d4e60348b8e","86c4e0d3fafd42f984715deb3c07c729","5a40c81949c1410e881dd28d60e10f17","ae8e7aef400f4ef289299090dda52b17","c35ed830f7d244438a52933af74b8b1b","c6c516c3792e428aa3795363ab8d66d5","84efcfbbac854f40801c2d0b28bb19cb","6685483b3ad744a18ee8bb824858a633","044675e6622e4051b78d13a471512592","db18d86e90e448af8d5e5de9c1eaac8b","9ce62ea377734130a252c78f05154139","dde539958ef9446cb366a81b3b1734e4","9e51445189604364a170f3328a91766a","7bd4ae0762e74255b6c60ea5b9d8457e","141c40d0e0ff4686994f0f7e5dbc6769","75939cfbd04f47bcb17a5ae934e37bfe","658669fc1c74421c87c622a67f06cd17","08156e823f314338a9d4d93457b90702","949b545a4b5948b08c324d3b7c953552","6e10d03f52324dc8b3fea34fcdffb44e","952f5f0985e741b8b2f694a200183248","58b02450c8c34e5ab56097a222db2555","a2483b4e2d3543968eb2be1721d3c5ce","bbfc1fb0af2e4f70813eaaec0adc152d","f29ca3fc85b445af98ebbb86a1aa9e6c","e1451054a0934eb483ddd48cdc34fe79","672192d0eed244df9906adcce7d8adf1","4a48816c1f9546ce8ce393850bb0b39d","cfc4be70210448f9b8923f093446cb8b","d724062527654d19bb54c5a40c7b979b","addc51d83cf641c786694eae9caae328","71648ad79eec48dabc12576d9a58f920","05538824abd04898a5cac3df50c36d4b","204b8f33565d4af69da9c5985d70966e","f119de49d35b48ef8e96babaac89b755","67ac2340424b4b61950193d3ae76440a","700ceea2b78e458b9949f9aa110c0032","eb657af8bfe5460cba8f1e57af913d59","956bc6dc873e478a87903bc2b2088ca6","c319a59ee6f74d54a9e4f57e9251c196","313f078eb4c7448f9a592b481760f191","313ef6e387a8438388e52741db990f87","fbc04eb8b1194ac5ae6236bf6ed51856","4deaad1c49e24a98976605c00bf9a543","980e0e38f6064da4a6e3cec7ee5d296e","72ff4663cbbf48e49952f04b11ce15ec","e6e2952b03fb4c2582e44fb626aeaba5","81dba03ce0fd499caefba0a50bedc15a","f85c0609530b4fe58d14a36ce4f0e439","d936323bb5f94c38a6e4ad72eacc78f2","d6ab3f7c6ae3440ca19b90fbe717305a","ef115c5fe48143f1b198464a841c3faf"]},"id":"nXtEEiWsLEwh","executionInfo":{"status":"ok","timestamp":1763997017418,"user_tz":-330,"elapsed":131705,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"eb53eb2a-1e4c-4556-a0df-3468b0ab0cc0"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_fewshot_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2b2604650f4dbf82d8c9a93580b2eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a23ad9af34b74f4c9b07f90286799885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044675e6622e4051b78d13a471512592"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e10d03f52324dc8b3fea34fcdffb44e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"addc51d83cf641c786694eae9caae328"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313ef6e387a8438388e52741db990f87"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2648\n","  - BLEU: 0.0415\n","  - BERTScore F1: 0.8809\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.8000\n","  - Jaccard Index: 0.3387\n","  - Micro F1: 0.4425\n","  - Macro F1: 0.3851\n","  - Weighted F1: 0.3963\n","\n","Q&A Generation:\n","  - BLEU: 0.0285\n","  - Diversity: 0.7974\n","  - Answerability: 0.1975\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.1600\n","  - Recall@10: 0.0640\n","  - F1@10: 0.0914\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/FewShot Prompting/llama-3.1-8b-instant/evaluation_final.json\n"]}]}]}