{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMAXitSY7Wa3fe0xSOrL+5c"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eKFOvtS_oDEP"},"outputs":[],"source":["import math\n","from typing import Dict, Tuple, List\n","import numpy as np\n","import pandas as pd\n","\n","# ----------------------------\n","# I/O CONFIG\n","# ----------------------------\n","INPUT_CSV = \"D:/Sarah_Professional/LJMU Research/Dataset/updated_Excel.xlsx\"       # <-- change me\n","OUT_EXP   = \"D:/Sarah_Professional/LJMU Research/Output/experiment_set_100.csv\"\n","OUT_HOLD  = \"D:/Sarah_Professional/LJMU Research/Output/holdout_set_50.csv\"\n","\n","# ----------------------------\n","# SAMPLING CONFIG\n","# ----------------------------\n","RANDOM_SEED = 42\n","EXPERIMENT_TOTAL = 100\n","HOLDOUT_TOTAL = 50\n","MIN_PER_TOPIC_EXPERIMENT = 6   # with 15 topics → feasible (100/15 ≈ 6.67)\n","MIN_PER_TOPIC_HOLDOUT = 3      # with 15 topics → feasible (50/15 ≈ 3.33)\n","\n","# Enforced (canonical) 15 topics — adjust to YOUR final list of 15\n","ENFORCED_TOPICS = [\n","    \"Artificial Intelligence\", \"Machine Learning\", \"Deep Learning\",\n","    \"Natural Language Processing\", \"Generative AI\", \"Agentic AI\",\n","    \"Data Science\", \"Python Programming\", \"Reinforcement Learning\",\n","    \"Time Series\", \"Mlops\", \"Langchain\", \"Langraph\",\n","    \"Statistics\", \"Prompt Engineering\"\n","]\n","# Note: anything unmapped can go to \"Other\" (not enforced for minima)\n","\n","# ----------------------------\n","# PLAYLIST → TOPIC MAPPING\n","# 1) Exact-name dictionary (fast path)\n","# 2) Fallback keyword rules (best-effort)\n","# ----------------------------\n","PLAYLIST_TO_TOPIC: Dict[str, str] = {\n","    # ==== EXAMPLES — extend/replace with your real playlists ====\n","    # Artificial Intelligence\n","    \"AI Fundamentals\": \"Artificial Intelligence\",\n","    \"AI Technical Tutorials\": \"Artificial Intelligence\",\n","\n","    # Machine Learning\n","    \"AI and Machine Learning with Google Cloud\": \"AI and ML\",\n","    \"Intro to Machine Learning\": \"Machine Learning\",\n","    \"Machine Learning with Python\": \"Machine Learning\",\n","\n","    # Deep Learning\n","    \"Deep Learning | Udacity\": \"Deep Learning\",\n","    \"Deep Learning With Tensorflow 2.0, Keras and Python\": \"Deep Learning\",\n","\n","    # Natural Language Processing\n","    \"NLP Tutorial Python\": \"Natural Language Processing\",\n","    \"Natural Language Processing\": \"Natural Language Processing\",\n","\n","    # Generative AI\n","    \"Generative AI\": \"Generative AI\",\n","    \"Gen AI Tutorials\": \"Generative AI\",\n","\n","    # Agentic AI\n","    \"Agentic AI\": \"Agentic AI\",\n","\n","    # Data Science\n","    \"Intro to Data Science\": \"Data Science\",\n","\n","    # Python Programming\n","    \"Python Training - Complete Python Training Course\": \"Python Programming\",\n","\n","    # Reinforcement Learning\n","    \"Reinforcement Learning 101\": \"Reinforcement Learning\",\n","\n","    # Time Series\n","    \"Time Series Crash Course\": \"Time Series\",\n","\n","    # MLOps\n","    \"Machine Learning Engineering for Production(Mlops)\": \"Mlops\",\n","\n","    # LangChain\n","    \"Updated Langchain\": \"Langchain\",\n","\n","    # Langraph\n","    \"LangGraph Crash Course: From Basic to Building Powerful Agents | 2025\": \"Langraph\",\n","\n","    # Statistics\n","    \"Statistics in Machine learning\": \"Statistics\",\n","\n","    # Prompt Engineering\n","    \"Prompt Engineering Full Course with LLM\": \"Prompt Engineering\",\n","}\n","\n","# Fallback keyword rules (case-insensitive) if a playlist_name\n","# isn't in PLAYLIST_TO_TOPIC. Add patterns as you like.\n","KEYWORD_RULES: List[Tuple[str, str]] = [\n","    (\"\\\\bAI\\\\b|artificial intelligence\", \"Artificial Intelligence\"),\n","    (\"\\\\bML\\\\b|machine learning\", \"Machine Learning\"),\n","    (\"deep learning|neural network|cnn|rnn|transformer\", \"Deep Learning\"),\n","    (\"\\\\bNLP\\\\b|natural language processing|text mining\", \"Natural Language Processing\"),\n","    (\"generative ai|genai|diffusion|llm\", \"Generative AI\"),\n","    (\"agentic ai|multi-agent|autonomous agent|swarm\", \"Agentic AI\"),\n","    (\"data science|data analysis|eda\", \"Data Science\"),\n","    (\"python\", \"Python Programming\"),\n","    (\"reinforcement learning|q-learning|policy gradient|rl\", \"Reinforcement Learning\"),\n","    (\"time series|temporal|forecast\", \"Time Series\"),\n","    (\"mlops|ml ops|deployment|monitoring|model ops\", \"Mlops\"),\n","    (\"langchain\", \"Langchain\"),\n","    (\"langraph\", \"Langraph\"),\n","    (\"statistics|probability|bayes|hypothesis testing\", \"Statistics\"),\n","    (\"prompt engineering|prompting\", \"Prompt Engineering\"),\n","]\n","\n","import re\n","def infer_topic_from_playlist(playlist_name: str) -> str:\n","    \"\"\"Map playlist_name → topic using dict first, then keyword rules.\"\"\"\n","    if not isinstance(playlist_name, str) or not playlist_name.strip():\n","        return \"Other\"\n","    # Exact map (case sensitive on key); try case-normalization too\n","    if playlist_name in PLAYLIST_TO_TOPIC:\n","        return PLAYLIST_TO_TOPIC[playlist_name]\n","    # try normalized lookup\n","    norm_key = playlist_name.strip()\n","    if norm_key in PLAYLIST_TO_TOPIC:\n","        return PLAYLIST_TO_TOPIC[norm_key]\n","    # Keyword rules\n","    lower = playlist_name.lower()\n","    for pattern, topic in KEYWORD_RULES:\n","        if re.search(pattern, lower):\n","            return topic\n","    return \"Other\"\n","\n","# ----------------------------\n","# LENGTH BINNING\n","# ----------------------------\n","def length_bin(num_tokens: int) -> str:\n","    if num_tokens < 1000: return \"Short\"\n","    if num_tokens <= 3000: return \"Medium\"\n","    return \"Long\"\n","\n","def approx_tokens(text: str) -> int:\n","    if not isinstance(text, str): return 0\n","    return len(text.split())\n","\n","def ensure_columns(df: pd.DataFrame) -> pd.DataFrame:\n","    df = df.copy()\n","    # topic from playlist_name\n","    if \"topic\" not in df.columns:\n","        if \"playlist_name\" not in df.columns:\n","            raise ValueError(\"DataFrame must have 'topic' or 'playlist_name'.\")\n","        df[\"topic\"] = df[\"playlist_name\"].apply(infer_topic_from_playlist)\n","\n","    # tokens\n","    if \"num_tokens\" not in df.columns:\n","        if \"transcript\" not in df.columns:\n","            raise ValueError(\"Need 'num_tokens' or 'transcript' to compute length bins.\")\n","        df[\"num_tokens\"] = df[\"transcript\"].apply(approx_tokens)\n","\n","    df[\"length_bin\"] = df[\"num_tokens\"].apply(length_bin)\n","    return df\n","\n","# ----------------------------\n","# ALLOCATION HELPERS\n","# ----------------------------\n","def largest_remainder_allocation(capacity: Dict[str, int], totalsize: int, seed: int) -> Dict[str, int]:\n","    \"\"\"Proportional allocation with caps + largest remainder tie-break.\"\"\"\n","    sizes = {k: max(0, int(v)) for k, v in capacity.items()}\n","    N = sum(sizes.values())\n","    if totalsize > N:\n","        raise ValueError(f\"Requested {totalsize} but only {N} available across groups.\")\n","    if N == 0 or totalsize <= 0:\n","        return {k: 0 for k in sizes}\n","\n","    reals = {k: (totalsize * sizes[k] / N) for k in sizes}\n","    floors = {k: min(sizes[k], int(math.floor(x))) for k, x in reals.items()}\n","    picked = sum(floors.values())\n","    remaining = totalsize - picked\n","\n","    rng = np.random.default_rng(seed)\n","    keys = list(sizes.keys())\n","    rng.shuffle(keys)\n","    keys.sort(key=lambda k: (reals[k] - math.floor(reals[k])), reverse=True)\n","\n","    caps = {k: sizes[k] - floors[k] for k in sizes}\n","    for k in keys:\n","        if remaining == 0: break\n","        if caps[k] > 0:\n","            floors[k] += 1\n","            caps[k] -= 1\n","            remaining -= 1\n","\n","    assert sum(floors.values()) == totalsize\n","    return floors\n","\n","def allocate_per_topic(\n","    df: pd.DataFrame,\n","    total: int,\n","    min_per_topic: int,\n","    seed: int,\n","    enforced_topics: List[str]\n",") -> Dict[str, int]:\n","    \"\"\"\n","    Give each ENFORCED topic at least min_per_topic (if possible),\n","    then distribute the remainder proportionally across ALL topics with capacity\n","    (including 'Other', if present).\n","    \"\"\"\n","    topic_sizes = df.groupby(\"topic\").size().to_dict()\n","\n","    # base minima only for enforced topics\n","    base = {t: 0 for t in topic_sizes}\n","    for t in topic_sizes:\n","        if t in enforced_topics:\n","            base[t] = min(min_per_topic, topic_sizes[t])\n","\n","    base_sum = sum(base.values())\n","    if base_sum > total:\n","        raise ValueError(f\"Sum of minima ({base_sum}) exceeds total ({total}). \"\n","                         f\"Lower min_per_topic or increase total.\")\n","\n","    # Remaining capacity per topic after base\n","    capacity = {t: max(0, topic_sizes[t] - base[t]) for t in topic_sizes}\n","    remainder = total - base_sum\n","    extra = largest_remainder_allocation(capacity, remainder, seed)\n","    quotas = {t: base.get(t, 0) + extra.get(t, 0) for t in topic_sizes}\n","    return quotas\n","\n","def allocate_within_topic_bins(topic_df: pd.DataFrame, quota: int, seed: int) -> Dict[Tuple[str,str], int]:\n","    \"\"\"\n","    Split a topic's quota across Short/Medium/Long.\n","    - If quota >= #non-empty bins → give each non-empty bin at least 1, then distribute remainder by capacity.\n","    - Else → allocate to largest bins first.\n","    \"\"\"\n","    topic = topic_df[\"topic\"].iloc[0]\n","    bin_sizes = topic_df.groupby(\"length_bin\").size().to_dict()\n","    nonempty = [b for b, n in bin_sizes.items() if n > 0]\n","    if quota <= 0 or len(nonempty) == 0:\n","        return {(topic, b): 0 for b in bin_sizes}\n","\n","    base = {b: 0 for b in bin_sizes}\n","    if quota >= len(nonempty):\n","        for b in nonempty: base[b] = 1\n","        remaining = quota - len(nonempty)\n","    else:\n","        # fewer quota than bins → fill biggest first\n","        largest = sorted(nonempty, key=lambda b: bin_sizes[b], reverse=True)\n","        for b in largest[:quota]: base[b] = 1\n","        remaining = 0\n","\n","    if remaining > 0:\n","        capacity = {b: max(0, bin_sizes[b] - base[b]) for b in bin_sizes}\n","        extra = largest_remainder_allocation(capacity, remaining, seed)\n","        for b in extra:\n","            base[b] += extra[b]\n","\n","    # respect bin caps\n","    for b in base:\n","        base[b] = min(base[b], bin_sizes[b])\n","\n","    return {(topic, b): base[b] for b in bin_sizes}\n","\n","# ----------------------------\n","# SAMPLING\n","# ----------------------------\n","def stratified_sample_2stage(\n","    df: pd.DataFrame,\n","    total: int,\n","    min_per_topic: int,\n","    seed: int,\n","    enforced_topics: List[str]\n",") -> pd.DataFrame:\n","    \"\"\"\n","    Stage 1: per-topic quotas (minima for enforced_topics).\n","    Stage 2: within-topic, split quota across length bins.\n","    Then sample per (topic, length_bin).\n","    \"\"\"\n","    df = ensure_columns(df)\n","\n","    topic_quotas = allocate_per_topic(\n","        df, total=total, min_per_topic=min_per_topic, seed=seed, enforced_topics=enforced_topics\n","    )\n","\n","    bin_targets: Dict[Tuple[str, str], int] = {}\n","    for topic, q in topic_quotas.items():\n","        tdf = df[df[\"topic\"] == topic]\n","        within = allocate_within_topic_bins(tdf, q, seed+hash(topic) % 100000)\n","        bin_targets.update(within)\n","\n","    rng = np.random.default_rng(seed)\n","    parts = []\n","    grouped = df.groupby([\"topic\", \"length_bin\"], dropna=False)\n","    for (topic, b), n in bin_targets.items():\n","        if n <= 0: continue\n","        if (topic, b) not in grouped.groups:  # empty stratum\n","            continue\n","        stratum = grouped.get_group((topic, b))\n","        if n >= len(stratum):\n","            parts.append(stratum)\n","        else:\n","            parts.append(stratum.sample(n=n, random_state=int(rng.integers(0, 2**31-1))))\n","\n","    if not parts:\n","        return df.iloc[0:0].copy()\n","    out = pd.concat(parts, axis=0).sample(frac=1.0, random_state=seed)  # shuffle\n","    assert len(out) == sum(topic_quotas.values()) == total, \"Sampling mismatch.\"\n","    return out\n","\n","def build_experiment_and_holdout(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    1) Pick 100 with ≥6 per enforced topic, split across bins.\n","    2) From the remaining pool, pick 50 with ≥3 per enforced topic, split across bins.\n","    \"\"\"\n","    df = ensure_columns(df)\n","\n","    if len(df) < (EXPERIMENT_TOTAL + HOLDOUT_TOTAL):\n","        raise ValueError(f\"Need at least {EXPERIMENT_TOTAL + HOLDOUT_TOTAL} rows; got {len(df)}.\")\n","\n","    exp_df = stratified_sample_2stage(\n","        df, total=EXPERIMENT_TOTAL, min_per_topic=MIN_PER_TOPIC_EXPERIMENT,\n","        seed=RANDOM_SEED, enforced_topics=ENFORCED_TOPICS\n","    )\n","    remaining = df.drop(exp_df.index)\n","    hold_df = stratified_sample_2stage(\n","        remaining, total=HOLDOUT_TOTAL, min_per_topic=MIN_PER_TOPIC_HOLDOUT,\n","        seed=RANDOM_SEED+1, enforced_topics=ENFORCED_TOPICS\n","    )\n","    return exp_df, hold_df\n","\n","# ----------------------------\n","# MAIN: read input, run, write output\n","# ----------------------------\n","if __name__ == \"__main__\":\n","    df = pd.read_excel(INPUT_CSV)\n","\n","    # Create 'topic' from 'playlist_name' if needed (mapping + keyword fallback)\n","    if \"topic\" not in df.columns and \"playlist_name\" in df.columns:\n","        df[\"topic\"] = df[\"playlist_name\"].apply(infer_topic_from_playlist)\n","\n","    exp_df, hold_df = build_experiment_and_holdout(df)\n","\n","    # Optional sanity prints\n","    def report(x: pd.DataFrame, name: str):\n","        print(f\"\\n{name} size = {len(x)}\")\n","        print(\"Per topic counts:\")\n","        print(x.groupby(\"topic\").size().rename(\"count\").sort_values(ascending=False))\n","        print(\"\\nPer topic × length_bin:\")\n","        print(x.groupby([\"topic\", \"length_bin\"]).size().rename(\"count\"))\n","\n","    report(exp_df, \"Experiment (100)\")\n","    report(hold_df, \"Hold-out (50)\")\n","\n","    exp_df.to_csv(OUT_EXP, index=False)\n","    hold_df.to_csv(OUT_HOLD, index=False)\n","    print(f\"\\nSaved:\\n - {OUT_EXP}\\n - {OUT_HOLD}\")\n"]}]}