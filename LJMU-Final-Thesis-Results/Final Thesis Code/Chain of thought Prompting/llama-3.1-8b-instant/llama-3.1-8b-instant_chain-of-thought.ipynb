{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVuL9P8jDNJA9RMWf4w+qJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d59908df93ad417097497e4b1eac5b79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9202bbc355594392be2fe8c86ad54f1a","IPY_MODEL_2dd09a2251214c9baa23138f12583261","IPY_MODEL_8b3ba2b33397401fa861981d5bcf8e75"],"layout":"IPY_MODEL_e8e68b324a164f28861f8226e63a6da6"}},"9202bbc355594392be2fe8c86ad54f1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8f9418136234698a406e61d3dc81757","placeholder":"​","style":"IPY_MODEL_53b45dc4c4f24d09b76c1d62da68f429","value":"tokenizer_config.json: 100%"}},"2dd09a2251214c9baa23138f12583261":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_597d2b7c44dd4968b63bca2887cf3055","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81c498b26f0c4aa2b2fb1d3295e6a5d3","value":25}},"8b3ba2b33397401fa861981d5bcf8e75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20e679086791420eba9f03fc849085fc","placeholder":"​","style":"IPY_MODEL_bd9210398a054852a4c17956d91cf436","value":" 25.0/25.0 [00:00&lt;00:00, 2.04kB/s]"}},"e8e68b324a164f28861f8226e63a6da6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8f9418136234698a406e61d3dc81757":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53b45dc4c4f24d09b76c1d62da68f429":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"597d2b7c44dd4968b63bca2887cf3055":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81c498b26f0c4aa2b2fb1d3295e6a5d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20e679086791420eba9f03fc849085fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd9210398a054852a4c17956d91cf436":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9182805e6cc432886cdb6cd01df36f8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8332d2f710834555a093b74b4e2ff90d","IPY_MODEL_a09190cac6a54d428cfd49a1b6dba6ae","IPY_MODEL_edcb3c16ae24479390dcea232d1e263d"],"layout":"IPY_MODEL_6512db1a1c8d464ab541ff03bed43a5f"}},"8332d2f710834555a093b74b4e2ff90d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fff9bca3fc22429c9f334ca71996432c","placeholder":"​","style":"IPY_MODEL_d5285fc1062147ceaf60c725f9723924","value":"config.json: 100%"}},"a09190cac6a54d428cfd49a1b6dba6ae":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8ed9d6fae244211b36617ff2d094bb1","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4da795e45d744ba592135b914cfa3742","value":482}},"edcb3c16ae24479390dcea232d1e263d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffb1280341954c14bdf3af2fc161bfb9","placeholder":"​","style":"IPY_MODEL_f58f767e9b48486b8798b6a0814f85ab","value":" 482/482 [00:00&lt;00:00, 44.5kB/s]"}},"6512db1a1c8d464ab541ff03bed43a5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fff9bca3fc22429c9f334ca71996432c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5285fc1062147ceaf60c725f9723924":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8ed9d6fae244211b36617ff2d094bb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da795e45d744ba592135b914cfa3742":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ffb1280341954c14bdf3af2fc161bfb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f58f767e9b48486b8798b6a0814f85ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e13e9b73da964a7fa34dd08e597a90ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_584e33f16d6c4a0b92bd4bb15c7b788c","IPY_MODEL_fda76d64d13343a7abd568b8ab8e0e44","IPY_MODEL_2ad3581eaa0a4594a957ee0608bba1af"],"layout":"IPY_MODEL_7d6946f9cdc940d5acafc8bf1486b14a"}},"584e33f16d6c4a0b92bd4bb15c7b788c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d9f335847664dc7b74c721b463c6f11","placeholder":"​","style":"IPY_MODEL_6c23c2aa7c8b4cc098919ef51df6511a","value":"vocab.json: 100%"}},"fda76d64d13343a7abd568b8ab8e0e44":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d087175834af43209d5fa75e2f6c227e","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a264d43220654e4b9109c06adf803d0c","value":898823}},"2ad3581eaa0a4594a957ee0608bba1af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27001c8c13264df6b80d5e0a25d8babf","placeholder":"​","style":"IPY_MODEL_d92ec171b9024265bd3e631539262402","value":" 899k/899k [00:00&lt;00:00, 3.62MB/s]"}},"7d6946f9cdc940d5acafc8bf1486b14a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d9f335847664dc7b74c721b463c6f11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c23c2aa7c8b4cc098919ef51df6511a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d087175834af43209d5fa75e2f6c227e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a264d43220654e4b9109c06adf803d0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"27001c8c13264df6b80d5e0a25d8babf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d92ec171b9024265bd3e631539262402":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b3f315d16a244bdaa7e5776c2165b4e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00daf0540d7042f0aa38f373c87b30ee","IPY_MODEL_c7415e80b32b437291144c93f0858ab9","IPY_MODEL_3d680f2be1044237813881ad0a03ee3d"],"layout":"IPY_MODEL_8a8397bebd7a420d8549f0ca4ceab15d"}},"00daf0540d7042f0aa38f373c87b30ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30010bb3454f48c58729abd0ebdeaffe","placeholder":"​","style":"IPY_MODEL_41ecac99f59c46ec842b8ac9b2832ee6","value":"merges.txt: 100%"}},"c7415e80b32b437291144c93f0858ab9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d3a95be50ad460b9433f1b1b9eb6b3f","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a12437d68fc452bb4de47987fcb1c96","value":456318}},"3d680f2be1044237813881ad0a03ee3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_41580711e7dd47818da21024f2d2ecdd","placeholder":"​","style":"IPY_MODEL_008d175b4d7d4aaf8cddc2211e1aab8b","value":" 456k/456k [00:00&lt;00:00, 2.84MB/s]"}},"8a8397bebd7a420d8549f0ca4ceab15d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30010bb3454f48c58729abd0ebdeaffe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41ecac99f59c46ec842b8ac9b2832ee6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d3a95be50ad460b9433f1b1b9eb6b3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a12437d68fc452bb4de47987fcb1c96":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41580711e7dd47818da21024f2d2ecdd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"008d175b4d7d4aaf8cddc2211e1aab8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32fff08c25fa46a6b7505d1b2fe44673":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_31c56cbff4b749629fd20b1256403598","IPY_MODEL_1b74d7a35c1d4d44902a4162bb653d82","IPY_MODEL_b78dfb68e18843069817d5ef88ae68c3"],"layout":"IPY_MODEL_e955d848cdd9454ab690ebc63accc686"}},"31c56cbff4b749629fd20b1256403598":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a046b245ddca4b42bb5b1820a0cbe5b9","placeholder":"​","style":"IPY_MODEL_b2b19197d39a41e380132f1212d3e64b","value":"tokenizer.json: 100%"}},"1b74d7a35c1d4d44902a4162bb653d82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f378cd3a6aee4f4b91ecfd1dee3718d2","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d02570e18684d709746b4c4b984a9f7","value":1355863}},"b78dfb68e18843069817d5ef88ae68c3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b87e0081f5142a1b2902f28fdd09dac","placeholder":"​","style":"IPY_MODEL_bb17b3c7630c441aab97aeab38e4bab4","value":" 1.36M/1.36M [00:00&lt;00:00, 4.15MB/s]"}},"e955d848cdd9454ab690ebc63accc686":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a046b245ddca4b42bb5b1820a0cbe5b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2b19197d39a41e380132f1212d3e64b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f378cd3a6aee4f4b91ecfd1dee3718d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d02570e18684d709746b4c4b984a9f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b87e0081f5142a1b2902f28fdd09dac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb17b3c7630c441aab97aeab38e4bab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f2ff194e0f1402f8ff6512d2f5452b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f33b74ff82fc44e7ba34a025a4e86ff8","IPY_MODEL_a82441d9e6c041c2ab689bb979a68de4","IPY_MODEL_8935961519304b5ea8ad9e2505376628"],"layout":"IPY_MODEL_17928b86ad1641d5941a60ee2f0e57ce"}},"f33b74ff82fc44e7ba34a025a4e86ff8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fbb1689c58841ec8ccc239571f8e629","placeholder":"​","style":"IPY_MODEL_b86b43214d764dd883ddef05a70f9d24","value":"model.safetensors: 100%"}},"a82441d9e6c041c2ab689bb979a68de4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e0040c7d4f44f589cacc1862c94d2c9","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81b1b732dcf24918b890285c8a3d5261","value":1421700479}},"8935961519304b5ea8ad9e2505376628":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa8b21a30127405d874d148112f02417","placeholder":"​","style":"IPY_MODEL_eaf03941192c4e3b8a6a99380028a809","value":" 1.42G/1.42G [00:14&lt;00:00, 121MB/s]"}},"17928b86ad1641d5941a60ee2f0e57ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fbb1689c58841ec8ccc239571f8e629":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86b43214d764dd883ddef05a70f9d24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e0040c7d4f44f589cacc1862c94d2c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81b1b732dcf24918b890285c8a3d5261":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa8b21a30127405d874d148112f02417":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eaf03941192c4e3b8a6a99380028a809":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":6,"metadata":{"id":"Bwa4TicMpr5h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763453369874,"user_tz":-330,"elapsed":12091,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"c283fabf-2141-4bf5-a619-bce3c6a56638"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.34.1)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n","Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"q65lO701XSM1","executionInfo":{"status":"ok","timestamp":1763435831757,"user_tz":-330,"elapsed":15,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"f63d475c-bb55-4b57-f37a-4c52870484d1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.1-8b-instant/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.1-8b-instant_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.1-8b-instant\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"q1cTkAPpXS0U","executionInfo":{"status":"error","timestamp":1763403028701,"user_tz":-330,"elapsed":9103285,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"b779a25c-9190-4d03-92e0-474de23352de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning through human feedback is a framework that enhances the training process of algorithms by incorporating human input. This framework, applied in chat GPT, utilizes a rewards model to assess generated responses, leading to improved decision-making and faster learning. The iterative training process with human feedback enables chat GPT to generate high-quality responses, and its application can be extended to various reinforcement learning algorithms, such as DQ learning and proximal policy optimization, to boost their performance and capabilities.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary goal of the rewards model in the context of chat GPT?\n","A: The primary goal of the rewards model is to assess and score the quality of answers generated by chat GPT.\n","Q: How does human feedback contribute to reinforcement learning, as illustrated with Frank's grid world adventure?\n","A: Human feedback accelerates the learning process.\n","Q: What is the name of the reinforcement learning algorithm used to fine-tune chat GPT, in conjunction with the rewards model?\n","A: Proximal policy optimization.\n","Q: What is the purpose of the rewards model in the context of reinforcement learning through human feedback?\n","A: The rewards model is used to guide and accelerate the learning process, allowing the algorithm to make more informed decisions.\n","Q: What is the name of the framework that integrates human feedback into the training process of a reinforcement learning algorithm?\n","A: Reinforcement learning through human feedback.\n","\n","KEY CONCEPTS:\n","\n","reinforcement learning, human feedback, gradient descent, proximal policy optimization, Q-learning, DQ learning, reward model, GPT architecture, attention mechanism, back propagation, reinforcement learning algorithm, reinforcement learning through human feedback\n","\n","============================================\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","This tutorial explores example code using CVX opt and kernels applied to a Support Vector Machine (SVM), drawing from Christopher Bishop's 'Pattern Recognition and Machine Learning' book and Matthew Blondell's GitHub repository. CVX opt is useful for visualizing kernel effects but not typically used in practice. The tutorial focuses on understanding how kernels affect the SVM, visualizing nonlinear and soft margin effects, and implementing support vector machines using the CVXOPT library. It covers initialization, kernel selection, and penalty parameter C, and demonstrates the use of different kernels, including the polynomial kernel. The instructor also discusses the importance of the penalty parameter C in determining whether the SVM is a hard or soft margin classifier, and provides examples of linearly separable and non-linearly separable data. The tutorial sets the stage for the next tutorial, which will cover all parameters of scikit-learn's support vector classifier and discuss how to classify more than two classes using support vector machines.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Deep Learning', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main topic of this tutorial?\n","A: Working with CVX opt and kernels applied to a Support Vector Machine.\n","Q: Where did the example code for this tutorial come from?\n","A: Matthew Blondell's GitHub.\n","Q: What is CVX opt primarily used for in this tutorial?\n","A: To directly see the impact of a kernel and where it's being injected and modifying the initial formal Support Vector machine.\n","Q: What is the name of the quadratic programming solver used in this tutorial?\n","A: The actual quadratic programming solver that we're going to use, which does the equation 1/2 x^t P * X + q^t * X subject to constraints.\n","Q: What is the main purpose of using CVX opt in this tutorial?\n","A: For educational purposes to look at the kernels and how they affect the Support Vector machine.\n","\n","KEY CONCEPTS:\n","\n","support Vector machine, CVX opt, kernels, quadratic programming, solver, gradient descent, pattern recognition, machine learning, support Vector machine, lib svm, pyit learn\n","\n","============================================\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","This section delves into the fundamentals of prompt engineering, focusing on understanding the inputs provided to large language models, known as prompts. Prompts come in various forms, offering context and constraints for text output generation. Seven types of prompts are identified, including question and statement prompts, and those with multiple inputs or constraints. Understanding these types is crucial for selecting the right prompt to achieve desired output and impact output complexity and quality. Key prompt features include length, language, context, and constraints, which can be summarized as expectations and execution methods. By defining these features, more accurate output can be obtained, as illustrated through examples and the process of deconstructing prompts, which involves breaking down prompts into individual components to better comprehend key features and constraints.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What are the inputs given to large language models in prompt engineering?\n","A: Prompts\n","Q: What are the key features of a prompt in prompt engineering?\n","A: Length of the prompt, specific language used, context or constraints included, tone of the text, style, specific requirements for the text, or things that you want in the output\n","Q: What is the process of breaking down a prompt into individual components to better understand its key features and constraints?\n","A: Deconstruction of the prompt\n","Q: What is the purpose of deconstructing a prompt in prompt engineering?\n","A: To identify the elements, key features, and constraints in the prompt\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, large language models, gradient descent, attention mechanism, prompt deconstruction, key features, constraints, prompt types, question prompts, statement prompts, multiple inputs, problem-solving prompts, contextual constraints, tone of the text, style of the text, specific requirements, output constraints, prompt optimization, SEO optimization, pre-trained models\n","\n","============================================\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","Artificial Intelligence (AI) agents are autonomous problem solvers that utilize tools to complete tasks. The React Agent Pattern, a popular method for creating AI agents, mimics human thought processes by incorporating thinking, acting, and observing cycles. This pattern is often combined with tools like language models and APIs to enable AI agents to tackle complex problems. The React Agent Pattern will be further examined through coding examples in subsequent sections.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary function of an AI agent in the context of Artificial Intelligence?\n","A: AI agents are capable of thinking on their own and making autonomous decisions.\n","Q: What is the purpose of tools in the context of AI agents?\n","A: Tools are specific functions that agents can use to complete tasks, such as a calculator tool or a search engine tool.\n","Q: What is the React Agent Pattern, and how does it mimic human thinking?\n","A: The React Agent Pattern stands for reasoning plus acting and mimics how human beings think by first thinking about a problem, then taking action, observing the result, and repeating the cycle until a final answer is obtained.\n","Q: What is the role of the LL (Language Model) in the React Agent Pattern?\n","A: The LL first thinks about the user's prompt or problem, decides if it can answer by itself or if it should use a particular tool, and then observes the output of the tool.\n","Q: What is the relationship between an LL and an agent in the context of the React Agent Pattern?\n","A: An agent is created by equipping the LL (brain) with tools, such as the ability to make API calls or run a Python function.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence (AI) agents, problem solvers, autonomous decisions, react agent pattern, reasoning plus acting, think-action-observe loop, Lang chain, LLM, tools, API calls, Google search, python function, agent creation, Lang graph\n","\n","============================================\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","This section analyzes the reflection agent system, tracing its components and interactions to understand the workflow and collaboration with other systems. The instructor demonstrates using Lang chain with LSmith, a tracing tool, to generate and refine a tweet, highlighting the power of reflection agents in deep thinking and output refinement through multiple iterations.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Python Programming', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of tracing the reflection agent system in this section?\n","A: To understand exactly what is happening and how it works together with the other system to deliver the final refined viral tweet.\n","Q: What is the system being built in this section?\n","A: The reflection agent system.\n","Q: What is the goal of the refined viral tweet?\n","A: Not explicitly stated in this transcript chunk, but implied to be a desired outcome.\n","Q: What website is being referenced in this section?\n","A: Smith.chain.\n","\n","KEY CONCEPTS:\n","\n","reflection agent system, viral tweet, agent system, gradient descent is not mentioned but 'refined' could imply optimization which is related to 'gradient descent' but not explicitly mentioned so I will not include it, working together, deliver final product, trace system, understand system, system working together\n","\n","============================================\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","This section focuses on utilizing Lang chains chat models, specifically the Open Artificial Intelligence (AI) API, to communicate with APIs. To begin, the required package, L chain Das open aai, is installed, and the chat open AI class is imported and initialized with a chosen model. The 'invoke' keyword is used to make API calls, passing in any string, and the API key is provided through an EnV file or the 'dotenv' import. If the OpenAI API is inaccessible due to insufficient balance, the user must top up their account and run the script again. The instructor demonstrates how to use Lang chains chat models to access various models and APIs, and plans to send the model an entire conversation history to improve its responses in the next section.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Artificial Intelligence', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What type of APIs will we be predominantly working with in this section?\n","A: Open Artificial Intelligence (AI) APIs\n","Q: What is the command needed to install the L chain Das open AI package?\n","A: L chain Das open AI\n","Q: Why is the latest model (GBT 40) more expensive than the GPD 3 model?\n","A: Because it is the most advanced model\n","Q: What is the name of the class we need to import from the langchain open AI module?\n","A: The chat open AI class\n","Q: What is the name of the model being used in this example?\n","A: GBT 40\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence (AI), open AI APIs, Lang chains chat models, L chain Das open aai, gradient descent, attention mechanism, command line interface, terminal, module, class, keyword parameter, latest models, GPT-3, GPT-40\n","\n","============================================\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","In Python, the sort method for lists containing strings prioritizes words starting with uppercase letters, followed by those starting with lowercase letters, resulting in two separate sorts. When the list contains both strings and numbers, numbers are placed first, followed by strings. This behavior is essential to consider when working with mixed data types, and may necessitate additional processing to achieve a specific sorting order.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What happens when you sort a list of strings in Python that contains both uppercase and lowercase letters?\n","A: The list is sorted with uppercase letters first and then lowercase letters, both alphabetically.\n","Q: Can Python's sort method handle lists that contain both strings and numbers?\n","A: Yes, it can, but it puts the numbers first and the strings.\n","Q: What is the effect of using the reverse method on a sorted list of strings and numbers in Python?\n","A: The numbers are moved to the end of the list and the strings are moved to the beginning.\n","Q: Why might you need to ensure all strings in a list are either all lowercase or all uppercase before sorting them in Python?\n","A: To avoid the sorting behavior of putting uppercase letters first and then lowercase letters.\n","Q: What happens when you insert a number into a list of strings and then sort the list in Python?\n","A: The number is placed at the beginning of the sorted list, followed by the strings.\n","\n","KEY CONCEPTS:\n","\n","gradient descent, attention mechanism, sort method, reverse alphabetical order, uppercase letter, lowercase letter, comestibles, volatile combination, alphabetical order, insert method\n","\n","============================================\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: \n","Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","The effective combination of human and artificial intelligence (AI) decision-making, known as augmented intelligence, can significantly improve decision-making outcomes. While AI excels in tasks where it's confident, humans are better suited for cases where AI is uncertain, particularly in complex or rare situations. To maximize augmented intelligence, it's essential to consider human cognitive bias and present AI recommendations in a way that minimizes bias, such as using optional displays. By leveraging the strengths of both parties, humans and AI can work together to make more informed decisions, moving from subjective to quantifiable decisions.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What type of decision-making tasks is Artificial Intelligence (AI) more likely to perform better at than humans?\n","A: statistically, Artificial Intelligence (AI) will make a better job of deciding for other tasks\n","Q: What is the main issue with the current fraud detection system?\n","A: the analysts are overwhelmed with 90 percent of those alerts being false positives\n","Q: What does a confidence score of 100 percent in the graph represent?\n","A: a prediction is certain that it is a real alert\n","Q: What happens when the Artificial Intelligence (AI) is not sure about a given prediction?\n","A: it's not such a case, and the success rate is lower\n","Q: How do human performance curves typically compare to Artificial Intelligence (AI) performance curves?\n","A: they are typically a little bit flatter\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence (AI), fraud detection system, false positives, confidence score, success rate, performance curve, human bias, holistic curves, decision making, machine learning algorithm\n","\n","============================================\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: \n","Build generative apps faster with Vertex AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","Google Cloud AI has introduced six new Vertex AI APIs to address technical challenges in building generative applications for enterprises. The APIs, including document understanding, improved embedding, vector search, ranking, grounded generation, and fact-checking, aim to provide high-quality solutions to common problems. Designed to be simple and easily integratable, these APIs enable developers to focus on building unique solutions for their use cases, leveraging Google's expertise and knowledge.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main challenge developers face when building generative applications for Enterprises?\n","A: How do you make those applications reliably access the right Enterprise data in order to produce responses which are accurate and consistent.\n","Q: What is the purpose of the document understanding API?\n","A: To understand the structure of documents and help improve the quality of applications that process them.\n","Q: What is the main advantage of the Vector search API?\n","A: It is one of the most performant and scalable and cost-efficient applications for doing embedding retrieval.\n","Q: What is the purpose of the ranking API?\n","A: To check how good each result is in answering a question and help bubble up the most relevant information.\n","Q: What is the purpose of the grounded generation API?\n","A: To produce well-grounded and accurate answers with citations to the reference information.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","The singular value decomposition (SVD) is a powerful tool in science and engineering, decomposing a matrix X into three matrices: U, Σ, and V. U and V are unitary matrices that preserve vector angles and lengths. The SVD can be interpreted geometrically, where X maps a sphere of unit vectors to an ellipsoid, with singular values determining the ellipsoid's elongation and orientation. This geometric interpretation is crucial for understanding the SVD and its applications in data analysis and machine learning.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Machine Learning', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What is the property of unitary matrices U and V in the SVD of a matrix X?\n","A: They preserve the angles between any two vectors in the vector space that they're transforming.\n","Q: What is the effect of a unitary transformation on the inner product of two vectors x and y?\n","A: The inner product remains unchanged if both vectors are mapped through the unitary transformation.\n","Q: What is the geometric interpretation of the matrix X in terms of unitary transformations?\n","A: It can be thought of as rotating vectors in a way that preserves angles and lengths, mapping a sphere of unit length vectors into an ellipsoid.\n","Q: What is the difference between the transpose and complex conjugate transpose of a matrix?\n","A: The complex conjugate transpose is used for complex-valued matrices, where the complex conjugate of each element is taken in addition to transposing the matrix.\n","Q: What is the role of singular values in the geometric interpretation of the SVD?\n","A: They determine the elongation or squishing of the ellipsoid, with larger singular values corresponding to longer principal axes.\n","\n","KEY CONCEPTS:\n","\n","singular value decomposition, unitary matrices, economy size SVD, gradient descent, Fourier transform, complex conjugate transpose, inner product, unit length vectors, ellipsoid, principal axes, singular values, left singular vectors, right singular vectors, coordinate transformation, data-driven generalization, variance of the column space, variance of the row space\n","\n","============================================\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","This video demonstrates how to build a generative Artificial Intelligence (AI) powered application using Google Gemini Pro 1.5. The presenter showcases a demo video and engages in hands-on applications, highlighting the capabilities of Gemini Pro 1.5, which can work with both text and images. The model can create an API key and utilize it to develop complex generative applications, such as video and audio content, and process large PDF files. The application can be installed using pip and imported into a Jupyter notebook, allowing users to understand how it works and utilize its capabilities. The presenter also demonstrates generating text from an image using the Google Gemini Pro Vision model and showcases the model's ability to combine text and image generation, eliminating the need for separate models. Google Gemini Pro 1.5 has improved features compared to its predecessor, Gemini 1.0 Pro, and has a vast capacity, allowing for the development of complex generative applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the name of the YouTube channel where this video is being hosted?\n","A: Krishn's YouTube channel\n","Q: What is Google Gemini Pro 1.5 capable of doing?\n","A: It can work with both text and images\n","Q: What is the purpose of the first 1-minute video in this tutorial?\n","A: To give an idea of what Google Gemini Pro can do\n","Q: What is the name of the newest model being referred to in the video?\n","A: Gemini\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence, Generative AI, Google Gemini Pro, Multi-model, API key, Long context understanding, Experimental feature, End to end projects, Hands-On application, Multimodel\n","\n","============================================\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","Evaluating and testing prompt engineering models involves using various metrics such as perplexity, accuracy, and human evaluation to assess their performance. These metrics measure the model's ability to predict word sequences, generate correct responses, and produce high-quality output. To improve models, developers analyze generated responses, identify common errors, and test them on different data sets and tasks to evaluate their ability to generalize to new data. Continuous evaluation and testing are essential to ensure the model's performance and identify areas for improvement.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","prompt engineering models, evaluation matrices, perplexity, accuracy, human evaluation, gradient descent, cross validation, model evaluation, testing techniques, model debugging, model improvement, large language models, pre-trained language models\n","\n","============================================\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","Artificial Intelligence (AI) systems can be categorized into three main types: generative AI, AI agents, and agentic AI. Generative AI creates new content, while AI agents perform tasks using tools and memory. Agentic AI involves multiple agents working together to achieve complex goals, often using tools and other agents to make decisions. As tasks become more complex, AI systems become more sophisticated, with agentic AI being the most advanced. Agentic AI systems can be built using frameworks like N8N and used for tasks such as onboarding employees or booking flights, characterized by autonomous decision-making, multi-step reasoning, and tool usage.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Generative AI', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main difference between generative Artificial Intelligence (AI) and Artificial Intelligence (AI) agents?\n","A: Generative AI can create new content based on patterns learned from existing data, while AI agents can perform tasks autonomously using tools and other agents to reach a goal.\n","Q: What is the purpose of providing access to tools and APIs in an AI system?\n","A: To enable the AI system to perform more complex tasks, such as booking flights or checking visa eligibility, by leveraging external tools and data.\n","Q: What is an example of a complex task that an AI agent can perform?\n","A: Booking a flight to New Delhi in May with specific criteria, such as sunny weather and a budget less than $1,600, and also suggesting hotels and airport taxis.\n","Q: What is the difference between an AI agent and an agentic AI system?\n","A: An AI agent is a single program that performs a task autonomously, while an agentic AI system is a collection of multiple agents that work together to achieve a complex goal.\n","Q: What is the role of a generative AI model in an agentic AI system?\n","A: It serves as a core component, providing the ability to create new content based on patterns learned from existing data, which is then used by the agentic AI system to perform more complex tasks.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence (AI), Generative Artificial Intelligence (AI), Large Language Model (LLM), Artificial Intelligence (AI) Agent, Agentic Artificial Intelligence (AI), Multi-step Reasoning, Multi-step Planning, Autonomous Decision Making, Tool Usage, N8N, Agno Framework, Agent with Tools and Instruction, Agent with Knowledge, Human in the Loop\n","\n","============================================\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: \n","Covariance in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","In data analysis, variance and covariance are crucial concepts. Covariance measures the relationship between two random variables, such as house size and price, and can be calculated using a specific equation. It indicates the direction and strength of the relationship, but can be positive or negative depending on whether the variables increase or decrease together. However, covariance does not indicate the strength of the relationship, which is a limitation. To overcome this, the Pearson correlation coefficient is introduced, which will be discussed further in the next video.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the topic that will be discussed next in the video?\n","A: Variance and covariance\n","Q: What are the two random variables used as an example in the video?\n","A: Size of the house and price of forms\n","Q: What is the formula for calculating the covariance between two variables X and Y?\n","A: 1/n summation of (X_i - mu_X) * (Y_i - mu_Y)\n","Q: What does a positive covariance value indicate?\n","A: That X is increasing and Y is also increasing\n","Q: What is the main disadvantage of using covariance to quantify the relationship between variables?\n","A: It does not indicate how much positivity or negativity there is in the relationship\n","Q: What is the purpose of using covariance in data analysis?\n","A: To quantify the relationship between random variables in a particular data set\n","\n","KEY CONCEPTS:\n","\n","variance covariance, data pre-processing, data analysis, covariance equation, variance of X, covariance of X and Y, Pearson correlation coefficient, random variables, mean of X and Y, relationship between X and Y, quantifying a relationship\n","\n","============================================\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","In reinforcement learning, an agent learns an optimal policy to maximize a numerical reward signal by interacting with an environment, making decisions based on observations, and receiving feedback. The ultimate goal is to maximize the cumulative reward over time. The agent updates its value for each state-action pair based on experience, using a reward function that assigns a numerical value to each state or action. This function reflects the agent's goals and preferences, and can be positive for winning, negative for losing, or zero for drawing in episodic tasks, or the amount of profit earned in continuous tasks like Stock Market trading.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the ultimate goal of reinforcement learning?\n","A: To learn the optimal policy that maximizes a numerical reward signal.\n","Q: How does an agent in a reinforcement learning environment receive feedback?\n","A: Through a reward signal that provides feedback on the quality of the action taken.\n","Q: What is the reward function in the context of reinforcement learning?\n","A: A mathematical expression that assigns a numerical value to each state or action of the agent, reflecting the trading goal and preferences.\n","\n","KEY CONCEPTS:\n","\n","reinforcement learning, objective, gradient descent, trial and error learning, optimal policy, value function, policy based method, cumulative reward, reward signal, episodic task, continuous task, parameterize objective, reward function, risk adjusted measure, sharp ratio, certino ratio\n","\n","============================================\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1791179432.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;31m# 10. RUN GENERATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGeneration completed. Run separate evaluation script next.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1791179432.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m             \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m             \u001b[0mqa_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m             \u001b[0mconcepts_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_concepts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1791179432.py\u001b[0m in \u001b[0;36mgenerate_qa\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0mTRANSCRIPT\u001b[0m \u001b[0mCHUNK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;31m\\\u001b[0m\u001b[0;34m\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0mqas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generated_questions\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1791179432.py\u001b[0m in \u001b[0;36mllm_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGLOBAL_MIN_GAP\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnow\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLAST_TS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Respecting global wait: sleeping {wait:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.1-8b-instant/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.1-8b-instant_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.1-8b-instant\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxMrjw9_9m4s","executionInfo":{"status":"ok","timestamp":1763446667899,"user_tz":-330,"elapsed":10836140,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"3c963133-95ab-4b1d-ad8c-0ad3a6c161b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Groq API key loaded ✓\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","Skipping row 7 (already processed)\n","Skipping row 8 (already processed)\n","Skipping row 9 (already processed)\n","Skipping row 10 (already processed)\n","Skipping row 11 (already processed)\n","Skipping row 12 (already processed)\n","Skipping row 13 (already processed)\n","Skipping row 14 (already processed)\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","In Python, a dictionary is a data type consisting of key-value pairs, where keys are immutable and items are separated by commas. Dictionaries are useful for mapping items, such as stock prices, and have various functions like items, keys, and values. They can be created from lists using the zip function and manipulated by accessing, changing, and deleting key-value pairs. Understanding dictionaries is essential for data science and working with pandas, making them a fundamental structure in Python.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Data Science', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of a dictionary in Python?\n","A: Dictionaries are useful for mapping one item to another.\n","Q: What is the syntax for declaring a dictionary in Python?\n","A: A dictionary is declared within curly brackets, e.g., D = {key1: value1, key2: value2, ...}.\n","Q: What is the difference between a key and a value in a dictionary?\n","A: A key is immutable and must be something that can't change, such as a string or a number, while a value can be any type of object.\n","Q: How do you access a value associated with a key in a dictionary?\n","A: You use the syntax D[key], where D is the dictionary name and key is the key you want to access.\n","Q: How do you change a value associated with a key in a dictionary?\n","A: You use the syntax D[key] = new_value, where D is the dictionary name, key is the key you want to change, and new_value is the new value you want to assign to that key.\n","\n","KEY CONCEPTS:\n","\n","key value pairs, immutable keys, dictionary declaration, zip function, tuple, dictionary manipulation, data science, pandas, dictionary items, dictionary keys, dictionary values\n","\n","============================================\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: \n","Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","Artificial Intelligence (AI) significantly improves an organization's security posture by reducing breach containment time. IBM's 2023 report found that AI and automation users contained breaches 108 days faster on average. User Behavior Analytics (UBA) with AI and machine learning detects and responds to Insider threats quickly and precisely, with an average cost of $4 million. UBA integrated with a SIM system helps security professionals detect and respond to insider threats effectively, analyzing user behavior, identifying anomalies, and detecting potential threats. By integrating AI and automation, organizations can accelerate investigations, shift focus to proactive defense, and stay ahead of emerging threats.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the potential benefit of using Artificial Intelligence (AI) in security?\n","A: It can take 108 fewer days on average to identify and contain a data breach.\n","Q: According to IBM's cost of a data breach report 2023, what was the average cost of an Insider threat for an organization?\n","A: $4.\n","Q: What is a key concern for organizations of all sizes, according to the cost of a data breach report?\n","A: Insider threats.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence, Machine Learning, User Behavior Analytics, Insider Threats, Data Breach, Security Posture, Artificial Intelligence and Automation, Cost of a Data Breach Report\n","\n","============================================\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","This is an introductory segment to a YouTube channel, where the host, Krishak, greets viewers and establishes the current time. The purpose of this opening is to set the tone and context for the content that will follow. Meta has announced Lama 3, an open-source large language model surpassing its predecessor in performance metrics. Trained on a massive dataset of 50 trillion tokens, Lama 3 supports an 8K context length and demonstrates exceptional accuracy in tasks such as language nuances and complex tasks like translation and dialog generation. The model's performance is comparable to or surpasses that of paid models, making it a significant advancement in artificial intelligence. The speaker also discusses the comprehensive approach to responsibility taken by companies and provides instructions on how to access and use the Meta Llama 3 model, which is available on Meta, Hugging Face, and Kaggle.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Generative AI']\n","\n","GENERATED Q&A:\n","\n","Q: What is the name of the person hosting this YouTube channel?\n","A: Krishak\n","Q: What time is it according to the host?\n","A: 2 a.m.\n","Q: What is the current time of day being referred to in the video?\n","A: The middle of the night\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","This video will guide viewers through the process of creating a decision boundary using Python code, focusing on the scikit-learn library and its documentation. The instructor will demonstrate how to use the library's functions, specifically Naive Bayes, and by the end of the next video or two, viewers will be able to write the code themselves, with a focus on Gaussian Naive Bayes.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the name of the Python library used in this lesson?\n","A: scikit-learn, often abbreviated as sk-learn\n","Q: What is the name of the algorithm used in this lesson?\n","A: Naive Bayes\n","Q: What type of Naive Bayes is being used in this lesson?\n","A: Gaussian Naive Bayes\n","Q: What is the purpose of searching Google for the library and algorithm?\n","A: To use the documentation of the library to figure out how to use some of the functions that it has\n","\n","KEY CONCEPTS:\n","\n","gradient descent, decision boundary, scikit-learn, Naive Bayes, Gaussian Naive Bayes, Python code, algorithm, derivation, use cases, classifier\n","\n","============================================\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: \n","Log Normal Distribution in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","This discussion focuses on Gaussian and log normal distributions, two fundamental probability distributions in statistics. The Gaussian distribution, also known as the normal distribution, follows a bell curve and is characterized by its mean and standard deviation. Key properties of the Gaussian distribution include the 68-95-99.7 rule, where 68% of data falls within one standard deviation, 95% within two, and 99.7% within three. The log normal distribution is a probability distribution of a random variable whose logarithm is normally distributed. The instructor emphasizes the importance of understanding these distributions in data analysis and provides techniques, such as log normalization, to scale down data and improve model accuracy.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What percentage of data falls within one standard deviation in a Gaussian distribution?\n","A: 68 percent\n","Q: What type of distribution is denoted by log of X being normally distributed?\n","A: Log normal distribution\n","Q: Why is standard scaling important in machine learning?\n","A: It allows for the same scale of values across different features, which can improve model accuracy\n","\n","KEY CONCEPTS:\n","\n","Gaussian distribution, normal distribution, empirical formula, bell curve, log normal distribution, standard normal distribution, standard scaler, log normalization, regression algorithm, classification algorithm, domain knowledge, mean, Sigma, standard deviation, normalization, scaling down\n","\n","============================================\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","This video series presents an end-to-end deep learning project in agriculture, focusing on disease detection in potato plants using a mobile application. The project involves data collection, model building, and deployment on Google Cloud with a backend server and mobile app integration. The process includes image classification using convolutional neural networks, data cleaning, and pre-processing. The trained model is exported to disk and deployed to Google Cloud, where it is served using TF serving and fast API. A website is built using React JS, and the model is converted to a TF Lite model for mobile app deployment. The project covers various topics, including back-end servers, ML ops, and front-end deployment, making it suitable for those looking to become machine learning engineers or data scientists.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Deep Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","deep learning, machine learning, TF serving, fast API, Google Cloud, Google Cloud functions, React Native, convolutional neural network, artificial intelligence, agriculture domain, end-to-end application, data collection, model building, backend server, mobile app, disease detection\n","\n","============================================\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","Artificial intelligence (AI) applications with large language models (LLMs) exhibit varying levels of autonomy, ranging from deterministic code to state machines or agents. The progression from human-driven to agent-executed is depicted in a diagram, with each level offering increased complexity and autonomy. Code lacks cognitive architecture and cannot handle real-life complexity, while LLM calls and chains provide more flexibility but still have limitations. Routers enable the AI to decide next steps, but lack memory and learning capabilities. State machines or agents combine previous levels, enabling loops, human review, and refinement, representing the most advanced and autonomous level of autonomy in LLM applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","This advanced section of prompt engineering covers complex topics and techniques to become experts in the field. It delves into handling different types of prompts, including text-based, image-based, and audio-based, and explores advanced techniques for fine-tuning pre-trained large language models. The section also discusses best practices for data preprocessing and cleaning, and examines how to deploy prompt engineering models in production using frameworks like TensorFlow Serving and Flask. Additionally, it addresses the ethical considerations of prompt engineering, including fairness and privacy, and provides practical exercises for advanced models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What are some advanced topics and techniques in prompt engineering that can help become an expert in this field?\n","A: Advanced topics and techniques include handling different types of prompts, fine-tuning pre-trained large language models, and best practices for data pre-processing and cleaning.\n","Q: How can a model be trained to identify different breeds of dogs from pictures?\n","A: A model can be trained on a dataset of images of dogs with each image labeled with its corresponding breed, and then use a pre-trained model to identify the breed of a dog in a new image.\n","Q: What is multitask learning in the context of fine-tuning pre-trained large language models?\n","A: Multitask learning involves training a model on multiple tasks simultaneously to learn more robust representations that can generalize to different users.\n","Q: What is distillation in the context of fine-tuning pre-trained large language models?\n","A: Distillation involves training a smaller model to mimic the behavior of a larger model, making the smaller model more efficient and faster to run.\n","Q: Why is tokenization an important step in data pre-processing for prompt engineering models?\n","A: Tokenization involves breaking down text into smaller units, such as words or subwords, to help the model understand the meaning of the text more accurately.\n","Q: What is TensorFlow Serving, and how can it be used to deploy prompt engineering models in production?\n","A: TensorFlow Serving is a framework for serving machine learning models in a production environment, and it can be used to deploy prompt engineering models.\n","Q: What are some ethical considerations in prompt engineering, such as bias and fairness?\n","A: Ethical considerations include ensuring that data used to train models is diverse and inclusive to avoid unfair and discriminatory outcomes.\n","Q: What are some advanced techniques for fine-tuning pre-trained large language models, such as multitask learning and distillation?\n","A: Advanced techniques include multitask learning, distillation, and self-supervised learning to increase the accuracy and performance of prompt engineering models.\n","Q: Why is data pre-processing and cleaning important for prompt engineering models?\n","A: Data pre-processing and cleaning, including tokenization, normalization, and data augmentation, is important to pass clean data to prompt engineering models.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, multitask learning, distillation, pre-trained large language models, data pre-processing, tokenization, normalization, data augmentation, deployment in production, ethical considerations, fairness, privacy, fine-tuning, self-supervised learning, Keras Library, ResNet 50, BERT, Inception, logistic regression, multinomial regression, cross entropy loss, Adam Optimizer, TensorFlow Serving, Flask API, open AI API\n","\n","============================================\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","This lecture explores the application of singular value decomposition (SVD) in image classification, specifically through the technique of eigenfaces. The instructor demonstrates how to load and cluster images of Arnold Schwarzenegger and Sylvester Stallone using SVD, finding eigenfaces as linear combinations of the original images and projecting new images into the eigenface space for classification. The lecture also discusses the use of SVD for image compression and classification, as well as the limitations of this approach. Furthermore, it touches on the integration of three-dimensional geometry to enhance face classification accuracy, as seen in Facebook's early work in this area.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","singular value decomposition, eigenfaces, principal component analysis, SVD, eigenvalues, eigenvectors, image classification, face recognition, deep neural network architectures, 3D geometry, stereo vision\n","\n","============================================\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","The Lang chain is a sequence of models used to generate responses, allowing applications to leverage the strengths of each model for more accurate and helpful answers. Large language models (LLMs) are intelligent but limited in interacting with the real world, requiring a framework like LangChain to access APIs, databases, and more. LangChain acts as a bridge between LLMs and the real world, enabling AI to act in the real world and making it smarter and more capable. It also facilitates switching between different LLMs without modifying code, making it a crucial component in building applications that require both LLM reasoning and real-world interaction.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of the example of planning a vacation to Paris in the context of understanding Lang chain?\n","A: To illustrate a simple problem that can be solved with the help of a ChBd.\n","Q: What happens behind the scenes when you enter a query to a ChBd application?\n","A: The query is sent to an LLM model.\n","Q: What type of models might a ChBd application use?\n","A: A lot of models, including Chity 3.\n","\n","KEY CONCEPTS:\n","\n","LLM model, Chit-Chat application, Natural Language Processing, Model architecture, Chatbot\n","\n","============================================\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","Residual analysis is a crucial step in time series forecasting, allowing data scientists to diagnose model performance and identify areas for improvement. By analyzing residuals, data scientists can detect trends or inconsistencies in the model, check for autocorrelation and bias, and make adjustments to improve its accuracy. Residuals are the difference between actual and fitted values, and they can be used to evaluate a model's performance, identify issues such as autocorrelation and bias, and make corrections when rebuilding the model. This analysis is essential for building accurate forecasting models and can be a useful tool for data scientists.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main purpose of residual analysis in time series forecasting?\n","A: To diagnose the performance of a model and improve it by identifying what the model is doing and what it's missing.\n","Q: What is the difference between residuals and errors in time series analysis?\n","A: Residuals are the difference between the fitted values and the actual values, while errors are the difference between the forecast and the actual values.\n","Q: What are the two key things to look for in residual analysis to ensure a good model?\n","A: The residual should have no autocorrelation or partial autocorrelation, and the mean of the residual should be zero.\n","Q: What is the purpose of the Yule-Walker test in residual analysis?\n","A: To test whether the residuals are independently distributed, i.e., whether they have any serial correlation.\n","Q: What is the implication of a significant Yule-Walker test result?\n","A: It indicates that the residuals have some form of serial correlation, which means the model has missed some information in the data.\n","\n","KEY CONCEPTS:\n","\n","time series crash course, residual analysis, forecasting methods, gradient descent, attention mechanism, Winters model, exponential smoothing model, autocorrelation function, partial autocorrelation function, Young Box test, serial correlation, statistical significance\n","\n","============================================\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","This instructional video guides viewers through building a Text-to-SQL agent using a large language model. The agent is built using LangGraph, Next.js, and WatsonX AI, and is designed to interact with a database using SQL knowledge. The process involves setting up a Next.js project, modifying the boilerplate code, and implementing a ReAct agent to connect to models on WatsonX.ai. The agent is then used to query a SQLite in-memory database, generating SQL queries to retrieve data. The instructor highlights the importance of guardrails to prevent the agent from having unlimited control over the database. The video covers key topics, including setting up the agent, connecting to the database, and implementing a tool to execute SQL queries. The agent is demonstrated to query the database for various questions, showcasing its capabilities and potential applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of building an Artificial Intelligence (AI) agent in this video?\n","A: To build an agent that's able to use SQL knowledge to connect to your databases.\n","Q: What is LangGraph used for in this project?\n","A: To build a ReAct agent.\n","Q: What is the role of Next.js in this project?\n","A: To create a frontend application.\n","Q: What type of database is being used in this project?\n","A: An in-memory database using SQLite.\n","Q: What is the purpose of the input box in the Home component?\n","A: To type a message to the large language model.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence, large language models, SQL knowledge, ReAct agent, Next.js, in-memory database, SQLite, VS Code, Tailwind, client-side component, server-side code\n","\n","============================================\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","Prompt engineering is a specialized field in natural language processing that focuses on building models to generate high-quality text outputs in response to prompts or input. These models are based on pre-trained large language models fine-tuned for specific tasks and inputs. The key benefit of prompt engineering is generating accurate, coherent, and contextually appropriate text outputs, which is crucial for applications like chatbots, language translation, and content generation. However, prompt engineering models may face challenges with complex prompts and biased outputs due to underlying data or model architecture.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, natural language processing, pre-trained large language models, fine-tuning, gradient descent, attention mechanism, GPT, Transformer, bias and inaccuracy, underlying data, model architecture, prompt analysis, deconstructing prompts, key features and constraints\n","\n","============================================\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning is a machine learning paradigm where an agent learns to maximize a numerical reward signal by mapping situations to actions. Q-learning is a value-based method that determines a value function to quantify the total reward and learns an optimal policy to achieve its goals. The agent updates Q-values using the Bellman equation and gradient update rule, with the learning rate set to 0.1. Q-learning is an off-policy algorithm that allows the behavior policy to collect data separately from the target policy, and multiple episodes are performed to learn the Q values that dictate the policy to achieve a target reward.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three primary machine learning paradigms?\n","A: Supervised learning, unsupervised learning, and reinforcement learning.\n","Q: What is the primary objective of value-based methods in reinforcement learning?\n","A: To determine a value function that quantifies the total reward.\n","Q: What is the difference between a state value function and a state-action value function?\n","A: A state value function takes a state as input and outputs a real number, while a state-action value function takes a state and action as input and outputs a real number, also known as a Q value.\n","Q: What is the goal of Q-learning?\n","A: To effectively learn the state-action value function such that the total reward is maximized.\n","Q: What is the Bellman equation used for in Q-learning?\n","A: It defines a recursive relationship between Q values.\n","\n","KEY CONCEPTS:\n","\n","reinforcement learning, q-learning, value-based methods, policy-based methods, state value function, state-action value function, q-value, bellman equation, exploration policy, behavior policy, target policy, discount factor, grid world, machine learning paradigms, supervised learning, unsupervised learning\n","\n","============================================\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","A logistic classifier is a type of linear classifier that uses a linear function to generate predictions from input data. This function is achieved through matrix multiplication of the inputs with a matrix of weights and a bias term. The model's weights and bias are trained to optimize its predictions. Classification is performed by converting scores into probabilities using a softmax function, which ensures the probabilities sum to 1 and are large for high scores and small for low scores.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Artificial Intelligence', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What type of classifier is a logistic classifier?\n","A: A linear classifier.\n","Q: How does a logistic classifier generate its predictions?\n","A: It applies a linear function to the input by multiplying the inputs with a matrix.\n","Q: What is the purpose of the weights and bias in a logistic classifier?\n","A: To train the model by finding the values for the weights and bias that are good at performing predictions.\n","Q: Why do we need to turn scores into probabilities in logistic regression?\n","A: To get the probability of the correct class to be very close to one and the probability for every other class to be close to zero.\n","Q: What is the property of proper probabilities?\n","A: They sum to 1 and are large when the scores are large and small when the scores are comparatively smaller.\n","\n","KEY CONCEPTS:\n","\n","logistic classifier, linear classifier, gradient descent, matrix multiply, weights, bias, softmax function, logits, probability, classification\n","\n","============================================\n","Saved row 29\n","\n","All rows processed.\n","\n","Generation completed. Run separate evaluation script next.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k748VTqZCmlX","executionInfo":{"status":"ok","timestamp":1763453880116,"user_tz":-330,"elapsed":21087,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"99fc8197-ac96-412b-9d04-facc17873a51"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_cot.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.1-8b-instant/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["d59908df93ad417097497e4b1eac5b79","9202bbc355594392be2fe8c86ad54f1a","2dd09a2251214c9baa23138f12583261","8b3ba2b33397401fa861981d5bcf8e75","e8e68b324a164f28861f8226e63a6da6","e8f9418136234698a406e61d3dc81757","53b45dc4c4f24d09b76c1d62da68f429","597d2b7c44dd4968b63bca2887cf3055","81c498b26f0c4aa2b2fb1d3295e6a5d3","20e679086791420eba9f03fc849085fc","bd9210398a054852a4c17956d91cf436","c9182805e6cc432886cdb6cd01df36f8","8332d2f710834555a093b74b4e2ff90d","a09190cac6a54d428cfd49a1b6dba6ae","edcb3c16ae24479390dcea232d1e263d","6512db1a1c8d464ab541ff03bed43a5f","fff9bca3fc22429c9f334ca71996432c","d5285fc1062147ceaf60c725f9723924","c8ed9d6fae244211b36617ff2d094bb1","4da795e45d744ba592135b914cfa3742","ffb1280341954c14bdf3af2fc161bfb9","f58f767e9b48486b8798b6a0814f85ab","e13e9b73da964a7fa34dd08e597a90ea","584e33f16d6c4a0b92bd4bb15c7b788c","fda76d64d13343a7abd568b8ab8e0e44","2ad3581eaa0a4594a957ee0608bba1af","7d6946f9cdc940d5acafc8bf1486b14a","5d9f335847664dc7b74c721b463c6f11","6c23c2aa7c8b4cc098919ef51df6511a","d087175834af43209d5fa75e2f6c227e","a264d43220654e4b9109c06adf803d0c","27001c8c13264df6b80d5e0a25d8babf","d92ec171b9024265bd3e631539262402","5b3f315d16a244bdaa7e5776c2165b4e","00daf0540d7042f0aa38f373c87b30ee","c7415e80b32b437291144c93f0858ab9","3d680f2be1044237813881ad0a03ee3d","8a8397bebd7a420d8549f0ca4ceab15d","30010bb3454f48c58729abd0ebdeaffe","41ecac99f59c46ec842b8ac9b2832ee6","4d3a95be50ad460b9433f1b1b9eb6b3f","0a12437d68fc452bb4de47987fcb1c96","41580711e7dd47818da21024f2d2ecdd","008d175b4d7d4aaf8cddc2211e1aab8b","32fff08c25fa46a6b7505d1b2fe44673","31c56cbff4b749629fd20b1256403598","1b74d7a35c1d4d44902a4162bb653d82","b78dfb68e18843069817d5ef88ae68c3","e955d848cdd9454ab690ebc63accc686","a046b245ddca4b42bb5b1820a0cbe5b9","b2b19197d39a41e380132f1212d3e64b","f378cd3a6aee4f4b91ecfd1dee3718d2","2d02570e18684d709746b4c4b984a9f7","7b87e0081f5142a1b2902f28fdd09dac","bb17b3c7630c441aab97aeab38e4bab4","2f2ff194e0f1402f8ff6512d2f5452b6","f33b74ff82fc44e7ba34a025a4e86ff8","a82441d9e6c041c2ab689bb979a68de4","8935961519304b5ea8ad9e2505376628","17928b86ad1641d5941a60ee2f0e57ce","7fbb1689c58841ec8ccc239571f8e629","b86b43214d764dd883ddef05a70f9d24","7e0040c7d4f44f589cacc1862c94d2c9","81b1b732dcf24918b890285c8a3d5261","aa8b21a30127405d874d148112f02417","eaf03941192c4e3b8a6a99380028a809"]},"id":"uSt-UuxTo_vQ","executionInfo":{"status":"ok","timestamp":1763454032114,"user_tz":-330,"elapsed":144210,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"f8e1d0ee-20e7-4160-ba50-b8e6bb06410d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_cot.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d59908df93ad417097497e4b1eac5b79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9182805e6cc432886cdb6cd01df36f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e13e9b73da964a7fa34dd08e597a90ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b3f315d16a244bdaa7e5776c2165b4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32fff08c25fa46a6b7505d1b2fe44673"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f2ff194e0f1402f8ff6512d2f5452b6"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2974\n","  - BLEU: 0.0611\n","  - BERTScore F1: 0.8893\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3121\n","  - Micro F1: 0.4403\n","  - Macro F1: 0.4043\n","  - Weighted F1: 0.4004\n","\n","Q&A Generation:\n","  - BLEU: 0.0436\n","  - Diversity: 0.6621\n","  - Answerability: 0.7319\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4733\n","  - Recall@10: 0.1893\n","  - F1@10: 0.2705\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.1-8b-instant/evaluation_final.json\n"]}]}]}