{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/87RBIS/rwfNc+zUwvTJn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ff92994b72fd44188ae42f5c493de387":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ef276c150b047a0838dabdf8fcda02f","IPY_MODEL_66895fabef2645958ad4b13c45334d33","IPY_MODEL_ac012fa3626248229d2cb6f0677b1a31"],"layout":"IPY_MODEL_ad1a6d8d5c3041b0b7d54d5396cefa84"}},"5ef276c150b047a0838dabdf8fcda02f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2323df27c5b4712bd6b7da7d3e0ac10","placeholder":"​","style":"IPY_MODEL_4d4425db6e914c96a592f09a8261710e","value":"tokenizer_config.json: 100%"}},"66895fabef2645958ad4b13c45334d33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_04c8d3ef99f64215b81a1acb5ee7b0c6","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d4975354c084d158628f6e8991e8afe","value":25}},"ac012fa3626248229d2cb6f0677b1a31":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9f455ce2de64e60b0eef248fa739af3","placeholder":"​","style":"IPY_MODEL_e8eb1cc78961445cb5fac5030682d928","value":" 25.0/25.0 [00:00&lt;00:00, 2.25kB/s]"}},"ad1a6d8d5c3041b0b7d54d5396cefa84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2323df27c5b4712bd6b7da7d3e0ac10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d4425db6e914c96a592f09a8261710e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"04c8d3ef99f64215b81a1acb5ee7b0c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d4975354c084d158628f6e8991e8afe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9f455ce2de64e60b0eef248fa739af3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8eb1cc78961445cb5fac5030682d928":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dfce95e23324d32ab3cab2912a7a0f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4b6fc074996f41a4a59b7d84960e3970","IPY_MODEL_f66c1ade0b8940b3a491383012319b6f","IPY_MODEL_9ebc7aa317e24f5f97ec1915aa818f1a"],"layout":"IPY_MODEL_0670da65b93e4d719c91f9cab93142c5"}},"4b6fc074996f41a4a59b7d84960e3970":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe9234c242744ce98b9cc9ca776a0e3b","placeholder":"​","style":"IPY_MODEL_7cabbad07a0a48be896aed0c86985e01","value":"config.json: 100%"}},"f66c1ade0b8940b3a491383012319b6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_863f28b1c997440c933249f77e268514","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_129e3585b48847608ce7cc8a612b8dd6","value":482}},"9ebc7aa317e24f5f97ec1915aa818f1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5736c2cff36c468ea87f736e63b1e126","placeholder":"​","style":"IPY_MODEL_ed9e05dc5afa467f9937e64780c2e118","value":" 482/482 [00:00&lt;00:00, 39.4kB/s]"}},"0670da65b93e4d719c91f9cab93142c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe9234c242744ce98b9cc9ca776a0e3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cabbad07a0a48be896aed0c86985e01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"863f28b1c997440c933249f77e268514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129e3585b48847608ce7cc8a612b8dd6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5736c2cff36c468ea87f736e63b1e126":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed9e05dc5afa467f9937e64780c2e118":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"afc5bcc05bd74d0ebebc06a167af46d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7453564f7f954c68b0055740f9625b5c","IPY_MODEL_60f31b8af0f64dc3befb3063c12a0774","IPY_MODEL_0e6c822678724961bb1a3d1b25a7b40f"],"layout":"IPY_MODEL_e35eb975705d42d08d74519fa21ad39b"}},"7453564f7f954c68b0055740f9625b5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cc616b131d84ecbafa3343f5d284083","placeholder":"​","style":"IPY_MODEL_9eca702235c449419003779427298e00","value":"vocab.json: 100%"}},"60f31b8af0f64dc3befb3063c12a0774":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de6be06c80de4b76972794f654827eed","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d08a350501744e10b05bfb9ee1ba7de4","value":898823}},"0e6c822678724961bb1a3d1b25a7b40f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7da9501a795145288ad4edf78dada214","placeholder":"​","style":"IPY_MODEL_4218e86cf7a34fe587340fbc781788d9","value":" 899k/899k [00:00&lt;00:00, 11.5MB/s]"}},"e35eb975705d42d08d74519fa21ad39b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cc616b131d84ecbafa3343f5d284083":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9eca702235c449419003779427298e00":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de6be06c80de4b76972794f654827eed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d08a350501744e10b05bfb9ee1ba7de4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7da9501a795145288ad4edf78dada214":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4218e86cf7a34fe587340fbc781788d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10527112d7a049fa9d1aea4830c1729e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4323e8e9f041435c9ccb5c3f6b86543f","IPY_MODEL_fb0022471abb404f82a0598b9e93eac1","IPY_MODEL_b6d0a098212e45b2bd043622fdb45ae7"],"layout":"IPY_MODEL_f44017ae651a43b7a7ac69dbd3aef062"}},"4323e8e9f041435c9ccb5c3f6b86543f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf201c6229e847baadf9689ecfe6d58c","placeholder":"​","style":"IPY_MODEL_16a683dc3fdd4781ba25f4e3d8539a23","value":"merges.txt: 100%"}},"fb0022471abb404f82a0598b9e93eac1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cdf33e39389a4cb8a66c9062496657e7","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db9e0e93a35344cb9b68f74121c80ee6","value":456318}},"b6d0a098212e45b2bd043622fdb45ae7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e5a06f352e4aeb861a3a5c13046a0b","placeholder":"​","style":"IPY_MODEL_a1b9c42e6f80466ba67157c51ed49a6b","value":" 456k/456k [00:00&lt;00:00, 9.42MB/s]"}},"f44017ae651a43b7a7ac69dbd3aef062":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf201c6229e847baadf9689ecfe6d58c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16a683dc3fdd4781ba25f4e3d8539a23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cdf33e39389a4cb8a66c9062496657e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db9e0e93a35344cb9b68f74121c80ee6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73e5a06f352e4aeb861a3a5c13046a0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1b9c42e6f80466ba67157c51ed49a6b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"491588ca677c4fd48cbb527b0a1d9ef8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c575d647649a4adeb7173ff82972ca92","IPY_MODEL_f22279a4e3ea472ea345f3fc623507ab","IPY_MODEL_6f236792f3274024a62e4ba6f9b5ffc6"],"layout":"IPY_MODEL_8074a79e8db2422ba863cdd03b914c79"}},"c575d647649a4adeb7173ff82972ca92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a11a23d9326a479bbeb6c5dccb6b1086","placeholder":"​","style":"IPY_MODEL_41e4ccce1d63475ca842515a5fddcbb1","value":"tokenizer.json: 100%"}},"f22279a4e3ea472ea345f3fc623507ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbbdacc1c2f74712a31c72f5d6984b0a","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ff8eb735bd34e15a2a9c801427a3fe3","value":1355863}},"6f236792f3274024a62e4ba6f9b5ffc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_163376dccfca412295841e890611673e","placeholder":"​","style":"IPY_MODEL_5df55be92c2a4e98a088c54cf5c35a3a","value":" 1.36M/1.36M [00:00&lt;00:00, 14.2MB/s]"}},"8074a79e8db2422ba863cdd03b914c79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a11a23d9326a479bbeb6c5dccb6b1086":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41e4ccce1d63475ca842515a5fddcbb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbbdacc1c2f74712a31c72f5d6984b0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ff8eb735bd34e15a2a9c801427a3fe3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"163376dccfca412295841e890611673e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5df55be92c2a4e98a088c54cf5c35a3a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e286329b8ab47d19e5814094395e83d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_480e5765fe644a84b228644b18811738","IPY_MODEL_5d114d1de6894472808a7da28f7138e9","IPY_MODEL_a4baf6b1a7f94d269fb187789e7b3ac6"],"layout":"IPY_MODEL_a676d3b7e2634b8eada0d58554d60daa"}},"480e5765fe644a84b228644b18811738":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b8311554d8540658e5e055492544de2","placeholder":"​","style":"IPY_MODEL_a8e85b818a7745fbbb07b71f680edf80","value":"model.safetensors: 100%"}},"5d114d1de6894472808a7da28f7138e9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_791a605f0a1f4f71bf11798b09df41ff","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ed839fe859846f0afbf0dd8084f0380","value":1421700479}},"a4baf6b1a7f94d269fb187789e7b3ac6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c44cdb9bc874f5a9275dfbcbd0917fc","placeholder":"​","style":"IPY_MODEL_ad87c8a0197141658f62ae48ae31e4c0","value":" 1.42G/1.42G [00:17&lt;00:00, 237MB/s]"}},"a676d3b7e2634b8eada0d58554d60daa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b8311554d8540658e5e055492544de2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8e85b818a7745fbbb07b71f680edf80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"791a605f0a1f4f71bf11798b09df41ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ed839fe859846f0afbf0dd8084f0380":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c44cdb9bc874f5a9275dfbcbd0917fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad87c8a0197141658f62ae48ae31e4c0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"gBspXl0zpsPY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763971092868,"user_tz":-330,"elapsed":24886,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"28752a36-5cf4-4e82-d267-af3cecc72083"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=617ea3a8200c99a4152bc813e158945984393fe410bd6f71c369257d655b9cc9\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"AAIhodg3qTMk","executionInfo":{"status":"ok","timestamp":1763971092886,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"e7d6ae88-3ef2-4fc3-9ee5-d6f1125d8bcc"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/kimi-k2-instruct-0905/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"kimi-k2-instruct-0905_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"moonshotai/kimi-k2-instruct-0905\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6J3dt84uqV0B","outputId":"b1e0f347-21e6-4f9a-8bf4-04a8a1acdc0d","executionInfo":{"status":"ok","timestamp":1763992142201,"user_tz":-330,"elapsed":21045340,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning with human feedback (RLHF) integrates human evaluative signals into the reinforcement-learning loop to accelerate convergence toward behaviors that align with human preferences. In a grid-world illustration, an agent named Frank shows how mentor-provided rankings or corrections supplement Q-learning, DQN, or PPO updates, reducing costly exploration. The identical mechanism scales to large language models: human annotators rank multiple candidate outputs, a reward model is trained to reproduce these rankings, and PPO fine-tunes the generative parameters to maximize the predicted reward, producing answers that better reflect user expectations. RLHF thus unifies low-level control and high-level language tasks under one human-in-the-loop framework, improving sample efficiency and policy alignment while remaining architecture-agnostic.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Generative AI', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: Which reinforcement-learning algorithms can be combined with human feedback according to the video?\n","A: All of the listed ones—Q-learning, DQN, and Proximal Policy Optimization—can be used with human feedback.\n","Q: In Frank’s grid-world example, how does human guidance change the learning outcome?\n","A: It accelerates Frank’s learning by nudging him toward preferred actions, letting him reach the +10 reward faster.\n","Q: What is the first step in using RLHF to improve ChatGPT?\n","A: Train a reward model that takes a question–answer pair and outputs a score indicating answer quality.\n","Q: After the reward model is trained, how is it used during ChatGPT fine-tuning?\n","A: The reward model scores each response that ChatGPT generates; this score becomes the reward signal for Proximal Policy Optimization, guiding back-propagation to improve the model over many iterations.\n","Q: What is the overall purpose of Reinforcement Learning from Human Feedback (RLHF)?\n","A: To integrate human judgments into the RL training loop, guiding and speeding up learning so the agent produces higher-quality, human-preferred decisions or responses.\n","\n","KEY CONCEPTS:\n","\n","reinforcement learning with human feedback, reward model, proximal policy optimization, grid world, human advisor, iterative training process, back propagation, loss function, ranking responses, fine-tuning ChatGPT\n","\n","============================================\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","The tutorial demonstrates an educational implementation of support-vector machines in CVXopt, walking through quadratic-programming formulation, custom kernels, and soft-margin tuning to visualize their nonlinear classification effects. Drawing on Bishop’s textbook and open code, it contrasts CVXopt’s pedagogical clarity with production-oriented libsvm, supplies links to concise and MIT quadratic-programming primers, and details NumPy-based workflow: kernel selection (linear, polynomial, RBF), penalty C adjustment, and extraction of support vectors, weights, and bias for sign-based prediction. Companion utilities generate separable, overlapping, or nonlinear datasets and animate kernel-induced feature-space mappings, illustrating how kernel choice and C control decision boundaries and support-vector counts. The session closes by noting that linear kernels permit explicit weight vectors whereas nonlinear ones require kernel evaluations at prediction, and previews a final scikit-learn SVC tutorial covering hyperparameters, multi-class strategies, and best practices.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main educational goal of the code example discussed in this part of the tutorial?\n","A: To show the direct impact of applying kernels to a support-vector machine and to visualize nonlinear boundaries and soft-margin effects.\n","Q: Which quadratic-programming solver is used in the demonstration code?\n","A: CVXopt.\n","Q: According to the speaker, what production-level library would you likely use instead of CVXopt if you were writing your own SVM?\n","A: LIBSVM.\n","Q: Where did the presenter obtain the example code and visualization?\n","A: From Matthew Blondel’s GitHub repository.\n","Q: Besides the GitHub repo, which textbook is referenced as a source for the modeled information?\n","A: Christopher Bishop’s “Pattern Recognition and Machine Learning.”\n","\n","KEY CONCEPTS:\n","\n","kernel trick, support vector machine, CVXOPT quadratic programming solver, soft margin SVM, nonlinear visualization, Matthew Blondell’s GitHub example, Christopher Bishop’s PRML book, libSVM library\n","\n","============================================\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","Prompts are the primary interface to large language models, determining output quality by explicitly encoding context, constraints, and stylistic requirements such as length, tone, and format. The material systematically distinguishes seven prompt archetypes, illustrates iterative refinement cycles that convert ambiguous requests into precise instructions across factual, code-generation, and essay-writing tasks, and presents deconstruction heuristics that isolate objectives, assumptions, and evaluation criteria. Mastery of these techniques enables learners to progress from crafting isolated prompts to embedding them within broader pre-trained model pipelines, ensuring reproducible, evaluable, and task-aligned generative behaviour.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Generative AI', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What are the two main things that define a prompt and determine the accuracy of the model’s output?\n","A: What you expect and how you want it to be done (including any constraints like tone, style, or format).\n","Q: List three of the seven prompt types mentioned in the transcript.\n","A: Question prompts, statement prompts, and prompts with multiple inputs or constraints.\n","Q: Why did the speaker add “give a one-word answer” to the prompt “What is the capital of France?”\n","A: To constrain the model so it would return only the word “Paris” instead of extra contextual information.\n","Q: When deconstructing an existing prompt, what specific elements should you identify?\n","A: The specific language used, the core requirement, and all constraints such as word count, format, or optimization rules.\n","Q: In the transcript’s example of generating an essay on World War II, what was the constraint and how did it affect the output?\n","A: The constraint was “in 500 words,” which caused the model to produce an essay of exactly that length.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, large language models, question prompts, statement prompts, prompt constraints, prompt deconstruction, context specification, output constraints, tone specification, SEO optimization constraints\n","\n","============================================\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","AI agents transcend fixed chains and routers by autonomously selecting actions; they invoke domain-specific tools such as search or calculation to solve complex tasks. The ReAct framework operationalises this autonomy through iterative think-act-observe cycles: the LLM generates a reasoning trace, calls a tool with structured arguments via LangChain, observes the returned evidence, and repeats until the query is satisfied, mirroring human problem-solving. Equipping an LLM with tools instantiates an agent, and forthcoming sections will first demonstrate minimal ReAct construction in LangChain before detailing LangGraph’s enhancements for state persistence, controllability, and scalability.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Agentic AI', 'LangChain', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What distinguishes an AI agent from chains and routers?\n","A: Agents can make autonomous decisions about which steps to take, whereas chains and routers only follow predefined instructions.\n","Q: In the ReAct pattern, what happens after the LLM provides the action input?\n","A: Control returns to the system (LangChain), which executes the tool with the LLM-suggested arguments and sends the output back to the LLM for observation.\n","Q: Why does the ReAct pattern alternate between 'think', 'act', and 'observe'?\n","A: It mimics human problem-solving: think about the problem, act with a tool, observe the result, and repeat the cycle until the answer is found.\n","Q: What two components must be combined to create an agent, according to the speaker’s simple diagram?\n","A: The reasoning brain (LLM) and tools (special functions like calculators or search engines).\n","Q: When would the ReAct cycle repeat instead of stopping?\n","A: When the LLM observes that the tool’s output is insufficient and the problem requires additional steps to solve.\n","\n","KEY CONCEPTS:\n","\n","react agent pattern, autonomous decision-making, tool integration, think-action-observe loop, LangChain tool execution, LLM reasoning ability, multi-step problem solving, agent-tool architecture, control flow handoff, API-equipped agents\n","\n","============================================\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","The tutorial demonstrates integrating LangSmith tracing into a LangChain reflection-agent pipeline to iteratively refine a tweet. After configuring environment variables, each execution of the six-step generation–reflection cycle is automatically streamed to LangSmith, producing a detailed, node-level trace. Learners inspect how the reflective agent critiques its own output, successively adding emojis, hashtags, and engagement hooks, thereby illustrating the mechanics and practical utility of iterative self-improvement in language agents.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Agentic AI', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main purpose of tracing the reflection agent system in this section?\n","A: To understand exactly what is happening where so we can see how both systems work together to deliver the final refined viral tweet.\n","Q: What final output are the two systems collaborating to produce?\n","A: A refined viral tweet.\n","Q: Which website does the speaker plan to use for the tracing demonstration?\n","A: smith.chain\n","\n","KEY CONCEPTS:\n","\n","reflection agent system, refined viral tweet, chain tracing\n","\n","============================================\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","The tutorial walks through installing LangChain’s ChatOpenAI package, instantiating a GPT-4o chat model, and invoking prompts via the OpenAI API. After encountering an authentication error, learners store the API key in a .env file and load it with python-dotenv, enabling successful requests that return structured JSON. Emphasis is placed on selecting cost-effective models (GPT-3.5 vs. GPT-4o), ensuring sufficient account credit, and extracting only the content field from verbose responses. A quick test confirms the model correctly returns 7 for √49, and the segment concludes by previewing the next step: sending full conversation history to enable context-aware replies.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Python Programming', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: Which specific LangChain chat model package must be installed to work with OpenAI APIs?\n","A: langchain-openai\n","Q: Why did the initial pip install command fail in the transcript?\n","A: Because a percentage sign (%) was left in the copied command that needed to be removed.\n","Q: Which OpenAI model does the instructor choose to initialize and why?\n","A: gpt-4o, because it is the latest model released by OpenAI at the time of the video.\n","Q: If someone is concerned about cost, which alternative model does the instructor suggest?\n","A: GPT-3.5\n","Q: What Python import statement is used to bring the ChatOpenAI class into the script?\n","A: from langchain_openai import ChatOpenAI\n","\n","KEY CONCEPTS:\n","\n","LangChain chat models, ChatOpenAI class, OpenAI API integration, GPT-4o model, pip install langchain-openai, model initialization, latest model cost, GPT-3 fallback\n","\n","============================================\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","Python’s default sort places uppercase-initial strings before lowercase-initial ones within each alphabetic subgroup; reversing the list inverts this order. Numeric items are always collated before any string, and their relative position is also reversed by descending sorts. To obtain case-insensitive ordering, strings must be normalized to a uniform case prior to sorting.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: When Python sorts a list of strings, how does it order words that start with uppercase letters versus those that start with lowercase letters?\n","A: Python puts all words beginning with uppercase letters first, sorted alphabetically among themselves, followed by all words beginning with lowercase letters, also sorted alphabetically.\n","Q: If you reverse a sorted list of mixed-case strings, what order will the uppercase and lowercase words appear in?\n","A: The lowercase words will appear first in reverse alphabetical order, followed by the uppercase words in reverse alphabetical order.\n","Q: What does Python do when sorting a list that contains both strings and numbers?\n","A: Python places all numbers first, followed by all strings, each group sorted among itself.\n","Q: Why might you normalize the case of strings before sorting them alphabetically?\n","A: To prevent Python from separating uppercase and lowercase words, ensuring a single alphabetical sequence regardless of case.\n","Q: Where will the number 18 appear in a list after sorting and then reversing that list?\n","A: After sorting it will be at the beginning; after reversing the sorted list it will be at the end.\n","\n","KEY CONCEPTS:\n","\n","case-sensitive sorting, mixed-type list sorting, Python sort method, reverse sorting, uppercase precedence, lowercase precedence, alphabetical ordering, numeric precedence\n","\n","============================================\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: \n","Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","High-volume fraud-detection triage illustrates how to optimise human–AI collaboration: an AI model that flags 90 % false-positive alerts is best deployed on extreme-confidence cases, while intermediate scores are routed to analysts whose contextual reasoning outperforms the algorithm near 50 % confidence. Augmented-intelligence systems therefore automate clear-cut decisions and reserve ambiguous alerts for humans, reducing workload without sacrificing accuracy. However, presentation design is critical: displaying AI recommendations alongside alerts risks automation bias, whereas optional disclosure—letting reviewers form an initial judgment before requesting advice—preserves autonomy yet lowers uptake when accuracy is revealed. Framing task allocation as an evidence-based, quantifiable choice rather than a subjective preference shifts system design toward selecting the most effective agent—human, AI, or hybrid—for each case, maximising collective performance while mitigating cognitive bias.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: In the fraud-detection example, what percentage of daily alerts are false positives that overwhelm analysts?\n","A: 90 percent\n","Q: What does the Y-axis of the described graph represent?\n","A: The success rate of predictions about whether an alert is real or false\n","Q: What does an AI confidence score of 0 % indicate about an alert?\n","A: That the AI predicts the alert is definitely a false positive\n","Q: How does the shape of a typical AI performance curve compare with a human performance curve?\n","A: The AI curve rises sharply at very low and very high confidence scores, while the human curve is typically flatter\n","Q: According to the transcript, when should an AI system ideally handle an alert instead of a human analyst?\n","A: When the AI has very high or very low confidence in its prediction, leading to a high success rate\n","\n","KEY CONCEPTS:\n","\n","fraud detection system, false positive rate, confidence score, AI performance curve, human performance curve, decision delegation, alert triage, prediction uncertainty, human-AI collaboration, threshold optimization\n","\n","============================================\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: \n","Build generative apps faster with Vertex AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","Vertex AI’s six new APIs provide composable, stateless primitives that convert Google’s search and ads infrastructure into enterprise-grade generative building blocks: a document-understanding service extracts structure from complex files; enhanced embeddings, scalable vector search and hybrid retrieval raise recall; a ranking API re-orders candidates by relevance; a grounded-generation model returns attributed answers; and a check-grounding endpoint validates claims against sources. By exposing these loosely coupled components, Google enables developers to assemble retrieval-augmented systems without managing infrastructure, concentrating effort on domain-specific logic while inheriting Google-scale retrieval quality and citation-level verifiability.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main goal of the new Vertex AI APIs discussed in the talk?\n","A: To solve common technical challenges developers face when building enterprise generative applications, letting them focus on unique features while Google handles document processing, retrieval, and grounding.\n","Q: Which API is designed to parse complex enterprise documents with sections, tables, and graphs?\n","A: The Document Understanding API, which uses Google’s document-AI know-how to extract structure and improve retrieval and answer quality.\n","Q: How does the Ranking API improve generative responses?\n","A: It scores each retrieved result for relevance to the user’s question, bubbling up the best evidence so the LLM can produce higher-quality, better-grounded answers.\n","Q: What capability does the ‘check grounding’ API provide?\n","A: It fact-checks every sentence of a statement against supplied evidence, labeling each sentence as supported, unsupported, or (in upcoming releases) contradicted.\n","Q: Why are these APIs described as easy to integrate into existing developer workflows?\n","A: They are stateless, standalone primitives with clear interfaces, available in popular frameworks like LangChain and LlamaIndex, so developers can quickly test, combine, and deploy them alongside third-party or open-source tools.\n","\n","KEY CONCEPTS:\n","\n","document understanding API, embedding API improvements, vector search hybrid retrieval, ranking API relevance scoring, grounded generation API with citations, check grounding fact verification, enterprise data grounding, stateless API primitives, chain/llama index integration, planet-scale embedding retrieval\n","\n","============================================\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","Singular-value decomposition X = UΣVᵀ factorises any real matrix into two unitary rotations and a diagonal scaling, preserving vector angles and lengths. Because unitary operators act as rigid rotations of the data cloud, they allow coordinate changes without distorting intrinsic geometry, a property shared by transforms such as the Fourier operator. The economy-size SVD retains only the leading columns corresponding to the rank of X. Geometrically, multiplication by X sends the unit sphere in row space to an ellipsoid in column space whose principal axes are oriented by the columns of U and stretched by the singular values; the adjoint mapping performs the dual operation. This data-driven rotation therefore exposes dominant directions of variance, even for rectangular matrices, providing a foundation for low-rank approximation, principal-component analysis and related dimensionality-reduction techniques.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Data Science', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What defining property makes U and V in the SVD unitary matrices?\n","A: They satisfy U Uᵀ = Uᵀ U = I and V Vᵀ = Vᵀ V = I, meaning their transpose is their inverse.\n","Q: Why is the economy-size SVD matrix Û only partially unitary?\n","A: Because Û is rectangular (M × N with M < N), so ÛᵀÛ = I but ÛÛᵀ ≠ I.\n","Q: Geometrically, what does multiplying a sphere of unit vectors by X produce?\n","A: It maps the sphere into an ellipsoid whose principal-axis lengths are the singular values of X and whose orientation is given by the left singular vectors.\n","Q: If X is 2×3, what dimension is the ellipsoid formed by mapping the 3-D unit sphere through X?\n","A: It becomes a 2-D ellipsoid in the 2-D column space of X.\n","Q: How does a unitary transformation affect the inner product ⟨x, y⟩?\n","A: It leaves it unchanged: ⟨Ux, Uy⟩ = ⟨x, y⟩ for all vectors x and y, preserving angles and lengths.\n","\n","KEY CONCEPTS:\n","\n","singular value decomposition, unitary matrices, economy size SVD, complex conjugate transpose, Fourier transform as unitary, angle and length preservation, inner product invariance, geometric ellipsoid mapping, left and right singular vectors, singular values as stretch factors\n","\n","============================================\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","This session provides a comprehensive tutorial on building generative-AI applications with Google Gemini Pro 1.5, a unified multimodal model supporting text and images within a one-million-token context window. Participants obtain a no-cost API key, configure a secure Python environment, and iteratively test the model’s capacity to process extensive documents, exemplified by a 402-page Apollo 11 transcript from which it extracts quotes, matches sketches to events, and cites timestamps. The workflow demonstrates simplified multimodal prompting—combining images and text without preprocessing—and streaming responses for reduced latency. Consolidating prior separate Pro and Vision endpoints, Gemini 1.5 enables end-to-end projects such as PDF-query RAG systems and blog-post generation from photographs, positioning the model as a scalable tool for large-scale, cross-modal reasoning tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Python Programming', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main focus of Krish's video?\n","A: Building generative AI-powered applications using Google Gemini Pro 1.5.\n","Q: What does it mean when Krish says Gemini Pro 1.5 is 'multimodal'?\n","A: It means the model can work with both text and images.\n","Q: According to the transcript, what will viewers see in the first minute of the video?\n","A: A 1-minute demo video from Google showcasing what Gemini Pro 1.5 can do.\n","Q: What practical steps will Krish demonstrate after the demo?\n","A: Creating an API key and running hands-on code that uses both images and text with Gemini Pro 1.5.\n","Q: Why does Krish mention his previous Gemini Pro playlist?\n","A: To highlight that he has already created many end-to-end projects with earlier Gemini versions.\n","\n","KEY CONCEPTS:\n","\n","Google Gemini Pro 1.5, generative AI application, multimodal model, long context understanding, API key creation, end-to-end projects, text and image processing, experimental feature\n","\n","============================================\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","The session provides a systematic framework for evaluating and refining prompt-engineered models, introducing perplexity, accuracy, and human-judgment metrics to quantify predictive quality and response correctness. A live demonstration on a small translation dataset confirms high accuracy, while subsequent discussion addresses iterative debugging via error-pattern analysis, generalization testing across diverse datasets and tasks, and the use of visualizations and cross-validation. Continuous assessment is emphasized as essential for sustaining performance, preparing participants for advanced prompt-engineering techniques in later lessons.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three commonly mentioned evaluation metrics for prompt-engineering models?\n","A: Perplexity, accuracy, and human evaluation.\n","Q: What does lower perplexity indicate about a language model?\n","A: It indicates that the model predicts the sequence of words better.\n","Q: Why is it useful to test a prompt-engineering model on multiple datasets or tasks?\n","A: Testing on varied data helps determine the model’s ability to generalize to new or unseen data.\n","Q: Which technique is described for debugging and improving a model after evaluation?\n","A: Analyzing the generated responses to identify common errors or patterns, then fine-tuning the model accordingly.\n","Q: Is model evaluation a one-time step or an ongoing process?\n","A: It is an ongoing process; the model should be continually evaluated and tested to ensure sustained performance.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering evaluation, perplexity metric, human evaluation, evaluate_translation function, model debugging, cross-validation testing, response pattern analysis, model generalization, fine-tuning techniques, visualization tools for errors\n","\n","============================================\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","The presentation distinguishes three ascending levels of AI capability. Generative AI, grounded in large language models, generates novel text or media from learned patterns but remains constrained by a fixed knowledge cutoff. AI agents extend the LLM ‘brain’ with tools and memory, enabling autonomous execution of narrow tasks such as live flight search and booking. Agentic AI coordinates multiple agents, tools and knowledge sources to accomplish multi-step, long-horizon objectives—illustrated by planning a weather-sensitive, budget-constrained itinerary while verifying visa requirements—thereby exhibiting greater autonomy and coordination. Development frameworks (N8N, Agno, LangGraph) facilitate construction of these systems, with architectural complexity progressing from simple question-answering to collaborative, goal-oriented workflows.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Agentic AI', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the key difference between generative AI and an AI agent?\n","A: Generative AI only creates new content (text, image, video) based on learned patterns, whereas an AI agent can take actions—like booking a flight—by using tools, memory, and knowledge to complete tasks autonomously.\n","Q: Why can’t a standalone LLM give tomorrow’s flight price?\n","A: A standalone LLM has a knowledge-cutoff date and no live data access; only when given an API like Expedia can it fetch current prices.\n","Q: What makes a system ‘agentic AI’ rather than just an ‘AI agent’?\n","A: Agentic AI handles complex, multi-step goals—often with multiple cooperating agents, advanced planning, and higher autonomy—while a simple AI agent performs narrower, usually single tasks.\n","Q: Name two tools or APIs mentioned that agents can use to expand their capabilities.\n","A: Expedia (travel) API and AccuWeather API.\n","Q: In the transcript’s final project example, what autonomous actions does the onboarding agentic AI perform?\n","A: It adds the new employee to the HRMS system, sends a welcome email, and notifies the manager.\n","\n","KEY CONCEPTS:\n","\n","generative Artificial Intelligence (AI), large language model (LLM), knowledge cutoff date, AI agent, tool use in AI, agentic Artificial Intelligence (AI), multi-step reasoning, autonomous decision making, multi-agent coordination, human-in-the-loop control\n","\n","============================================\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: \n","Covariance in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","Covariance quantifies the direction of linear association between two random variables by averaging the product of their deviations from their respective means. Positive values indicate that the variables tend to increase together, whereas negative values suggest inverse movement. Using the example of house size and price, the lecture demonstrates how this statistic signals co-movement but remains uninformative about the strength of the relationship. Because covariance is expressed in the product of the original units and is unbounded, it cannot be used to compare the magnitude of associations across different variable pairs or scales. Consequently, the lecture positions covariance as a foundational yet limited measure that naturally leads to the introduction of Pearson’s correlation coefficient, a standardized metric that will subsequently address the issue of strength and enable meaningful comparisons.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What does covariance quantify between two random variables?\n","A: It quantifies how the two variables change together—whether they increase or decrease in tandem.\n","Q: In the covariance formula Cov(X,Y)=1/n Σ(Xi–μX)(Yi–μY), what do μX and μY represent?\n","A: μX is the mean of the random variable X and μY is the mean of the random variable Y.\n","Q: If larger house sizes consistently pair with higher prices, what sign will the covariance between size and price have?\n","A: Positive covariance.\n","Q: When X increases and Y decreases, what sign will the covariance have?\n","A: Negative covariance.\n","Q: Why is covariance alone insufficient for measuring the strength of a linear relationship?\n","A: Because it only indicates the direction (positive or negative) but not the magnitude of the relationship; Pearson correlation coefficient is needed to gauge strength.\n","\n","KEY CONCEPTS:\n","\n","covariance formula, variance-covariance relationship, positive covariance, negative covariance, Pearson correlation coefficient, random variable relationship, mean deviation product, covariance interpretation, covariance limitations, quantifying variable dependence\n","\n","============================================\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning is framed as the problem of discovering an optimal policy that maximises cumulative reward, mirroring human goal-directed behaviour under temporal and feedback constraints. Reward functions numerically encode task success and, through repeated interaction, agents iteratively refine state-action value estimates. Episodic examples such as Tic-Tac-Toe and continuing tasks like stock trading illustrate how trial-and-error updates convert immediate rewards into long-term value predictions, enabling systematic improvement of decision-making performance.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","optimal policy, cumulative reward, reward function, trial-and-error learning, value-based methods, policy-based methods, episodic tasks, continuous tasks, risk-adjusted measures, state-action value update\n","\n","============================================\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","Python dictionaries are presented as mutable mappings that bind immutable keys to values, created with curly braces and colons, commas delimiting items. Core operations covered include querying via items(), keys(), values(); instantiation from zipped lists; element access and mutation through square-bracket indexing; deletion with del; length retrieval; and extraction of key or value lists. These competencies establish the foundational knowledge required for subsequent data-science applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What characters are used to separate key-value pairs and items in a Python dictionary?\n","A: A colon separates the key and value within each item, and commas separate the individual items.\n","Q: Why can a tuple be used as a dictionary key while a list cannot?\n","A: A key must be immutable; tuples are immutable, whereas lists can change and therefore are not allowed.\n","Q: How would you retrieve the list of all keys from a dictionary named prices?\n","A: Use prices.keys() to obtain the list of keys.\n","Q: If you have two equal-length lists, how can you build a dictionary from them in one line?\n","A: Zip the lists together and pass the result to dict(), e.g. dict(zip(list1, list2)).\n","Q: What Python statement deletes the entry with key 'x' from dictionary d?\n","A: del d['x'] removes the key 'x' and its associated value from the dictionary.\n","\n","KEY CONCEPTS:\n","\n","key-value pair, immutable key, dict constructor, zip function, dictionary comprehension, dict.items(), dict.keys(), dict.values(), del statement, dictionary indexing\n","\n","============================================\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: \n","Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","AI-driven User Behaviour Analytics (UBA) embedded in IBM QRadar SIEM shortens breach lifecycles by 108 days and halves the US$4 million average cost of insider incidents. After a seven-day unsupervised learning phase, machine-learning models baseline individual and peer-group behaviour, then flag deviations as risk-scored, MITRE-mapped offences. Analysts triage alerts within minutes through dashboards, watch-lists, timeline views and relationship graphs; AI-generated insights and confidence metrics accelerate investigations, while continuous human feedback refines detection. By shifting security operations from reactive alert handling to proactive, evidence-based defence, UBA reduces dwell time, limits financial impact and strengthens overall organisational security posture against insider threats that conventional controls overlook.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: According to IBM’s 2023 Cost of a Data Breach Report, how many fewer days on average did organizations that extensively used AI and automation take to identify and contain a breach compared with those that did not?\n","A: 108 fewer days.\n","Q: What is the average cost of an insider-threat incident to an organization as cited in the transcript?\n","A: $4.9 million.\n","Q: What does UBA stand for and how does AI enhance it?\n","A: User Behavior Analytics; AI (and machine learning) enables faster and more precise detection of and response to insider threats.\n","Q: Why are insider threats described as a major concern for organizations?\n","A: They affect organizations of all sizes and carry a high average cost per incident.\n","Q: What overall benefit does the transcript suggest AI and automation provide to security teams?\n","A: They help security professionals stay ahead of emerging threats and improve their organization’s security posture.\n","\n","KEY CONCEPTS:\n","\n","user behavior analytics, insider threat detection, AI-driven breach containment, machine learning security automation, behavioral anomaly detection, AI-based threat intelligence\n","\n","============================================\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","Meta’s release of the open-source Llama 3 family (8B and 70B parameters, trained on 50 trillion tokens) attains state-of-the-art results on reasoning, code generation and instruction-following benchmarks, outperforming comparable open models and rivaling proprietary systems such as GPT-4 and Gemini Pro 1, while exhibiting fewer false refusals and stronger multi-task competence. The segment situates Llama 3 within Meta’s responsible-AI pipeline—define, model, train, evaluate, improve—and showcases the Llama Guard transparency accessory. Practical access is governed by a lightweight approval workflow: developers register on Meta’s portal, submit a use-case form, receive a signed download link, and may alternatively fetch model cards from Hugging Face or Kaggle; GitHub documentation supplies checkpoint details and installation commands. Local inference requires obtaining weights in Transformers or native format, executing provided setup scripts, and consulting bundled recipes; a forthcoming tutorial promises applied demonstrations.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI']\n","\n","GENERATED Q&A:\n","\n","Q: Who is speaking in the transcript?\n","A: Krishak\n","Q: What platform is being used in the transcript?\n","A: YouTube\n","Q: What time is mentioned in the transcript?\n","A: 2 a.m.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","The instructor provides a deliberate, step-by-step walkthrough of re-implementing the previously shown Python decision-boundary script, assuring students that mastery will follow within one or two sessions. Learners are explicitly directed to consult Google for navigating scikit-learn documentation, with targeted searches for the Naive Bayes algorithm. Gaussian Naive Bayes is identified as the specific variant employed for the classifier, while deeper theoretical exposition is postponed until after students have successfully executed the code, prioritising hands-on competence over conceptual detail.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: Which Python library is used in the lesson for implementing the classifier?\n","A: scikit-learn (often abbreviated sklearn).\n","Q: What specific type of Naive Bayes classifier is implemented in the code shown?\n","A: Gaussian Naive Bayes.\n","Q: Why does the instructor suggest searching Google for 'sklearn Naive Bayes'?\n","A: To access the library’s documentation and find guidance on using its functions.\n","Q: What will viewers be able to do by the end of the next one or two videos?\n","A: Write the Python code for the decision boundary themselves.\n","Q: Besides the derivation of the Naive Bayes formula, what else does the documentation page provide?\n","A: A bunch of use cases for the algorithm.\n","\n","KEY CONCEPTS:\n","\n","Gaussian Naive Bayes, scikit-learn, decision boundary, Naive Bayes classifier, sklearn documentation, Python library integration\n","\n","============================================\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: \n","Log Normal Distribution in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","The session extends the discussion of Gaussian distributions to the log-normal family, whose defining property is the normality of ln(X). Empirical examples—income distributions and product-review lengths—illustrate the right-skewed, heavy-tailed behaviour that distinguishes log-normal from Gaussian data. Correct model specification hinges on recognising this difference: standardisation is appropriate for Gaussian variables, whereas log-normal variables require a preliminary log-transformation to restore normality. These preprocessing steps ensure that features share a comparable scale, thereby enhancing the performance and stability of downstream statistical and machine-learning models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What percentage of data falls within one standard deviation of the mean in a Gaussian distribution?\n","A: 68 percent of the total distribution falls within one standard deviation.\n","Q: How is a random variable X said to follow a log-normal distribution?\n","A: X follows a log-normal distribution if the natural logarithm of X (ln X) is normally (Gaussian) distributed with some mean μ and standard deviation σ.\n","Q: Give two real-world examples mentioned that follow a log-normal distribution.\n","A: Income of people and the length of product reviews/comments on e-commerce sites like Amazon.\n","Q: Why is it useful to convert data from Gaussian or log-normal form into standard normal distribution before feeding it to a model?\n","A: Converting to standard normal (mean 0, std 1) puts all features on the same scale, which increases model accuracy regardless of the algorithm used.\n","Q: If a marketing-spend column is log-normally distributed, what preprocessing step lets you scale it to the same standard-normal scale as a Gaussian-distributed R&D column?\n","A: Take the natural log of every marketing value so the transformed data become Gaussian, then apply the standard-score formula (x−μ)/σ to obtain standard normal distribution.\n","\n","KEY CONCEPTS:\n","\n","log-normal distribution, Gaussian distribution, standard normal distribution, empirical rule, bell curve, log normalization, standard scaler, mean and standard deviation, data scaling for model accuracy, distribution identification for preprocessing\n","\n","============================================\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","The tutorial series presents an end-to-end deep-learning system that enables potato growers to mitigate early- and late-blight losses through automated leaf-disease classification. A data-science team at AtliQ Agriculture curates and augments field images, trains a convolutional neural network, exports the model to TensorFlow Serving, and exposes predictions via a FastAPI backend hosted on Google Cloud. Farmers access diagnoses through a React-Native mobile interface, receiving timely treatment recommendations that reduce crop loss. The curriculum guides learners through the complete supervised-learning pipeline: data acquisition, cleansing, augmentation, CNN training, model quantization to TensorFlow Lite, and deployment options comparing cloud-based inference against on-device execution. Architectural variants—standalone JavaScript web app versus mobile client—illustrate trade-offs between latency, accuracy, and resource constraints. Prerequisites include Python and CNN fundamentals; subsequent sessions detail data handling and optimization, providing a reproducible portfolio project.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Machine Learning', 'Mlops']\n","\n","GENERATED Q&A:\n","\n","Q: What are the two potato diseases the project aims to detect?\n","A: Early blight and late blight.\n","Q: Why is accurate disease identification important for farmers?\n","A: Because the treatments for early blight and late blight are different, so correct identification prevents economic loss.\n","Q: Which deep learning technique will the mobile app use to classify plant images?\n","A: Convolutional neural network (CNN).\n","Q: Name two technologies mentioned for deploying and serving the model.\n","A: TF Serving and Google Cloud Functions.\n","Q: What is the farmer required to do to receive a disease diagnosis from the app?\n","A: Take a picture of the potato plant with the mobile app.\n","\n","KEY CONCEPTS:\n","\n","convolutional neural network, TF serving, FastAPI backend, Google Cloud functions, React Native mobile app, early blight detection, late blight detection, end-to-end ML pipeline, agriculture disease classification, ML Ops deployment\n","\n","============================================\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The lecture presents a six-level taxonomy of autonomy in LLM-based systems. Level 0, static code, requires exhaustive manual specification. Level 1 adds a single LLM call, enabling flexible but limited responses. Level 2 chains decompose tasks into fixed sequences of specialist prompts, improving reliability yet remaining brittle. Level 3 routers dynamically select among predefined chains, but lack memory or learning. Level 4 state-machine agents, exemplified by LangGraph, embed the LLM in a control loop: they iteratively plan, delegate sub-tasks, request human feedback, and refine outputs until success criteria are met before autonomously invoking tools. Level 5 envisions fully autonomous agents that additionally set and revise their own goals and action designs; these remain experimental. The progression highlights increasing adaptability, contextual memory, and self-direction, framing practical design trade-offs between control, transparency, and open-ended capability.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Langraph', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main drawback of using hard-coded rules (zero autonomy) in LLM applications?\n","A: You must write rules for every possible scenario, making it impossible to handle real-life complexity.\n","Q: Why can a single LLM call struggle when asked to perform multiple tasks in one prompt?\n","A: Because one model cannot be an expert at everything; giving it several tasks at once leads to confused or mixed-up responses.\n","Q: How do chains improve upon a single LLM call, and what limitation do they still retain?\n","A: Chains use multiple specialist LLMs in sequence, each excelling at one sub-task, but they always follow the same rigid, human-defined steps.\n","Q: What new capability does a router add that chains lack?\n","A: A router lets the LLM itself decide which predefined chain or tool to invoke next, rather than following a fixed path.\n","Q: What key features distinguish a state-machine/agent architecture from simpler routers or chains?\n","A: It supports loops, human-in-the-loop approval, multi-agent delegation, memory of past steps, and adaptive learning so the system can refine outputs and avoid repeating mistakes.\n","\n","KEY CONCEPTS:\n","\n","LLM autonomy levels, zero autonomy code, single LLM call, AI chains, router LLM, state machine agent, LangGraph, human-in-the-loop approval, multi-agent hierarchy, agent executed control flow\n","\n","============================================\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","The text presents a comprehensive pipeline for developing production-grade prompt-driven systems, progressing from foundational prompt-engineering techniques to expert-level multimodal prompts that integrate text, image, and audio. It covers advanced fine-tuning strategies including multitask learning and knowledge distillation, alongside rigorous data preprocessing steps such as tokenization and normalization. Deployment considerations emphasize scalable infrastructure using TensorFlow Serving or Flask APIs, while ethical imperatives address bias mitigation, fairness, and privacy protection. Practical exercises consolidate these elements, guiding readers to iteratively build, optimize, and responsibly deploy high-performance, ethically aligned models, thereby bridging research innovations with real-world accountability and operational robustness.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Generative AI', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three main modalities of prompts discussed in the transcript?\n","A: Text-based, image-based, and audio-based prompts.\n","Q: Why is multitask learning used when fine-tuning large language models?\n","A: It trains the model on several tasks at once so it learns richer, more general representations that perform better across different users and use-cases.\n","Q: How does model distillation improve deployment efficiency?\n","A: A compact ‘student’ model is trained to imitate the outputs of a larger ‘teacher’ model, yielding faster inference while retaining most of the accuracy.\n","Q: Which two data-pre-processing steps are explicitly mentioned as best practices before training?\n","A: Tokenization (splitting text into words or sub-words) and normalization (e.g., converting everything to lowercase).\n","Q: Name two production deployment options for prompt-engineering models that are recommended in the transcript.\n","A: TensorFlow Serving and building a Flask web API.\n","\n","KEY CONCEPTS:\n","\n","multitask learning, knowledge distillation, tokenization, text normalization, TensorFlow Serving, Flask deployment, ethical bias mitigation, audio-based prompts, image-based prompts, fine-tuning pre-trained LLMs\n","\n","============================================\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","The lecture illustrates eigenfaces for face classification by performing SVD on 40 aligned 200×175-pixel grayscale images of Arnold Schwarzenegger and Sylvester Stallone. After mean-centering the 35 000-pixel vectors, the leading principal components (“eigen heroes”) embed images in a 3-D latent space where the actors form cleanly separated clusters; test images classify accurately. Repeating the protocol with Taylor Swift yields stronger separation from Stallone but unexpected overlap between Swift and Arnold, demonstrating that pixel-based eigenfaces can conflate skin and hair colour with identity. The experiment underscores both the power and the limitations of unsupervised, correlation-driven image representations for recognition tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Statistics', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What preprocessing steps are applied to each 200×175 image before building the data matrix?\n","A: Convert to grayscale if color, crop and align so faces fill the same box, then reshape into a 35,000-pixel column vector.\n","Q: Why is the mean ‘average action-hero’ face subtracted from every image before computing the SVD?\n","A: Centering the data (subtracting the mean) is required so the SVD yields the principal components—here the eigenfaces—of the dataset.\n","Q: In the 3-D eigenface space, which pair achieves the cleanest cluster separation: Arnold vs. Stallone, Taylor vs. Stallone, or Arnold vs. Taylor?\n","A: Taylor Swift versus Stallone shows the best separation; Arnold and Taylor overlap most because their similar fair skin and light hair dominate the pixel correlations.\n","Q: How can a new test image be classified as Arnold or Stallone after the eigenfaces are computed?\n","A: Project the test image into the first three eigenface coordinates and assign it to whichever cluster (Arnold or Stallone) it lands closest to in that space.\n","Q: What limitation of this eigenface approach is highlighted by the Arnold–Taylor overlap?\n","A: The method relies on pixel correlations, so superficial features like skin tone and hair color can outweigh perceived semantic differences, leading to counter-intuitive classifications.\n","\n","KEY CONCEPTS:\n","\n","singular value decomposition, eigenfaces, principal component analysis, image classification, eigen action heroes, face space projection, economy SVD, average face subtraction, three-dimensional face inference, skin tone correlation bias\n","\n","============================================\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","LangChain is presented through a vacation-planning scenario—booking a Paris flight, reserving a hotel, and selecting restaurants—demonstrating that isolated large language models cannot autonomously execute real-world tasks. While LLMs excel at reasoning, they lack direct access to external systems; LangChain overcomes this limitation by orchestrating models with APIs, databases, and services, enabling end-to-end automation of bookings, data queries, emails, and other actions. The framework preserves modular flexibility, allowing developers to swap underlying models without altering application code, thereby transforming LLMs from conversational agents into actionable, context-aware components of production systems.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Prompt Engineering', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: What specific vacation-planning tasks does the speaker ask the LLM to handle in the example?\n","A: Book a flight to Paris, book a hotel for the same day, and suggest good restaurants.\n","Q: According to the transcript, what happens immediately after the speaker presses enter?\n","A: The query is sent to an LLM model.\n","Q: Why does the speaker pause before pressing enter in the demonstration?\n","A: To explain what will happen behind the scenes once the query is submitted.\n","Q: Which day of the week does the speaker want to travel to Paris?\n","A: This Saturday.\n","\n","KEY CONCEPTS:\n","\n","LangChain, LLM model, ChaGPT application, trip planning, restaurant recommendation\n","\n","============================================\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","Residual analysis is presented as an essential diagnostic for improving time-series forecasts. Residuals—differences between fitted and observed values—should be uncorrelated around a zero mean; deviations signal model bias or missing structure. Using airline passenger data, autocorrelation function plots, the Ljung-Box test and histograms evaluate Holt-Winters forecasts, revealing significant residual autocorrelation and prompting model revision. The histogram shows near-symmetry with a small negative mean, indicating low bias relative to passenger scale. Systematic residual inspection thus guides targeted corrections, an intuitive Python-based step strongly recommended for all forecasting workflows.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Statistics', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the difference between residuals and errors in time-series modelling?\n","A: Residuals are the differences between the fitted values (on training data the model has seen) and the actual values, whereas errors are the differences on unseen test data.\n","Q: Why should the mean of the residuals be zero?\n","A: A non-zero mean indicates a systematic bias—consistent over- or under-forecasting—so a mean of zero shows the model is unbiased.\n","Q: Which two key properties should residuals exhibit for a well-specified time-series model?\n","A: Residuals should have no autocorrelation (or partial autocorrelation) and their mean should be zero.\n","Q: What does the Ljung-Box test check in residual analysis?\n","A: It tests the null hypothesis that residuals are independently distributed (no serial correlation); significant p-values suggest correlation remains.\n","Q: How can residual analysis guide model improvement when autocorrelation is detected?\n","A: Significant autocorrelation indicates the model has missed information (e.g., seasonality); capturing that structure (e.g., adding seasonal terms) can improve forecasts.\n","\n","KEY CONCEPTS:\n","\n","residual analysis, autocorrelation function, partial autocorrelation function, Ljung-Box test, Holt-Winters model, fitted values vs forecast, zero-mean residuals, seasonality detection, exponential smoothing, serial correlation\n","\n","============================================\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","This tutorial details building a conversational text-to-SQL AI agent using LangGraph, Next.js, Tailwind, and watsonx.ai’s Mistral Large. A ReAct-style agent is scaffolded inside a TypeScript Next.js app, exposing a chat interface that serializes conversation history to a server-side actions.ts handler. After configuring Watsonx credentials, the frontend input becomes a controlled component whose messages are stored as LangChain BaseMessage arrays; UI feedback is managed via an isLoading flag. A SQLite database (customer/order tables) is seeded on mount and exposed through a GetFromDB LangChain tool, letting the agent generate and execute SQL while shielding the schema with quoted identifiers. Starting with simple counts, the agent progresses to multi-table joins, demonstrating autonomous querying and highlighting the need for access guardrails.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Langraph', 'Python Programming', 'Agentic AI']\n","\n","GENERATED Q&A:\n","\n","Q: Which three main technologies are used to build the SQL-speaking AI agent in the video?\n","A: LangGraph for the ReAct agent, Next.js for the frontend, and watsonx.ai for the models.\n","Q: Why does the presenter choose Tailwind CSS for the project?\n","A: To avoid having to write any custom CSS.\n","Q: What type of database is used during development, and how is it run?\n","A: An in-memory SQLite database.\n","Q: What must you do before starting the Next.js application after running create-next-app?\n","A: Move into the newly created project directory.\n","Q: In the Next.js app, what distinction is made about component execution that affects how the Home component is configured?\n","A: Components can run either client-side or server-side; the Home component is explicitly set to run client-side.\n","\n","KEY CONCEPTS:\n","\n","ReAct agent, LangGraph, Text2SQL agent, in-memory SQLite database, Next.js client-side component, Tailwind CSS, watsonx.ai models, create-next-app CLI, TypeScript boilerplate, SQL knowledge extraction\n","\n","============================================\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","The lecture frames prompt engineering as a systematic methodology for eliciting accurate, coherent, and context-sensitive outputs from large pre-trained language models, positioning it within the broader evolution of natural-language processing. It contrasts data-driven prompting with brittle rule-based systems, enumerates advantages such as rapid deployment and adaptability, and acknowledges persistent limitations including encoded bias, ambiguity propagation, and hallucination. The session concludes by mapping the course trajectory from foundational prompt analysis through iterative refinement to advanced fine-tuning, establishing the conceptual scaffold for subsequent technical modules.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","GENERATED Q&A:\n","\n","Q: What is prompt engineering and which family of models does it build on?\n","A: Prompt engineering is a specialized NLP field that builds on pre-trained large language models such as OpenAI’s GPT, Google Bard, or Hugging Face Transformers to generate high-quality text outputs from prompts.\n","Q: Why is prompt engineering considered more effective than rule- or keyword-based text generation?\n","A: It produces outputs that are more accurate, coherent, and contextually appropriate, leading to better user experience in applications like chatbots, translation, and content generation.\n","Q: Name two limitations of prompt engineering models mentioned in the transcript.\n","A: They may struggle with complex or ambiguous prompts and can generate biased or inaccurate outputs due to limitations in the underlying data or model architecture.\n","Q: What foundational skill will you learn first in this course regarding prompt analysis?\n","A: You will learn how to deconstruct prompts and identify their key features and constraints.\n","Q: By the end of the introductory section, what overall understanding should you have?\n","A: You should understand what prompt engineering is, why it matters, its benefits and limitations, and what the rest of the course will teach you.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, large language models, fine-tuning pre-trained models, prompt analysis, contextually appropriate text generation, bias in language models, ambiguous prompt handling, rule-based vs prompt-based approaches, natural language processing, transformer architectures\n","\n","============================================\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","Q-learning is an off-policy, value-based reinforcement-learning algorithm that iteratively estimates the optimal state-action value function Q by applying the Bellman equation. After positioning the method within the three learning paradigms and contrasting value-based with policy-based approaches, the lecture uses a grid-world to show how an agent explores, receives rewards, and updates a Q-table via temporal-difference learning. Each step computes the observed return, compares it with the current Q-value to obtain a TD error, and adjusts the estimate by a learning-rate-scaled update, exemplified by revisions from 1 to 0.815 and from 1.5 to 1.27. Episodes terminate at ±10; the same table is reused across many stochastic episodes until convergence. Once Q-values stabilise, the agent exploits the maximum entries to derive the optimal target policy, while the behaviour policy remains exploratory, thereby satisfying the off-policy property of Q-learning.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three main machine-learning paradigms mentioned in the transcript?\n","A: Supervised learning, unsupervised learning, and reinforcement learning.\n","Q: How does a value-based RL method differ from a policy-based one?\n","A: Value-based methods first learn a value function that estimates total reward and then derive the optimal policy from it, whereas policy-based methods search directly for the policy that maximizes total reward.\n","Q: In Q-learning, what do the rows and columns of the Q-table represent?\n","A: Rows represent states and columns represent possible actions; each cell holds the Q-value for that state-action pair.\n","Q: Why does the agent use a behavior policy that chooses actions randomly during learning?\n","A: Random exploration ensures the agent tries diverse actions and discovers rewards it might miss if it always picked the currently highest Q-value, helping it learn the optimal policy.\n","Q: According to the Bellman equation used in the example, what two components make up the observed Q-value for taking action ‘right’ from state S1?\n","A: The immediate reward received after transitioning to state S2 (−1) plus the discounted maximum Q-value available from that next state (γ · max Q(S2, a)).\n","\n","KEY CONCEPTS:\n","\n","Q-learning, value-based reinforcement learning, state-action value function, Bellman equation, behavior policy, target policy, discount factor gamma, exploration policy, optimal policy, state value function\n","\n","============================================\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","The lecture presents logistic classification as a linear method that maps input features to class predictions through matrix multiplication, parameterized by learnable weights W and bias b. Training adjusts these parameters to maximize the score for the correct class. Because each image receives one label, raw scores are transformed into probabilities via the softmax function, guaranteeing non-negative outputs that sum to unity and encode relative confidence, thereby enabling principled probabilistic classification.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What type of classifier is a logistic classifier and what operation does it perform on the input?\n","A: A linear classifier; it performs a giant matrix multiply (linear function) on the input vector.\n","Q: Which symbols are used for the inputs, weights, and bias term in the model?\n","A: X for inputs, W for weights, and b for the bias term.\n","Q: Why is the softmax function applied to the scores (logits) in logistic classification?\n","A: To convert arbitrary scores into proper probabilities that sum to 1, making the correct class probability near 1 and others near 0.\n","Q: What property must ‘proper probabilities’ produced by softmax satisfy?\n","A: They must sum to 1.\n","Q: During training, which parameters of the logistic classifier are optimized to improve predictions?\n","A: The weights (W) and the bias (b).\n","\n","KEY CONCEPTS:\n","\n","logistic classifier, linear classifier, softmax function, logits, matrix multiply, weights and bias, probability normalization, multi-class classification\n","\n","============================================\n","Saved row 29\n","\n","All rows processed.\n","\n","Generation completed. Run separate evaluation script next.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E_jDCKiKJjR_","executionInfo":{"status":"ok","timestamp":1763992556463,"user_tz":-330,"elapsed":4514,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"271bbbd4-e014-425f-b836-ad2baaca7f3f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_cot.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/kimi-k2-instruct-0905/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["ff92994b72fd44188ae42f5c493de387","5ef276c150b047a0838dabdf8fcda02f","66895fabef2645958ad4b13c45334d33","ac012fa3626248229d2cb6f0677b1a31","ad1a6d8d5c3041b0b7d54d5396cefa84","e2323df27c5b4712bd6b7da7d3e0ac10","4d4425db6e914c96a592f09a8261710e","04c8d3ef99f64215b81a1acb5ee7b0c6","2d4975354c084d158628f6e8991e8afe","b9f455ce2de64e60b0eef248fa739af3","e8eb1cc78961445cb5fac5030682d928","0dfce95e23324d32ab3cab2912a7a0f9","4b6fc074996f41a4a59b7d84960e3970","f66c1ade0b8940b3a491383012319b6f","9ebc7aa317e24f5f97ec1915aa818f1a","0670da65b93e4d719c91f9cab93142c5","fe9234c242744ce98b9cc9ca776a0e3b","7cabbad07a0a48be896aed0c86985e01","863f28b1c997440c933249f77e268514","129e3585b48847608ce7cc8a612b8dd6","5736c2cff36c468ea87f736e63b1e126","ed9e05dc5afa467f9937e64780c2e118","afc5bcc05bd74d0ebebc06a167af46d3","7453564f7f954c68b0055740f9625b5c","60f31b8af0f64dc3befb3063c12a0774","0e6c822678724961bb1a3d1b25a7b40f","e35eb975705d42d08d74519fa21ad39b","8cc616b131d84ecbafa3343f5d284083","9eca702235c449419003779427298e00","de6be06c80de4b76972794f654827eed","d08a350501744e10b05bfb9ee1ba7de4","7da9501a795145288ad4edf78dada214","4218e86cf7a34fe587340fbc781788d9","10527112d7a049fa9d1aea4830c1729e","4323e8e9f041435c9ccb5c3f6b86543f","fb0022471abb404f82a0598b9e93eac1","b6d0a098212e45b2bd043622fdb45ae7","f44017ae651a43b7a7ac69dbd3aef062","bf201c6229e847baadf9689ecfe6d58c","16a683dc3fdd4781ba25f4e3d8539a23","cdf33e39389a4cb8a66c9062496657e7","db9e0e93a35344cb9b68f74121c80ee6","73e5a06f352e4aeb861a3a5c13046a0b","a1b9c42e6f80466ba67157c51ed49a6b","491588ca677c4fd48cbb527b0a1d9ef8","c575d647649a4adeb7173ff82972ca92","f22279a4e3ea472ea345f3fc623507ab","6f236792f3274024a62e4ba6f9b5ffc6","8074a79e8db2422ba863cdd03b914c79","a11a23d9326a479bbeb6c5dccb6b1086","41e4ccce1d63475ca842515a5fddcbb1","cbbdacc1c2f74712a31c72f5d6984b0a","8ff8eb735bd34e15a2a9c801427a3fe3","163376dccfca412295841e890611673e","5df55be92c2a4e98a088c54cf5c35a3a","1e286329b8ab47d19e5814094395e83d","480e5765fe644a84b228644b18811738","5d114d1de6894472808a7da28f7138e9","a4baf6b1a7f94d269fb187789e7b3ac6","a676d3b7e2634b8eada0d58554d60daa","1b8311554d8540658e5e055492544de2","a8e85b818a7745fbbb07b71f680edf80","791a605f0a1f4f71bf11798b09df41ff","4ed839fe859846f0afbf0dd8084f0380","8c44cdb9bc874f5a9275dfbcbd0917fc","ad87c8a0197141658f62ae48ae31e4c0"]},"id":"LSNizfXoqcY6","executionInfo":{"status":"ok","timestamp":1763992699736,"user_tz":-330,"elapsed":143261,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"ea81a6cb-7c8b-428b-81d2-75320d524f3d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_cot.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff92994b72fd44188ae42f5c493de387"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dfce95e23324d32ab3cab2912a7a0f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afc5bcc05bd74d0ebebc06a167af46d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10527112d7a049fa9d1aea4830c1729e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"491588ca677c4fd48cbb527b0a1d9ef8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e286329b8ab47d19e5814094395e83d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2133\n","  - BLEU: 0.0135\n","  - BERTScore F1: 0.8672\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3551\n","  - Micro F1: 0.4922\n","  - Macro F1: 0.4714\n","  - Weighted F1: 0.4671\n","\n","Q&A Generation:\n","  - BLEU: 0.0227\n","  - Diversity: 0.7718\n","  - Answerability: 0.5511\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5267\n","  - Recall@10: 0.2107\n","  - F1@10: 0.3010\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/kimi-k2-instruct-0905/evaluation_final.json\n"]}]}]}