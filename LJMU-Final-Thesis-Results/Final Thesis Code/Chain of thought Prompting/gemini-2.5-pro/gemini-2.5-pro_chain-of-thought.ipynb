{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/d0/G3julK5oy33rqpfl/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"943cc92da5f54543acc1a4295c79d96f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c00ef7e1149c4a3bb9a60917bafcf36d","IPY_MODEL_0ddcd53401e64fe98b8e0b883bf9a30b","IPY_MODEL_94d963562eda4c16b4a691b69e469725"],"layout":"IPY_MODEL_47287ad47392401e8a2e898ef7a95f3f"}},"c00ef7e1149c4a3bb9a60917bafcf36d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5bb028e0a814aca9c5619da777ffbaf","placeholder":"​","style":"IPY_MODEL_6b47c8224aa74a109d4fde087296dc51","value":"tokenizer_config.json: 100%"}},"0ddcd53401e64fe98b8e0b883bf9a30b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f37a3055506c413fab0b722e1a2e4943","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_281c0cc53f064ce8927ab75a21f7d1ec","value":25}},"94d963562eda4c16b4a691b69e469725":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6f3c10dcf444600aa3332bcf09c7567","placeholder":"​","style":"IPY_MODEL_501221449e3a4caa921edbabb69819e4","value":" 25.0/25.0 [00:00&lt;00:00, 1.83kB/s]"}},"47287ad47392401e8a2e898ef7a95f3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5bb028e0a814aca9c5619da777ffbaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b47c8224aa74a109d4fde087296dc51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f37a3055506c413fab0b722e1a2e4943":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"281c0cc53f064ce8927ab75a21f7d1ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6f3c10dcf444600aa3332bcf09c7567":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"501221449e3a4caa921edbabb69819e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ec07873d7384123a7f8a46aa20d839e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9300698d8b048af8f77b24ec1fe2df2","IPY_MODEL_2bb5f6a654e3487792c04dc7d32b0330","IPY_MODEL_58221be7330d4d0aa1feaf023a65cf5b"],"layout":"IPY_MODEL_f8618486cbed47739526ef3d8d8f041b"}},"a9300698d8b048af8f77b24ec1fe2df2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_46a7ca7e566446a49f91b0b43c3413c8","placeholder":"​","style":"IPY_MODEL_fcfa999a61b94424a94a2f7b504ec07b","value":"config.json: 100%"}},"2bb5f6a654e3487792c04dc7d32b0330":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd9b32c499e94ab0ab2a88a8a6f3bdd3","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e827ca3c8f114e908c30202340f0a439","value":482}},"58221be7330d4d0aa1feaf023a65cf5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c3e4697c3f74dfaaa14271031545a53","placeholder":"​","style":"IPY_MODEL_caca24c2f23e4ec3bc215d785ead9129","value":" 482/482 [00:00&lt;00:00, 24.1kB/s]"}},"f8618486cbed47739526ef3d8d8f041b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46a7ca7e566446a49f91b0b43c3413c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcfa999a61b94424a94a2f7b504ec07b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd9b32c499e94ab0ab2a88a8a6f3bdd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e827ca3c8f114e908c30202340f0a439":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c3e4697c3f74dfaaa14271031545a53":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"caca24c2f23e4ec3bc215d785ead9129":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"614ebef586764f198c74c0b0a231a0ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e12d66ccc0f84a2899464c060add6d5b","IPY_MODEL_26c37d0d226a4239bf99d89359a52edc","IPY_MODEL_e5f8bcb7391f48218e3ba5bfb8778789"],"layout":"IPY_MODEL_198c384aa4fb4e558cc27ab74c836c93"}},"e12d66ccc0f84a2899464c060add6d5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57f960cc6a6f4065b334073df2c05ac7","placeholder":"​","style":"IPY_MODEL_7d02a2da1d6c427ca32883d8373bb76d","value":"vocab.json: 100%"}},"26c37d0d226a4239bf99d89359a52edc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39f570dc9dc44b4cb4c8bfc0909ebf3c","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70bfbcc2fa384883a12fd8603be33be1","value":898823}},"e5f8bcb7391f48218e3ba5bfb8778789":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30661c6dcac44f96b4065607b887215f","placeholder":"​","style":"IPY_MODEL_bfd8b192dd22491f8a02462074381d31","value":" 899k/899k [00:00&lt;00:00, 7.20MB/s]"}},"198c384aa4fb4e558cc27ab74c836c93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57f960cc6a6f4065b334073df2c05ac7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d02a2da1d6c427ca32883d8373bb76d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39f570dc9dc44b4cb4c8bfc0909ebf3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70bfbcc2fa384883a12fd8603be33be1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30661c6dcac44f96b4065607b887215f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd8b192dd22491f8a02462074381d31":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85e19da7dc25456d90d44009735054e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ede2b2fb40e140a78cc01fc997755d8d","IPY_MODEL_9e469f10716348d4a5f22620df8fff6a","IPY_MODEL_e1297da0a288410e9bb9ac890db84688"],"layout":"IPY_MODEL_f8ff4bce0df645a995a9807fd20e06fc"}},"ede2b2fb40e140a78cc01fc997755d8d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49f89c244fb1485d86b7fb0407db447a","placeholder":"​","style":"IPY_MODEL_ed3245fa005745f2841638d163e8d767","value":"merges.txt: 100%"}},"9e469f10716348d4a5f22620df8fff6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e16b319d86044f0bcbd2492ab2b0d7e","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3521247653b84a7d9c4e3bafbf755352","value":456318}},"e1297da0a288410e9bb9ac890db84688":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_843aa4738ff94c2d8ec38692d0c421ba","placeholder":"​","style":"IPY_MODEL_5210019eb0df450d9550912d50c663ce","value":" 456k/456k [00:00&lt;00:00, 5.54MB/s]"}},"f8ff4bce0df645a995a9807fd20e06fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49f89c244fb1485d86b7fb0407db447a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed3245fa005745f2841638d163e8d767":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e16b319d86044f0bcbd2492ab2b0d7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3521247653b84a7d9c4e3bafbf755352":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"843aa4738ff94c2d8ec38692d0c421ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5210019eb0df450d9550912d50c663ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3328689540a8498598fa14e68dc7d35a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_19f6272e05ea421b9430dff61a33af5f","IPY_MODEL_a85a445df7304f2991069249968dc45e","IPY_MODEL_55ecf21c0fbf4a679fc1c39ab3262515"],"layout":"IPY_MODEL_a15d695c18b14b1d8d5a629ab8400826"}},"19f6272e05ea421b9430dff61a33af5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97520cc6431e4d7da0b33cabebae3209","placeholder":"​","style":"IPY_MODEL_3284ababe4b7414baf08c33b10ea2bc0","value":"tokenizer.json: 100%"}},"a85a445df7304f2991069249968dc45e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f192ac0b86a14a20b8576cbe717a7bc8","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_30a24df512cd4cb8937aae878d2f5b75","value":1355863}},"55ecf21c0fbf4a679fc1c39ab3262515":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f4f59f40b6143849cfa1e2a6876ece1","placeholder":"​","style":"IPY_MODEL_d825104e6f504b9e9734029cf46d3d7c","value":" 1.36M/1.36M [00:00&lt;00:00, 47.4MB/s]"}},"a15d695c18b14b1d8d5a629ab8400826":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97520cc6431e4d7da0b33cabebae3209":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3284ababe4b7414baf08c33b10ea2bc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f192ac0b86a14a20b8576cbe717a7bc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30a24df512cd4cb8937aae878d2f5b75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5f4f59f40b6143849cfa1e2a6876ece1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d825104e6f504b9e9734029cf46d3d7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b70492d14f814f08a64c8debe3aaffab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0810481463694e82a799100db034e5ac","IPY_MODEL_37caceec7c3a4b7bbe4307e68e0511b3","IPY_MODEL_594e7be3eaed4881a1f24fdcd565d678"],"layout":"IPY_MODEL_7a62d8e37f1745ffb6daffbc45af1743"}},"0810481463694e82a799100db034e5ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0161a0ae5ab94ccb8d30f7289242defc","placeholder":"​","style":"IPY_MODEL_432f7fac6d9e402fac9449b2e7d24d84","value":"model.safetensors: 100%"}},"37caceec7c3a4b7bbe4307e68e0511b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a42d5f12a14d4a649e4b5a50ca32e0ca","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c875698e01684ad9bbb68ad05cad0f07","value":1421700479}},"594e7be3eaed4881a1f24fdcd565d678":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_430865064ce34fd0aa15ea9e7e0745f5","placeholder":"​","style":"IPY_MODEL_c50b1d6a2fec4bd293a61fbcad7e7499","value":" 1.42G/1.42G [00:24&lt;00:00, 36.9MB/s]"}},"7a62d8e37f1745ffb6daffbc45af1743":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0161a0ae5ab94ccb8d30f7289242defc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"432f7fac6d9e402fac9449b2e7d24d84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a42d5f12a14d4a649e4b5a50ca32e0ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c875698e01684ad9bbb68ad05cad0f07":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"430865064ce34fd0aa15ea9e7e0745f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c50b1d6a2fec4bd293a61fbcad7e7499":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DoA_y1P96s9j"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PeFR37cx4Um","executionInfo":{"status":"ok","timestamp":1763814056006,"user_tz":-330,"elapsed":21981,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"43696d10-28bc-457d-a86b-808f3f0c0524"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=0eb16849f675bc5c5906614dec15a31d053f15bb874d9bf93998fb931d6d4b1c\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"NT80VkVzm6x-","executionInfo":{"status":"ok","timestamp":1763814056020,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"ddb081a0-8d4d-4156-e297-9dbb15882fd5"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import google.generativeai as genai\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-pro_cot_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key9.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Outputs will go to:\", BASE_OUT)\n","print(\"Gemini PRO API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG (PRO-OPTIMISED)\n","#####################################################################\n","MODEL_NAME     = \"gemini-2.5-pro\"\n","MAX_CHARS      = 2200          # safer for PRO JSON stability\n","GLOBAL_MIN_GAP = 110           # seconds – conservative for PRO\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","logger.info(\"Starting Gemini PRO CoT pipeline\")\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if Gemini mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GEMINI CALL WITH GLOBAL WAIT & RETRIES (PRO-SAFE)\n","#####################################################################\n","def gemini_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting PRO global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    cfg = {\"temperature\": temperature}\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = model.generate_content(prompt, generation_config=cfg)\n","            LAST_TS = time.time()\n","            return (getattr(resp, \"text\", \"\") or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Gemini PRO call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"Gemini PRO call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = gemini_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = gemini_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = gemini_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = gemini_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. MAIN PIPELINE (AUTOSAVE + RESUME) – NO EVALUATION HERE\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input file must contain a 'Reference Summary' column for evaluation later.\")\n","\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already processed.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error processing row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        print(\"\\n--- SUMMARY ---\\n\", summary)\n","        print(\"\\n--- TOPICS ---\\n\", topics)\n","        print(\"\\n--- Q&A ---\\n\", qa_text)\n","        print(\"\\n--- KEY CONCEPTS ---\\n\", concepts_text)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave after each row\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Row {idx} saved to {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed. Final output saved to:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. To evaluate, run the SEPARATE evaluation script.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"LA92vI77m7B2","outputId":"a1fbb3f2-6bc5-4e3f-bf6b-ab8d579b4030","executionInfo":{"status":"ok","timestamp":1763832269580,"user_tz":-330,"elapsed":18076534,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Outputs will go to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro\n","Gemini PRO API key loaded ✓\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","--- SUMMARY ---\n"," The optimal allocation of decision-making tasks between humans and Artificial Intelligence hinges on their complementary strengths. AI systems typically exhibit high accuracy at the extremes of confidence but perform poorly in uncertain, mid-confidence ranges. Conversely, humans often excel in these ambiguous situations by applying external context. This dynamic suggests a hybrid model of augmented intelligence, where human judgment and AI assistance are combined for superior outcomes. However, the success of this synergistic partnership is critically dependent on mitigating human cognitive biases, such as automation bias. The presentation of AI-generated information is a key factor; for example, allowing users to optionally request AI input can reduce bias compared to a forced display. Understanding these interface dynamics is crucial for designing effective human-AI collaborations and developing quantifiable methods to assign a given task to the most appropriate decision-maker, whether human, AI, or a combined team.\n","\n","--- TOPICS ---\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","--- Q&A ---\n"," Q: In the performance curve graph described, what do the X and Y axes represent?\n","A: The Y-axis tracks the success rate of a prediction, while the X-axis represents the confidence score of that prediction, ranging from 0% to 100%.\n","Q: According to the transcript, when does an AI system achieve its highest success rate?\n","A: An AI system has a high success rate when its confidence score is either very low (predicting an event is not real) or very high (predicting an event is real). Its success rate is lower when it is unsure.\n","Q: In the fraud detection example, what specific problem are the financial analysts facing that an AI could help alleviate?\n","A: The financial analysts are overwhelmed with thousands of alerts each day, 90 percent of which are false positives. An AI could help alleviate this workload.\n","Q: How does the transcript describe the difference between a typical AI performance curve and a human one?\n","A: A typical AI performance curve shows high success at the extremes of confidence and a dip in the middle. In contrast, a human performance curve is described as being \"a little bit flatter.\"\n","\n","--- KEY CONCEPTS ---\n"," fraud detection system, false positives, confidence score, AI performance curve, human performance curves, human bias, success rate\n","Row 7 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","--- SUMMARY ---\n"," Google has introduced a new suite of Vertex AI APIs to enable developers to more efficiently build grounded generative AI applications for enterprise environments. These tools are designed to address the critical challenge of ensuring AI-generated responses are accurate and reliably based on an organization's specific data sources. The suite comprises APIs for advanced document understanding, improved embedding models, and an enhanced vector search with hybrid capabilities. To further improve reliability, it includes a ranking API to surface the most relevant information, a specialized model for generating answers with citations, and a dedicated fact-checking API. Leveraging Google's internal technology, these high-quality solutions are offered as simple, standalone primitives and are integrated into popular frameworks like LangChain to facilitate straightforward adoption by the developer community.\n","\n","--- TOPICS ---\n"," ['Generative AI', 'Artificial Intelligence', 'Agentic AI']\n","\n","--- Q&A ---\n"," Q: What is the primary challenge, referred to as \"grounding,\" that the new Vertex AI APIs are designed to solve for enterprise generative applications?\n","A: The primary challenge of \"grounding\" is ensuring that generative applications can reliably access the correct enterprise data to produce responses that are accurate and consistent.\n","Q: The speaker highlights two main features that set the new Vertex AI APIs apart. What are they?\n","A: The two main features are high quality and the embedding of Google's unique \"know-how.\" This means the APIs leverage technology and concepts from planet-scale Google products like Google Search, YouTube, and Google Ads.\n","Q: A developer needs to verify if a statement generated by an AI model is factually supported by a set of source documents. Which of the new APIs is specifically designed for this task?\n","A: The \"Check Grounding\" API is designed for this purpose. It fact-checks a statement against provided evidence, sentence by sentence, to confirm if it is supported by the source information.\n","Q: How has Google designed the new Vertex AI APIs to be easily integrated into developers' existing workflows?\n","A: The APIs are designed as simple, standalone, stateless \"primitives\" with clear interfaces. Google is also integrating them into popular frameworks like LangChain and LlamaIndex, allowing developers to easily prototype and combine them with other tools.\n","\n","--- KEY CONCEPTS ---\n"," Grounding, Document understanding API, Embedding API, Vector search, Hybrid search, Ranking API, Grounded generation API, Check grounding\n","Row 8 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","--- SUMMARY ---\n"," The Singular Value Decomposition (SVD) provides a fundamental factorization of a matrix X into its constituent components, including the unitary matrices U and V. These matrices represent transformations that preserve vector lengths and angles, functioning as pure rotations that maintain the data's intrinsic geometric structure. While U and V are unitary, the full matrix X is generally not. The geometric interpretation of SVD is powerful, describing how the linear transformation represented by X maps a sphere of unit vectors into an ellipsoid. The orientation of this resulting ellipsoid is determined by the singular vectors, which are the columns of U and V. Concurrently, the singular values correspond directly to the lengths of the ellipsoid's principal axes, quantifying the magnitude of stretching or compression along each of these orthogonal directions, thereby revealing the transformation's primary actions.\n","\n","--- TOPICS ---\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","--- Q&A ---\n"," Q: What are the defining properties of a unitary transformation, and what is its geometric effect on vectors?\n","A: A unitary transformation preserves the angles between vectors and the lengths of vectors. Geometrically, this means it rotates vectors in space without changing their length or the angles between them.\n","Q: In the geometric interpretation of the SVD, what happens when a sphere of unit vectors is multiplied by a matrix X?\n","A: When a sphere of unit vectors is multiplied by the matrix X, it is transformed into an ellipsoid. The orientation of this ellipsoid is determined by the singular vectors (U or V), and the amount it is stretched or squished along its principal axes is determined by the singular values (Sigma).\n","Q: How does the notation for matrix transposition differ for real-valued versus complex-valued data as explained in the transcript?\n","A: For real-valued data, the standard transpose (denoted by a superscript T) is used. For complex-valued data, the complex conjugate transpose (denoted by a star, *) is used, which involves both transposing the matrix and taking the complex conjugate of each element.\n","Q: The transcript uses an analogy to explain unitary transformations. What is this analogy and what does it illustrate?\n","A: The analogy is of constellations in the night sky. As the sky rotates, the constellations move, but the angles between the stars within them remain fixed. This illustrates how a unitary transformation rotates a vector space while preserving the geometric relationships (angles and lengths) between the vectors within it.\n","\n","--- KEY CONCEPTS ---\n"," Singular Value Decomposition (SVD), Unitary matrices, Unitary transformations, Economy size SVD, Complex conjugate transpose, Geometric interpretation, Left singular vectors, Singular values, Coordinate transformation, Principal axes\n","Row 9 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","--- SUMMARY ---\n"," This tutorial outlines the development of generative AI applications using Google's Gemini 1.5 Pro, emphasizing its one-million-token context window and unified multimodal capabilities. The model's advanced capacity is demonstrated through its ability to process extensive documents, such as a 402-page Apollo 11 transcript, and interpret visual data from abstract drawings. The guide details the initial setup process, which includes obtaining an API key, installing the `google-generativeai` Python library, and configuring the environment to select the \"gemini-1.5-pro-latest\" model. Core functionalities covered include submitting text-based queries and utilizing advanced features like response streaming for an improved user experience. A significant focus is placed on the model's integrated multimodality, which streamlines development by allowing simultaneous processing of text and image inputs within a single framework, a notable improvement over previous versions that required separate models.\n","\n","--- TOPICS ---\n"," ['Generative AI', 'Artificial Intelligence', 'Python Programming']\n","\n","--- Q&A ---\n"," Q: What specific AI model is the focus of the video tutorial?\n","A: The video tutorial focuses on building applications using Google Gemini Pro 1.5.\n","Q: Based on the transcript, what does it mean for Gemini Pro 1.5 to be a \"multimodal\" model?\n","A: A multimodal model, in the context of Gemini Pro 1.5, means it is able to work with both text and images.\n","Q: What are the three main parts of the planned video demonstration?\n","A: The video will first show a demo from Google, then a hands-on coding application using images and text, and finally, it will explain how to create and use an API key.\n","Q: What experimental feature of the Gemini 1.5 model is mentioned in the demo clip?\n","A: The demo clip mentions an experimental feature called \"long context understanding.\"\n","\n","--- KEY CONCEPTS ---\n"," Generative Artificial Intelligence, Google Gemini Pro 1.5, Multimodal model, Long context understanding, End-to-end projects, API key\n","Row 10 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","--- SUMMARY ---\n"," The refinement of prompt engineering models depends on a systematic, iterative process encompassing evaluation, testing, and debugging. This procedure begins with an initial assessment using key performance metrics, including perplexity to measure predictive capability, accuracy to determine output correctness, and direct human evaluation. Following this, a debugging phase analyzes response patterns to identify and correct errors, facilitating model fine-tuning. To ensure robustness and prevent overfitting, the model must then be tested on diverse datasets and tasks to confirm its ability to generalize. This cyclical methodology of evaluation, debugging, and testing constitutes a crucial and ongoing framework for maintaining and progressively improving model performance over time. This continuous loop is fundamental to developing reliable and effective language models.\n","\n","--- TOPICS ---\n"," ['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","--- Q&A ---\n"," Q: What are the three commonly used metrics mentioned for evaluating prompt engineering models?\n","A: The three commonly used metrics mentioned are perplexity, accuracy, and human evaluation.\n","Q: How does the metric of perplexity evaluate a language model's performance?\n","A: Perplexity measures how well a language model predicts a sequence of words. A lower perplexity score indicates that the language model is performing better.\n","Q: According to the transcript, what is a common technique for debugging and improving a model after its initial evaluation?\n","A: A common technique is to analyze the generated responses to identify common errors or patterns, which then allows for fine-tuning the model to make improvements.\n","Q: Why is it important to test prompt engineering models on different datasets or tasks?\n","A: Testing on different data or tasks is important to determine the model's ability to generalize its performance to new or unseen data.\n","Q: The transcript describes the evaluation and testing of prompt engineering models as what kind of process?\n","A: It is described as an ongoing process, where you continuously evaluate and test the model as you use it to ensure it continues to perform well.\n","\n","--- KEY CONCEPTS ---\n"," prompt engineering models, evaluation metrics, perplexity, accuracy, human evaluation, large language model, model fine-tuning, model generalization, cross validation\n","Row 11 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","--- SUMMARY ---\n"," This analysis presents a conceptual framework that distinguishes between Generative AI, AI Agents, and Agentic AI, framing them as a progression of increasing complexity and autonomy. At the foundational level, Generative AI, based on Large Language Models (LLMs), creates novel content but is constrained by a static knowledge cutoff. The next stage, the AI Agent, enhances this capability by integrating external tools and real-time knowledge, enabling it to perform specific, autonomous actions to complete well-defined tasks. This moves beyond simple content generation toward task execution. The most advanced form, Agentic AI, represents sophisticated systems, potentially comprising one or more collaborating agents, designed to achieve complex, multi-step objectives. Such systems require advanced planning, multi-step reasoning, and coordination to autonomously handle intricate goals, signifying a significant leap in artificial intelligence.\n","\n","--- TOPICS ---\n"," ['Generative AI', 'Agentic AI', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is the core function of Generative AI as described in the transcript?\n","A: Generative AI is an Artificial Intelligence that can create new content, such as text, images, or video, based on patterns it has learned from existing data like Wikipedia and Google Books.\n","Q: How does an AI Agent differ from a simple Generative AI with only a Large Language Model (LLM)?\n","A: An AI Agent is a program that can take input, think, and act to complete a task, going beyond the simple Q&A of a Generative AI. It achieves this by using tools (like APIs), memory, and knowledge, and it possesses a degree of autonomy to make independent decisions for specific tasks.\n","Q: According to the transcript, what distinguishes an Agentic AI system from a single AI Agent?\n","A: An Agentic AI system is designed to handle more complex, multi-step goals that require planning and coordination. It can consist of one or more AI agents working autonomously, often collaborating and using various tools to reach a goal, demonstrating a higher level of autonomous decision-making than a single agent handling a narrow task.\n","Q: Imagine you ask an AI to plan a 7-day trip that requires checking weather forecasts, finding flights within a budget, and verifying visa status. Based on the transcript, which type of AI system would be required for this and why?\n","A: This would require an Agentic AI system. The task is complex, involving multi-step reasoning and planning (checking weather, then flights, then visa). It requires coordinating multiple tools (weather API, travel API) and potentially multiple specialized agents (a flight booking agent and an immigration agent) to achieve the overall goal autonomously.\n","\n","--- KEY CONCEPTS ---\n"," Generative Artificial Intelligence, Large Language Model (LLM), Knowledge cutoff, AI Agent, Tool usage, Autonomous decision making, Agentic AI, Multi-step reasoning, Multi-step planning\n","Row 12 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","--- SUMMARY ---\n"," Covariance is a statistical measure that quantifies the directional relationship between two random variables, such as house size and price. The sign of the covariance value indicates whether the variables move together (positive) or in opposite directions (negative). The lecture presents the mathematical formula for covariance, noting its conceptual similarity to the variance formula, as the variance of a variable is effectively its covariance with itself. A primary limitation of this metric is that its magnitude is not standardized, meaning it reveals only the direction of the linear relationship but not its strength. This shortcoming is overcome by other statistical tools, most notably the Pearson correlation coefficient, which provides a normalized measure of the association's strength and direction. This distinction is crucial for accurately interpreting the degree to which two variables are related.\n","\n","--- TOPICS ---\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","--- Q&A ---\n"," Q: What is the primary purpose of using covariance in data analysis?\n","A: The primary purpose of covariance is to quantify the relationship between two random variables to determine if they tend to increase or decrease together.\n","Q: How does the sign of the covariance value (positive or negative) indicate the relationship between two variables?\n","A: A positive covariance value indicates that as one variable increases, the other variable also tends to increase. A negative covariance value indicates that as one variable increases, the other tends to decrease.\n","Q: What is the mathematical relationship between the covariance of a variable with itself, Cov(X, X), and its variance?\n","A: The covariance of a variable with itself, Cov(X, X), is the same as the variance of that variable, Var(X).\n","Q: According to the transcript, what is the main limitation of covariance that leads to the use of other techniques like the Pearson correlation coefficient?\n","A: The main limitation is that while covariance indicates the direction of a relationship (positive or negative), it does not specify the strength or magnitude of that relationship. It doesn't quantify 'how much' positive or negative the relationship is.\n","\n","--- KEY CONCEPTS ---\n"," covariance, variance, random variables, positive covariance, negative covariance, mean of a random variable, Pearson correlation coefficient\n","Row 13 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","--- SUMMARY ---\n"," The fundamental objective in reinforcement learning (RL) is to derive an optimal policy that maximizes a cumulative numerical reward signal over time. An agent accomplishes this through a trial-and-error process, interacting with an environment and utilizing reward signals as feedback to assess the quality of its actions. The precise formulation of this objective depends on the task's nature. For episodic tasks with clear start and end points, such as a game of Tic-Tac-Toe, rewards are typically assigned to terminal outcomes like winning or losing. In contrast, for continuous tasks without a fixed endpoint, like algorithmic stock trading, the objective is parameterized via a reward function. This mathematical expression is specifically engineered to reflect desired goals, such as maximizing profit or optimizing for risk-adjusted returns, guiding the agent's behavior in an ongoing process.\n","\n","--- TOPICS ---\n"," ['Reinforcement Learning', 'Machine Learning', 'Agentic AI']\n","\n","--- Q&A ---\n"," Q: What is the primary objective of a reinforcement learning (RL) agent as described in the transcript?\n","A: The primary objective of an RL agent is to learn an optimal policy that maximizes the cumulative numerical reward signal over time.\n","Q: Explain the role of the reward signal in the learning process of an RL agent.\n","A: The reward signal provides feedback to the agent on the quality of its actions. The agent learns through a trial-and-error process, where it takes actions, observes the resulting rewards, and updates its policy to favor actions that lead to higher cumulative rewards.\n","Q: According to the transcript, how are rewards typically defined in an episodic task like a Tic-Tac-Toe game?\n","A: In the Tic-Tac-Toe game example, the agent receives a positive reward (e.g., +1) for winning, a negative reward (e.g., -1) for losing, and no reward (0) for a draw or for moves made while the game is still in progress.\n","Q: How can the objective be parameterized for a continuous task like stock market trading?\n","A: For a continuous task like stock trading, the objective is parameterized by defining a reward function. This can be a mathematical expression that assigns a numerical value based on the profit or loss of each trade, or it could use risk-adjusted measures like the Sharpe ratio.\n","\n","--- KEY CONCEPTS ---\n"," reinforcement learning problem, optimal policy, cumulative reward, reward signal, episodic and continuous tasks, trial and error learning, reward function, value-based methods, policy-based methods, state-action pair\n","Row 14 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","--- SUMMARY ---\n"," Python dictionaries are a core data structure designed for storing information in key-value pairs, referred to as items. A dictionary is declared using curly brackets, with the critical constraint that each key must be an immutable type, such as a string or number. Dictionaries can be instantiated literally or created programmatically, for example, by merging two lists with the `dict()` and `zip()` functions. Essential operations include accessing and modifying values via their keys using square bracket notation and deleting items with the `del` keyword. Furthermore, built-in methods such as `.keys()`, `.values()`, and `.items()` facilitate the inspection of the dictionary's contents. This versatile structure is fundamental for mapping related data and is considered an essential tool in data science applications for its efficiency and flexibility in handling structured information.\n","\n","--- TOPICS ---\n"," ['Python Programming', 'Data Science']\n","\n","--- Q&A ---\n"," Q: What is the fundamental structure of a Python dictionary as described in the transcript?\n","A: A Python dictionary consists of key-value pairs, which are referred to as items. The key and value in a pair are separated by a colon, the items are separated by commas, and the entire dictionary is enclosed in curly brackets.\n","Q: What is the primary requirement for a dictionary key, and which data type is explicitly mentioned as being unsuitable for this role?\n","A: A dictionary key must be immutable, meaning it cannot be changed. The transcript states that a list, for example, could not be used as a key because it is mutable.\n","Q: If you have two lists, one for keys and one for values, what two functions would you use together to create a dictionary from them?\n","A: You would use the `zip()` function to pair the corresponding elements from the two lists, and then pass the result to the `dict()` function to create the dictionary.\n","Q: How can you access, modify, and delete the item associated with the key `5` in a dictionary named `my_dict`?\n","A: To access the value, you use `my_dict[5]`. To modify it to 'Z', you use the assignment `my_dict[5] = 'Z'`. To delete the item, you use the command `del my_dict[5]`.\n","Q: What do the `.items()`, `.keys()`, and `.values()` methods each return when called on a dictionary?\n","A: The `.items()` method returns the key-value pairs (the items). The `.keys()` method returns a list of the keys. The `.values()` method returns the values from the dictionary.\n","\n","--- KEY CONCEPTS ---\n"," key-value pairs, immutable keys, dictionary methods (.items(), .keys(), .values()), dict() constructor, zip() function, key-based indexing, updating dictionary values, deleting dictionary items\n","Row 15 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","--- SUMMARY ---\n"," Artificial Intelligence (AI) provides a significant advantage in cybersecurity by enhancing defenses against emerging threats. According to IBM's 2023 \"Cost of a Data Breach Report,\" extensive AI and automation can reduce breach identification and containment time by an average of 108 days. A key application is User Behavior Analytics (UBA), which utilizes machine learning to establish baselines of normal user activity and detect anomalies indicative of insider threats. When integrated into Security Information and Event Management (SIEM) solutions like IBM QRadar, UBA empowers analysts with dashboards that prioritize risky users and visualize trends. This system facilitates rapid investigation through event timelines, Indicators of Compromise (IOCs), and mappings to the MITRE ATT&CK framework. By accelerating analysis from hours or days to mere minutes, this AI-driven approach enables security teams to shift from a reactive to a more proactive defense posture.\n","\n","--- TOPICS ---\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","--- Q&A ---\n"," Q: According to IBM's 2023 \"Cost of a Data Breach Report,\" what was the key advantage for organizations that extensively used AI and automation?\n","A: Organizations that extensively used AI and automation were able to identify and contain a data breach 108 fewer days on average compared to those that did not.\n","Q: What specific security challenge does the transcript state can be addressed by User Behavior Analytics (UBA) with AI and machine learning?\n","A: User Behavior Analytics (UBA) with AI and machine learning can help security teams detect and respond to insider threats quickly and precisely.\n","Q: Based on the transcript, why is it important for security professionals to find ways to improve their organization's security posture?\n","A: It is important because they need to stay ahead of emerging threats and because security incidents, such as insider threats, can be very costly for an organization.\n","Q: What two technologies are presented as a combined solution for enhancing an organization's security against internal risks?\n","A: The transcript presents User Behavior Analytics (UBA) combined with Artificial Intelligence (AI) and machine learning as a solution.\n","\n","--- KEY CONCEPTS ---\n"," User Behavior Analytics, Insider Threats, Artificial Intelligence, Machine Learning, AI and automation, Data breach containment, Security posture, Threat detection\n","Row 16 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","--- SUMMARY ---\n"," Meta has released Llama 3, a new open-source large language model available in 8 billion and 70 billion parameter versions. This model represents a significant advancement, having been trained on a 15 trillion token dataset with a doubled 8K context length, resulting in state-of-the-art performance. Benchmark evaluations demonstrate that Llama 3 is highly competitive with leading proprietary models like GPT-4 and outperforms other open-source alternatives on benchmarks such as MMLU and HumanEval, showcasing strong reasoning and code generation. However, it exhibits a comparative weakness in advanced mathematics. The release is framed by Meta's responsible AI approach, including system-level tools. To access the model, users must submit a request on the official Meta website. Upon approval, the model weights and tokenizer are available for download, with distribution also occurring through platforms like Hugging Face and Kaggle, supported by a GitHub repository for implementation guidance.\n","\n","--- TOPICS ---\n"," ['Generative AI', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is the name of the speaker?\n","A: The speaker's name is Krishak.\n","Q: On what platform is the speaker creating content?\n","A: The speaker is creating content for their YouTube channel.\n","Q: According to the transcript, what time was it when the speaker was recording?\n","A: It was 2 a.m. when the speaker was recording.\n","\n","--- KEY CONCEPTS ---\n"," \n","Row 17 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","--- SUMMARY ---\n"," This section introduces the scikit-learn (sklearn) Python library as a foundational tool for machine learning implementation. A practical workflow is demonstrated, emphasizing the use of search engines to locate official documentation for specific algorithms like Naive Bayes. This method allows users to identify appropriate functions and understand their correct application. The instruction highlights that consulting external documentation is a fundamental and essential skill for practitioners. The overarching goal is to equip students with the ability to independently write Python code to create machine learning classifiers and visualize their corresponding decision boundaries, thereby reinforcing concepts covered previously. This approach empowers learners to move from theoretical understanding to practical application by leveraging industry-standard tools and resources effectively.\n","\n","--- TOPICS ---\n"," ['Machine Learning', 'Python Programming', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is the full name and common abbreviation of the Python library featured in the lesson?\n","A: The library is called scikit-learn, which is often abbreviated as sk-learn.\n","Q: According to the transcript, what specific algorithm was used to create the decision boundary classifier?\n","A: The algorithm used was Gaussian Naive Bayes, which is a specific type of Naive Bayes algorithm.\n","Q: Why does the speaker suggest using Google to search for terms like 'sklearn Naive Bayes'?\n","A: The speaker suggests using Google to find the official documentation for the scikit-learn library, which explains how to use its various functions.\n","Q: What is the stated learning objective for the viewer by the end of the upcoming videos?\n","A: The objective is for the viewer to be able to write the Python code that creates the decision boundary themselves.\n","\n","--- KEY CONCEPTS ---\n"," decision boundary, scikit-learn, Naive Bayes, classifier, Naive Bayes formula, Gaussian Naive Bayes\n","Row 18 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","--- SUMMARY ---\n"," This analysis contrasts the symmetrical Gaussian (normal) distribution with the right-skewed log-normal distribution, where a variable's logarithm is normally distributed. Common examples of log-normal data include income levels and the length of product reviews. The primary application discussed is its utility in machine learning for feature scaling. By identifying a feature's underlying distribution, appropriate transformations can be applied to enhance model performance. Specifically, features exhibiting a log-normal pattern can undergo a logarithmic transformation to approximate a Gaussian distribution. This transformed data can then be standardized, a process known as log normalization. This technique is crucial for bringing features with disparate scales onto a comparable basis, thereby improving the accuracy and reliability of predictive models by ensuring that all variables contribute appropriately to the learning process.\n","\n","--- TOPICS ---\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","--- Q&A ---\n"," Q: What is the defining characteristic of a random variable that follows a log-normal distribution?\n","A: A random variable X is said to follow a log-normal distribution if the logarithm of that variable, log(X), is normally distributed (i.e., follows a Gaussian distribution).\n","Q: How does the shape of a log-normal distribution curve typically differ from a Gaussian (bell) curve as described in the transcript?\n","A: A Gaussian distribution has a symmetrical bell curve. In contrast, a log-normal distribution is described as being right-skewed, where the right-hand side of the curve gets extended and becomes \"fatter,\" not coming down symmetrically like a bell curve.\n","Q: According to the transcript, what is a practical application of identifying a feature's distribution (e.g., log-normal) in machine learning?\n","A: Identifying a feature's distribution is crucial for feature scaling to improve model accuracy. For a log-normal feature, you can take the logarithm of its values to transform it into a Gaussian distribution. This can then be converted to a standard normal distribution, putting it on the same scale as other features and potentially increasing the model's accuracy.\n","Q: What are two real-world examples mentioned in the transcript that often follow a log-normal distribution?\n","A: The transcript provides two main examples: the income of people and the length of comments or product reviews on e-commerce sites like Amazon.\n","\n","--- KEY CONCEPTS ---\n"," Gaussian distribution, Log normal distribution, Empirical formula, Bell curve, Standard normal distribution, Standard scaling, Log normalization, Right-skewed distribution\n","Row 19 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","--- SUMMARY ---\n"," This research outlines an end-to-end deep learning project for the agricultural domain, aimed at mitigating economic losses for potato farmers through the early detection of early and late blight. The proposed solution involves a mobile application where users can photograph a potato leaf, which is then classified by a Convolutional Neural Network (CNN) as healthy or diseased. The project encompasses the entire machine learning lifecycle, from data collection and pre-processing with data augmentation to model building. The development plan features a two-phase deployment: a web application for initial testing, followed by a React Native mobile app. For mobile deployment, the model is optimized via TensorFlow Lite quantization to enhance performance on edge devices. The technical architecture utilizes TensorFlow Serving and FastAPI for the back-end, with the final system deployed on Google Cloud Platform, demonstrating a complete MLOps workflow.\n","\n","--- TOPICS ---\n"," ['Deep Learning', 'Data Science', 'Mlops']\n","\n","--- Q&A ---\n"," Q: What is the primary business problem that the end-to-end deep learning project aims to solve for potato farmers?\n","A: The project aims to solve the problem of economic losses faced by potato farmers due to plant diseases. By enabling early and accurate disease detection, farmers can apply appropriate treatments and prevent waste.\n","Q: What are the two specific potato plant diseases mentioned, and why is it crucial to differentiate between them?\n","A: The two diseases are early blight, caused by a fungus, and late blight, caused by a microorganism. It is crucial to differentiate them because their treatments are different, so accurate identification is necessary for effective treatment.\n","Q: Describe the user interaction with the final application. What does a farmer need to do, and what information do they receive?\n","A: A farmer uses a mobile app to take a picture of a potato plant. The application then processes the image using a deep learning model and informs the farmer whether the plant is healthy or if it has one of the specific diseases.\n","Q: Which technologies are mentioned for building the backend server, deploying the model, and creating the mobile app?\n","A: The backend server will be built using FastAPI. The model will be deployed to Google Cloud (GCP) using TF serving and Google Cloud Functions. The mobile app will be written in React Native.\n","\n","--- KEY CONCEPTS ---\n"," end-to-end deep learning project, Machine Learning (ML) Ops, TF serving, fast API, model deployment, Google Cloud functions, React Native, deep learning, convolutional neural network, technical architecture\n","Row 20 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","--- SUMMARY ---\n"," This analysis outlines a spectrum of autonomy in Large Language Model (LLM) applications, beginning with deterministic code as a baseline with no autonomy. The progression advances to single LLM calls for discrete tasks and then to chains, which link multiple specialized models in a fixed sequence to address more complex problems. A greater degree of autonomy is introduced with routers, where an LLM dynamically directs user requests to the most suitable chain based on the input. The most sophisticated level presented is the state machine, or agent, which facilitates cyclical and adaptive workflows. This advanced architecture supports iterative refinement, incorporates memory, and allows for human-in-the-loop supervision, signifying a fundamental shift from rigid, human-defined processes to flexible, agent-driven execution systems capable of handling dynamic tasks.\n","\n","--- TOPICS ---\n"," ['Agentic AI', 'Langraph', 'LangChain']\n","\n","--- Q&A ---\n"," Q: What is the primary disadvantage of using a single LLM call for complex tasks?\n","A: The main disadvantage is that trying to get everything done in one shot often leads to confused or mixed-up responses, as a single LLM cannot be an expert at multiple things simultaneously.\n","Q: How does a 'Router' improve upon the 'Chain' level of autonomy in LLM applications?\n","A: Unlike chains, which follow a rigid, predefined sequence of steps, a router acts like a 'smart traffic cop.' The AI itself decides which path or chain to take next based on the user's input, introducing a level of decision-making.\n","Q: According to the transcript, what key capability distinguishes a 'State Machine' (or Agent) from a 'Router'?\n","A: A State Machine, or Agent, can have cycles and loops, allowing it to go back, remember previous conversations, learn from mistakes, and iteratively refine its output. Routers, while able to choose a path, cannot do this and lack cycles.\n","Q: An application is designed to create social media content. When a user provides a topic, the application automatically generates a LinkedIn post, a Twitter post, and a blog post simultaneously using three parallel, predefined processes. Which level of autonomy does this describe?\n","A: This describes the 'Chains' level of autonomy, where multiple specialists (in this case, LLM calls for each platform) work in a fixed, predefined sequence or in parallel to accomplish a larger task.\n","Q: Why are 'Chains' and 'Routers' considered 'human-driven' while 'State Machines' are considered 'agent-executed'?\n","A: Chains and Routers are considered 'human-driven' because they are one-directional and follow paths defined by a human. A State Machine is 'agent-executed' because the control flow is managed by the LLM itself, allowing for cycles, loops, and iterative refinement, which is a form of intelligence.\n","\n","--- KEY CONCEPTS ---\n"," Levels of autonomy, Single LLM call, LLM Chains, Router, State machine, AI agents, Human in the loop, Multi-agent systems, Control flow\n","Row 21 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","--- SUMMARY ---\n"," This overview transitions from foundational principles to advanced topics within prompt engineering, detailing the management of diverse prompt modalities such as text, images, and audio. It explains how pre-trained models can be leveraged for tasks like image classification. The discussion then explores sophisticated fine-tuning techniques, including multitask learning, which trains a model on several tasks simultaneously to improve robustness, and distillation, a method for creating smaller, more efficient models. Furthermore, the critical role of data pre-processing is emphasized, with methods like tokenization and normalization being essential for ensuring high-quality model inputs. The summary also addresses the practicalities of deploying models into production environments using frameworks like Flask and concludes by highlighting the paramount importance of ethical considerations, such as mitigating bias, ensuring fairness, and protecting user privacy.\n","\n","--- TOPICS ---\n"," ['Prompt Engineering', 'Deep Learning', 'Machine Learning']\n","\n","--- Q&A ---\n"," Q: What are two advanced techniques for fine-tuning pre-trained models mentioned in the transcript?\n","A: The transcript discusses two advanced fine-tuning techniques: multitask learning, which involves training a model on multiple tasks simultaneously, and distillation, which involves training a smaller model to mimic the behavior of a larger one.\n","Q: How does the process of 'distillation' make a model more efficient?\n","A: Distillation makes a model more efficient by training a smaller, faster model to mimic the behavior of a larger, more complex model. This allows the smaller model to run more quickly while still benefiting from the knowledge of the larger one.\n","Q: According to the transcript, what is a primary ethical concern related to the data used for training prompt engineering models, and what is its potential consequence?\n","A: A primary ethical concern is bias, which can occur when the training data is not representative of the whole population. This can lead to unfair and discriminatory outcomes.\n","Q: What are two key data pre-processing steps discussed, and what purpose does each serve?\n","A: The two key steps are tokenization and normalization. Tokenization involves breaking text into smaller units (like words or sub-words) to help the model understand meaning more accurately. Normalization involves converting text to a standard format (e.g., all lowercase) to avoid confusion between words that are spelled the same but have different meanings.\n","\n","--- KEY CONCEPTS ---\n"," Fine-tuning pre-trained models, Multitask learning, Model distillation, Tokenization and normalization, Model deployment in production, TensorFlow Serving, Ethical considerations in AI, Self-supervised learning, Data augmentation\n","Row 22 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","--- SUMMARY ---\n"," This lecture demonstrates the application of Singular Value Decomposition (SVD) in facial recognition through the eigenfaces method. The process begins by creating a dataset of images to compute an average face. SVD is then applied to the mean-subtracted data to extract the principal components, or eigenfaces, which represent the most significant variations in the dataset. Each individual image is subsequently projected onto these dominant eigenfaces, reducing its dimensionality to a low-dimensional feature space where distinct clusters form for classification. While examples with celebrity faces illustrate the method's capability, a significant limitation is exposed: its reliance on simple pixel correlations can lead to counterintuitive groupings based on features like skin or hair color. This highlights the 'shallowness' of the approach compared to modern techniques that infer more sophisticated features, such as 3D geometry.\n","\n","--- TOPICS ---\n"," ['Machine Learning', 'Artificial Intelligence', 'Data Science']\n","\n","--- Q&A ---\n"," Q: What is the crucial step performed on the entire set of face images before computing the Singular Value Decomposition (SVD), and what is its purpose?\n","A: After calculating the average face from all the images, this average face is subtracted from every individual image. This process centers the data, which is a key step for performing Principal Component Analysis (PCA) to find the most significant features, or \"eigenfaces.\"\n","Q: How is a high-dimensional face image classified into a category (e.g., \"Arnold\" or \"Stallone\") using the low-dimensional eigenface space?\n","A: The high-dimensional image vector is projected into a low-dimensional space defined by the first few most dominant eigenfaces. This creates a cluster of points for each person in the training data. A new test image is then projected into the same space, and it is classified based on which cluster its corresponding point is closest to.\n","Q: The analysis surprisingly showed more overlap between Arnold Schwarzenegger and Taylor Swift than between Arnold and Sylvester Stallone. What reason does the transcript suggest for this result?\n","A: The transcript suggests this is due to \"shallow\" features. Both Arnold Schwarzenegger and Taylor Swift have fair skin and blonde hair, resulting in lighter pixels on average. This leads to a higher mathematical correlation in their image data, making them appear more similar in eigenface space despite other differences.\n","Q: According to the transcript, what technological advancement allowed Facebook's facial recognition to improve and eventually match human-level accuracy?\n","A: Facebook's facial recognition improved by moving beyond 2D image analysis. They began to infer the three-dimensional geometry of the human head from 2D images, using information like shadows to determine the depth of facial features. This 3D mapping provided the extra performance needed to match human accuracy.\n","\n","--- KEY CONCEPTS ---\n"," Singular Value Decomposition (SVD), Eigenfaces, Principal Component Analysis (PCA), Mean subtraction, Image vectorization, Projection into principal components, Feature space, Image classification, Dominant modes\n","Row 23 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","--- SUMMARY ---\n"," Large Language Models (LLMs) possess advanced reasoning abilities but are inherently limited by their inability to interact with the external world to perform actions. A complex user query, such as planning a vacation, illustrates this challenge, as it requires multiple distinct steps like booking travel and reserving accommodations that a standard LLM cannot execute. The LangChain framework is introduced as a crucial bridge to overcome this limitation, enabling the development of applications that connect LLMs to external systems like APIs and databases. This allows the AI to act on user requests by executing real-world tasks such as making reservations, sending emails, or querying private data. Furthermore, LangChain provides a modular structure, offering developers the flexibility to interchange different LLMs without rewriting core application logic, thereby enhancing the functionality and adaptability of AI-powered systems.\n","\n","--- TOPICS ---\n"," ['LangChain', 'Agentic AI', 'Generative AI']\n","\n","--- Q&A ---\n"," Q: According to the transcript, what is the immediate action taken after a user submits a query to an application like ChatGPT?\n","A: The user's query is sent to an LLM (Large Language Model).\n","Q: What three specific tasks did the example user ask the AI to perform for their trip to Paris?\n","A: The user asked the AI to book a flight, book a hotel for the same day, and suggest good restaurants.\n","Q: What does the transcript suggest about the relationship between an application like ChatGPT and the models it uses?\n","A: The transcript indicates that an application is the interface that sends a user's query to an underlying LLM, and it may use a variety of different models, such as the mentioned 'charity 3'.\n","\n","--- KEY CONCEPTS ---\n"," LangChain, ChatGPT, LLM model, query, ChatGPT application, GPT-3\n","Row 24 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","--- SUMMARY ---\n"," Residual analysis is a critical diagnostic technique for evaluating and improving time series forecasting models. Residuals, defined as the difference between a model's fitted predictions and the actual observed values, are used to determine if all predictable information has been captured. Ideally, residuals should be unbiased, with a mean of zero, and exhibit no autocorrelation, signifying that no systematic patterns remain in the errors. Practitioners employ diagnostic checks to verify these properties, such as plotting a histogram of residuals to assess for a symmetrical, zero-centered distribution and using Autocorrelation Function (ACF) plots or statistical methods like the Ljung-Box test to detect correlation. The presence of bias or significant autocorrelation, often indicated by a low p-value in statistical tests, signals that the model is misspecified. This analysis is a crucial step in an iterative process of model refinement to enhance overall forecasting accuracy.\n","\n","--- TOPICS ---\n"," ['Time Series', 'Data Science', 'Statistics']\n","\n","--- Q&A ---\n"," Q: What is the definition of a residual in the context of time series analysis?\n","A: A residual is the difference between the fitted value (y-hat) generated by a model and the actual value (y) of the time series for the data the model was trained on.\n","Q: According to the transcript, what are the two primary characteristics an analyst should look for in the residuals of a well-fitted time series model?\n","A: The two key characteristics are: 1) The residuals should have no autocorrelation, indicating the model has captured all the information in the data. 2) The mean of the residuals should be zero, which signifies the forecast is not biased (i.e., not consistently over- or under-forecasting).\n","Q: How does the transcript distinguish between a model's 'residuals' and its 'errors'?\n","A: Residuals are calculated on the data the model was trained on (the difference between fitted values and actual training data). In contrast, errors are calculated on data the model has not seen before, such as a test set, to measure forecast performance.\n","Q: What is the null hypothesis of the Ljung-Box test, and what does a very low p-value from this test imply about a model's performance?\n","A: The null hypothesis of the Ljung-Box test is that the residuals are independently distributed (i.e., they have no correlation). A very low p-value implies that this hypothesis should be rejected, meaning the residuals have significant serial correlation that the model failed to capture.\n","\n","--- KEY CONCEPTS ---\n"," residual analysis, fitted value, autocorrelation, partial autocorrelation, Ljung-Box test, Holt-Winters model, exponential smoothing, serial correlation, biased forecast\n","Row 25 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","--- SUMMARY ---\n"," This tutorial outlines the development of an AI agent that translates natural language into database queries using a technology stack comprising LangGraph, a Next.js frontend, watsonx.ai models, and an in-memory SQLite database. The process begins with establishing the user interface and server-side actions, including state management for the conversation and configuring the LLM connection. A core component is the creation and seeding of the SQLite database; its schema is provided as context to the agent. A custom LangChain tool is then engineered to execute SQL queries, which the ReAct agent is instructed to use via a detailed system prompt. This prompt guides the model in generating syntactically correct SQL. The tutorial demonstrates the agent's capabilities with both simple and complex queries and concludes by highlighting the critical security consideration of implementing guardrails to restrict the LLM's database access.\n","\n","--- TOPICS ---\n"," ['Agentic AI', 'Langraph', 'Natural Language Processing']\n","\n","--- Q&A ---\n"," Q: What is the primary goal of the AI agent being developed in this project?\n","A: The goal is to build an Artificial Intelligence (AI) agent that is capable of using its knowledge of SQL to communicate with a database.\n","Q: Which specific technologies and frameworks are mentioned for building this text-to-SQL application?\n","A: The project uses LangGraph to build a ReAct agent, Next.js for the frontend, models from watsonx.ai, and an in-memory SQLite database.\n","Q: According to the transcript, what is the stated reason for using Tailwind in the Next.js application?\n","A: Tailwind is used in the project so that the developer does not have to write any CSS.\n","Q: In the context of Next.js, what two execution environments for components are mentioned in the transcript?\n","A: The transcript mentions that in Next.js, components can be run either client-side or server-side in the background.\n","\n","--- KEY CONCEPTS ---\n"," AI agent, large language models, Text2SQL agent, LangGraph, ReAct agent, watsonx.ai, in-memory database, SQLite, Next.js\n","Row 26 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","--- SUMMARY ---\n"," Prompt engineering is a specialized discipline within Natural Language Processing (NLP) focused on designing effective inputs to guide pre-trained large language models (LLMs) like GPT and BERT. Its primary objective is to generate outputs that are more accurate, coherent, and contextually appropriate than those produced by traditional rule-based methods. This capability is crucial for advancing applications such as chatbots, automated translation, and content generation. However, the field faces notable limitations, including difficulties in managing complex or ambiguous prompts and the potential for models to produce biased or inaccurate results stemming from their underlying data or architecture. An introductory course on this subject typically covers the fundamentals of prompt analysis and the core principles of effective prompt construction, providing a comprehensive overview of this emerging area of study.\n","\n","--- TOPICS ---\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is prompt engineering, according to the transcript?\n","A: Prompt engineering is a specialized field in natural language processing that focuses on building models, based on pre-trained large language models, that can generate high-quality text outputs in response to specific prompts or inputs.\n","Q: What is the key benefit of prompt engineering compared to traditional rule-based or keyword-based approaches?\n","A: The key benefit is that prompt engineering allows for the generation of text outputs that are more accurate, coherent, and contextually appropriate, which significantly impacts user experience in applications like chatbots and language translation.\n","Q: A prompt engineering model generates a biased and inaccurate output. According to the transcript, what are two potential reasons for this limitation?\n","A: The two potential reasons mentioned are that the output may be biased or inaccurate due to the underlying data the model was trained on, or because of the model's architecture.\n","Q: What foundational skill related to analyzing prompts will be covered in the first part of the course?\n","A: The first part of the course will cover the basics of prompt analysis, which includes how to deconstruct prompts and identify their key features and constraints.\n","\n","--- KEY CONCEPTS ---\n"," prompt engineering, natural language processing, pre-trained large language models, fine-tuning, rule-based approaches, keyword-based approaches, model architecture, prompt analysis\n","Row 27 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","--- SUMMARY ---\n"," Q-learning is a prominent value-based reinforcement learning (RL) algorithm that determines an optimal policy by learning a state-action value function. Unlike policy-based methods, it estimates Q-values—the expected cumulative reward for an action in a given state—and stores them in a Q-table. The core of the algorithm is an iterative update process driven by the Bellman equation. At each step, a target Q-value is calculated from the immediate reward and the discounted maximum Q-value of the next state. The discrepancy between this target and the current estimate is the Temporal Difference (TD) error, which, scaled by a learning rate, is used to refine the Q-table. This process is repeated over multiple episodes until the Q-values converge, establishing an optimal target policy. Because the learned policy can differ from the behavior policy used for exploration, Q-learning is classified as an off-policy method.\n","\n","--- TOPICS ---\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What are the two main types of reinforcement learning algorithms, and how do they differ in their approach to maximizing reward?\n","A: The two main types are value-based and policy-based methods. Value-based methods determine a value function that quantifies the total reward, which is then used to find the optimal policy. Policy-based methods determine the optimal policy directly to maximize the total reward.\n","Q: Explain the difference between a state value function (V) and a state-action value function (Q).\n","A: A state value function (V) takes a state as input and quantifies how good it is to be in that specific state. A state-action value function (Q) takes both a state and an action as input, and it quantifies how good it is to take that specific action while in that state.\n","Q: What is the primary goal of the Q-learning algorithm?\n","A: The goal of Q-learning is to learn the Q-values for a table of states and actions, where these learned values help an agent follow a policy that maximizes its total reward.\n","Q: In the initial stages of the grid world example, the agent uses an 'exploration policy' or 'behavior policy'. What does this mean for the agent's decision-making process?\n","A: Using an exploration or behavior policy means the agent's actions are based on random chance. This prevents the agent from only choosing actions that already have a high Q-value, allowing it to explore the environment more thoroughly.\n","\n","--- KEY CONCEPTS ---\n"," Q-learning, Reinforcement learning, Value-based methods, Policy-based methods, State-action value function, Optimal policy, Q-table, Behavior policy, Bellman equation, Discount factor\n","Row 28 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","--- SUMMARY ---\n"," A logistic classifier operates as a fundamental linear model, generating predictions by applying a linear function to input data. This process involves multiplying an input vector by a matrix of weights and adding a bias term to produce a set of raw scores, commonly referred to as logits. The primary objective during the training phase is to optimize these weights and biases to maximize the model's predictive accuracy. For single-label classification tasks, these logits are subsequently transformed using a softmax function. This crucial step normalizes the scores into a valid probability distribution, ensuring that the probabilities assigned to all possible classes sum to one. By doing so, the softmax function effectively assigns a high probability to the most likely class while attributing low probabilities to incorrect alternatives, thereby facilitating a clear and interpretable classification decision based on the model's confidence.\n","\n","--- TOPICS ---\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","--- Q&A ---\n"," Q: According to the transcript, what type of classifier is a logistic classifier, and what mathematical operation does it use to generate predictions?\n","A: A logistic classifier is a linear classifier. It applies a linear function, which is described as a \"giant matrix multiply,\" to the inputs to generate its predictions.\n","Q: What is the goal of \"training\" a logistic classifier, and which specific parameters are adjusted during this process?\n","A: The goal of training is to find the values for the weights (W) and bias (b) that are good at performing predictions. The weights and bias are the parameters that are adjusted during the training process.\n","Q: How does a logistic classifier convert its output scores into probabilities for a task where each input has only one correct label?\n","A: It uses a softmax function to turn the scores (also called logits) into proper probabilities. This function ensures the probabilities sum to 1, with the goal of making the probability for the correct class close to one and all others close to zero.\n","Q: In the context of logistic regression, what is another term for the scores generated by the linear function?\n","A: In the context of logistic regression, scores are also often called logits.\n","\n","--- KEY CONCEPTS ---\n"," logistic classifier, linear classifier, linear function, matrix multiply, weights and bias, softmax function, logits, logistic regression\n","Row 29 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","All rows processed. Final output saved to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n","\n","Generation completed. To evaluate, run the SEPARATE evaluation script.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS & PATHS (EVALUATION ONLY)\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json\n","from typing import List, Dict, Any\n","\n","import numpy as np\n","import pandas as pd\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","# ---- Paths (PRO) ----\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/evaluation_COT.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 2. GOLD TOPIC RULES (YOUR KEYWORD-BASED VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str) -> List[str]:\n","    text = (ref_sum or \"\").lower()\n","    matched: List[str] = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 3. TOKENISER & TOPIC LABEL SPACE\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\n","    \"been\",\"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\n","    \"across\",\"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str) -> List[str]:\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", str(text).lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. EVALUATION FUNCTION (WITH OVERLAP ACCURACY)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame) -> Dict[str, Any]:\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","\n","    overlap_acc = []      # your Accuracy A (any overlap = 1)\n","    jaccard_list = []\n","\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    # store label lists for macro/weighted F1\n","    all_gold_labels: List[List[str]] = []\n","    all_pred_labels: List[List[str]] = []\n","\n","    # global TP/FP/FN for true micro F1\n","    global_tp = 0\n","    global_fp = 0\n","    global_fn = 0\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        if idx not in df_ref.index:\n","            continue\n","\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"]\n","        ref_s = ref_summary if isinstance(ref_summary, str) else str(ref_summary or \"\")\n","\n","        gen_sum_raw = row.get(\"summary\", \"\")\n","        gen_sum = gen_sum_raw if isinstance(gen_sum_raw, str) else str(gen_sum_raw or \"\")\n","\n","        # -------------------- Summarisation --------------------\n","        r = rouge.score(ref_s, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_s.split()], gen_sum.split(), smoothing_function=smooth)\n","        P, R, F1 = bert_score([gen_sum], [ref_s], lang='en', verbose=False)\n","\n","        sum_r.append(float(r))\n","        sum_b.append(float(b))\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_s)\n","        pred_raw = row.get(\"topic_classification\", \"\") or \"\"\n","        pred = [x.strip() for x in str(pred_raw).split(\",\") if x.strip()]\n","\n","        set_gold = set(gold)\n","        set_pred = set(pred)\n","\n","        # Accuracy A: 1 if ANY overlap, else 0\n","        overlap = 1.0 if (set_gold & set_pred) else 0.0\n","        overlap_acc.append(overlap)\n","\n","        # Jaccard\n","        inter = len(set_gold & set_pred)\n","        union = len(set_gold | set_pred)\n","        jaccard = inter / union if union > 0 else 0.0\n","        jaccard_list.append(jaccard)\n","\n","        # accumulate for global micro-F1\n","        global_tp += inter\n","        global_fp += len([p for p in pred if p not in gold])\n","        global_fn += len([g for g in gold if g not in pred])\n","\n","        all_gold_labels.append(gold)\n","        all_pred_labels.append(pred)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row.get(\"Q_and_A\", \"\") or \"\"\n","        qs = [l[2:].strip() for l in str(qa_text).splitlines()\n","              if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(float(np.mean(bleu_vals)))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_s))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts_tokens = [c.lower() for c in tokenize(ref_s)]\n","        ref_top = ref_concepts_tokens[:25]\n","\n","        tp_k = len([\n","            p for p in pred_concepts[:10]\n","            if any(p in r or r in p for r in ref_top)\n","        ])\n","\n","        p_val = tp_k / 10.0\n","        r_val = tp_k / max(1, len(ref_top)) if ref_top else 0.0\n","        f_val = 2 * p_val * r_val / (p_val + r_val) if (p_val + r_val) else 0.0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f_val)\n","\n","    # ---- TRUE global micro-F1 for topics ----\n","    if (2 * global_tp + global_fp + global_fn) > 0:\n","        micro_f1 = 2 * global_tp / (2 * global_tp + global_fp + global_fn)\n","    else:\n","        micro_f1 = 0.0\n","\n","    # ---- Macro & Weighted F1 over label space ----\n","    label2idx = {l: i for i, l in enumerate(VALID_TOPICS)}\n","    n = len(all_gold_labels)\n","    L = len(VALID_TOPICS)\n","\n","    y_true_bin = np.zeros((n, L), dtype=int)\n","    y_pred_bin = np.zeros((n, L), dtype=int)\n","\n","    for i, gold in enumerate(all_gold_labels):\n","        for g in gold:\n","            if g in label2idx:\n","                y_true_bin[i, label2idx[g]] = 1\n","\n","    for i, pred in enumerate(all_pred_labels):\n","        for p in pred:\n","            if p in label2idx:\n","                y_pred_bin[i, label2idx[p]] = 1\n","\n","    _, _, macro_f1, _ = precision_recall_fscore_support(\n","        y_true_bin, y_pred_bin, average=\"macro\", zero_division=0\n","    )\n","    _, _, weighted_f1, _ = precision_recall_fscore_support(\n","        y_true_bin, y_pred_bin, average=\"weighted\", zero_division=0\n","    )\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)) if sum_r else 0.0,\n","            \"BLEU\": float(np.mean(sum_b)) if sum_b else 0.0,\n","            \"BERTScore F1\": float(np.mean(sum_bert)) if sum_bert else 0.0\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc)) if overlap_acc else 0.0,\n","            \"Jaccard Index\": float(np.mean(jaccard_list)) if jaccard_list else 0.0,\n","            \"Micro F1\": float(micro_f1),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)) if qa_bleu else 0.0,\n","            \"Diversity\": float(np.mean(qa_div)) if qa_div else 0.0,\n","            \"Answerability\": float(np.mean(qa_ans)) if qa_ans else 0.0\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)) if kc_p else 0.0,\n","            \"Recall@10\": float(np.mean(kc_r)) if kc_r else 0.0,\n","            \"F1@10\": float(np.mean(kc_f)) if kc_f else 0.0\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 5. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(eval_summary, f, indent=2, ensure_ascii=False)\n","\n","print(\"\\nSaved evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"id":"jOGu39awm7S3","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["943cc92da5f54543acc1a4295c79d96f","c00ef7e1149c4a3bb9a60917bafcf36d","0ddcd53401e64fe98b8e0b883bf9a30b","94d963562eda4c16b4a691b69e469725","47287ad47392401e8a2e898ef7a95f3f","e5bb028e0a814aca9c5619da777ffbaf","6b47c8224aa74a109d4fde087296dc51","f37a3055506c413fab0b722e1a2e4943","281c0cc53f064ce8927ab75a21f7d1ec","f6f3c10dcf444600aa3332bcf09c7567","501221449e3a4caa921edbabb69819e4","6ec07873d7384123a7f8a46aa20d839e","a9300698d8b048af8f77b24ec1fe2df2","2bb5f6a654e3487792c04dc7d32b0330","58221be7330d4d0aa1feaf023a65cf5b","f8618486cbed47739526ef3d8d8f041b","46a7ca7e566446a49f91b0b43c3413c8","fcfa999a61b94424a94a2f7b504ec07b","cd9b32c499e94ab0ab2a88a8a6f3bdd3","e827ca3c8f114e908c30202340f0a439","6c3e4697c3f74dfaaa14271031545a53","caca24c2f23e4ec3bc215d785ead9129","614ebef586764f198c74c0b0a231a0ca","e12d66ccc0f84a2899464c060add6d5b","26c37d0d226a4239bf99d89359a52edc","e5f8bcb7391f48218e3ba5bfb8778789","198c384aa4fb4e558cc27ab74c836c93","57f960cc6a6f4065b334073df2c05ac7","7d02a2da1d6c427ca32883d8373bb76d","39f570dc9dc44b4cb4c8bfc0909ebf3c","70bfbcc2fa384883a12fd8603be33be1","30661c6dcac44f96b4065607b887215f","bfd8b192dd22491f8a02462074381d31","85e19da7dc25456d90d44009735054e2","ede2b2fb40e140a78cc01fc997755d8d","9e469f10716348d4a5f22620df8fff6a","e1297da0a288410e9bb9ac890db84688","f8ff4bce0df645a995a9807fd20e06fc","49f89c244fb1485d86b7fb0407db447a","ed3245fa005745f2841638d163e8d767","6e16b319d86044f0bcbd2492ab2b0d7e","3521247653b84a7d9c4e3bafbf755352","843aa4738ff94c2d8ec38692d0c421ba","5210019eb0df450d9550912d50c663ce","3328689540a8498598fa14e68dc7d35a","19f6272e05ea421b9430dff61a33af5f","a85a445df7304f2991069249968dc45e","55ecf21c0fbf4a679fc1c39ab3262515","a15d695c18b14b1d8d5a629ab8400826","97520cc6431e4d7da0b33cabebae3209","3284ababe4b7414baf08c33b10ea2bc0","f192ac0b86a14a20b8576cbe717a7bc8","30a24df512cd4cb8937aae878d2f5b75","5f4f59f40b6143849cfa1e2a6876ece1","d825104e6f504b9e9734029cf46d3d7c","b70492d14f814f08a64c8debe3aaffab","0810481463694e82a799100db034e5ac","37caceec7c3a4b7bbe4307e68e0511b3","594e7be3eaed4881a1f24fdcd565d678","7a62d8e37f1745ffb6daffbc45af1743","0161a0ae5ab94ccb8d30f7289242defc","432f7fac6d9e402fac9449b2e7d24d84","a42d5f12a14d4a649e4b5a50ca32e0ca","c875698e01684ad9bbb68ad05cad0f07","430865064ce34fd0aa15ea9e7e0745f5","c50b1d6a2fec4bd293a61fbcad7e7499"]},"executionInfo":{"status":"ok","timestamp":1763832716738,"user_tz":-330,"elapsed":156063,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"03f594e3-064a-4c1c-e8ac-20de1a78aa42"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/gemini-2.5-pro_cot_full_output.xlsx\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"943cc92da5f54543acc1a4295c79d96f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ec07873d7384123a7f8a46aa20d839e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614ebef586764f198c74c0b0a231a0ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e19da7dc25456d90d44009735054e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3328689540a8498598fa14e68dc7d35a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b70492d14f814f08a64c8debe3aaffab"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3113\n","  - BLEU: 0.0747\n","  - BERTScore F1: 0.8922\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.4055\n","  - Micro F1: 0.5414\n","  - Macro F1: 0.5212\n","  - Weighted F1: 0.5339\n","\n","Q&A Generation:\n","  - BLEU: 0.0210\n","  - Diversity: 0.7063\n","  - Answerability: 0.5689\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4267\n","  - Recall@10: 0.1707\n","  - F1@10: 0.2438\n","\n","Saved evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-pro/evaluation_COT.json\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0qtJuEOhm7fO"},"execution_count":null,"outputs":[]}]}