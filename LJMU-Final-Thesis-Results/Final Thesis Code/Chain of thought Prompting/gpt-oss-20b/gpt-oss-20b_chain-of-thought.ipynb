{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNfqh2MkjW/DRl8BlGnrAO2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"a09721d1b4cb495b99d4b5eec4747d8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c72cb593e2ea49ba919d224cd1351790","IPY_MODEL_72e7d287824047b9b239704664cd16aa","IPY_MODEL_cff77ac23de0465181c18c828e895ea6"],"layout":"IPY_MODEL_a4824c3bcd8a4ae1b6d3ac7c024ea439"}},"c72cb593e2ea49ba919d224cd1351790":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12ea0e27776d4ea09ad2b88c64b110b2","placeholder":"​","style":"IPY_MODEL_b4f7882e1af9464e9227ae8c82b42be5","value":"tokenizer_config.json: 100%"}},"72e7d287824047b9b239704664cd16aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_27ae493eef244136821ebf8cb72e8346","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d389c51fadd444ea51a21a45cdfd4d8","value":25}},"cff77ac23de0465181c18c828e895ea6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68f12d86877e41b097c53ccddb7c6693","placeholder":"​","style":"IPY_MODEL_86988b9b00a54c33a15b4070529b692a","value":" 25.0/25.0 [00:00&lt;00:00, 2.69kB/s]"}},"a4824c3bcd8a4ae1b6d3ac7c024ea439":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12ea0e27776d4ea09ad2b88c64b110b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4f7882e1af9464e9227ae8c82b42be5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"27ae493eef244136821ebf8cb72e8346":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d389c51fadd444ea51a21a45cdfd4d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68f12d86877e41b097c53ccddb7c6693":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86988b9b00a54c33a15b4070529b692a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe4d830bdfae4e8a811b397fc02b1902":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7b8bcfadb9c4495ab57ff296115ce7c5","IPY_MODEL_ceba5b16a51842fbb15eaae199ad65a5","IPY_MODEL_833f137747a74f16a9ed3b73e818b06d"],"layout":"IPY_MODEL_b8952738d7f14b0a821dc8207619b75b"}},"7b8bcfadb9c4495ab57ff296115ce7c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e678b2b4aa943f5902b856b9b842975","placeholder":"​","style":"IPY_MODEL_8fee27b6d8c94701b727f5a23b5c32f4","value":"config.json: 100%"}},"ceba5b16a51842fbb15eaae199ad65a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2f951c1ca7ab4654a253ee14820fb7fb","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_05d5e063361a47a187dd9bf92563f8f4","value":482}},"833f137747a74f16a9ed3b73e818b06d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ae4c29af46345a9b7bbd694b1868ab4","placeholder":"​","style":"IPY_MODEL_15681648db42470a9602cf876cf58075","value":" 482/482 [00:00&lt;00:00, 50.6kB/s]"}},"b8952738d7f14b0a821dc8207619b75b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e678b2b4aa943f5902b856b9b842975":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fee27b6d8c94701b727f5a23b5c32f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2f951c1ca7ab4654a253ee14820fb7fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05d5e063361a47a187dd9bf92563f8f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4ae4c29af46345a9b7bbd694b1868ab4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15681648db42470a9602cf876cf58075":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3cd40b3e6c74882996a454ee2ee0a30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d199262aefb9477b815c2a9cce58e83c","IPY_MODEL_2a35679d197a4bffa531f5f2b16a5cf3","IPY_MODEL_b48c151f6cbd46ef9b9d0fd9a5265452"],"layout":"IPY_MODEL_6b0d75c480b94a2182d10351b025c076"}},"d199262aefb9477b815c2a9cce58e83c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0775be2116db47c1ac188c75f42b89c4","placeholder":"​","style":"IPY_MODEL_855f68db05e04fad82ab10922d34267f","value":"vocab.json: 100%"}},"2a35679d197a4bffa531f5f2b16a5cf3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_952b3c71cf2846bea7a8138355c20d3b","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81a63daa361546e8adf66437813397eb","value":898823}},"b48c151f6cbd46ef9b9d0fd9a5265452":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b462712818147d5b44a6cdf3951cb24","placeholder":"​","style":"IPY_MODEL_ffd4130707b4451895c3162812d2df68","value":" 899k/899k [00:00&lt;00:00, 13.4MB/s]"}},"6b0d75c480b94a2182d10351b025c076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0775be2116db47c1ac188c75f42b89c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855f68db05e04fad82ab10922d34267f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"952b3c71cf2846bea7a8138355c20d3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81a63daa361546e8adf66437813397eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b462712818147d5b44a6cdf3951cb24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffd4130707b4451895c3162812d2df68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a110edae0da340699ffbad1471acaf77":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_626823cad6f945c68f1de8431b1ec5b3","IPY_MODEL_f1e94541ea5c44a79daa3ae7982d249c","IPY_MODEL_3e3b15fe610c40ae977c2393b5df0220"],"layout":"IPY_MODEL_656646fea55343ff8e632f16b4079de2"}},"626823cad6f945c68f1de8431b1ec5b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ecd268cd96774709a882597654edeb55","placeholder":"​","style":"IPY_MODEL_6c21aa9fa88146e3bf0e92f723f54b80","value":"merges.txt: 100%"}},"f1e94541ea5c44a79daa3ae7982d249c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a20bb05a5b2450693879d9861b61bd4","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_12deac19872948018552fe89ca62df18","value":456318}},"3e3b15fe610c40ae977c2393b5df0220":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_78a5f4de4dea4b4293a1bb08751e4028","placeholder":"​","style":"IPY_MODEL_dc25fd3e75df463bb66b680b5e2cf634","value":" 456k/456k [00:00&lt;00:00, 30.9MB/s]"}},"656646fea55343ff8e632f16b4079de2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecd268cd96774709a882597654edeb55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c21aa9fa88146e3bf0e92f723f54b80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a20bb05a5b2450693879d9861b61bd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12deac19872948018552fe89ca62df18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"78a5f4de4dea4b4293a1bb08751e4028":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc25fd3e75df463bb66b680b5e2cf634":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3563c8a761814f96aa9774e477763553":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7d6681b69ae40dba972f943c4158f98","IPY_MODEL_984cfca9ccdd4175b06ccce675c37a62","IPY_MODEL_ee125e369ea94da297ffcbec328633c7"],"layout":"IPY_MODEL_fbc344af36c44803a16391586298cded"}},"c7d6681b69ae40dba972f943c4158f98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6515a3e923f45d5bdbb08497fb64fa7","placeholder":"​","style":"IPY_MODEL_870d2e152b734b4399561d721e864998","value":"tokenizer.json: 100%"}},"984cfca9ccdd4175b06ccce675c37a62":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a717b9a4b301499b97a6e9202d040baa","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0dbff8706d95469d96be3a297359c51c","value":1355863}},"ee125e369ea94da297ffcbec328633c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3549debd867b4fdfaf8a3e87361a4273","placeholder":"​","style":"IPY_MODEL_8b4cf344609f43ca80fc5ba729e583ba","value":" 1.36M/1.36M [00:00&lt;00:00, 24.2MB/s]"}},"fbc344af36c44803a16391586298cded":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6515a3e923f45d5bdbb08497fb64fa7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"870d2e152b734b4399561d721e864998":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a717b9a4b301499b97a6e9202d040baa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dbff8706d95469d96be3a297359c51c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3549debd867b4fdfaf8a3e87361a4273":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b4cf344609f43ca80fc5ba729e583ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b752415cc2845f699d35126a747e382":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_13a149805c1b4570b7043041df4e6317","IPY_MODEL_94de3539f5d847fdaa6f2122dd848d56","IPY_MODEL_c7e5062fbd2b4fb0b1d7700bba1eb403"],"layout":"IPY_MODEL_86e0318a3de0484bbe8a5dd8810ab1f2"}},"13a149805c1b4570b7043041df4e6317":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa9ff7be7cfb48a4929263e2f4ab3f3f","placeholder":"​","style":"IPY_MODEL_5fdc3795a86b4a0aa2010567098fa730","value":"model.safetensors: 100%"}},"94de3539f5d847fdaa6f2122dd848d56":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bcba733c2c044d69ad3262087167ccd","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f899dc184ca4e18b0d451602d250790","value":1421700479}},"c7e5062fbd2b4fb0b1d7700bba1eb403":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8884a7fadf2408d9937710e60e0d245","placeholder":"​","style":"IPY_MODEL_297088d00a5545419b63576703429191","value":" 1.42G/1.42G [00:22&lt;00:00, 62.7MB/s]"}},"86e0318a3de0484bbe8a5dd8810ab1f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa9ff7be7cfb48a4929263e2f4ab3f3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5fdc3795a86b4a0aa2010567098fa730":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3bcba733c2c044d69ad3262087167ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f899dc184ca4e18b0d451602d250790":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8884a7fadf2408d9937710e60e0d245":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"297088d00a5545419b63576703429191":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"-DqwO7-5zsoo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763902000567,"user_tz":-330,"elapsed":24439,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"d4c618bb-64be-4050-cb68-5faa9cc7f13b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=568c3644dbbcb5f58ed0b0540f9089ccad04e3cdb557b610b729efb0ba5f0e67\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"n9195GOYGSfH","executionInfo":{"status":"ok","timestamp":1763902000605,"user_tz":-330,"elapsed":34,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"deaa8564-066e-461d-a61c-7c07d1693b64"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"openai/gpt-oss-20b\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Oipw983mGSlF","executionInfo":{"status":"error","timestamp":1763885014467,"user_tz":-330,"elapsed":15198,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"dd7d89bb-4156-448e-f289-77a6f677e83f"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","The video provides an introductory overview of reinforcement learning with human feedback (RLHF), using a grid‑world scenario to illustrate how an agent—named Frank—learns to navigate toward a reward. Initially, Frank is trained with conventional reinforcement learning algorithms such as Q‑learning, DQN, and proximal policy optimization (PPO). The presentation then demonstrates how incorporating human guidance accelerates learning and aligns the agent’s behavior with human preferences. Building on this foundation, the speaker explains the training pipeline for ChatGPT: a reward model is first constructed to evaluate the quality of generated responses, and this model is subsequently employed within a PPO framework to iteratively fine‑tune the language model. The process is shown to enhance answer quality and ensure that the model’s outputs better reflect desired human values. Overall, the video highlights RLHF’s versatility across algorithms and its practical application in large‑scale language model training.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Generative AI', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary goal for Frank in the grid world described in the video?\n","A: Frank’s goal is to reach the square that contains the +10 reward spot.\n","Q: According to the transcript, how does human feedback affect Frank’s learning process?\n","A: Human feedback accelerates Frank’s learning by nudging him toward better actions, helping him reach the reward spot more efficiently.\n","Q: What are the two main steps involved in training ChatGPT using reinforcement learning through human feedback, as described in the transcript?\n","A: First, a reward model is trained to score the quality of answers; second, this reward model is used with the proximal policy optimization algorithm to fine‑tune ChatGPT by incorporating the reward into its loss function and performing back‑propagation.\n","Q: Which reinforcement learning algorithms are mentioned as compatible with human feedback in the video?\n","A: The transcript lists Q‑learning, DQ learning, proximal policy optimization, and states that all of the above can be used with human feedback.\n","\n","KEY CONCEPTS:\n","\n","reinforcement learning through human feedback, grid world environment, Q-learning, Deep Q-learning, proximal policy optimization, reward model, GPT architecture, fine-tuning via PPO\n","\n","============================================\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","{\"generated_summary\":\"This tutorial segment introduces support vector machines (SVMs) and demonstrates how kernel functions can be explored using the CVXOPT library. The presenter references external resources—including Matthew Blondell’s GitHub code, a related blog post, and Christopher Bishop’s textbook—to contextualize the discussion. CVXOPT is positioned as a pedagogical tool for visualizing kernel-induced decision boundaries, while the speaker notes that production systems typically rely on libraries such as libsvm or scikit‑learn. A concise quadratic‑programming example illustrates the solver workflow, encouraging hands‑on experimentation. The talk also outlines the theoretical underpinnings of soft‑margin SVMs, the role of the penalty parameter \\(C\\), and the transition from linear to nonlinear kernels. Finally, the presenter points to a comprehensive MIT tutorial for deeper insig\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main purpose of using CVX opt in this tutorial?\n","A: To directly observe how a kernel affects the support vector machine and to visualize the resulting changes.\n","Q: Which library is typically used for implementing SVMs instead of CVX opt?\n","A: libsvm.\n","Q: What objective does the quadratic programming solver in CVX opt minimize?\n","A: It minimizes ½ xᵀP x + qᵀx subject to the constraints Gx ≤ h and Ax = b.\n","Q: Which book is referenced for additional information on the models discussed?\n","A: Christopher Bishop's \"Pattern Recognition and Machine Learning.\"\n","Q: Where did the example code used in the tutorial originate?\n","A: It was taken from Matthew Blondell's GitHub.\n","\n","KEY CONCEPTS:\n","\n","CVX opt, Support Vector Machine, Kernel methods, Quadratic programming solver, Soft margin, Nonlinear decision boundary, Quadratic programming constraints, libsvm\n","\n","============================================\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","The transcript provides an introductory overview of prompt engineering, outlining how prompts—inputs to large language models—establish context and constraints that shape generated text. It categorizes seven distinct prompt types, including question, statement, and multi‑input prompts, and discusses how the choice of type influences output quality and complexity. Key prompt attributes such as length, specificity, contextual detail, and explicit constraints (e.g., tone, style, word limits) are identified as critical for defining expected responses. The speaker demonstrates the impact of prompt refinement, illustrating that specifying concise requirements (such as a one‑word answer) can enhance accuracy. Additionally, the discussion covers deconstructing prompts to uncover underlying expectations and constraints. The session concludes by preparing learners for subsequent lessons focused on constructing prompt‑engineering models using pre‑trained language models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What are the seven types of prompts mentioned in the transcript?\n","A: The transcript lists seven types of prompts: question prompts, statement prompts, prompts with multiple inputs, prompts with constraints, and three additional types that are not explicitly named but are implied as part of the seven.\n","Q: Why is it important to define both what you expect and how you want it to be done in a prompt?\n","A: Defining both what you expect and how you want it to be done helps create a clear prompt, which leads to more accurate and efficient output from the large language model.\n","Q: Give an example of how adding a constraint changes the output of a prompt, based on the transcript.\n","A: When asking for the capital of France, a simple prompt returns “Paris” plus extra information. Adding the constraint “give a one word answer” changes the output to just “Paris.”\n","Q: What is the purpose of deconstructing a prompt?\n","A: Deconstructing a prompt involves breaking it into its components—specific language, requirements, and constraints—to better understand what the model should produce and how to guide it.\n","Q: How does the transcript suggest you handle constraints when writing a prompt for an essay?\n","A: It suggests specifying the essay topic and the word limit as constraints, e.g., “write an essay on World War II in 500 words,” so the model generates an essay that meets the 500‑word requirement.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, large language models, prompt types, prompt constraints, prompt deconstruction, prompt specificity, tone constraints, word count constraint\n","\n","============================================\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","Artificial Intelligence agents are autonomous problem‑solvers that decide their own actions, contrasting with chains and routers that strictly follow predefined instructions. These agents leverage specialized tools—such as calculators or search engines—to accomplish tasks. A prevalent architectural design for constructing such agents is the REACT pattern, which emulates human reasoning by cycling through think, act, and observe stages. In this loop, a language model first interprets the prompt, selects an appropriate tool and its arguments, the tool is executed by the system, and the resulting output is fed back to the model for further reasoning. This iterative process continues until a satisfactory answer is achieved, providing a foundational framework for coding a basic REACT agent with LangChain. The approach underscores the synergy between autonomous decision‑making, tool utilization, and iterative refinement in modern AI agent design.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'LangChain', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What distinguishes AI agents from chains and routers in terms of decision-making?\n","A: AI agents can autonomously decide what steps to take on their own, whereas chains and routers simply follow specific instructions given to them.\n","Q: What are the main steps of the React agent pattern and what does each step represent?\n","A: The React pattern consists of: 1) Think – the LLM considers the problem; 2) Action – the LLM decides whether to answer directly or use a tool; 3) Action Input – the LLM provides arguments for the chosen tool; 4) Observe – the system executes the tool and returns its output to the LLM; 5) The cycle repeats (Think → Action → Observe) until a final answer is reached.\n","Q: How does the LLM interact with tools during the React pattern cycle?\n","A: The LLM first determines if a tool is needed, then supplies the appropriate arguments (action input). The tool is executed by the system, and its output is fed back to the LLM, which then observes the result and decides whether further actions are required.\n","Q: What role does LangChain play in executing tools within the React pattern?\n","A: LangChain receives the tool name and arguments from the LLM, executes the specified function or API call, and returns the output back to the LLM for observation.\n","Q: Why might a multi-step problem require repeated cycles in the React pattern?\n","A: If the initial tool output does not fully solve the problem, the LLM must think again, choose another action, and observe the new result, repeating the cycle until the final answer is achieved.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence agents, Autonomous decision-making, Tool usage in agents, React agent pattern, Think‑Action‑Observe loop, LangChain framework, LLM reasoning, Multi‑step problem solving\n","\n","============================================\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","The presentation details the construction and operation of a reflection agent system designed to refine viral tweets through iterative feedback. It explains how the system integrates a generation agent, which produces initial tweet drafts, and a reflection agent, which critiques and guides subsequent revisions, thereby enabling multi‑step refinement. The speaker demonstrates setting up a LangChain project with LSMITH integration, highlighting the role of environment variables for API keys and tracing. A practical walkthrough shows the workflow of generating, reflecting, and refining tweets, with each iteration recorded by LSMITH to provide comprehensive traceability. The discussion references the smith.chain website as a resource illustrating the system’s architecture. Overall, the talk illustrates how the reflection agent collaborates with other modules to produce a refined final tweet, validating the effectiveness of the integrated, traceable approach for complex content generation tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Agentic AI', 'Generative AI', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What system is being traced in this section?\n","A: The reflection agent system.\n","Q: What is the purpose of tracing the reflection agent system?\n","A: To understand exactly what is happening where and how both systems work together to deliver the final refined viral tweet.\n","Q: Which website is mentioned for tracing the system?\n","A: smith.chain.\n","Q: What is the final output that the systems aim to produce?\n","A: A refined viral tweet.\n","Q: Why does the speaker want to trace the reflection agent system?\n","A: To gain a clear understanding of how both systems collaborate to produce the final refined viral tweet.\n","\n","KEY CONCEPTS:\n","\n","reflection agent system, viral tweet generation, system integration, refinement pipeline, AI-driven content creation, website smith.chain\n","\n","============================================\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","The tutorial guides users through integrating LangChain’s OpenAI chat model into a Python workflow. It begins with installing the \"langchain-openai\" package, correcting a syntax error, and importing the necessary classes. An instance of the chat model is created, with the instructor highlighting the trade‑off between GPT‑4o’s advanced performance and GPT‑3.5’s lower cost. Interaction with the model is demonstrated using the \"invoke\" method, passing a prompt and capturing the response. A common error—missing API key—is addressed by creating a .env file, storing the key, and loading it via python‑dotenv, ensuring secure, global access. The session shows how to extract the response’s content field and notes that insufficient credits can be remedied by topping up the OpenAI account. Finally, the instructor outlines extending the approach to include full conversation history for more context‑aware responses, emphasizing practical configuration, model selection, and prompt preparation for effective LLM communication.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Python Programming', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What command is used to install the LangChain package for OpenAI chat models?\n","A: The command is `pip install langchain-openai`.\n","Q: Which class from the LangChain OpenAI module is imported to work with chat models?\n","A: The class imported is `ChatOpenAI`.\n","Q: How do you initialize a chat model instance for the GPT-4.0 model using LangChain?\n","A: You initialize it with `ChatOpenAI(model_name=\"gpt-4.0\")`.\n","Q: Why might someone choose to use GPT-3 instead of GPT-4.0 in this context?\n","A: Because GPT-4.0 is more advanced and can be more expensive, so GPT-3 is a cheaper alternative.\n","Q: What issue was encountered during installation and how was it resolved?\n","A: The installation failed due to a stray percentage sign; removing the percentage sign allowed the package to install successfully.\n","\n","KEY CONCEPTS:\n","\n","LangChain, OpenAI API, ChatOpenAI class, GPT-4 model, GPT-3 model, pip install command, model initialization, parameter passing\n","\n","============================================\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","The video provides a concise demonstration of Python’s built‑in sorting behavior for lists containing strings and mixed data types. It first illustrates that, by default, the sort method orders strings alphabetically with uppercase characters preceding lowercase ones, and that the same method can reverse the sequence when requested. The presenter then examines a heterogeneous list comprising both numeric values and strings, noting that numeric elements are positioned before string elements in the sorted output; reversing the list subsequently moves the numeric element to the end. Throughout, the speaker emphasizes the importance of normalizing case (e.g., converting all strings to a common case) when a specific ordering is required. These examples collectively clarify the default ordering rules of Python’s sorting algorithm and highlight practical considerations for handling mixed‑type collections.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the result of sorting a list that contains both uppercase and lowercase strings in Python?\n","A: Python sorts the list by placing all strings that start with an uppercase letter first, sorted alphabetically, followed by all strings that start with a lowercase letter, also sorted alphabetically.\n","Q: How does Python's sort method handle a list that contains both numbers and strings?\n","A: When sorting a list with both numbers and strings, Python places all numbers at the beginning of the list, followed by all strings.\n","Q: What happens to the order of elements when the reverse parameter is used with the sort method on a mixed-case string list?\n","A: Using reverse=True sorts the list in reverse order: lowercase strings appear first in reverse alphabetical order, followed by uppercase strings in reverse alphabetical order.\n","Q: Why might you want to convert all strings to either lowercase or uppercase before sorting a list?\n","A: Converting all strings to the same case prevents Python from separating uppercase and lowercase items into two groups, allowing a single alphabetical order.\n","Q: If you insert the number 18 into the sixth position of a list and then sort it, where will 18 appear in the sorted list?\n","A: After sorting, the number 18 will appear at the very beginning of the list, before all string elements.\n","\n","KEY CONCEPTS:\n","\n","Python list sorting, sort method behavior, case-sensitive sorting order, reverse alphabetical sorting, mixed type (strings and numbers) sorting, list insertion at specific index, stable sort algorithm, type coercion rules in Python sorting, alphabetical ordering of strings, handling uppercase vs lowercase in sorting\n","\n","============================================\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: \n","Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","The discussion examines how task characteristics and actor strengths determine whether humans, artificial intelligence (AI), or a hybrid approach should be employed, using fraud‑detection as a primary example. AI efficiently filters high‑confidence alerts, while human analysts excel at ambiguous or rare cases, leading to a complementary workflow that maximises accuracy and efficiency. The analysis contrasts pure AI, human, and augmented intelligence (AI‑human collaboration) performance, noting that augmented systems achieve the highest success rates for moderate‑confidence predictions but are vulnerable to human cognitive bias. The presentation highlights the influence of information display—forced versus optional AI outputs—on analyst reliance, automation bias, and trust, showing that optional displays reduce bias and that explicit accuracy metrics can paradoxically lower AI adoption. Overall, the material underscores the importance of transparent, well‑designed interfaces and high‑quality human input to optimise hybrid decision‑making and shift from subjective to quantifiable judgments.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main problem faced by financial analysts in the fraud detection system described in the transcript?\n","A: They are overwhelmed by thousands of alerts each day, with about 90 percent being false positives.\n","Q: How does the transcript describe the relationship between an AI system's confidence score and its success rate?\n","A: The AI's confidence score is plotted on the X-axis and the success rate on the Y-axis; high confidence scores (near 100%) correspond to high success rates, while low confidence scores (near 0%) also correspond to high success rates for false positives, and the middle range shows lower success rates.\n","Q: According to the transcript, how do human performance curves compare to AI performance curves in this context?\n","A: Human performance curves are described as being flatter than AI performance curves, indicating less variation in success rate across confidence levels.\n","Q: What decision does the transcript suggest should be made regarding which alerts are handled by AI versus human analysts?\n","A: The transcript implies that alerts with high confidence scores (indicating likely real alerts) should be handled by AI, while alerts where the AI is uncertain (mid-range confidence) should be reviewed by skilled financial analysts.\n","Q: Why might an AI system say \"I don't know\" about a prediction, according to the transcript?\n","A: When the AI is not sure about a given prediction—i.e., its confidence score is in the middle range—it indicates uncertainty, leading to a lower success rate and the AI effectively saying \"I don't know.\"\n","\n","KEY CONCEPTS:\n","\n","fraud detection system, confidence score, success rate, performance curve, false positives, human bias, AI decision-making, alert classification\n","\n","============================================\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: \n","Build generative apps faster with Vertex AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","During a session at Cloud NEX, Google Cloud AI’s product manager Dimitrius unveiled six Vertex AI APIs aimed at accelerating the development of enterprise‑scale generative applications. The new primitives include a document‑understanding API that parses complex documents to enhance retrieval, an improved embedding API, a scalable vector‑search service with hybrid search capabilities, a ranking API that re‑orders retrieved results to improve answer quality, a grounded‑generation API that produces citations, and a check‑grounding API that fact‑checks statements against evidence. All services are stateless, built on Google’s large‑scale infrastructure, and designed for seamless integration into existing workflows and popular frameworks such as LangChain and LlamaIndex. By focusing on quality and ease of adoption, these APIs promise to lower the barrier to entry for enterprises seeking to deploy sophisticated generative AI solutions.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Natural Language Processing', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary challenge that the new Vertex AI APIs aim to solve for developers building generative applications for enterprises?\n","A: They aim to reliably access the right enterprise data to produce accurate and consistent responses, addressing technical challenges developers face repeatedly.\n","Q: Name the six new Vertex AI APIs introduced in the talk and give a one-sentence description of each.\n","A: 1) Document Understanding API – parses complex documents to improve retrieval and answer generation. 2) Embedding API – provides high-performance embeddings for text. 3) Vector Search API – scalable, cost‑efficient embedding retrieval with hybrid search. 4) Ranking API – ranks retrieved results to surface the most relevant answers. 5) Grounded Generation API – uses Gemini to generate grounded answers with citations. 6) Check Grounding API – fact‑checks statements against provided evidence.\n","Q: How does the Ranking API enhance the quality of answers produced by an LLM?\n","A: It evaluates each retrieved result’s relevance to the question and promotes the most relevant information, improving the LLM’s answer quality.\n","Q: What is the purpose of the Grounded Generation API and how does it differ from a standard generative model?\n","A: It fine‑tunes Gemini to take a question and evidence, producing well‑grounded answers that include citations to the reference information, ensuring factual accuracy.\n","Q: Describe how the APIs are designed to facilitate seamless integration into developers’ workflows.\n","A: They are stateless primitives with clear interfaces, easy to try and understand, and are being integrated into popular frameworks like LangChain and LlamaIndex for rapid prototyping and combination with other APIs.\n","\n","KEY CONCEPTS:\n","\n","Vertex AI APIs, Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Check Grounding API, Hybrid Search\n","\n","============================================\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","{\"generated_summary\":\"The lecture presents the singular value decomposition (SVD) of a matrix \\(X\\) as \\(X=U\\Sigma V^{T}\\), where \\(U\\) and \\(V\\) are unitary (orthogonal in the real case) and \\(\\Sigma\\) is diagonal with non‑negative singular values. It distinguishes between the full SVD, which includes all singular vectors, and the economy (truncated) form, noting that only the truncated \\(U\\) and \\(V\\) retain orthonormality. The discussion emphasizes that unitary transformations preserve inner products and norms when applied to both vectors, with the transpose sufficing for real data and the conjugate transpose required for complex data. Geometrically, \\(X\\) maps the unit sphere to an ellipsoid whose semi‑axes are the singular values, illustrating how the SVD captures the principal directions and scaling of the linear map. The lecture concludes by highlighting applications such as dimen\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Data Science', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","Q: What condition must a matrix satisfy to be considered unitary?\n","A: A matrix U is unitary if U U^T = I and U^T U = I (for real matrices) or U U* = I and U* U = I (for complex matrices).\n","Q: How does a unitary transformation affect the angles and lengths between any two vectors in a vector space?\n","A: A unitary transformation preserves the angles and lengths between vectors; the inner product between any two vectors remains unchanged when both are transformed by the same unitary matrix.\n","Q: In the economy SVD of a matrix X, which identity relation holds for the reduced U matrix?\n","A: For the economy SVD, the reduced U matrix (denoted ψU) satisfies ψU^T ψU = I, but ψU ψU^T is not the identity.\n","Q: Geometrically, what shape does the matrix X map the unit sphere into, and what determines the lengths of its principal axes?\n","A: X maps the unit sphere into an ellipsoid. The lengths of the ellipsoid’s principal axes are given by the singular values of X, and the orientation of the ellipsoid is determined by the left singular vectors.\n","Q: When dealing with complex-valued data, how does the transpose operation change in the context of the SVD?\n","A: For complex-valued data, the transpose is replaced by the complex conjugate transpose (denoted by a star). Thus, X* is used instead of X^T, and the unitary conditions involve X* rather than X^T.\n","\n","KEY CONCEPTS:\n","\n","singular value decomposition, unitary matrices, economy size SVD, unitary transformations, Fourier transform, complex conjugate transpose, geometric interpretation of SVD (ellipsoid mapping), left and right singular vectors\n","\n","============================================\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","The video provides a comprehensive overview of Google Gemini Pro 1.5, a multimodal generative AI that processes text and images within a one‑million‑token context window. It begins with a demonstration of the model’s capabilities, followed by a step‑by‑step tutorial on acquiring an API key, installing the Python client, and configuring the model in Jupyter or Colab. The presenter compares Gemini 1.5 Pro’s extended context length to earlier Gemini releases and to GPT‑4 Turbo, highlighting advantages for complex multimodal tasks. Practical examples include querying a large Apollo 11 transcript PDF, extracting specific content, and identifying scenes from drawings. The session also covers model listing, troubleshooting prompt errors, and enabling streaming for incremental responses. Finally, the speaker discusses future enhancements such as a speed API and the competitive position of Gemini within the broader landscape of large language models, encouraging further exploration of its end‑to‑end application potential.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main focus of the video described in the transcript?\n","A: The video focuses on building generative AI-powered applications using Google Gemini Pro 1.5.\n","Q: What type of content does Google Gemini Pro 1.5 support?\n","A: Google Gemini Pro 1.5 is a multimodal model that can work with both text and images.\n","Q: How long is the demo video that the speaker plans to show?\n","A: The demo video is approximately one minute long.\n","Q: What will the speaker demonstrate after the initial one‑minute demo?\n","A: After the demo, the speaker will show hands‑on applications, including running code with images and text, and discuss how to create and use an API key for Gemini Pro 1.5.\n","Q: What experimental feature is mentioned in the transcript?\n","A: The transcript mentions an experimental feature for long context understanding in the newest Gemini model.\n","\n","KEY CONCEPTS:\n","\n","Google Gemini Pro 1.5, generative AI powered application, multimodal model, API key integration, long context understanding, image and text processing, end-to-end projects, experimental feature\n","\n","============================================\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","The video provides a comprehensive overview of evaluating and testing prompt‑engineering models, focusing on quantitative metrics such as perplexity and accuracy alongside qualitative human assessment. It demonstrates how to calculate these metrics on a custom dataset, citing an example that achieved perfect accuracy. The speaker then discusses systematic debugging techniques, including error‑pattern analysis and fine‑tuning, and emphasizes the necessity of evaluating models across diverse datasets and tasks to assess generalization. Visualization tools and cross‑validation methods are recommended to gain deeper insights into model performance. The presentation concludes by underscoring that evaluation is an iterative, ongoing process and hints at forthcoming lessons that will delve into advanced product‑engineering concepts. This structured approach equips practitioners with both the theoretical framework and practical tools needed for rigorous model assessment.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Machine Learning', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three commonly used metrics for evaluating prompt engineering models mentioned in the transcript?\n","A: Perplexity, accuracy, and human evaluation.\n","Q: Why is a lower perplexity considered better for a language model?\n","A: Because perplexity measures how well a language model predicts a sequence of words; the lower the perplexity, the better the model's predictive performance.\n","Q: What does the accuracy metric indicate in the context of prompt engineering models?\n","A: Accuracy indicates how many of the generated responses are correct.\n","Q: Describe one technique for debugging and improving prompt engineering models as discussed in the transcript.\n","A: Analyzing the generated responses to identify common errors or patterns, which allows for fine‑tuning and improvement of the model.\n","Q: Why is it important to test prompt engineering models on different datasets or tasks?\n","A: Testing on different datasets or tasks helps determine the model's ability to generalize to new or unseen data, ensuring consistent performance across varied scenarios.\n","\n","KEY CONCEPTS:\n","\n","perplexity metric, accuracy metric, human evaluation, debugging via error pattern analysis, cross-validation testing, visualization tools for response analysis, generalization testing on unseen data, fine-tuning for improvement\n","\n","============================================\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","Generative artificial intelligence (AI) systems employ large language models to synthesize text, images, or video by leveraging learned statistical patterns. Their outputs, however, are constrained by a fixed knowledge cutoff and they lack the capacity to execute real‑world actions. An AI agent augments this generative core with external tool interfaces, contextual memory, and a degree of autonomous decision‑making, thereby enabling the completion of narrowly scoped tasks such as booking a flight through travel APIs. Building upon this foundation, agentic AI orchestrates multiple agents, integrating planning, multi‑step reasoning, and coordinated tool usage. This architecture permits the resolution of complex, multi‑faceted objectives—for example, constructing a comprehensive travel itinerary that incorporates visa verification, weather forecasts, and accommodation recommendations. Consequently, the evolution from generative AI to agentic AI markedly broadens both the range and sophistication of tasks that can be autonomously performed, reflecting a shift from passive content generation to proactive, goal‑driven problem solving.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main difference between generative AI and an AI agent as described in the transcript?\n","A: Generative AI only creates new content (text, images, video) based on patterns learned from existing data, while an AI agent can use tools, memory, and knowledge to act and complete tasks autonomously.\n","Q: How does an AI agent obtain up-to-date flight prices that a pure LLM cannot?\n","A: It calls a travel API (such as Xedia) or performs a web search to fetch the latest flight price information.\n","Q: What additional capability does agentic AI have compared to a single AI agent?\n","A: Agentic AI can involve multiple agents working together, enabling multi‑step reasoning, planning, and coordination to achieve complex goals.\n","Q: In the transcript, what role does the \"knowledge\" component play for an AI agent?\n","A: Knowledge acts like tools (hammers, screwdrivers) that help the agent make better decisions, such as selecting the cheapest flight or determining suitable travel options.\n","\n","KEY CONCEPTS:\n","\n","large language model, generative artificial intelligence, AI agent, agentic artificial intelligence, tool integration, multi-step reasoning, autonomous decision making, workflow automation with N8N, multi-agent coordination, knowledge base integration\n","\n","============================================\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: \n","Covariance in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","The video provides a concise introduction to covariance, framing it as a foundational statistical measure for assessing the linear relationship between two random variables—for instance, house size and price. It outlines the covariance formula, emphasizing that the metric captures how deviations from each variable’s mean co‑vary. Positive covariance values indicate that the variables tend to increase together, whereas negative values reveal an inverse relationship. The discussion notes that when the two variables are identical, covariance reduces to variance, and it highlights the limitation that covariance lacks a standardized scale, making it difficult to compare the strength of associations across different variable pairs. To address this shortcoming, the speaker previews the Pearson correlation coefficient, which normalizes covariance and provides a dimensionless measure of linear association.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the formula for covariance between two random variables X and Y as described in the transcript?\n","A: Covariance(X,Y) = (1/n) * Σ_{i=1}^{n} (X_i - μ_X) * (Y_i - μ_Y).\n","Q: According to the transcript, how does covariance relate to variance when X and Y are the same variable?\n","A: When X and Y are the same variable, covariance(X,X) equals the variance of X.\n","Q: What does a positive covariance indicate about the relationship between X and Y?\n","A: A positive covariance indicates that as X increases, Y also tends to increase.\n","Q: What limitation of covariance is mentioned in the transcript that leads to the use of Pearson correlation coefficient?\n","A: Covariance only indicates the direction (positive or negative) of the relationship but does not quantify the strength or magnitude of that relationship.\n","Q: In the example of house size and price, what would you expect the covariance to be and why?\n","A: The covariance would be positive because as house size increases, the price also increases, indicating a positive relationship.\n","\n","KEY CONCEPTS:\n","\n","covariance, variance, covariance formula, Pearson correlation coefficient, random variables, mean of a random variable, positive covariance, negative covariance\n","\n","============================================\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","The video provides a concise yet comprehensive overview of how objectives are defined within reinforcement learning (RL). It begins by framing an objective as a goal assigned to an agent, analogous to a human task constrained by time, and explains that the core aim in RL is to learn an optimal policy that maximizes cumulative reward. The speaker emphasizes that reward signals serve as feedback on action quality, guiding the agent through trial‑and‑error learning. Illustrative examples include episodic tasks such as Tic‑Tac‑Toe, where discrete rewards are given for winning, losing, or drawing, and continuous tasks like stock trading, where rewards may be expressed as profit or risk‑adjusted metrics. The discussion underscores the critical role of reward‑function design: it must accurately reflect desired outcomes and steer the agent toward optimal behavior. Overall, the video highlights the interplay between objective specification, reward shaping, and policy optimization in RL systems.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary objective of a reinforcement learning problem as described in the transcript?\n","A: To learn an optimal policy that maximizes the cumulative reward over time.\n","Q: How does the reward signal function in reinforcement learning according to the transcript?\n","A: It provides feedback to the agent about the quality of its actions, allowing the agent to update its policy through trial and error.\n","Q: In the Tic‑Tac‑Toe example, what rewards are assigned for winning, losing, and drawing?\n","A: Winning gives a positive reward (+1), losing gives a negative reward (−1), and drawing gives no reward (0).\n","Q: How can a reward function be parameterized for a continuous task like stock market trading?\n","A: By assigning a numerical value such as profit or loss of each trade, or a risk‑adjusted measure like the Sharpe or Sortino ratio, to each state or action.\n","Q: What are the two main approaches mentioned for learning the optimal policy?\n","A: Value‑based methods and policy‑based methods, or a combination of both.\n","\n","KEY CONCEPTS:\n","\n","optimal policy, cumulative reward, value-based methods, policy-based methods, reward function, trial and error learning, episodic task, continuous task\n","\n","============================================\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199314, Requested 1559. Please try again in 6m17.136s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199302, Requested 1559. Please try again in 6m11.952s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199279, Requested 1559. Please try again in 6m2.016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199244, Requested 1448. Please try again in 4m58.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199232, Requested 1448. Please try again in 4m53.76s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199209, Requested 1448. Please try again in 4m43.824s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199174, Requested 1450. Please try again in 4m29.567999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199162, Requested 1450. Please try again in 4m24.383999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199139, Requested 1450. Please try again in 4m14.448s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","Python dictionaries are immutable-keyed collections that map keys to values, represented by curly braces and colon-separated key-value pairs. The tutorial outlines built‑in methods—items(), keys(), and values()—for inspecting dictionary contents, and demonstrates dictionary creation via literal syntax or by zipping two lists with dict(zip(...)). It explains accessing and updating entries through indexing, removing entries with del, and determining dictionary size using len(). Additionally, the discussion covers conversion of keys or values into list structures. Emphasis is placed on dictionaries as a foundational data structure in data‑science workflows, particularly when interfacing with pandas, highlighting their role in efficient data manipulation and retrieval. The material provides a concise yet comprehensive overview of dictionary operations essential for Python programming and data‑analysis applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: \n","Fight Insider Threats with AI-infused SIEM\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199746, Requested 1182. Please try again in 6m40.896s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199734, Requested 1182. Please try again in 6m35.712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199711, Requested 1182. Please try again in 6m25.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4079427031.py\u001b[0m in \u001b[0;36mllm_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             resp = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199711, Requested 1182. Please try again in 6m25.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4079427031.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;31m# 10. RUN GENERATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGeneration completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4079427031.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0mqa_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4079427031.py\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mTRANSCRIPT\u001b[0m \u001b[0mCHUNK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;31m\\\u001b[0m\u001b[0;34m\"\\\"\\\"{c}\\\"\\\"\\\"\"\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0msummary_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generated_summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4079427031.py\u001b[0m in \u001b[0;36mllm_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"LLaMA call failed (attempt {attempt}/{retries}): {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LLaMA call failed after all retries; returning empty text.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"openai/gpt-oss-20b\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"C_caviCgvoqt","executionInfo":{"status":"error","timestamp":1763890169668,"user_tz":-330,"elapsed":5063773,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"4d53ee9b-64bc-4f59-81f1-6713369e376c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Groq API key loaded ✓\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","Skipping row 7 (already processed)\n","Skipping row 8 (already processed)\n","Skipping row 9 (already processed)\n","Skipping row 10 (already processed)\n","Skipping row 11 (already processed)\n","Skipping row 12 (already processed)\n","Skipping row 13 (already processed)\n","Skipping row 14 (already processed)\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","Python dictionaries are associative data structures that map immutable keys to arbitrary values, with syntax defined by curly brackets and colon separators. Keys must be hashable types such as strings or numbers, whereas values may be any Python object. Standard methods—items(), keys(), and values()—provide iterable views of the dictionary’s contents, often converted to tuples or lists. Dictionaries can be instantiated directly or constructed from parallel key and value sequences via dict(zip(keys, values)). Element access uses square‑bracket indexing; assignment updates values, and the del statement removes entries. The built‑in len() function reports the number of key‑value pairs, and keys() or values() can be cast to list objects for further manipulation. These structures underpin many data‑science workflows, particularly in conjunction with pandas, where dictionaries serve as convenient sources for DataFrame construction and configuration management.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the syntax for separating key-value pairs and items within a Python dictionary?\n","A: Key-value pairs are separated by colons (:), and items are separated by commas (,). The entire dictionary is enclosed in curly brackets ({}).\n","Q: How can you create a dictionary from two lists using the zip function?\n","A: Use the dict constructor with zip: `my_dict = dict(zip(list1, list2))`. This pairs each element of list1 with the corresponding element of list2 to form key-value pairs.\n","Q: What happens if you try to use a list as a key in a dictionary?\n","A: It will raise an error because dictionary keys must be immutable; lists are mutable and therefore cannot be used as keys.\n","Q: How can you change the value associated with a specific key in a dictionary?\n","A: Access the key with square brackets and assign a new value, e.g., `my_dict[5] = 'Z'` changes the value for key 5 to 'Z'.\n","Q: What function would you use to delete an entry from a dictionary and how does it affect the dictionary's length?\n","A: Use `del my_dict[5]` to delete the entry with key 5. After deletion, the dictionary's length decreases by one, as shown by `len(my_dict)`.\n","\n","KEY CONCEPTS:\n","\n","dictionary, key-value pair, immutable key, dict() constructor, zip() function, dictionary methods (items, keys, values), del statement for dictionary entry, dictionary to list conversion\n","\n","============================================\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: \n","Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","Security professionals aim to preempt emerging threats, and AI can cut breach detection and containment times by an average of 108 days, according to IBM’s 2023 Cost of a Data Breach report. The report highlights that organizations employing extensive AI and automation achieve faster containment than those that do not. This session explores how user behavior analytics (UBA), powered by machine learning, can rapidly and accurately detect insider threats—incidents that average $4 million in cost. UBA models normal activity to flag anomalies, and when integrated with a security information and event management (SIEM) platform, it enables analysts to prioritize risks, review alerts, and investigate incidents through dashboards that display risk rankings, offense timelines, and MITRE ATT&CK mappings. After a roughly seven‑day learning period, AI‑driven investigations reduce analysis time from hours to minutes, generate natural‑language summaries, and allow analyst feedback to refine future detections, thereby streamlining threat detection and freeing analysts for proactive defense.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What impact does AI and automation have on the time it takes to identify and contain a data breach, according to IBM's 2023 Cost of a Data Breach report?\n","A: Organizations that extensively used AI and automation identified and contained data breaches 108 days faster on average than those that did not.\n","Q: What is the average cost of an insider threat to an organization, as reported in the transcript?\n","A: The average cost of an insider threat to an organization was $4.\n","Q: Which analytical approach is highlighted in the transcript as a way to detect and respond to insider threats?\n","A: User Behavior Analytics (UBA) combined with Artificial Intelligence (AI) and machine learning.\n","Q: Why is staying ahead of emerging threats important for security professionals, according to the transcript?\n","A: Because it helps improve an organization's security posture and reduces the impact and cost of data breaches.\n","Q: What type of organizations were surveyed in IBM's 2023 Cost of a Data Breach report?\n","A: The survey included over 500 organizations.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence (AI), Machine Learning, User Behavior Analytics (UBA), Insider Threats, Data Breach Containment, Security Posture, AI-driven Automation, Threat Detection and Response\n","\n","============================================\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","In the opening segment, Krishak greets viewers in a relaxed, late‑night vlog style before announcing Meta’s new open‑source Llama 3 model. He outlines Llama 3’s architecture—8 billion and 70 billion parameter variants trained on a 50 trillion‑token corpus, an 8 K context window, and superior performance on language, translation, dialogue, reasoning, code generation, and instruction following relative to Llama 2 and other open‑source models. Benchmark results show Llama 3 approaching paid LLMs such as Google Gemini Pro, with competitive accuracy on MML, human evaluation, and GSM math tasks, though still slightly behind GPT‑style models. The presenter then presents a responsible deployment framework comprising define, prepare, train, evaluate, improve, and a meta‑guard for transparency. Practical guidance follows: users can obtain the model via Meta, Hugging Face, or Kaggle, download signed checkpoints, and run provided scripts for local inference. Further demonstrations are promised in a subsequent video.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Natural Language Processing', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: Who is speaking in the transcript?\n","A: Krishak\n","Q: What platform is the speaker welcoming viewers to?\n","A: YouTube channel\n","Q: According to the transcript, what time is it?\n","A: 2 a.m.\n","Q: What does the speaker say they are doing at the moment?\n","A: They are welcoming viewers to their YouTube channel.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","The instructor outlines the methodology for constructing a decision boundary in Python, focusing on the scikit‑learn library. He stresses the importance of consulting official documentation—often accessed via Google searches—and notes common terminology such as ‘sklearn’ and the relevance of Naive Bayes classifiers. The discussion centers on Gaussian Naive Bayes, detailing its theoretical foundation and practical utility in classification tasks. While the current segment refrains from delving into low‑level code, it sets the stage for subsequent tutorials that will guide learners through hands‑on implementation, ensuring a clear understanding of each procedural step. Overall, the presentation emphasizes a structured instructional approach, balancing conceptual explanations with a roadmap for future, more detailed coding exercises.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: Which Python library is the speaker using to implement the Naive Bayes classifier?\n","A: scikit-learn, often abbreviated as sklearn.\n","Q: What specific variant of Naive Bayes does the speaker use in the code?\n","A: Gaussian Naive Bayes.\n","Q: According to the transcript, what tool does the speaker recommend for finding documentation on library functions?\n","A: Using Google to search for the library and the algorithm name.\n","Q: What is the main purpose of the Python code discussed in the transcript?\n","A: To generate a decision boundary for the classifier.\n","\n","KEY CONCEPTS:\n","\n","scikit-learn, Naive Bayes, Gaussian Naive Bayes, decision boundary, Python code, algorithm derivation, classifier\n","\n","============================================\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: \n","Log Normal Distribution in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","The lecture provides a concise overview of Gaussian and log‑normal distributions, emphasizing the empirical 68‑95‑99.7 rule that characterises the normal curve’s symmetry about its mean. It defines a log‑normal variable as one whose logarithm follows a normal distribution and demonstrates how to confirm this property by plotting log‑transformed data. Illustrative examples—including human height, income, product‑review length, and sentiment scores—highlight the distinct shapes of these distributions and their practical relevance. The speaker then addresses data standardisation: Gaussian observations are converted to a standard normal via (x‑μ)/σ, whereas log‑normal observations are first log‑transformed before standardisation. This procedure aligns disparate scales, facilitating more accurate regression and classification models. Overall, the talk underscores the importance of appropriate distributional assumptions and normalisation techniques in statistical modelling.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What percentages of the total distribution fall within one, two, and three standard deviations of a Gaussian distribution according to the transcript?\n","A: Approximately 68% within one standard deviation, 95% within two, and 99.97% within three.\n","Q: How is a log‑normal distribution defined in terms of the logarithm of the variable?\n","A: A random variable X is log‑normal if log(X) follows a normal (Gaussian) distribution.\n","Q: What mathematical operation converts a Gaussian distribution into a standard normal distribution?\n","A: Subtract the mean μ from each value and divide by the standard deviation σ: (X – μ)/σ.\n","Q: Why is standard scaling applied to features with different distributions before modeling?\n","A: It brings all features to the same scale (mean 0, standard deviation 1), which improves model accuracy.\n","\n","KEY CONCEPTS:\n","\n","Gaussian distribution, log-normal distribution, standard normal distribution, standard scaling, log transformation, empirical rule (68-95-99.7 rule), bell curve, law of normalization\n","\n","============================================\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","The presentation details an end‑to‑end deep‑learning pipeline for detecting potato leaf diseases, specifically early and late blight. It begins with data acquisition, cleaning, and augmentation (rotations, flips, contrast adjustments) to build a robust training set. A convolutional neural network is trained, exported, and served via TensorFlow Serving, which supports multiple model versions. The inference layer is exposed through a FastAPI backend, and the entire stack is deployed on Google Cloud Platform, with Cloud Functions acting as a serverless bridge to a React Native mobile application that captures leaf images and displays real‑time predictions. For web access, a React‑JS front‑end is built. The workflow also incorporates model optimization: TensorFlow Lite quantization reduces model size for edge inference, with plans to run the TFLite model directly on mobile devices. The speaker briefly promotes his brother’s software firm, AtliQ Technologies, while emphasizing the project’s practical value for resumes and its adaptability to other domains.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Machine Learning', 'Mlops']\n","\n","GENERATED Q&A:\n","\n","Q: How many videos will the end-to-end deep learning project series contain?\n","A: The series will have a total of seven to eight videos.\n","Q: What are the two common potato diseases mentioned, and what causes each?\n","A: Early blight, caused by a fungus, and late blight, caused by a specific microorganism.\n","Q: Why is it important to accurately identify whether a potato plant has early blight or late blight?\n","A: Because the treatments for early blight and late blight differ, so accurate identification allows farmers to apply the correct treatment and prevent economic loss.\n","Q: What technologies will be used to build the backend server and deploy the model?\n","A: The backend server will be built using FastAPI, and the model will be deployed to Google Cloud Platform (GCP) with Google Cloud Functions.\n","Q: What role does the mobile app play in the described system?\n","A: The mobile app, written in React Native, allows farmers to take a picture of a potato plant, which the app then analyzes using the deployed deep learning model to determine if the plant is healthy or has early or late blight.\n","\n","KEY CONCEPTS:\n","\n","end-to-end deep learning project, ML Ops with TF serving, FastAPI backend server, Google Cloud Functions deployment, React Native mobile app, potato disease detection, convolutional neural network, agriculture domain AI application\n","\n","============================================\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198687, Requested 3516. Please try again in 15m51.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198675, Requested 3516. Please try again in 15m46.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198652, Requested 3516. Please try again in 15m36.576s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198617, Requested 3405. Please try again in 14m33.503999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198605, Requested 3405. Please try again in 14m28.319999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198582, Requested 3405. Please try again in 14m18.383999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198547, Requested 3407. Please try again in 14m4.127999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198535, Requested 3407. Please try again in 13m58.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198511, Requested 3407. Please try again in 13m48.575999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198477, Requested 1730. Please try again in 1m29.424s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The transcript delineates a progressive hierarchy of autonomy in large language model (LLM) applications, ranging from deterministic, hard‑coded code to nascent fully autonomous agents. Initially, static code offers no autonomy yet fails to cope with real‑world complexity. A single LLM invocation introduces flexibility, enabling the handling of one task per prompt, but it struggles with multi‑task demands. Chains of specialist LLMs decompose tasks into sequential steps, thereby expanding capability while remaining rigid and devoid of decision‑making. Routers add a decision layer, allowing the LLM to select appropriate chains based on input; however, they still lack memory and iterative refinement. State‑machine agents integrate routing with loops, memory, and human‑in‑the‑loop approval, facilitating iterative improvement and adaptive learning, and represent the most sophisticated stage before fully autonomous agents, which are still emerging. This framework underscores the incremental trade‑offs between control, flexibility, and autonomy in LLM‑driven systems.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198465, Requested 1730. Please try again in 1m24.24s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198441, Requested 1730. Please try again in 1m13.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198549, Requested 1802. Please try again in 2m31.632s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198537, Requested 1802. Please try again in 2m26.447999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198514, Requested 1802. Please try again in 2m16.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198479, Requested 1749. Please try again in 1m38.496s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198467, Requested 1749. Please try again in 1m33.312s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198444, Requested 1749. Please try again in 1m23.376s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198409, Requested 1751. Please try again in 1m9.12s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 198397, Requested 1751. Please try again in 1m3.936s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","{\"generated_summary\":\"\"}\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, fine-tuning pre-trained large language models, multitask learning, knowledge distillation, tokenization and normalization, deployment with TensorFlow Serving, deployment with Flask API, ethical considerations in prompt engineering\n","\n","============================================\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199746, Requested 3086. Please try again in 20m23.424s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199734, Requested 3086. Please try again in 20m18.24s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199711, Requested 3086. Please try again in 20m8.303999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199745, Requested 3213. Please try again in 21m17.856s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199734, Requested 3213. Please try again in 21m13.104s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199710, Requested 3213. Please try again in 21m2.735999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3416631285.py\u001b[0m in \u001b[0;36mllm_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             resp = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199710, Requested 3213. Please try again in 21m2.735999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3416631285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;31m# 10. RUN GENERATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGeneration completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3416631285.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mqa_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mconcepts_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_concepts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3416631285.py\u001b[0m in \u001b[0;36mclassify_topic\u001b[0;34m(transcript, summary)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0mTRANSCRIPT\u001b[0m \u001b[0mCHUNK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;31m\\\u001b[0m\u001b[0;34m\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicted_topics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3416631285.py\u001b[0m in \u001b[0;36mllm_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"LLaMA call failed (attempt {attempt}/{retries}): {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LLaMA call failed after all retries; returning empty text.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key4.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"openai/gpt-oss-20b\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5EkrOQdnwG5C","executionInfo":{"status":"ok","timestamp":1763908670032,"user_tz":-330,"elapsed":6662901,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"51329f9d-c8d9-4ab6-d65f-2bfce28f0e7e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Groq API key loaded ✓\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","Skipping row 7 (already processed)\n","Skipping row 8 (already processed)\n","Skipping row 9 (already processed)\n","Skipping row 10 (already processed)\n","Skipping row 11 (already processed)\n","Skipping row 12 (already processed)\n","Skipping row 13 (already processed)\n","Skipping row 14 (already processed)\n","Skipping row 15 (already processed)\n","Skipping row 16 (already processed)\n","Skipping row 17 (already processed)\n","Skipping row 18 (already processed)\n","Skipping row 19 (already processed)\n","Skipping row 20 (already processed)\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The transcript delineates a spectrum of autonomy levels for large language model (LLM) applications, from deterministic code to fully autonomous agents. It contrasts single LLM calls—capable of handling isolated tasks but limited in multi‑task prompts—with chains that delegate specialized sub‑tasks yet adhere to rigid sequences. Routers add decision‑making by selecting appropriate chains based on user input, though they lack memory or learning. State‑machine agents integrate routing with iterative loops, human‑in‑the‑loop approval, and adaptive refinement, thereby enabling contextual awareness and continuous improvement. Deterministic code offers predictable behavior but lacks flexibility; single calls provide simplicity but struggle with complex, multi‑step tasks; chains improve modularity but remain linear; routers introduce dynamic selection but no learning; state‑machine agents bridge the gap between rigid pipelines and fully autonomous systems. The discussion traces the evolution from human‑driven workflows to agent‑executed processes, outlining the strengths and constraints of each autonomy tier.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Agentic AI', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main disadvantage of a code-based LLM application?\n","A: It requires writing rules for every possible scenario, making it impossible to handle real‑life complexity.\n","Q: How does a single LLM call differ from a chain in handling tasks?\n","A: A single LLM call processes one input and produces one output, while a chain breaks a complex task into multiple specialist steps, each handled by a separate LLM.\n","Q: What key feature distinguishes a router from a chain?\n","A: A router uses an LLM to decide which chain to route the request to based on the user’s input, adding decision‑making that a fixed chain lacks.\n","Q: Why is a state machine (agent) considered more autonomous than a router?\n","A: It can loop, remember previous context, include human‑in‑loop approvals, refine outputs, and the LLM controls the entire flow, whereas a router only selects a path.\n","Q: What role does the \"head of content agent\" play in the described hierarchy?\n","A: It receives the user’s request, delegates tasks to sub‑agents (e.g., LinkedIn writer, blog writer, publisher), manages approvals, and returns the final output to the user.\n","\n","KEY CONCEPTS:\n","\n","levels of autonomy in LLM applications, single LLM call, chain architecture with specialists, router for dynamic routing, state machine / agent with loops and memory, human-in-loop approval step, tool integration via Python functions, autonomous agents (e.g., BabyAGI, AutoGPT)\n","\n","============================================\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","This section advances prompt engineering by addressing complex topics such as handling text, image, and audio prompts, fine‑tuning pre‑trained large language models, and deploying them in production. It introduces multitask learning and model distillation to enhance robustness and efficiency, while emphasizing best practices for data preprocessing—including tokenization, normalization, and augmentation. Deployment strategies such as TensorFlow Serving and Flask APIs are discussed, alongside ethical considerations encompassing bias, fairness, and privacy. The material culminates in practical exercises that integrate prompt handling, advanced fine‑tuning techniques, and ethical deployment, thereby preparing learners for real‑world applications of large language models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Machine Learning', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What types of prompts are mentioned in the transcript and how should they be handled?\n","A: The transcript mentions text-based prompts, image-based prompts, audio-based prompts, and combinations of these. Handling them involves preprocessing each type appropriately—e.g., using image models like ResNet50 or VGG16 for images, tokenization and normalization for text, and suitable audio processing for audio prompts—before fine‑tuning a pre‑trained model to generate the desired output.\n","Q: Describe the multitask learning technique for fine‑tuning large language models as explained in the transcript.\n","A: Multitask learning involves training a single model on multiple tasks simultaneously. In the transcript, a Python function is built that uses cross‑entropy loss and an Adam optimizer to train on a small dataset containing source and target texts. The model learns to predict multiple attributes or features at once, leading to more robust representations that generalize better across different users.\n","Q: What is model distillation and why is it advantageous according to the transcript?\n","A: Model distillation trains a smaller model to mimic the behavior of a larger, more powerful model. The smaller model runs faster while retaining much of the larger model’s performance. The transcript describes a function that runs forward passes on both models, computes a distillation loss, and updates the smaller model’s parameters, resulting in an efficient, high‑speed model.\n","Q: Which preprocessing steps for text data are highlighted as best practices in the transcript?\n","A: The transcript highlights tokenization—breaking text into words or sub‑words—and normalization—converting text to a standard format such as all lowercase. These steps help the model understand text meaning more accurately and avoid confusion between similarly spelled words.\n","Q: Name two deployment options for prompt engineering models mentioned in the transcript.\n","A: The transcript mentions TensorFlow Serving as a framework for serving models in production, and building a Flask web application to expose the model via an API.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, fine-tuning pre-trained large language models, multitask learning, knowledge distillation, data preprocessing (tokenization, normalization, augmentation), deployment with TensorFlow Serving, deployment with Flask API, ethical considerations (bias, fairness, privacy)\n","\n","============================================\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","In the lecture, the instructor demonstrates the application of singular value decomposition (SVD) to facial image data for extracting eigenfaces and conducting basic classification. Using a modest set of twenty photographs of Arnold Schwarzenegger and Sylvester Stallone, the class first computes a mean face, centers the images, and then performs an economy‑size SVD to obtain principal components. The resulting eigenfaces are visualised, and each image is projected onto the leading three components, revealing distinct clusters that separate the two actors. The same procedure is replicated with images of Taylor Swift to illustrate how facial similarity can influence separability. This exercise illustrates how dimensionality reduction via SVD captures salient facial features, enabling efficient representation and facilitating rudimentary classification tasks. It also highlights the limitations of 2‑D approaches when faces exhibit high similarity. The discussion concludes with a brief note on Facebook’s shift toward 3‑D geometry‑based recognition systems.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Data Science', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What preprocessing step is performed on each face image before it is added to the matrix used for SVD?\n","A: Each image is converted to greyscale (if not already), cropped and roughly aligned so that the faces occupy the same box, and then reshaped into a tall skinny column vector.\n","Q: How is the average action‑hero face computed in the lecture?\n","A: All 40 face images (20 Arnold + 20 Stallone) are summed pixel‑wise and then divided by 40 to obtain the average face.\n","Q: What does the matrix B represent in the context of this example, and how is it constructed?\n","A: B is the matrix of mean‑centered face vectors; each column is a face image vector with the average face subtracted from it.\n","Q: Why do the first few eigenfaces look similar to the average face, and what do later eigenfaces reveal?\n","A: The first eigenfaces capture the most variance, often resembling the average face; later eigenfaces reveal more specific features such as glasses or eye shape.\n","Q: According to the lecture, why might Arnold Schwarzenegger and Taylor Swift be closer together in eigenface space than Arnold and Stallone, despite being a man and a woman?\n","A: Because Arnold and Taylor share similar skin tone and hair color, which dominate the correlation in the eigenface representation, leading to higher similarity in the first few principal components.\n","\n","KEY CONCEPTS:\n","\n","singular value decomposition, eigenfaces, principal component analysis, image classification, clustering in face space, mean face subtraction, projection onto principal components, image compression using SVD\n","\n","============================================\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","The transcript introduces LangChain through a vacation‑planning scenario, illustrating how a user might request a chatbot to book flights, reserve hotels, and recommend restaurants for a trip to Paris. It explains that the user’s query is sent to a large language model (LLM), which can reason and generate plans but cannot directly interact with the real world to execute actions such as reservations. The example underscores the limitation of LLMs, which provide information rather than perform tasks. To bridge this gap, a middleware framework is required; LangChain is presented as the most popular solution. It connects LLMs to external APIs, databases, email services, web browsing, and scraping tools, enabling developers to swap underlying models (e.g., GPT‑4 for a free Hugging Face model) without modifying application code. This integration extends AI capabilities into practical, real‑world applications while preserving flexibility and modularity.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Natural Language Processing', 'Prompt Engineering']\n","\n","GENERATED Q&A:\n","\n","Q: What simple problem is used to illustrate LangChain in the transcript?\n","A: Planning a vacation to Paris, including booking a flight, a hotel, and suggesting restaurants.\n","Q: What happens when the user presses enter in the example?\n","A: The query is sent to an LLM model.\n","Q: Which LLM model is mentioned as a possible choice in the example?\n","A: The transcript mentions using \"charity 3\".\n","Q: What does the example demonstrate about LangChain's capabilities?\n","A: It shows that LangChain can send user queries to an LLM and handle multiple tasks such as booking travel and recommending restaurants.\n","\n","KEY CONCEPTS:\n","\n","LangChain, LLM model, model chaining, prompt engineering, task orchestration, context management, API integration\n","\n","============================================\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","The video discusses residual analysis as a diagnostic tool for improving time‑series forecasting models. Residuals, defined as the difference between fitted values and actual observations, are examined for key properties: zero mean to avoid bias and lack of autocorrelation to confirm that systematic patterns have been captured. Using a Holt‑Winters exponential smoothing model on the Air Passengers dataset, the presenter extracts fitted values and residuals, plots autocorrelation and partial autocorrelation functions, and applies the Ljung‑Box test. The test rejects the null hypothesis of no autocorrelation (p < 0.05), indicating correlated residuals and prompting model reconsideration. A histogram of residuals shows a slight negative mean, suggesting minimal under‑prediction bias. The discussion emphasizes that residual inspection can reveal deficiencies such as autocorrelation or bias, enabling model refinement. The speaker notes that residual analysis is straightforward to perform, as demonstrated in a PyTorch tutorial, and recommends its routine use for forecasting practitioners.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Statistics', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the definition of a residual in time series analysis as described in the transcript?\n","A: A residual is the difference between the fitted value (the model’s prediction for a data point that was part of the training set) and the actual observed value of the time series.\n","Q: How do residuals differ from errors according to the transcript?\n","A: Residuals are calculated using fitted values from data the model has already seen (training data), whereas errors refer to predictions made on data the model has not seen before (test data).\n","Q: Why should the mean of residuals be zero, and what does a non‑zero mean indicate?\n","A: A zero mean indicates an unbiased forecast; a non‑zero mean shows systematic over‑ or under‑forecasting, meaning the model is consistently too high or too low and can be corrected by shifting the forecast up or down.\n","Q: Which statistical test is mentioned for checking autocorrelation in residuals, and what does its null hypothesis state?\n","A: The Yule‑Box test is used; its null hypothesis is that the residuals are independently distributed (i.e., no autocorrelation). Rejecting the null suggests serial correlation in the residuals.\n","\n","KEY CONCEPTS:\n","\n","residual analysis, autocorrelation, partial autocorrelation, Young's Box test, Holt-Winters exponential smoothing, fitted values, forecast bias, time series diagnostics\n","\n","============================================\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","The tutorial presents a step‑by‑step construction of a text‑to‑SQL AI agent using LangGraph and LangChain, hosted on IBM’s watsonx.ai platform, and integrated into a Next.js front‑end built with TypeScript and Tailwind CSS. It begins by initializing a Next.js project, creating a client‑side Home component, and setting up stateful input handling with React hooks to manage conversation history. The guide then installs LangGraph and LangChain libraries, defines a server‑side actions module that constructs a ReAct agent, and configures a Watsonx.ai chat interface with the Mistral Large model. A lightweight in‑memory SQLite database is seeded with customer and order tables; a LangChain tool, GetFromDB, is added to the agent to execute SQL queries. The narrative details UI updates to display human and AI messages, loading indicators, and guardrails that constrain the agent to use the database tool, culminating in a functional, database‑aware conversational agent demonstrated via the browser and VS Code output.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What programming framework and UI library are used to build the frontend of the AI agent described in the transcript?\n","A: Next.js is used for the frontend application and Tailwind is used for styling.\n","Q: Which database technology is employed for the in-memory database in this project?\n","A: SQLite is used as the in-memory database.\n","Q: What is the purpose of the ReAct agent mentioned in the transcript?\n","A: The ReAct agent is designed to use SQL knowledge from large language models to interact with databases.\n","Q: What command is used to create the boilerplate project for this application?\n","A: The command `create-next-app@latest` is used to set up the boilerplate project.\n","Q: How does the transcript suggest handling components in Next.js for client-side rendering?\n","A: It suggests adding the directive to run components as client-side components, ensuring they execute in the browser rather than server-side.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence agent, SQL knowledge, LangGraph, ReAct agent, Next.js, watsonx.ai, in-memory SQLite database, client-side component, server-side component, Tailwind CSS\n","\n","============================================\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","The transcript introduces prompt engineering as a specialized subfield of natural language processing that customizes large language models—such as GPT, Google BERT, and Hugging Face Transformers—to generate high‑quality, context‑appropriate text in response to user prompts. It argues that prompt engineering improves accuracy, coherence, and relevance over traditional rule‑based or keyword‑based approaches, thereby benefiting applications including chatbots, translation, and content generation. The speaker outlines the course’s objectives: to cover foundational concepts, prompt analysis techniques, and the advantages and limitations of the approach, while equipping learners to design, build, and evaluate their own prompt‑engineering models. By the conclusion, participants should understand the significance of prompt engineering and be prepared for subsequent lessons that delve deeper into practical implementation and evaluation strategies.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is prompt engineering and why is it important in natural language processing?\n","A: Prompt engineering is a specialized field within NLP that focuses on building models that generate high‑quality text outputs in response to prompts. It is important because it produces more accurate, coherent, and contextually appropriate text than traditional rule‑based or keyword‑based approaches, improving user experience in applications like chatbots, translation, and content generation.\n","Q: Which pre‑trained large language models are commonly used in prompt engineering?\n","A: Commonly used pre‑trained large language models include OpenAI’s GPT, Google’s BERT, and Hugging Face Transformers.\n","Q: What are some limitations of prompt engineering mentioned in the transcript?\n","A: Limitations include difficulty handling complex or ambiguous prompts, and the potential to generate biased or inaccurate outputs due to underlying data or model architecture.\n","Q: What topics will the course cover in its first part?\n","A: The first part covers an introduction to prompt engineering, its importance, basics of prompt analysis (deconstructing prompts and identifying key features and constraints), benefits and limitations, and an overview of the core structure and content of the course.\n","Q: How does prompt engineering differ from traditional rule‑based or keyword‑based approaches?\n","A: Unlike rule‑based or keyword‑based methods, prompt engineering uses fine‑tuned large language models to produce outputs that are more accurate, coherent, and contextually appropriate.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, large language models, fine-tuning, pre-trained models, prompt analysis, prompt deconstruction, bias in language models, contextual appropriateness, natural language processing\n","\n","============================================\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","The episode introduces reinforcement learning as the third major machine‑learning paradigm, contrasting it with supervised and unsupervised approaches. It defines reinforcement learning as the process of mapping states to actions to maximize cumulative reward and categorises algorithms into value‑based and policy‑based methods. Q‑learning, a value‑based technique, is explained in detail: it learns a state‑action value function (Q‑values) stored in a table, updated via the Bellman equation and temporal‑difference (TD) error. Concrete grid‑world examples illustrate how the TD error is computed and how Q‑values are adjusted using a learning rate, demonstrating convergence toward an optimal policy. The discussion highlights the distinction between the target policy derived from the Q‑table and the behavior policy used for exploration, underscoring Q‑learning’s off‑policy nature. The episode concludes that, after sufficient episodes, the Q‑table converges, enabling the agent to act optimally in the environment.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three machine learning paradigms discussed in the transcript?\n","A: Supervised learning, unsupervised learning, and reinforcement learning.\n","Q: How do value-based reinforcement learning methods differ from policy-based methods?\n","A: Value-based methods first determine a value function that quantifies total reward and then derive a policy from it, while policy-based methods directly determine an optimal policy that maximizes total reward.\n","Q: In Q-learning, what is the state-action value function commonly called?\n","A: It is called the Q value (or q(s,a)).\n","Q: What are the reward values assigned to the special squares in the grid world example?\n","A: The +10 reward square gives +10, the -10 poison square gives -10, and all other squares give -1.\n","Q: What role does the Bellman equation play in Q-learning?\n","A: It provides the recursive relationship used to calculate the observed Q value as the sum of the immediate reward and the discounted maximum future Q value.\n","\n","KEY CONCEPTS:\n","\n","Q-learning, reinforcement learning, value-based methods, policy-based methods, state-action value function, Bellman equation, discount factor, exploration policy\n","\n","============================================\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","{\"generated_summary\":\"Training a logistic classifier begins with a linear transformation of the input features—typically a matrix multiplication of the feature vector with a weight matrix \\(W\\) followed by the addition of a bias term \\(b\\). The resulting scores, or logits, are passed through the softmax function, which converts them into a probability distribution that sums to one. This mapping assigns a high probability to the correct class while suppressing probabilities for incorrect classes. The learning objective is to adjust the parameters \\(W\\) and \\(b\\) so that the predicted probability distribution aligns closely with the true labels, thereby maximizing classification accuracy. In practice, this alignment is achieved by minimizing a cross‑entropy loss via gradient‑based optimization, iteratively updating the weights and bias until the model reliably predicts the correct class fo\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Statistics', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What type of classifier is a logistic classifier and how does it process input data?\n","A: A logistic classifier is a linear classifier that processes input data by applying a linear function, which is essentially a giant matrix multiplication of the input vector X with a weight matrix W, plus a bias term b, to generate predictions for each output class.\n","Q: What symbols are used in the transcript to denote the inputs, weights, and bias in a logistic classifier?\n","A: The inputs are denoted by X, the weights by W, and the bias term by b.\n","Q: Why is the softmax function used in logistic regression according to the transcript?\n","A: The softmax function is used to convert the raw scores (logits) into proper probabilities that sum to 1, ensuring that the probability of the correct class is close to one and probabilities for other classes are close to zero.\n","Q: What property of the softmax function makes it suitable for turning any set of scores into probabilities?\n","A: The softmax function can take any set of scores and transform them into probabilities that sum to 1, with larger scores producing larger probabilities and smaller scores producing smaller probabilities.\n","\n","KEY CONCEPTS:\n","\n","logistic classifier, linear classifier, matrix multiplication, weight matrix, bias term, training model, softmax function, logits\n","\n","============================================\n","Saved row 29\n","\n","All rows processed.\n","\n","Generation completed.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gpt-oss-20b/gpt-oss-20b_cot.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gpt-oss-20b/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"id":"68qChTxYGSr8","colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["a09721d1b4cb495b99d4b5eec4747d8e","c72cb593e2ea49ba919d224cd1351790","72e7d287824047b9b239704664cd16aa","cff77ac23de0465181c18c828e895ea6","a4824c3bcd8a4ae1b6d3ac7c024ea439","12ea0e27776d4ea09ad2b88c64b110b2","b4f7882e1af9464e9227ae8c82b42be5","27ae493eef244136821ebf8cb72e8346","9d389c51fadd444ea51a21a45cdfd4d8","68f12d86877e41b097c53ccddb7c6693","86988b9b00a54c33a15b4070529b692a","fe4d830bdfae4e8a811b397fc02b1902","7b8bcfadb9c4495ab57ff296115ce7c5","ceba5b16a51842fbb15eaae199ad65a5","833f137747a74f16a9ed3b73e818b06d","b8952738d7f14b0a821dc8207619b75b","3e678b2b4aa943f5902b856b9b842975","8fee27b6d8c94701b727f5a23b5c32f4","2f951c1ca7ab4654a253ee14820fb7fb","05d5e063361a47a187dd9bf92563f8f4","4ae4c29af46345a9b7bbd694b1868ab4","15681648db42470a9602cf876cf58075","e3cd40b3e6c74882996a454ee2ee0a30","d199262aefb9477b815c2a9cce58e83c","2a35679d197a4bffa531f5f2b16a5cf3","b48c151f6cbd46ef9b9d0fd9a5265452","6b0d75c480b94a2182d10351b025c076","0775be2116db47c1ac188c75f42b89c4","855f68db05e04fad82ab10922d34267f","952b3c71cf2846bea7a8138355c20d3b","81a63daa361546e8adf66437813397eb","5b462712818147d5b44a6cdf3951cb24","ffd4130707b4451895c3162812d2df68","a110edae0da340699ffbad1471acaf77","626823cad6f945c68f1de8431b1ec5b3","f1e94541ea5c44a79daa3ae7982d249c","3e3b15fe610c40ae977c2393b5df0220","656646fea55343ff8e632f16b4079de2","ecd268cd96774709a882597654edeb55","6c21aa9fa88146e3bf0e92f723f54b80","1a20bb05a5b2450693879d9861b61bd4","12deac19872948018552fe89ca62df18","78a5f4de4dea4b4293a1bb08751e4028","dc25fd3e75df463bb66b680b5e2cf634","3563c8a761814f96aa9774e477763553","c7d6681b69ae40dba972f943c4158f98","984cfca9ccdd4175b06ccce675c37a62","ee125e369ea94da297ffcbec328633c7","fbc344af36c44803a16391586298cded","b6515a3e923f45d5bdbb08497fb64fa7","870d2e152b734b4399561d721e864998","a717b9a4b301499b97a6e9202d040baa","0dbff8706d95469d96be3a297359c51c","3549debd867b4fdfaf8a3e87361a4273","8b4cf344609f43ca80fc5ba729e583ba","9b752415cc2845f699d35126a747e382","13a149805c1b4570b7043041df4e6317","94de3539f5d847fdaa6f2122dd848d56","c7e5062fbd2b4fb0b1d7700bba1eb403","86e0318a3de0484bbe8a5dd8810ab1f2","aa9ff7be7cfb48a4929263e2f4ab3f3f","5fdc3795a86b4a0aa2010567098fa730","3bcba733c2c044d69ad3262087167ccd","3f899dc184ca4e18b0d451602d250790","a8884a7fadf2408d9937710e60e0d245","297088d00a5545419b63576703429191"]},"executionInfo":{"status":"ok","timestamp":1763908883388,"user_tz":-330,"elapsed":153324,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"2a3c174a-3951-4b56-b8f6-98844e8c980f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gpt-oss-20b/gpt-oss-20b_cot.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09721d1b4cb495b99d4b5eec4747d8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4d830bdfae4e8a811b397fc02b1902"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3cd40b3e6c74882996a454ee2ee0a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a110edae0da340699ffbad1471acaf77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3563c8a761814f96aa9774e477763553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b752415cc2845f699d35126a747e382"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2617\n","  - BLEU: 0.0372\n","  - BERTScore F1: 0.8740\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9667\n","  - Jaccard Index: 0.3642\n","  - Micro F1: 0.4946\n","  - Macro F1: 0.4530\n","  - Weighted F1: 0.4557\n","\n","Q&A Generation:\n","  - BLEU: 0.0244\n","  - Diversity: 0.7061\n","  - Answerability: 0.6300\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4733\n","  - Recall@10: 0.1893\n","  - F1@10: 0.2705\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gpt-oss-20b/evaluation_final.json\n"]}]}]}