{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP6V8JUZ/ncwe6/5hnDCqR/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"71166d610e37429588392bdcf4670cfb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_454fde030aea4eabb65b407a6b284703","IPY_MODEL_f4004c746dc44127accb330497582d8e","IPY_MODEL_25ae8e7e57f249b28fc94549b3f6e7a2"],"layout":"IPY_MODEL_3ac5fdc73225412fae1e89237636f587"}},"454fde030aea4eabb65b407a6b284703":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a38d1d80d74b419883411e35fa8bf14c","placeholder":"​","style":"IPY_MODEL_8a8c8a3895cc4f8593ba667bcb8502f7","value":"tokenizer_config.json: 100%"}},"f4004c746dc44127accb330497582d8e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d0e1ead01834dd799d0f2f2eeba44d5","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e3d5e5aa68446249ec5e24ec09ae0b6","value":25}},"25ae8e7e57f249b28fc94549b3f6e7a2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13ef6eff4574462fbf8c2768980d4bb3","placeholder":"​","style":"IPY_MODEL_f2333830b5854fc1ac2ceabe148bcb7e","value":" 25.0/25.0 [00:00&lt;00:00, 2.28kB/s]"}},"3ac5fdc73225412fae1e89237636f587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38d1d80d74b419883411e35fa8bf14c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a8c8a3895cc4f8593ba667bcb8502f7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d0e1ead01834dd799d0f2f2eeba44d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e3d5e5aa68446249ec5e24ec09ae0b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13ef6eff4574462fbf8c2768980d4bb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2333830b5854fc1ac2ceabe148bcb7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c8475f94c1574291b8088de3c3357dd6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_113e60306e2d4aa2ac7e3920aa9c6f3c","IPY_MODEL_922c3a9e66e64cc19ac702cb33b88600","IPY_MODEL_aee42b4f69a247349f5979af9dc0f0e7"],"layout":"IPY_MODEL_4af83151262241e19f3f2476863421bf"}},"113e60306e2d4aa2ac7e3920aa9c6f3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00e9d751d86d4ae79ce5a6a9110382f1","placeholder":"​","style":"IPY_MODEL_07ecd20717ca43cb9f55323f7349f4a7","value":"config.json: 100%"}},"922c3a9e66e64cc19ac702cb33b88600":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_987a449f77d44c17a7ed6a6439117fb7","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d745ac54cbd04f25b3f1f162cd50c1ec","value":482}},"aee42b4f69a247349f5979af9dc0f0e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbc15efbfd9746c5aeb4fbe4580fb7f3","placeholder":"​","style":"IPY_MODEL_0bd9fd58ecbd437da474157de1a4c332","value":" 482/482 [00:00&lt;00:00, 50.4kB/s]"}},"4af83151262241e19f3f2476863421bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00e9d751d86d4ae79ce5a6a9110382f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07ecd20717ca43cb9f55323f7349f4a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"987a449f77d44c17a7ed6a6439117fb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d745ac54cbd04f25b3f1f162cd50c1ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fbc15efbfd9746c5aeb4fbe4580fb7f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd9fd58ecbd437da474157de1a4c332":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4e8fe97eec0b4c1595973ed553a1e85b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f3149a40670b40c7855f3604192af484","IPY_MODEL_fd982bfdbacf420c928200fba1de209b","IPY_MODEL_bb415f7134cf4dcd825b1c0042e5c995"],"layout":"IPY_MODEL_31dbcf013fea4df2a54db03d986ea76a"}},"f3149a40670b40c7855f3604192af484":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8cd596b8fc9a4144826e3637e17fc545","placeholder":"​","style":"IPY_MODEL_1225c6a6225f4a57a86550414f359188","value":"vocab.json: 100%"}},"fd982bfdbacf420c928200fba1de209b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa4c05fd42734cf1ac9a88003d0b335f","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_927388aa32e648b4a615cc1b4c6d5bc8","value":898823}},"bb415f7134cf4dcd825b1c0042e5c995":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af2300b2ae6349c49803dc2d12810791","placeholder":"​","style":"IPY_MODEL_b9b48b7c75a34c7fb534bc71273229f3","value":" 899k/899k [00:00&lt;00:00, 6.95MB/s]"}},"31dbcf013fea4df2a54db03d986ea76a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cd596b8fc9a4144826e3637e17fc545":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1225c6a6225f4a57a86550414f359188":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aa4c05fd42734cf1ac9a88003d0b335f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"927388aa32e648b4a615cc1b4c6d5bc8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af2300b2ae6349c49803dc2d12810791":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9b48b7c75a34c7fb534bc71273229f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd273dfeb396461eb4143463c82efe69":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68cdef331c56486eaa85869e84e9c543","IPY_MODEL_f3c8a972fe96498f8dacd08a2c47d900","IPY_MODEL_3acbc3fa6d9c4327802b62dd80ec4362"],"layout":"IPY_MODEL_2a37448503e64860aaee7d0655bcd2db"}},"68cdef331c56486eaa85869e84e9c543":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c47e7326e1d433aaa2154f16f12543f","placeholder":"​","style":"IPY_MODEL_bf0211066ce54dff92419ea5b13e6f5a","value":"merges.txt: 100%"}},"f3c8a972fe96498f8dacd08a2c47d900":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9218062476544b9acbb348190f7a61c","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61aab4bca49e489fbac32849648d429d","value":456318}},"3acbc3fa6d9c4327802b62dd80ec4362":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_684faef3eafe4ddb90be25f0ff032ba5","placeholder":"​","style":"IPY_MODEL_a06f1e2032ff45d4be4114bc5a7e26e9","value":" 456k/456k [00:00&lt;00:00, 6.79MB/s]"}},"2a37448503e64860aaee7d0655bcd2db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c47e7326e1d433aaa2154f16f12543f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf0211066ce54dff92419ea5b13e6f5a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9218062476544b9acbb348190f7a61c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61aab4bca49e489fbac32849648d429d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"684faef3eafe4ddb90be25f0ff032ba5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a06f1e2032ff45d4be4114bc5a7e26e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"244ae61913634091a07076845e7be9e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7ec9f474baf4bbd9bdbaf4a4fa2ff7a","IPY_MODEL_1c247b27329d4f5bb92721deb2be5e3f","IPY_MODEL_8254763f76e24d0893b43fa2a4e83d68"],"layout":"IPY_MODEL_7578fade16ef494e96058c434beef2e1"}},"b7ec9f474baf4bbd9bdbaf4a4fa2ff7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db63b48649544f3f9cdb3dd8e562e308","placeholder":"​","style":"IPY_MODEL_67998eb65fde4c45bfba670f699c168d","value":"tokenizer.json: 100%"}},"1c247b27329d4f5bb92721deb2be5e3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f89391b07b684b85a28f0386e746e935","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f5b91033f8d4e21ac75da35d17a780c","value":1355863}},"8254763f76e24d0893b43fa2a4e83d68":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_727bfbfacab74688a69c3b7d9cc04d27","placeholder":"​","style":"IPY_MODEL_ae61d37f5eeb4327a3b8b34de055336d","value":" 1.36M/1.36M [00:00&lt;00:00, 5.28MB/s]"}},"7578fade16ef494e96058c434beef2e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db63b48649544f3f9cdb3dd8e562e308":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67998eb65fde4c45bfba670f699c168d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f89391b07b684b85a28f0386e746e935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f5b91033f8d4e21ac75da35d17a780c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"727bfbfacab74688a69c3b7d9cc04d27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae61d37f5eeb4327a3b8b34de055336d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a19e16527ec4792967c979765939958":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_835a379d53204f61bd321d5eee69efd7","IPY_MODEL_668cb219c39249fdbf98d1ec347b69b8","IPY_MODEL_519faf9729ba484ba2ae4f9ecfb00da0"],"layout":"IPY_MODEL_ca0a1b04e8a248a3a3303164a2131d35"}},"835a379d53204f61bd321d5eee69efd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ca8b25e872b4dd58c7078a9c523e0ae","placeholder":"​","style":"IPY_MODEL_4769d183645242dca8d7d34359970785","value":"model.safetensors: 100%"}},"668cb219c39249fdbf98d1ec347b69b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8398027468b24691842a3d10e881aec2","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcf2747535a64b4895fe156f4144ee15","value":1421700479}},"519faf9729ba484ba2ae4f9ecfb00da0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17713b11e84447be998b6f87c5711b12","placeholder":"​","style":"IPY_MODEL_0dfc7fe68e9f40a1be6e7d81d3122d81","value":" 1.42G/1.42G [00:21&lt;00:00, 193MB/s]"}},"ca0a1b04e8a248a3a3303164a2131d35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ca8b25e872b4dd58c7078a9c523e0ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4769d183645242dca8d7d34359970785":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8398027468b24691842a3d10e881aec2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcf2747535a64b4895fe156f4144ee15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"17713b11e84447be998b6f87c5711b12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0dfc7fe68e9f40a1be6e7d81d3122d81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"gBspXl0zpsPY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763806014870,"user_tz":-330,"elapsed":11682,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"aca1014e-7e6f-4296-dd43-9e5d21b50add"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=be4967a543f8cfc33d00dac17c677079e0c96151b90e4d71268123e7c9eb927e\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"AAIhodg3qTMk","executionInfo":{"status":"ok","timestamp":1763806014888,"user_tz":-330,"elapsed":16,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"eb50b88a-dfb4-42a9-e02f-58de4854c869"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-4-scout-17b-16e-instruct/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-4-scout-17b-16e-instruct_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6J3dt84uqV0B","outputId":"427f6841-9639-4abb-add4-2f634b5a0dc2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Groq API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","========== OUTPUT FOR ROW 0 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning with human feedback is an approach that integrates human input into the training process of reinforcement learning algorithms, such as Q-learning, Deep Q-learning, and Proximal Policy Optimization. This integration guides and accelerates the learning process, enabling algorithms to make more informed decisions. A notable application of this framework is ChatGPT, where human feedback is provided through a rewards model that assesses and scores the quality of generated responses. The iterative training process, which incorporates human feedback, enhances ChatGPT's capabilities, ultimately making it a powerful tool for generating high-quality responses. By leveraging human input, reinforcement learning algorithms can improve their performance and produce more accurate and relevant outputs, as demonstrated by the effectiveness of ChatGPT in generating high-quality responses.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary purpose of human feedback in reinforcement learning?\n","A: To guide and accelerate the learning process, allowing the algorithm to make more informed decisions\n","Q: Which reinforcement learning algorithms can be used along with human feedback?\n","A: All of the above (Q-learning, DQN learning, Proximal Policy Optimization)\n","Q: In the context of ChatGPT, what is the primary purpose of the rewards model?\n","A: To assess and score the quality of answers generated by ChatGPT\n","\n","KEY CONCEPTS:\n","\n","Reinforcement Learning with Human Feedback, Proximal Policy Optimization, Rewards Model, Q-Learning, DQ Learning, Grid World, Human-in-the-Loop Learning, Iterative Training Process, Fine-Tuning with Human Feedback\n","\n","============================================\n","Saved row 0\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","========== OUTPUT FOR ROW 1 ==========\n","\n","SUMMARY:\n","\n","This tutorial explores the application of kernels to Support Vector Machines (SVMs) using CVXopt, a quadratic programming solver. The example code demonstrates the impact of kernels on SVMs, particularly in nonlinear and soft margin cases. The tutorial covers various kernels, including linear, polynomial, and Gaussian kernels, and their role in handling non-linearly separable data. The instructor discusses the implementation of SVMs using CVXOPT, including the hard margin SVM and soft margin SVM, and demonstrates how to generate and classify linearly separable and non-linearly separable data. The lecture also includes visualizations of SVMs in higher dimensions and their application in lower dimensions. The tutorial aims to educate on the effects of kernels and provides resources for further learning, including a reference to Christopher Bishop's book and additional tutorials on quadratic programming. The instructor also previews the next tutorial, which will cover SVM parameters in scikit-learn and multi-class classification problems.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Artificial Intelligence', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of using CVX opt in this machine learning tutorial?\n","A: To directly see the impact of a kernel and where it's being injected and modifying the initial formal support Vector machine.\n","Q: What is the name of the quadratic programming solver being used in this tutorial?\n","A: CVX opt\n","Q: What is the equation that CVX opt solves?\n","A: Minimizing 1/2 x^T P x + q^T x subject to Gx <= h and Ax = b.\n","Q: Is CVX opt a library that you would typically use to implement a support Vector machine in practice?\n","A: No, you would almost certainly be using libsvm.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 1\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","========== OUTPUT FOR ROW 2 ==========\n","\n","SUMMARY:\n","\n","The foundation of prompt engineering lies in understanding prompts as inputs to large language models, which vary in complexity and context. There are seven types of prompts, including question and statement prompts with constraints. Key features of prompts, such as length, language, and context, help define expectations and constraints. Effective prompt engineering involves selecting the right prompt type and clearly defining expectations to obtain accurate outputs. By refining prompts, specific outputs can be achieved, as illustrated by examples ranging from a one-word answer to a 500-word essay. A well-crafted prompt is crucial for generating accurate text outputs, making prompt engineering a critical component of working with large language models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary function of prompts in prompt engineering?\n","A: Prompts are the inputs given to prompt engineering models, providing context and constraints for generating text outputs.\n","Q: What are the key features of a prompt that can impact the output of a large language model?\n","A: The key features of a prompt include its length, specific language used, context, and constraints, such as tone, style, and specific requirements for the text.\n","Q: How can deconstructing a prompt help in understanding its requirements?\n","A: Deconstructing a prompt involves breaking it down into individual components to better understand its key features and constraints, such as specific language, requirements, and output format.\n","Q: What is the purpose of specifying constraints in a prompt, such as tone or word count?\n","A: Specifying constraints in a prompt, such as tone or word count, helps define the output and ensures that the generated text meets the required criteria.\n","Q: How can rephrasing a prompt impact the output of a large language model?\n","A: Rephrasing a prompt can impact the output of a large language model, as seen in the example where the prompt 'What is the capital of France' was rephrased to 'What is the capital of France, give a one-word answer' to get a more accurate output.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, large language models, prompt types, key features of prompts, constraints in prompts, deconstructing prompts, prompt structure, specific language used in prompts, tone and style of text\n","\n","============================================\n","Saved row 2\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","========== OUTPUT FOR ROW 3 ==========\n","\n","SUMMARY:\n","\n","Artificial Intelligence (AI) agents are autonomous problem solvers that make decisions using tools, which are specific functions that facilitate task completion. The React Agent pattern, a key method for developing AI agents, emulates human thought processes through a cyclical process of thinking, acting, and observing. This pattern involves an agent assessing a problem, determining an action, executing that action via a tool, and then evaluating the outcome. This iterative cycle continues until a solution is found. Understanding the React Agent pattern is essential to grasping AI agents, and it will be applied in future practical coding exercises to illustrate its application and relevance in AI development.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Agentic AI', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What are AI agents in the context of Artificial Intelligence?\n","A: AI agents are problem solvers that can think on their own and make autonomous decisions.\n","Q: What is the React Agent pattern in Artificial Intelligence?\n","A: The React Agent pattern is a concept that mimics human thinking, consisting of a cycle of thinking, taking action, observing the result, and repeating the cycle until a solution is found.\n","Q: What are tools in the context of AI agents?\n","A: Tools are specific functions that agents can use to complete tasks, such as a calculator, search engine, or calendar.\n","Q: How does the React Agent pattern work in terms of think, action, and observe?\n","A: The React Agent pattern works by first thinking about a problem, then taking action to solve it, observing the result, and repeating the cycle of think, action, and observe until a solution is found.\n","Q: What constitutes an AI agent in simple terms?\n","A: An AI agent consists of a brain (reasoning ability) equipped with tools (such as the ability to make API calls or run a Python function), which together enable the agent to solve problems.\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence (AI) agents, React Agent Pattern, Reasoning plus Acting, Think-Action-Observation, Autonomous decisions, Tools for AI agents, Lang chain, Large Language Model (LLM)\n","\n","============================================\n","Saved row 3\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","========== OUTPUT FOR ROW 4 ==========\n","\n","SUMMARY:\n","\n","The instructor guides the group through tracing a reflection agent system, built to produce refined viral tweets, to understand its inner mechanics and collaboration with another system. Using the smith.chain website as a reference, the instructor demonstrates setting up a tracing project with Lang Chain and LSmith, an observability tool. They explain how reflection agents work through iterative exchanges between generation and reflection nodes, allowing for deep thinking and refinement. This process generates a well-crafted tweet through multiple iterations. The instructor highlights the power of reflection agents in complex tasks and previews the next topic, Reflexion agents, showcasing their potential in producing high-quality outputs.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Agentic AI', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of tracing the reflection agent system in this section?\n","A: To understand exactly what is happening where and how both systems are working together to deliver the final refined viral tweet.\n","Q: What is the ultimate goal that the speaker wants to achieve by understanding the reflection agent system?\n","A: To deliver a final refined viral tweet.\n","Q: Where does the speaker go to trace the reflection agent system?\n","A: A website called smith.chain.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 4\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","========== OUTPUT FOR ROW 5 ==========\n","\n","SUMMARY:\n","\n","This section guides students in working with Lang chains chat models, focusing on Open AI's APIs. The instructor demonstrates how to install the necessary package, import and initialize the ChatOpenAI class, and communicate with Open AI's APIs using the 'invoke' keyword. An error occurs due to a missing API key, which is resolved by creating an environment file and installing the 'python-dotenv' package. The instructor shows how to access the API, address potential issues with low balance, and extract specific content from the response object. A simple example is provided, sending a prompt to retrieve the square root of 49. The section explains how to use Lang chains chat models to interact with APIs, specifically Open AI, and sets the stage for the next section, which will cover sending conversation history to the model for more effective responses.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Artificial Intelligence', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What is the command to install the Lang chain chat model related to Open Artificial Intelligence (AI)?\n","A: L chain Das open aai\n","Q: What is the name of the class that needs to be imported from the 'longchain open Artificial Intelligence (AI)' module?\n","A: chat open Artificial Intelligence (AI)\n","Q: Which Open AI model is used in the example and why is it chosen?\n","A: gbt 40, because it is one of the latest models released by Open AI\n","Q: What alternative model can be used if the latest model is too expensive?\n","A: GPD 3\n","\n","KEY CONCEPTS:\n","\n","Lang chains, chat models, open Artificial Intelligence (AI) apis, chat open AI, L chain, open aai, chat open AI class, language model, GBT 3, GBT 4\n","\n","============================================\n","Saved row 5\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","========== OUTPUT FOR ROW 6 ==========\n","\n","SUMMARY:\n","\n","In Python, the sort method exhibits specific behaviors when handling lists containing strings and mixed data types. When sorting lists of strings, Python prioritizes uppercase letters over lowercase letters, resulting in separate alphabetical sorts. To achieve a unified sort, it is essential to ensure all strings are in the same case. For lists containing both strings and numbers, Python sorts numbers first, followed by strings, with ordering based on data type and then alphabetical order. Understanding these sorting behaviors is crucial for effective list management in Python, as it allows developers to anticipate and control the ordering of elements in lists, ensuring accurate and predictable results.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What happens when you sort a list of strings in Python that contains both uppercase and lowercase letters?\n","A: Python sorts the list by putting the words with uppercase letters first, followed by the words with lowercase letters, in alphabetical order.\n","Q: How does Python handle sorting a list that contains both strings and numbers?\n","A: Python puts the numbers first, followed by the strings, when sorting a list that contains both.\n","Q: What is a potential issue to consider when sorting lists that contain strings with different cases, and how can it be addressed?\n","A: The potential issue is that Python sorts uppercase letters before lowercase letters. To address this, you can convert all strings to lowercase or uppercase before sorting.\n","\n","KEY CONCEPTS:\n","\n","sort method, alphabetical order, uppercase letters, lowercase letters, list sorting, string sorting, numeric sorting, reverse sorting\n","\n","============================================\n","Saved row 6\n","\n","================================================================================\n","PROCESSING ROW 7: \n","Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 7 ==========\n","\n","SUMMARY:\n","\n","The integration of Artificial Intelligence (AI) and human decision-making, known as augmented intelligence, has the potential to improve decision-making outcomes. When combined, humans and AI algorithms can leverage their respective strengths to achieve better results. AI excels in decision-making tasks when confident, whereas humans outperform AI when it's uncertain, bringing in additional context and information. However, human cognitive bias can impact the effectiveness of augmented intelligence. The way AI-generated information is presented to human decision-makers can mitigate or exacerbate this bias. For instance, a forced display of AI recommendations can lead to automation bias, while an optional display allows humans to form their own impression before considering the AI's suggestion. To maximize the collaboration between humans and AI, it is essential to present AI-generated information effectively, minimizing human cognitive bias and enabling the strengths of both humans and AI to be leveraged.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is a common challenge faced by financial analysts in fraud detection systems?\n","A: They are overwhelmed with 90 percent of alerts being false positives.\n","Q: How do Artificial Intelligence (AI) performance curves typically behave in predicting real alerts?\n","A: They have a high success rate for very low and very high confidence scores, but a lower success rate when the AI is not sure.\n","Q: What is a key difference between Artificial Intelligence (AI) and human performance curves in prediction tasks?\n","A: Artificial Intelligence (AI) performance curves are more steeply sloped, while human performance curves are typically flatter.\n","\n","KEY CONCEPTS:\n","\n","fraud detection system, Artificial Intelligence (AI), confidence score, success rate, performance curve, false positives, human bias, holistic curves, prediction accuracy, decision making\n","\n","============================================\n","Saved row 7\n","\n","================================================================================\n","PROCESSING ROW 8: \n","Build generative apps faster with Vertex AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 8 ==========\n","\n","SUMMARY:\n","\n","Google Cloud AI's product manager, Demetrius, presented on accelerating the development of generative applications using Vertex AI. To address technical challenges in building enterprise generative applications, six new APIs were introduced. These APIs - document understanding, embedding, vector search, ranking, grounded generation, and check grounding - aim to enhance accuracy and consistency by providing reliable access to enterprise data. Designed to be simple, standalone, and stateless, the APIs facilitate easy integration into developers' workflows, allowing them to focus on creating unique solutions. By leveraging these APIs, developers can build generative applications more efficiently and effectively, overcoming key hurdles in the process.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Generative AI', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main focus of Demetrius' talk about building gen apps faster and better with Vertex Artificial Intelligence (AI)?\n","A: The main focus of Demetrius' talk is on solving technical challenges that developers face when building generative applications for Enterprises, particularly in grounding and accessing the right Enterprise data to produce accurate and consistent responses.\n","Q: What are the six new Vertex AI APIs announced for building gen apps, and what are their primary functions?\n","A: The six new Vertex AI APIs are: 1) Document Understanding API, 2) Embedding API, 3) Vector Search, 4) Ranking API, 5) Grounded Generation API, and 6) Check Grounding API. They are designed to help developers overcome technical challenges in building generative applications, including document processing, embedding retrieval, vector search, result ranking, grounded generation, and fact-checking.\n","Q: What sets the new Vertex AI APIs apart from others in the market?\n","A: The new Vertex AI APIs are set apart by their high quality, unique features, and the incorporation of Google's know-how and technology. They are designed to solve specific problems that developers face, and they leverage Google's expertise in areas such as document AI, search, and language models.\n","Q: How can developers seamlessly integrate the new Vertex AI APIs into their workflow?\n","A: The new Vertex AI APIs are designed as simple, standalone, and stateless APIs with clear interfaces, making it easy for developers to try, understand, and apply them to their problems. Additionally, they are being integrated into popular frameworks like LangChain and Llama Index, allowing developers to prototype and combine them with other APIs to build solutions.\n","\n","KEY CONCEPTS:\n","\n","Vertex Artificial Intelligence, Generative Applications, Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Grounding, Hybrid Search, Gemini Model\n","\n","============================================\n","Saved row 8\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","========== OUTPUT FOR ROW 9 ==========\n","\n","SUMMARY:\n","\n","The singular value decomposition (SVD) of a matrix X is represented as X = U Σ V^T, where U and V are unitary matrices that preserve angles and lengths of vectors, effectively acting as rotations in vector space. These unitary matrices have significant implications in science and engineering, particularly in data transformation applications. The SVD provides insight into the geometric structure of a matrix by transforming a sphere into an ellipsoid. In this transformation, the unitary matrices U and V represent rotations, while the singular values in Σ signify the degree of stretching or compaction. This decomposition is crucial for understanding the intrinsic properties of matrices and has far-reaching applications in various fields.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary property of unitary matrices that makes them important in science and engineering?\n","A: Unitary matrices preserve the angles between any two vectors in the vector space that they're transforming, as well as the lengths of vectors.\n","Q: How does the economy size SVD (Singular Value Decomposition) differ from the standard SVD in terms of matrix dimensions and properties?\n","A: In the economy SVD, the matrix U is an N by M matrix, and Sigma is an M by M matrix. Only U hat transpose U hat equals the identity, not U hat U hat transpose.\n","Q: What is a geometric interpretation of multiplying a vector by a unitary matrix, such as those obtained from SVD?\n","A: Multiplying a vector by a unitary matrix corresponds to rotating the vector in such a way that the angles between vectors and their lengths are preserved.\n","Q: If X is a matrix that can be decomposed using SVD into U, Sigma, and V, what does the transformation of a sphere of unit vectors in the column space of X into an ellipsoid reveal about X?\n","A: The transformation reveals that X maps the sphere into an ellipsoid, with the lengths of the principal axes of the ellipsoid given by the singular values of X, and the orientation given by the left singular vectors of X.\n","\n","KEY CONCEPTS:\n","\n","Singular Value Decomposition (SVD), Unitary Matrices, Economy Size SVD, Fourier Transform, Unitary Transformations, Complex Conjugate Transpose, Geometric Interpretation, Principal Axes, Singular Values, Left Singular Vectors\n","\n","============================================\n","Saved row 9\n","\n","================================================================================\n","PROCESSING ROW 10: \n","Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 10 ==========\n","\n","SUMMARY:\n","\n","This video tutorial guides viewers on building generative AI-powered applications using Google Gemini Pro 1.5, a multimodal model that can work with both text and images. The speaker showcases the model's capabilities, including processing large context windows of up to 1 million multimodal tokens, and demonstrates how to create an API key and implement applications that leverage its features. The Google Gemini Pro API offers a powerful tool for developing generative applications, capable of handling large inputs such as 1 million tokens, 1 hour of video, 11 hours of audio, and 30k lines of code. The tutorial covers the configuration of generative AI models, securing API keys, and model selection, and explores the capabilities of the Gemini Pro 1.5 model, including its ability to handle multimodal tasks and generate text and images simultaneously. The goal is to understand how to utilize the Google Gemini Pro API effectively and explore its potential uses in artificial intelligence applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Generative AI', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main topic of the video being presented by Krishn?\n","A: Building generative Artificial Intelligence (AI) powered applications using Google Gemini Pro 1.5\n","Q: What type of model is Google Gemini Pro 1.5?\n","A: A multimodal model, capable of working with both text and images\n","Q: What will Krishn demonstrate in the video after showing the demo provided by Google?\n","A: Hands-on applications by running some code, playing with images and text\n","\n","KEY CONCEPTS:\n","\n","Generative Artificial Intelligence, Google Gemini Pro, Multimodel, Long Context Understanding, API Key\n","\n","============================================\n","Saved row 10\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","========== OUTPUT FOR ROW 11 ==========\n","\n","SUMMARY:\n","\n","Evaluating and testing prompt engineering models is essential to ensure their performance and accuracy. This process involves using various metrics, such as perplexity, accuracy, and human evaluation, to measure a model's ability to generate meaningful responses. Perplexity assesses a model's prediction of word sequences, while accuracy evaluates the correctness of generated responses. Human evaluation provides a subjective assessment of response quality. To debug and improve models, errors and patterns are analyzed, and models are tested on diverse data sets and tasks to determine their ability to generalize. Through this ongoing process, models are validated and refined using tools and techniques like visualization and cross-validation, ultimately enhancing their reliability and effectiveness in generating high-quality responses.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What are some common matrices used to evaluate prompt engineering models?\n","A: perplexity, accuracy, and human evaluation\n","Q: What does perplexity measure in a language model?\n","A: how well a language model predicts a sequence of words\n","Q: How can you improve a prompt engineering model after identifying common errors or patterns in its generated responses?\n","A: by fine-tuning the model\n","Q: Why is testing a prompt engineering model on different data sets or tasks important?\n","A: to determine the model's ability to generalize on new or unseen data\n","Q: What is one technique for testing a model on different data sets?\n","A: cross-validation\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 11\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","========== OUTPUT FOR ROW 12 ==========\n","\n","SUMMARY:\n","\n","The speaker delineates the distinctions between Generative AI, AI Agents, and Agentic AI, highlighting their varying capabilities and complexities. Generative AI is characterized by its ability to create novel content, such as text, images, or videos, based on patterns gleaned from existing data. In contrast, AI Agents are designed to take input, process information, and act to accomplish tasks, often leveraging tools and memory. Agentic AI represents a more sophisticated system wherein multiple AI agents operate autonomously to achieve intricate goals, utilizing tools and collaborating with other agents. The capacity for task complexity escalates across these AI categories, with Agentic AI distinguished by its ability to plan, coordinate, and execute multi-step objectives.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Agentic AI', 'Generative AI']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Generative Artificial Intelligence (AI), Large Language Model (LLM), Artificial Intelligence (AI) agent, Agentic Artificial Intelligence (AI), Autonomous decision making, Multi-step reasoning, Tool usage, Knowledge cutoff date, API integration, LLM training data\n","\n","============================================\n","Saved row 12\n","\n","================================================================================\n","PROCESSING ROW 13: \n","Covariance in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 13 ==========\n","\n","SUMMARY:\n","\n","Covariance is a fundamental concept in data analysis and preprocessing that measures the relationship between two random variables. It quantifies how changes in one variable affect another, with a positive covariance indicating that as one variable increases, the other also tends to increase, and a negative covariance suggesting that as one variable increases, the other decreases. The covariance formula is calculated as the average of the products of the deviations from the means of the two variables. Although covariance provides valuable insights, it has limitations, such as not indicating the strength of the relationship between variables. This limitation will be addressed through the Pearson correlation coefficient, a topic that builds upon the foundational understanding of covariance. Understanding covariance is essential for further analysis and interpretation of data relationships.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","Covariance, Variance, Random Variables, Data Pre-processing, Data Analysis, Pearson Correlation Coefficient, Gradient of Relationship, Quantifying Relationships, Variance-Covariance Relationship\n","\n","============================================\n","Saved row 13\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning aims to learn an optimal policy that maximizes a numerical reward signal. An agent, which can be human or artificial, interacts with an environment, making decisions based on observations and receiving feedback in the form of rewards. The agent's ultimate goal is to maximize the cumulative reward over time through trial and error learning. It explores the environment, takes actions, and updates its policy based on resulting rewards. A reward function, a mathematical expression, assigns numerical values to each state or action, reflecting the trading goal and preferences. This process enables the agent to adapt and make informed decisions, optimizing its policy to achieve the defined objective.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the primary objective of a reinforcement learning problem?\n","A: To learn the optimal policy that maximizes a numerical reward signal.\n","Q: How is the objective defined in a reinforcement learning problem?\n","A: The objective is defined by a goal given to an agent, similar to how a manager gives a goal to an employee, and is achieved through trial and error learning.\n","Q: In a Tic-Tac-Toe game, what is the ultimate goal of a reinforcement learning agent and how is the reward structured?\n","A: The ultimate goal is to win the game by placing three marks of the same kind in a row, column, or diagonal, with a positive reward of +1 for winning, -1 for losing, and 0 for drawing.\n","Q: How can the reward function be defined for a continuous task like stock market trading?\n","A: The reward function can be defined as the amount of profit earned over a given period of time, or using risk-adjusted measures such as the Sharpe ratio or Sortino ratio.\n","Q: What is the purpose of the reward signal in reinforcement learning?\n","A: The reward signal provides feedback to the agent about the quality of its actions, guiding it to learn the optimal policy.\n","\n","KEY CONCEPTS:\n","\n","Reinforcement Learning, Optimal Policy, Cumulative Reward, Reward Function, Trial and Error Learning, Value-Based Method, Policy-Based Method, Objective Function, Numerical Reward Signal, Episodic Task, Continuous Task\n","\n","============================================\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","In Python, dictionaries are a fundamental data type comprising key-value pairs, enclosed in curly brackets, where each key is immutable and linked to a specific value. They are particularly useful for mapping items, such as stock prices, and play a crucial role in Python, especially in data science applications with libraries like pandas. Dictionaries support various key functions, including accessing items, keys, and values, as well as creating dictionaries from lists using the zip function. They can be manipulated by adding, editing, or deleting entries, and their length can be determined. Overall, dictionaries are a versatile and essential data type in Python, widely used for data organization and transformation, making them a vital tool for developers and data scientists alike.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is a dictionary in Python, and how is it declared?\n","A: A dictionary in Python is a data type that consists of key-value pairs, and it is declared within curly brackets.\n","Q: What are the requirements for a key in a Python dictionary?\n","A: The key in a Python dictionary must be immutable, such as a string or a number, but it cannot be a list.\n","Q: How do you access the value associated with a key in a Python dictionary?\n","A: You can access the value associated with a key in a Python dictionary by writing the name of the dictionary followed by the key in square brackets.\n","Q: What is the purpose of the `zip` function in creating a dictionary from two lists?\n","A: The `zip` function pairs corresponding values from two lists together into tuples, which can then be used to create a dictionary.\n","Q: How do you delete a particular entry within a Python dictionary?\n","A: You can delete a particular entry within a Python dictionary using the `del` function, specifying the dictionary and the key to be deleted.\n","\n","KEY CONCEPTS:\n","\n","key-value pairs, dictionary, immutable, zip function, dict function, dictionary comprehension, key access, value update, delete entry, dictionary manipulation\n","\n","============================================\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: \n","Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","The integration of Artificial Intelligence (AI) and machine learning in security operations can significantly enhance an organization's security posture. According to IBM's 2023 Cost of a Data Breach report, AI and automation can help contain data breaches 108 days faster on average. A key application of AI is in User Behavior Analytics (UBA), which detects and responds to insider threats quickly and precisely. When combined with a Security Information and Event Management (SIEM) solution like IBM's QRadar, UBA provides a powerful tool for security analysts to identify and contain threats. The QRadar UBA app enables analysts to prioritize risks, monitor alerts, and view risk categories, streamlining security operations and enabling a proactive defense. By leveraging AI and automation, organizations can improve their threat detection and response capabilities, reducing the average cost of an insider threat incident, which is $4 million.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What was the average reduction in days to identify and contain a data breach for organizations that extensively used AI and automation compared to those that didn't?\n","A: 108 days\n","Q: What is the average cost of an Insider threat for an organization according to the Cost of a Data Breach Report?\n","A: $4 million\n","Q: How can User Behavior Analytics (UBA) with Artificial Intelligence (AI) and machine learning help security teams?\n","A: Detect and respond to Insider threats quickly and precisely\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","Meta has introduced Llama 3, an open-source large language model (LLM) that offers state-of-the-art performance in language understanding, contextual reasoning, and complex tasks. The model is available in two variants, with 8 billion and 70 billion parameters, and has been trained on over 50 trillion tokens of data. Llama 3 excels in tasks such as translation, dialog generation, and code generation, outperforming other open-source models and competing with paid LLM models. The model's performance is competitive, with high accuracy in various benchmarks, although slightly lower than Gemin Pro 1 in certain areas. Meta has also introduced guidelines for responsible AI development, including Llama Guard, which provides transparency on the model's development. Access to Llama 3 is available through various platforms, including Meta, Hugging Face, and Kaggle, with provided instructions for downloading model weights and tokenizer, and utilizing the model for various tasks.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Other']\n","\n","GENERATED Q&A:\n","\n","Q: What is the name of the YouTube channel owner speaking in the transcript?\n","A: Krishak\n","Q: What time is it when the speaker is recording the video?\n","A: 2 a. m.\n","Q: Where is the speaker likely recording the video?\n","A: It is not specified, but likely at home or a quiet place\n","Q: What can viewers expect to find on this YouTube channel?\n","A: The transcript does not specify\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-4-scout-17b-16e-instruct/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-4-scout-17b-16e-instruct_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nGBAD_3yBxWN","executionInfo":{"status":"ok","timestamp":1763810989804,"user_tz":-330,"elapsed":4569718,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"79adddd4-5b4a-4486-d7bd-0a126f77c602"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Groq API key loaded ✓\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","Skipping row 7 (already processed)\n","Skipping row 8 (already processed)\n","Skipping row 9 (already processed)\n","Skipping row 10 (already processed)\n","Skipping row 11 (already processed)\n","Skipping row 12 (already processed)\n","Skipping row 13 (already processed)\n","Skipping row 14 (already processed)\n","Skipping row 15 (already processed)\n","Skipping row 16 (already processed)\n","Skipping row 17 (already processed)\n","Skipping row 18 (already processed)\n","Skipping row 19 (already processed)\n","Skipping row 20 (already processed)\n","Skipping row 21 (already processed)\n","Skipping row 22 (already processed)\n","Skipping row 23 (already processed)\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","Lang chain is a framework that enables the development of AI-powered applications by bridging the gap between large language models and the real world. To illustrate its utility, consider planning a vacation, such as asking a chatbot to book a flight, hotel, and suggest restaurants for a trip to Paris. Large language models, like those used in chat applications, can only reason and provide information, but cannot interact with external systems. Lang chain overcomes this limitation by allowing applications to access APIs, databases, and perform actions like sending emails. By leveraging the reasoning ability of large language models and enabling interaction with external systems, Lang chain has become a popular choice for building applications that can act in the real world, making it an essential tool for developers seeking to create more sophisticated AI-powered applications.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What is the user trying to accomplish with the help of a chatbot in this scenario?\n","A: The user wants to plan a trip to Paris, book a flight, book a hotel, and get suggestions for good restaurants.\n","Q: What happens to the user's query when they press enter?\n","A: The query is sent to a large language model (LLM).\n","Q: What type of application might use multiple models, including potentially 'charity 3'?\n","A: A chatbot application.\n","\n","KEY CONCEPTS:\n","\n","Lang chain, LLM model, Chbd application, Vacation planning, Flight booking, Hotel booking, Restaurant suggestion\n","\n","============================================\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","This video discusses the importance of residuals and residual analysis in time series forecasting. Residuals, the differences between fitted and actual values, help diagnose model performance by identifying issues such as autocorrelation and bias. A good model should have residuals with no autocorrelation or partial autocorrelation and a mean of zero. The speaker demonstrates how to perform residual analysis in Python using a Holt-Winters model and the statsmodels package. The analysis reveals significant autocorrelation in the residuals, indicating that the model needs revision. The residuals are approximately zero-centered and symmetrical, with a minor negative bias. Overall, the analysis highlights the need to address autocorrelation in residuals to improve model accuracy. Residual analysis is a useful tool for understanding and diagnosing model performance, and is recommended for those building forecasting models.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Machine Learning', 'Statistics']\n","\n","GENERATED Q&A:\n","\n","\n","\n","KEY CONCEPTS:\n","\n","residual analysis, time series forecasting, autocorrelation, partial autocorrelation, Ljung-Box test, residuals, fitted values, exponential smoothing model, Holt-Winters model, model diagnostics\n","\n","============================================\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","This video tutorial guides viewers on building an Artificial Intelligence (AI) agent that interacts with a database using SQL knowledge. The agent is built using LangGraph, Next.js, and models running on watsonx.ai, with an in-memory database using SQLite. The tutorial covers setting up a Next.js project, implementing a feature to populate messages with data from a large language model, and creating a ReAct agent to connect to models on watsonx.ai. It also involves setting up a database using SQLite 3, creating server-side functions for database interactions, and integrating a tool, GetFromDB, with a large language model to enable database querying. The goal is to enable the model to generate SQL and execute it with given tools, allowing the agent to answer questions by querying the database. The tutorial highlights the importance of implementing guardrails to prevent the model from having unlimited control over the database.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What type of agent will be built using LangGraph and models running on watsonx.ai?\n","A: A ReAct agent\n","Q: What is being used to set up the boilerplate project for the frontend application?\n","A: Next.js CLI\n","Q: What is the purpose of the input box and button being added to the Home component?\n","A: To type a message to the large language model\n","Q: Where will the code for the frontend application be run?\n","A: Client-side\n","Q: What database is being used for the application?\n","A: SQLite\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence agent, large language models, SQL knowledge, LangGraph, ReAct agent, Next.js, watsonx.ai, in-memory database, SQLite, client-side component\n","\n","============================================\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","Prompt engineering is a specialized field within natural language processing that focuses on developing models capable of generating high-quality text outputs in response to input prompts. By leveraging pre-trained large language models and fine-tuning them for specific tasks, prompt engineering enables the production of accurate, coherent, and contextually relevant text. This field plays a vital role in various applications, including chatbots, language translation, and content generation, where the quality of output significantly affects user experience. The fundamentals of prompt engineering encompass prompt analysis, benefits, limitations, and techniques for fine-tuning pre-trained models, providing a comprehensive introduction to this rapidly evolving field. As a result, prompt engineering has become crucial for creating effective human-computer interactions and automating content creation, making it an essential area of study in natural language processing.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is prompt engineering and why is it important in Natural Language Processing (NLP)?\n","A: Prompt engineering is a specialized field within NLP that focuses on building models to generate high-quality text outputs in response to prompts or inputs, allowing for more accurate, coherent, and contextually appropriate text outputs.\n","Q: What are the benefits of using prompt engineering over traditional rule-based or keyword-based approaches?\n","A: The key benefit of prompt engineering is that it allows us to generate text outputs that are more accurate, coherent, and contextually appropriate than traditional approaches, significantly impacting user experience and engagement.\n","Q: What are some limitations of prompt engineering models?\n","A: Prompt engineering models may struggle with complex and ambiguous prompts, or generate outputs that are biased and inaccurate due to underlying data or model architecture.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, natural language processing, pre-trained language models, fine-tuning, large language models, prompt analysis, text generation, chatbots, language translation, content generation\n","\n","============================================\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","Reinforcement learning is a machine learning paradigm where an agent learns to map situations to actions to maximize a numerical reward signal. Q-learning, a value-based method, uses a Q-value function to solve problems by determining the optimal policy to maximize the total reward. The Q-value function takes a state and action as input and outputs a real number that quantifies the total reward. Q-learning utilizes a Q-table to store Q-values and the Bellman equation to calculate the observed Q-value, which updates the Q-table. The process involves calculating the temporal difference error, the difference between Q-values at two time steps, and using it to update Q-values based on a gradient update rule with a learning rate. Q-learning is an off-policy algorithm that enables agents to learn from their environment, make decisions based on a Q-table, and refine Q-values over time. As Q-values stabilize, agents use them to determine the best policy to achieve a target reward, allowing for efficient learning and informed decision-making.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three primary machine learning paradigms discussed in the context of reinforcement learning?\n","A: supervised learning, unsupervised learning, and reinforcement learning\n","Q: What is the main goal of Q-learning in reinforcement learning?\n","A: to learn the state-action value function (Q-values) that maximizes the total reward\n","Q: What is the difference between a state value function (V) and a state-action value function (Q)?\n","A: A state value function takes a state as input and outputs a real number, while a state-action value function takes a state and action as input and outputs a real number (Q-value)\n","\n","KEY CONCEPTS:\n","\n","Q-learning, Reinforcement learning, Value-based methods, Policy-based methods, State-action value function, Bellman equation, Q-value, Discount factor, Exploration policy, Target policy\n","\n","============================================\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","A logistic classifier, also referred to as a linear classifier, utilizes a linear function to generate predictions from input data, such as image pixels. This linear function combines matrix multiplication with weights (W) and a bias term (b) to produce scores. The scores are then transformed into probabilities through a softmax function (S), ensuring that the probabilities sum to 1 and reflect the relative magnitude of the scores. The primary objective of model training is to determine the optimal values for the weights and bias, enabling the model to accurately perform classification by assigning high probabilities to correct classes and low probabilities to incorrect classes.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What type of classifier is a logistic classifier, and what operation does it perform on the input to generate predictions?\n","A: A logistic classifier is a linear classifier, and it applies a linear function, which is a giant matrix multiply, to the input to generate predictions.\n","Q: What is the purpose of using a softmax function in logistic regression?\n","A: The purpose of using a softmax function is to turn scores into proper probabilities that sum to 1, where the probability of the correct class is close to 1 and the probabilities for every other class are close to zero.\n","Q: What are the logits in the context of logistic regression?\n","A: The logits, in the context of logistic regression, refer to the scores before applying the softmax function.\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 29\n","\n","All rows processed.\n","\n","Generation completed. Run separate evaluation script next.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_cot.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["71166d610e37429588392bdcf4670cfb","454fde030aea4eabb65b407a6b284703","f4004c746dc44127accb330497582d8e","25ae8e7e57f249b28fc94549b3f6e7a2","3ac5fdc73225412fae1e89237636f587","a38d1d80d74b419883411e35fa8bf14c","8a8c8a3895cc4f8593ba667bcb8502f7","5d0e1ead01834dd799d0f2f2eeba44d5","7e3d5e5aa68446249ec5e24ec09ae0b6","13ef6eff4574462fbf8c2768980d4bb3","f2333830b5854fc1ac2ceabe148bcb7e","c8475f94c1574291b8088de3c3357dd6","113e60306e2d4aa2ac7e3920aa9c6f3c","922c3a9e66e64cc19ac702cb33b88600","aee42b4f69a247349f5979af9dc0f0e7","4af83151262241e19f3f2476863421bf","00e9d751d86d4ae79ce5a6a9110382f1","07ecd20717ca43cb9f55323f7349f4a7","987a449f77d44c17a7ed6a6439117fb7","d745ac54cbd04f25b3f1f162cd50c1ec","fbc15efbfd9746c5aeb4fbe4580fb7f3","0bd9fd58ecbd437da474157de1a4c332","4e8fe97eec0b4c1595973ed553a1e85b","f3149a40670b40c7855f3604192af484","fd982bfdbacf420c928200fba1de209b","bb415f7134cf4dcd825b1c0042e5c995","31dbcf013fea4df2a54db03d986ea76a","8cd596b8fc9a4144826e3637e17fc545","1225c6a6225f4a57a86550414f359188","aa4c05fd42734cf1ac9a88003d0b335f","927388aa32e648b4a615cc1b4c6d5bc8","af2300b2ae6349c49803dc2d12810791","b9b48b7c75a34c7fb534bc71273229f3","fd273dfeb396461eb4143463c82efe69","68cdef331c56486eaa85869e84e9c543","f3c8a972fe96498f8dacd08a2c47d900","3acbc3fa6d9c4327802b62dd80ec4362","2a37448503e64860aaee7d0655bcd2db","8c47e7326e1d433aaa2154f16f12543f","bf0211066ce54dff92419ea5b13e6f5a","c9218062476544b9acbb348190f7a61c","61aab4bca49e489fbac32849648d429d","684faef3eafe4ddb90be25f0ff032ba5","a06f1e2032ff45d4be4114bc5a7e26e9","244ae61913634091a07076845e7be9e8","b7ec9f474baf4bbd9bdbaf4a4fa2ff7a","1c247b27329d4f5bb92721deb2be5e3f","8254763f76e24d0893b43fa2a4e83d68","7578fade16ef494e96058c434beef2e1","db63b48649544f3f9cdb3dd8e562e308","67998eb65fde4c45bfba670f699c168d","f89391b07b684b85a28f0386e746e935","2f5b91033f8d4e21ac75da35d17a780c","727bfbfacab74688a69c3b7d9cc04d27","ae61d37f5eeb4327a3b8b34de055336d","0a19e16527ec4792967c979765939958","835a379d53204f61bd321d5eee69efd7","668cb219c39249fdbf98d1ec347b69b8","519faf9729ba484ba2ae4f9ecfb00da0","ca0a1b04e8a248a3a3303164a2131d35","2ca8b25e872b4dd58c7078a9c523e0ae","4769d183645242dca8d7d34359970785","8398027468b24691842a3d10e881aec2","bcf2747535a64b4895fe156f4144ee15","17713b11e84447be998b6f87c5711b12","0dfc7fe68e9f40a1be6e7d81d3122d81"]},"id":"LSNizfXoqcY6","executionInfo":{"status":"ok","timestamp":1763811605614,"user_tz":-330,"elapsed":150429,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"55d368f1-3efd-40d5-9f83-d624ac2e7f06"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_cot.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71166d610e37429588392bdcf4670cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8475f94c1574291b8088de3c3357dd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e8fe97eec0b4c1595973ed553a1e85b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd273dfeb396461eb4143463c82efe69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244ae61913634091a07076845e7be9e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a19e16527ec4792967c979765939958"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3030\n","  - BLEU: 0.0746\n","  - BERTScore F1: 0.8931\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9000\n","  - Jaccard Index: 0.3612\n","  - Micro F1: 0.4772\n","  - Macro F1: 0.4472\n","  - Weighted F1: 0.4234\n","\n","Q&A Generation:\n","  - BLEU: 0.0300\n","  - Diversity: 0.7149\n","  - Answerability: 0.7756\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5400\n","  - Recall@10: 0.2160\n","  - F1@10: 0.3086\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\n"]}]}]}