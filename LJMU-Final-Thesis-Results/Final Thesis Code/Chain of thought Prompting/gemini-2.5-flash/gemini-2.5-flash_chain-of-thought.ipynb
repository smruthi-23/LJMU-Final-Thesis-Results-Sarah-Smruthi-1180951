{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiR1bCm+Kkk152zBa0BwPl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"39675b1f73214b17a3748078102dd00a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ea68d9d32af4394a62d024eda55aee3","IPY_MODEL_acd22c6c7cf64304a4dbc7514a0d0551","IPY_MODEL_abfe00b302ee48c685ee6b123239ec21"],"layout":"IPY_MODEL_54709bb386f4482db48c8dc085018efc"}},"3ea68d9d32af4394a62d024eda55aee3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b92e454307d4d1aa56944e8cf56228b","placeholder":"​","style":"IPY_MODEL_09c7ede8c6aa4e198f000cee14fedf82","value":"tokenizer_config.json: 100%"}},"acd22c6c7cf64304a4dbc7514a0d0551":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5d4440d1994b41bea065ad836ee7e5c1","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7122974b484444398ec1711f4e53e474","value":25}},"abfe00b302ee48c685ee6b123239ec21":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f13196e974d45a0884dccb284648ce7","placeholder":"​","style":"IPY_MODEL_3dcb048d66b249c1926a7aec62bdc9cb","value":" 25.0/25.0 [00:00&lt;00:00, 1.73kB/s]"}},"54709bb386f4482db48c8dc085018efc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b92e454307d4d1aa56944e8cf56228b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09c7ede8c6aa4e198f000cee14fedf82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d4440d1994b41bea065ad836ee7e5c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7122974b484444398ec1711f4e53e474":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f13196e974d45a0884dccb284648ce7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dcb048d66b249c1926a7aec62bdc9cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9b5179e430d433fa0755a36a116342a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfb26da93042499a8530811bbac730f0","IPY_MODEL_05ccf3ef90c84af7925ac9556fdad7de","IPY_MODEL_65186fd6a6534cc7855c89a47e86c79f"],"layout":"IPY_MODEL_5dbc075c319941bba3822af2fee4aa86"}},"bfb26da93042499a8530811bbac730f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c77f1fa31f7945c8ba1d3170cb7ded5c","placeholder":"​","style":"IPY_MODEL_e536a1cb43064fabab997dd0bde56c88","value":"config.json: 100%"}},"05ccf3ef90c84af7925ac9556fdad7de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e6985a989994a48ad135437decd6c47","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb2231fbe10449278db6a92d0bd0659b","value":482}},"65186fd6a6534cc7855c89a47e86c79f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_487e52a0d9a1435893b1f95b57efcab2","placeholder":"​","style":"IPY_MODEL_afd81b5c06f24257a0d71b95d2af6fe9","value":" 482/482 [00:00&lt;00:00, 22.0kB/s]"}},"5dbc075c319941bba3822af2fee4aa86":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c77f1fa31f7945c8ba1d3170cb7ded5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e536a1cb43064fabab997dd0bde56c88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e6985a989994a48ad135437decd6c47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb2231fbe10449278db6a92d0bd0659b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"487e52a0d9a1435893b1f95b57efcab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"afd81b5c06f24257a0d71b95d2af6fe9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3a22c9d56834b35850c66446cd175ce":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d0a48c93048b413f90b7acf04ec8ac8c","IPY_MODEL_fc16cdcd5fb74fa9b613614118f33fc4","IPY_MODEL_9bb0d0c42cde46858a1ed4e599ad1341"],"layout":"IPY_MODEL_c7f16cc055a849d4b403ba0cf3911735"}},"d0a48c93048b413f90b7acf04ec8ac8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5eb1fe57c614a96bf9ba7431d7e3bd9","placeholder":"​","style":"IPY_MODEL_a64a0c866a0b49a9a1c29a493cc78dde","value":"vocab.json: 100%"}},"fc16cdcd5fb74fa9b613614118f33fc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_61536d63b3a4487c990ce4c18b894168","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd182a0436084d85b07334937ba8d957","value":898823}},"9bb0d0c42cde46858a1ed4e599ad1341":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15742ad399d247a0853708ae7c161d02","placeholder":"​","style":"IPY_MODEL_86a750e8d1764d67a5815876dd4d22e8","value":" 899k/899k [00:00&lt;00:00, 15.4MB/s]"}},"c7f16cc055a849d4b403ba0cf3911735":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5eb1fe57c614a96bf9ba7431d7e3bd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a64a0c866a0b49a9a1c29a493cc78dde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"61536d63b3a4487c990ce4c18b894168":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd182a0436084d85b07334937ba8d957":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"15742ad399d247a0853708ae7c161d02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86a750e8d1764d67a5815876dd4d22e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06cbd33535b54e1aaf170a00ba00ed5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f33a8b2e26754149b6c1719aedca342c","IPY_MODEL_fea6a00a9d0748a3bdcb4b6a89a2250d","IPY_MODEL_c2da9007dcbd466b8ab6d13bf217f084"],"layout":"IPY_MODEL_242b02fbd60c47fc86d54f77d9c21692"}},"f33a8b2e26754149b6c1719aedca342c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e31a3aacb99e4a90bb06a9f85fb96b1e","placeholder":"​","style":"IPY_MODEL_3fdafc60432d484f8a7c52da5b13c48b","value":"merges.txt: 100%"}},"fea6a00a9d0748a3bdcb4b6a89a2250d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39d7365954df4eaa96092fcf44453fd2","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_748555c764ef43efb93ea1d7b771097c","value":456318}},"c2da9007dcbd466b8ab6d13bf217f084":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34c5e781e2364d4b8167070b0ed459c1","placeholder":"​","style":"IPY_MODEL_e323640e2a114893872ef15cb4782837","value":" 456k/456k [00:00&lt;00:00, 18.3MB/s]"}},"242b02fbd60c47fc86d54f77d9c21692":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e31a3aacb99e4a90bb06a9f85fb96b1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fdafc60432d484f8a7c52da5b13c48b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39d7365954df4eaa96092fcf44453fd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"748555c764ef43efb93ea1d7b771097c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"34c5e781e2364d4b8167070b0ed459c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e323640e2a114893872ef15cb4782837":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1ff85f2fd33461cb50f55bca725d995":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57ff8b7683314087985a4ba34a69648e","IPY_MODEL_6ead688d71ec41fc9d978cc1943c0e72","IPY_MODEL_80e4bd449a7f49d9b52aa5c877cce68c"],"layout":"IPY_MODEL_93d99e54d5e84fae91ec3c0b553c47e0"}},"57ff8b7683314087985a4ba34a69648e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca57ac7413ed468a9033a63d0b03b988","placeholder":"​","style":"IPY_MODEL_1f25ce4b431f4e5b95529cf938c77447","value":"tokenizer.json: 100%"}},"6ead688d71ec41fc9d978cc1943c0e72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b901246795541dea60e9adbcc07356f","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a4c83a6e3dd7448fa53d3cd53edee313","value":1355863}},"80e4bd449a7f49d9b52aa5c877cce68c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2117abfcaa74828ababe4b6994ed502","placeholder":"​","style":"IPY_MODEL_86b961bce4094812b4d52bcf5053319b","value":" 1.36M/1.36M [00:00&lt;00:00, 16.9MB/s]"}},"93d99e54d5e84fae91ec3c0b553c47e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca57ac7413ed468a9033a63d0b03b988":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f25ce4b431f4e5b95529cf938c77447":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b901246795541dea60e9adbcc07356f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4c83a6e3dd7448fa53d3cd53edee313":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a2117abfcaa74828ababe4b6994ed502":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86b961bce4094812b4d52bcf5053319b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a33c712e0d9b4109a1730c1f56471bb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bb6439a2a6a4424af88fcbc7b901977","IPY_MODEL_2c3b546917824d4395bd39b817f4d7e3","IPY_MODEL_0b317389b48c4aaea89b34e92cdd0ae1"],"layout":"IPY_MODEL_d5e20f60f288450faf8b799cda5ca722"}},"6bb6439a2a6a4424af88fcbc7b901977":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dedc53609584432905b2ffc0a0e1889","placeholder":"​","style":"IPY_MODEL_2dc2b0e778ca46dc921978407f27cad5","value":"model.safetensors: 100%"}},"2c3b546917824d4395bd39b817f4d7e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a910b28d238434c93376c1ad6fb1742","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_22f46ef8d93f49678f2946a8c7ad29dd","value":1421700479}},"0b317389b48c4aaea89b34e92cdd0ae1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a41bfc91f9fd47cdb4c491dac0fe941e","placeholder":"​","style":"IPY_MODEL_bf03d80e3d7b4dcebe095f387e171c1c","value":" 1.42G/1.42G [00:17&lt;00:00, 248MB/s]"}},"d5e20f60f288450faf8b799cda5ca722":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dedc53609584432905b2ffc0a0e1889":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2dc2b0e778ca46dc921978407f27cad5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9a910b28d238434c93376c1ad6fb1742":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22f46ef8d93f49678f2946a8c7ad29dd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a41bfc91f9fd47cdb4c491dac0fe941e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bf03d80e3d7b4dcebe095f387e171c1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DoA_y1P96s9j"},"outputs":[],"source":["\n","\n"]},{"cell_type":"code","source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6PeFR37cx4Um","executionInfo":{"status":"ok","timestamp":1763183616388,"user_tz":-330,"elapsed":12951,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"e22d38b3-57be-4944-e526-e0a9faba07ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.10.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=39d9615da3e2052adf200fb585917ed350b4157a1f2c823de880c92277d7b42a\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"NT80VkVzm6x-","executionInfo":{"status":"ok","timestamp":1763183958129,"user_tz":-330,"elapsed":91,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"8e30e747-453d-44e0-efbf-b05c05e9dcaf"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","import google.generativeai as genai\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-flash_cot_full_output.xlsx\"\n","FINAL_EVAL_JSON   = BASE_OUT / \"evaluation_COT.json\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key1.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Outputs will go to:\", BASE_OUT)\n","print(\"Gemini API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"gemini-2.5-flash\"\n","MAX_CHARS      = 2600          # safer for JSON stability\n","GLOBAL_MIN_GAP = 70            # seconds – conservative for free Colab\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"),\n","                  logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","logger.info(\"Starting Gemini CoT pipeline\")\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)           # remove URLs\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)      # remove emails\n","    t = re.sub(r'\\s+', ' ', t)                             # collapse spaces\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if Gemini mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GEMINI CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def gemini_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    cfg = {\"temperature\": temperature}\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = model.generate_content(prompt, generation_config=cfg)\n","            LAST_TS = time.time()\n","            return (getattr(resp, \"text\", \"\") or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Gemini call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Gemini call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. PROPER CoT PROMPTS FOR 4 TASKS\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational YouTube transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\n","\"\"\"\n","        out = gemini_call(prompt, temperature=0.18)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\n","\"\"\"\n","    out2 = gemini_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at multi-label topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\n","\"\"\"\n","\n","    out = gemini_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    # cleanup + normalization\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\n","\"\"\"\n","    out = gemini_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\n","\"\"\"\n","    out = gemini_call(prompt, temperature=0.22)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. GOLD LABEL HELPERS (FOR EVALUATION)\n","#####################################################################\n","def tokenize(text: str) -> List[str]:\n","    return [t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", str(text).lower()) if t not in STOPWORDS]\n","\n","def gold_topics_from_ref_summary(ref_sum: str) -> List[str]:\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","    for t in VALID_TOPICS:\n","        if t.lower() != \"other\" and t.lower() in text:\n","            matched.append(t)\n","    return matched or [\"Other\"]\n","\n","def gold_questions_from_ref_summary(ref_sum: str, k: int = 5) -> List[str]:\n","    base = [\n","        \"What is the main topic discussed in the video?\",\n","        \"Why is this topic important?\",\n","        \"How is the core concept explained?\",\n","        \"What example is mentioned in the content?\",\n","        \"What is the key conclusion of the video?\"\n","    ]\n","    return base[:k]\n","\n","def gold_concepts_from_ref_summary(ref_sum: str, k: int = 25) -> List[str]:\n","    toks = [t for t in tokenize(ref_sum) if len(t) > 3]\n","    freq: Dict[str, int] = {}\n","    for t in toks:\n","        freq[t] = freq.get(t, 0) + 1\n","    ranked = sorted(freq.items(), key=lambda x: (-x[1], -len(x[0])))\n","    return [w.title() for w, _ in ranked[:k]]\n","\n","\n","#####################################################################\n","# 10. EVALUATION\n","#####################################################################\n","\n","def evaluate_outputs(df_out: pd.DataFrame, df_ref: pd.DataFrame) -> Dict[str, Any]:\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    topic_subset_acc, topic_jaccard, topic_micro_f1 = [], [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        if idx not in df_ref.index:\n","            continue\n","\n","        ref_sum = df_ref.loc[idx, \"Reference Summary\"]\n","        ref_s = ref_sum or \"\"\n","        gen_s = row[\"summary\"] or \"\"\n","        pred_topic = row[\"topic_classification\"]\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        kc_text = row[\"key_concepts\"] or \"\"\n","\n","        # Summarisation metrics\n","        r = rouge.score(ref_s, gen_s)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_s.split()], gen_s.split(), smoothing_function=smooth)\n","        P, R, F1 = bert_score([gen_s], [ref_s], lang='en', verbose=False)\n","        sum_r.append(float(r))\n","        sum_b.append(float(b))\n","        sum_bert.append(float(F1.mean()))\n","\n","        # ---- TOPIC CLASSIFICATION (MULTI-LABEL) ----\n","        gold = gold_topics_from_ref_summary(ref_sum)                # list\n","        pred = [t.strip() for t in row[\"topic_classification\"].split(\",\") if t.strip()]\n","\n","        # subset accuracy (exact match of sets)\n","        subset_acc = 1.0 if set(pred) == set(gold) else 0.0\n","\n","        # jaccard index\n","        inter = len(set(pred) & set(gold))\n","        union = len(set(pred) | set(gold))\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # micro F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n","        micro_f1 = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0.0\n","\n","        topic_subset_acc.append(subset_acc)\n","        topic_jaccard.append(jaccard)\n","        topic_micro_f1.append(micro_f1)\n","\n","\n","        # Q&A metrics\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","        gold_qs = gold_questions_from_ref_summary(ref_sum, k=5)\n","        if qs:\n","            bleu_vals = [sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                         for g in gold_qs for q in qs]\n","            bleu_q = float(np.mean(bleu_vals))\n","        else:\n","            bleu_q = 0.0\n","        toks = [t for q in qs for t in q.split()]\n","        div_q = float(len(set(toks)) / len(toks)) if toks else 0.0\n","        ref_tokens = set(tokenize(ref_sum))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        ans_q = float(ans_count / len(qs)) if qs else 0.0\n","        qa_bleu.append(bleu_q)\n","        qa_div.append(div_q)\n","        qa_ans.append(ans_q)\n","\n","        # Key concepts metrics\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","        ref_concepts = [c.lower() for c in gold_concepts_from_ref_summary(ref_sum, k=25)]\n","        tp = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_concepts)])\n","        p_val = tp / 10.0\n","        r_val = tp / max(1, len(ref_concepts))\n","        f_val = 0.0 if (p_val + r_val) == 0 else 2 * p_val * r_val / (p_val + r_val)\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f_val)\n","\n","    summary = {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)) if sum_r else 0.0,\n","            \"BLEU\": float(np.mean(sum_b)) if sum_b else 0.0,\n","            \"BERTScore F1\": float(np.mean(sum_bert)) if sum_bert else 0.0\n","        },\n","\n","      \"Topic Classification\": {\n","            \"Subset Accuracy\": float(np.mean(topic_subset_acc)),\n","            \"Jaccard Index\": float(np.mean(topic_jaccard)),\n","            \"Micro F1\": float(np.mean(topic_micro_f1))\n","        },\n","\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)) if qa_bleu else 0.0,\n","            \"Diversity\": float(np.mean(qa_div)) if qa_div else 0.0,\n","            \"Answerability\": float(np.mean(qa_ans)) if qa_ans else 0.0\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)) if kc_p else 0.0,\n","            \"Recall@10\": float(np.mean(kc_r)) if kc_r else 0.0,\n","            \"F1@10\": float(np.mean(kc_f)) if kc_f else 0.0\n","        }\n","    }\n","    return summary\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE (AUTOSAVE + RESUME)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input file must contain a 'Reference Summary' column for evaluation.\")\n","\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already processed.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topic = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error processing row {idx}: {e}\")\n","            summary = \"\"\n","            topic = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        print(\"\\n--- SUMMARY ---\\n\", summary)\n","        print(\"\\n--- TOPIC ---\\n\", topic)\n","        print(\"\\n--- Q&A ---\\n\", qa_text)\n","        print(\"\\n--- KEY CONCEPTS ---\\n\", concepts_text)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topic),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave after each row\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Row {idx} saved to {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed. Final output saved to:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN + EVALUATE\n","#####################################################################\n","df_out = run_pipeline()\n","df_ref = pd.read_excel(INPUT_FILE)\n","\n","eval_summary = evaluate_outputs(df_out, df_ref)\n","with open(FINAL_EVAL_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(eval_summary, f, indent=2, ensure_ascii=False)\n","\n","print(\"\\n================== FINAL EVALUATION METRICS ==================\")\n","for task, metrics in eval_summary.items():\n","    print(f\"{task:25s} | \" + \" | \".join(f\"{k}={v:.3f}\" for k, v in metrics.items()))\n","\n","print(\"\\nCombined Excel:\", FINAL_OUTPUT_FILE)\n","print(\"Evaluation JSON:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["39675b1f73214b17a3748078102dd00a","3ea68d9d32af4394a62d024eda55aee3","acd22c6c7cf64304a4dbc7514a0d0551","abfe00b302ee48c685ee6b123239ec21","54709bb386f4482db48c8dc085018efc","0b92e454307d4d1aa56944e8cf56228b","09c7ede8c6aa4e198f000cee14fedf82","5d4440d1994b41bea065ad836ee7e5c1","7122974b484444398ec1711f4e53e474","4f13196e974d45a0884dccb284648ce7","3dcb048d66b249c1926a7aec62bdc9cb","b9b5179e430d433fa0755a36a116342a","bfb26da93042499a8530811bbac730f0","05ccf3ef90c84af7925ac9556fdad7de","65186fd6a6534cc7855c89a47e86c79f","5dbc075c319941bba3822af2fee4aa86","c77f1fa31f7945c8ba1d3170cb7ded5c","e536a1cb43064fabab997dd0bde56c88","1e6985a989994a48ad135437decd6c47","cb2231fbe10449278db6a92d0bd0659b","487e52a0d9a1435893b1f95b57efcab2","afd81b5c06f24257a0d71b95d2af6fe9","f3a22c9d56834b35850c66446cd175ce","d0a48c93048b413f90b7acf04ec8ac8c","fc16cdcd5fb74fa9b613614118f33fc4","9bb0d0c42cde46858a1ed4e599ad1341","c7f16cc055a849d4b403ba0cf3911735","c5eb1fe57c614a96bf9ba7431d7e3bd9","a64a0c866a0b49a9a1c29a493cc78dde","61536d63b3a4487c990ce4c18b894168","bd182a0436084d85b07334937ba8d957","15742ad399d247a0853708ae7c161d02","86a750e8d1764d67a5815876dd4d22e8","06cbd33535b54e1aaf170a00ba00ed5b","f33a8b2e26754149b6c1719aedca342c","fea6a00a9d0748a3bdcb4b6a89a2250d","c2da9007dcbd466b8ab6d13bf217f084","242b02fbd60c47fc86d54f77d9c21692","e31a3aacb99e4a90bb06a9f85fb96b1e","3fdafc60432d484f8a7c52da5b13c48b","39d7365954df4eaa96092fcf44453fd2","748555c764ef43efb93ea1d7b771097c","34c5e781e2364d4b8167070b0ed459c1","e323640e2a114893872ef15cb4782837","e1ff85f2fd33461cb50f55bca725d995","57ff8b7683314087985a4ba34a69648e","6ead688d71ec41fc9d978cc1943c0e72","80e4bd449a7f49d9b52aa5c877cce68c","93d99e54d5e84fae91ec3c0b553c47e0","ca57ac7413ed468a9033a63d0b03b988","1f25ce4b431f4e5b95529cf938c77447","0b901246795541dea60e9adbcc07356f","a4c83a6e3dd7448fa53d3cd53edee313","a2117abfcaa74828ababe4b6994ed502","86b961bce4094812b4d52bcf5053319b","a33c712e0d9b4109a1730c1f56471bb1","6bb6439a2a6a4424af88fcbc7b901977","2c3b546917824d4395bd39b817f4d7e3","0b317389b48c4aaea89b34e92cdd0ae1","d5e20f60f288450faf8b799cda5ca722","3dedc53609584432905b2ffc0a0e1889","2dc2b0e778ca46dc921978407f27cad5","9a910b28d238434c93376c1ad6fb1742","22f46ef8d93f49678f2946a8c7ad29dd","a41bfc91f9fd47cdb4c491dac0fe941e","bf03d80e3d7b4dcebe095f387e171c1c"]},"id":"LA92vI77m7B2","executionInfo":{"status":"ok","timestamp":1763202592068,"user_tz":-330,"elapsed":14237334,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"0d373acc-9d9f-4910-c71c-662c12457e62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Outputs will go to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash\n","Gemini API key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","--- SUMMARY ---\n"," Reinforcement Learning with Human Feedback (RLHF) is a framework that integrates human input to enhance and accelerate the training of reinforcement learning algorithms. This approach leverages human guidance, acting as a mentor, to enable agents to learn more efficiently and produce outcomes aligned with human preferences, as exemplified in contexts like grid world navigation. A prominent real-world application of RLHF is its use in models such as ChatGPT. Here, the process involves two primary stages: first, a reward model is trained using human rankings of various AI-generated responses to accurately assess answer quality. Subsequently, this trained reward model, combined with an algorithm like Proximal Policy Optimization, fine-tunes the language model by iteratively scoring generated responses and using these scores to drive learning, thereby improving the model's capacity to generate high-quality, human-aligned outputs.\n","\n","--- TOPIC ---\n"," ['Reinforcement Learning', 'Generative AI', 'Deep Learning']\n","\n","--- Q&A ---\n"," Q: What is the primary benefit of integrating human feedback into a reinforcement learning algorithm?\n","A: Integrating human feedback allows the reinforcement learning algorithm to learn faster and produce responses that are more human-favored.\n","Q: According to the transcript, which reinforcement learning algorithms can be used along with human feedback?\n","A: Q-learning, DQ-learning, and Proximal Policy Optimization (PPO) can all be used along with human feedback.\n","Q: In the context of Frank's grid world, how did human feedback specifically help Frank learn?\n","A: The human mentor nudged Frank in the direction they thought was correct, which helped Frank learn faster and reach the good spot more efficiently than learning without human intervention.\n","Q: What are the two main parts of how ChatGPT makes use of reinforcement learning through human feedback?\n","A: The first part is to train a reward model to be a human advisor to ChatGPT. The second part is to use this reward model along with an algorithm called Proximal Policy Optimization (PPO) to fine-tune ChatGPT.\n","\n","--- KEY CONCEPTS ---\n"," reinforcement learning with human feedback, proximal policy optimization, reward model, grid world, Q-learning, DQ learning, fine-tune, back propagation\n","Row 0 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","--- SUMMARY ---\n"," This tutorial series segment focuses on applying kernels to Support Vector Machines (SVMs), primarily using CVXopt for educational visualization of kernel injection and soft margin effects in nonlinear classification. While CVXopt is not practical for real-world SVMs, it effectively illustrates the underlying quadratic programming mechanism, which minimizes a specific equation subject to constraints. The implementation details include various kernel functions—Gaussian, Polynomial, and Linear—and the role of the 'C' parameter in defining hard versus soft margins. The fitting process involves solving for optimal alphas, identifying support vectors, and calculating the intercept, with prediction based on data projection. Demonstrations cover scenarios from linearly separable to non-linearly separable and overlapping data, highlighting the necessity of kernels and soft margins. Future topics will conclude the SVM series, examine scikit-learn's `SupportVectorClassifier` parameters, and address extending binary SVMs to multi-class problems.\n","\n","--- TOPIC ---\n"," ['Machine Learning']\n","\n","--- Q&A ---\n"," Q: What is the primary focus of part 32 of the machine learning tutorial?\n","A: Part 32 of the tutorial focuses on working with CVXopt and applying kernels to Support Vector Machines to observe their impact and visualize the results.\n","Q: Who is credited for the example code used in this tutorial?\n","A: The example code was obtained from Matthew Blondell's GitHub, and it also models information from Christopher Bishop's 'Pattern Recognition and Machine Learning' book.\n","Q: What is the main reason for using CVXopt in this tutorial, and what alternative is generally recommended for implementing a Support Vector Machine?\n","A: CVXopt is used purely for educational purposes to directly see the impact of a kernel and where it's injected into the formal Support Vector Machine. However, for practical implementation, libsvm is almost certainly recommended.\n","Q: What specific aspects of Support Vector Machines will be visualized in this tutorial?\n","A: The tutorial will visualize the impact of kernels, nonlinear aspects, and the soft margin.\n","Q: What is the basic structure of the quadratic programming solver equation mentioned in the transcript?\n","A: The solver minimizes `1/2 x^T P * X + q^T * X` subject to the constraints `G(X) <= H` and `A * X = B`.\n","\n","--- KEY CONCEPTS ---\n"," CVX opt, kernels, support Vector machine, lib svm, soft margin, quadratic programming solver, scikit-learn, nonlinear classification\n","Row 1 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","--- SUMMARY ---\n"," Prompts serve as foundational inputs in prompt engineering, providing essential context and constraints for large language models (LLMs) like ChatGPT to generate specific text outputs. Understanding diverse prompt types—including questions, statements, or those with explicit limitations—is critical for selecting the most effective input to achieve high-quality results. Key features such as prompt length, specific language, and embedded context or constraints dictate the expected output and desired execution method. The lecture emphasizes that precisely defined prompts yield more accurate and controlled outputs, exemplified by specifying a one-word answer for factual queries or a word count for essays. Furthermore, prompt deconstruction, which involves dissecting a prompt into its constituent elements, facilitates a clearer identification of its core requirements and constraints, thereby optimizing comprehension and subsequent model performance.\n","\n","--- TOPIC ---\n"," ['Prompt Engineering', 'Generative AI', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: In prompt engineering, what are \"prompts\" and what is their primary function?\n","A: Prompts are the inputs given to prompt engineering models. They serve as the starting point for generating text outputs, providing context and constraints for large language models.\n","Q: According to the transcript, what are the key features that define a prompt?\n","A: The key features of a prompt include its length, the specific language used, and any context or constraints that are included. These can be summarized as 'what you expect' and 'how you want it to be done'.\n","Q: How does adding a specific constraint, such as 'give a one-word answer,' to a prompt like 'What is the capital of France?' affect the output from a large language model?\n","A: Adding such a constraint ensures that the large language model provides only the specific, constrained output (e.g., 'Paris'), rather than additional information or context that it might otherwise include.\n","Q: What is the process of 'deconstructing a prompt' and what is its purpose?\n","A: Deconstructing a prompt is the process of breaking it down into individual components to better understand its key features and constraints. This involves looking at things like the specific language used, the requirements, and any constraints such as output format or word count.\n","Q: Given the prompt: 'Write a blog on this theme in 1000 words and use SEO to optimize it,' identify the main requirement and the constraints.\n","A: The main requirement is to 'write a blog.' The constraints are to 'write in 1000 words' and 'use SEO to optimize it'.\n","\n","--- KEY CONCEPTS ---\n"," prompt engineering, prompts, large language models, text outputs, prompt types, prompt constraints, prompt deconstruction\n","Row 2 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","--- SUMMARY ---\n"," {\"generated_summary\":\"AI agents are characterized as autonomous problem-solvers capable of independent decision-making, distinguishing them from simpler AI components. These agents leverage \"tools,\" which are specific functions or abilities, to execute tasks. A prominent method for constructing such agents is the ReAct pattern, an acronym for Reasoning plus Acting. This pattern emulates human thought processes through an iterative cycle: \"Think\" for problem analysis, \"Action\" for selecting a step or tool, \"Action Input\" for providing necessary arguments, and \"Observe\" for evaluating the tool's output. This continuous cycle enables the agent to refine its approach until a solution is achieved. Fundamentally, an AI agent integrates the reasoning capabilities of a Large Language Model (LLM) with a suite of tools to attain complex objectives effectively.\"}\n","\n","--- TOPIC ---\n"," ['Agentic AI', 'Artificial Intelligence', 'LangChain']\n","\n","--- Q&A ---\n"," Q: What is the primary difference between AI agents and chains/routers, according to the transcript?\n","A: AI agents are capable of thinking on their own and making autonomous decisions, deciding for themselves what steps to take, whereas chains and routers follow specific instructions.\n","Q: What does the 'React' in 'React Agent Pattern' stand for, and what human cognitive process does it mimic?\n","A: The 'React' in 'React Agent Pattern' stands for Reasoning + Acting. It mimics how human beings think.\n","Q: Describe the main steps an LLM goes through in the React Agent Pattern cycle.\n","A: In the React Agent Pattern, the LLM first 'thinks' about the problem, then decides on an 'action' (potentially using a tool), provides 'action input' (arguments for the tool), and finally 'observes' the output of the tool's execution. This cycle repeats until the problem is solved.\n","Q: How are 'tools' defined in the context of AI agents, and what is their purpose?\n","A: Tools are specific functions that agents can use to complete tasks. They are special abilities given to AI, such as a calculator, search engine, or calendar tool, to help them solve problems.\n","Q: What two core components combine to form an AI agent, as described in the transcript?\n","A: An AI agent is formed by combining the reasoning ability of an LLM (which acts as the brain) with specific tools.\n","\n","--- KEY CONCEPTS ---\n"," AI agents, Autonomous decisions, Agent tools, ReAct agent pattern, Reasoning plus Acting, Think-Action-Observation loop, Action input, Large Language Model (LLM), LangChain\n","Row 3 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","--- SUMMARY ---\n"," The operational flow of a reflection agent system, designed for iterative refinement, is detailed, exemplified by its application in optimizing a viral tweet. Internal mechanisms and component interactions are traced using `smith.chain` (Lsmith), a visualization tool that monitors multi-step LangChain project execution by automatically streaming operational data after API key setup. The core workflow involves a generation agent drafting content, which a separate critiquing agent evaluates, providing detailed feedback. This feedback prompts the generation agent to revise the content iteratively, forming a generate-reflect-revise cycle. This iterative process, demonstrating how reflection agents facilitate deep thinking and refinement, leads to a highly polished final output. The methodology is broadly applicable to various complex tasks beyond simple tweet generation, highlighting its utility in achieving optimized outcomes through structured self-correction.\n","\n","--- TOPIC ---\n"," ['Agentic AI', 'LangChain', 'Mlops']\n","\n","--- Q&A ---\n"," Q: What is the main activity that will be performed in this section?\n","A: The main activity will be tracing the reflection agent system.\n","Q: What is the purpose of tracing the reflection agent system?\n","A: The purpose is to understand exactly what is happening where and how both systems are working together.\n","Q: What is the ultimate goal that the systems are working together to deliver?\n","A: The ultimate goal is to deliver a final refined viral tweet.\n","Q: Which specific website will be used to trace the system?\n","A: The website smith.chain will be used to trace the system.\n","\n","--- KEY CONCEPTS ---\n"," reflection agent system, system tracing, smith. chain, viral tweet generation, system refinement\n","Row 4 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","--- SUMMARY ---\n"," The integration of LangChain with OpenAI's chat models is established through the installation of `langchain-openai` and the initialization of the `ChatOpenAI` class. Users select specific models, such as `gpt-4o` or the more budget-friendly `gpt-3`, considering associated costs. API interaction is primarily managed via the `llm.invoke()` method. Secure authentication is critical, involving the storage of OpenAI API keys in `.env` files and their loading through the `python-dotenv` package. The system provides comprehensive API responses, encompassing content, arguments, and metadata, with functionality to extract only the pertinent `content` property. Troubleshooting guidance includes verifying a minimum $5 balance in OpenAI billing settings. The framework supports both single-turn prompts and the advanced transmission of full conversation histories to LLMs, significantly enhancing contextual awareness and the relevance of generated responses.\n","\n","--- TOPIC ---\n"," ['LangChain', 'Artificial Intelligence', 'Python Programming']\n","\n","--- Q&A ---\n"," Q: What is the command used to install the LangChain package for working with OpenAI chat models?\n","A: The command to install the package is `pip install langchain-openai`.\n","Q: Which specific class needs to be imported from the `langchain_openai` module to initialize an OpenAI chat model?\n","A: The `ChatOpenAI` class needs to be imported from the `langchain_openai` module.\n","Q: When initializing the `ChatOpenAI` model, what keyword parameter is used to specify the desired OpenAI model, and what are two examples of models mentioned?\n","A: The `model_name` keyword parameter is used to specify the desired model. Two examples mentioned are `gpt-4o` and `gpt-3`.\n","Q: According to the transcript, what is a primary reason someone might choose to use `gpt-3` instead of `gpt-4o`?\n","A: Someone might choose `gpt-3` because `gpt-4o` is the latest and most advanced model, which can be more expensive, especially if they are 'a little short on cash'.\n","\n","--- KEY CONCEPTS ---\n"," LangChain chat models, OpenAI APIs, chat OpenAI class, model initialization, GPT-4o, GPT-3\n","Row 5 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","--- SUMMARY ---\n"," Python's `sort()` method demonstrates distinct behaviors based on the data types present in a list. For lists containing only strings, `sort()` arranges words beginning with uppercase letters alphabetically first, followed by words starting with lowercase letters, also sorted alphabetically. Reversing this sort preserves the uppercase-then-lowercase grouping but inverts the alphabetical order within each group. When a list comprises a mix of strings and numbers, `sort()` prioritizes numbers, placing them at the beginning in numerical order, before sorting the strings alphabetically. Reversing a mixed-type sorted list positions the numbers at the end. A comprehensive understanding of these sorting conventions is fundamental for effective and predictable list manipulation in Python.\n","\n","--- TOPIC ---\n"," ['Python Programming']\n","\n","--- Q&A ---\n"," Q: How does Python's `sort()` method arrange strings in a list when some start with an uppercase letter and others with a lowercase letter?\n","A: The `sort()` method puts words that have a capital uppercase letter first and sorts them alphabetically, and then it sorts the ones with the lowercase first letter alphabetically.\n","Q: What is the order of elements when a list containing both strings and numbers is sorted using Python's `sort()` method?\n","A: When a list contains both strings and numbers, the `sort()` method puts the numbers first, followed by the strings.\n","Q: If a list of strings with mixed-case starting letters is sorted in reverse, what is the resulting order?\n","A: When sorted in reverse, the list will have the lowercase letters in reverse alphabetical order first, followed by the words with uppercase letters at the start, also in reverse alphabetical order.\n","Q: What action might be necessary if a user wants a single, continuous alphabetical sort for a list of strings, ignoring initial letter case?\n","A: If a user wants a single, continuous alphabetical sort, they might have to make sure all strings are either lowercase or all uppercase before sorting.\n","\n","--- KEY CONCEPTS ---\n"," Python list sorting, sort method, alphabetical sorting, uppercase letter precedence, lowercase letter sorting, reverse alphabetical order, mixed-type list sorting, number precedence in sorting\n","Row 6 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","--- SUMMARY ---\n"," This analysis investigates the optimal allocation of decision-making between humans and Artificial Intelligence (AI), particularly in contexts like fraud detection. It contrasts AI's high success at extreme confidence levels with its unreliability when uncertain, against humans' more consistent performance, especially in complex or ambiguous scenarios where contextual integration is vital. Augmented intelligence, which integrates human and AI strengths, is identified as the most effective approach, yielding superior success rates. However, its implementation requires careful consideration of human cognitive biases, notably automation bias. The presentation of AI recommendations is crucial; optional display, allowing initial human assessment, mitigates undue influence more effectively than forced display. Interestingly, displaying AI accuracy can paradoxically reduce human trust. The discussion underscores the importance of quantifying the optimal decision-maker for specific tasks and designing augmented intelligence interfaces to minimize bias, ultimately aiming for synergistic human-AI collaboration to enhance decision outcomes.\n","\n","--- TOPIC ---\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","--- Q&A ---\n"," Q: What is the main problem financial analysts face in the fraud detection system described?\n","A: Financial analysts are overwhelmed by thousands of events generated each day, with 90 percent of those alerts being false positives.\n","Q: According to the transcript, what do the X and Y axes represent in the graph used to answer the question, 'Is this a real alert?'\n","A: The Y-axis tracks the success rate of a prediction, and the X-axis tracks the confidence score of that prediction.\n","Q: How does an Artificial Intelligence (AI) system typically perform when it has very low or very high confidence scores, according to the provided text?\n","A: When an Artificial Intelligence (AI) system has very low confidence scores (predicting 'not a real alert') or very high confidence scores (predicting 'a real alert'), its predictions are correlated to a high success rate.\n","Q: In what specific situation does the transcript suggest a human is likely to outperform an Artificial Intelligence (AI) in making a decision?\n","A: A human is likely to do a better job than an Artificial Intelligence (AI) at a 50 percent confidence level, which is when the AI is unsure about a given prediction.\n","\n","--- KEY CONCEPTS ---\n"," Artificial Intelligence (AI), fraud detection system, false positives, confidence score, success rate, AI performance curve, human performance curves, holistic curves, human bias\n","Row 7 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","--- SUMMARY ---\n"," Demetrius Case from Google Cloud AI introduces new Vertex AI APIs designed to accelerate and enhance generative AI application development, particularly for enterprise use cases demanding grounded and accurate responses. Key offerings include a Document Understanding API for processing complex formats, an enhanced Embedding API, and improvements to Vector Search for scalable retrieval. A Ranking API helps prioritize relevant information, while the Grounded Generation API, powered by Gemini, produces well-grounded and cited answers. Additionally, a Check Grounding API fact-checks statements against provided evidence. These APIs leverage Google's extensive expertise and technology from planet-scale applications, offering high quality and simplifying development by addressing common technical challenges and integrating with popular frameworks like LangChain.\n","\n","--- TOPIC ---\n"," ['Generative AI', 'Artificial Intelligence', 'LangChain']\n","\n","--- Q&A ---\n"," Q: What is the primary challenge Demitrius identifies for developers building generative applications for enterprises, and how do the new Vertex AI APIs address it?\n","A: The primary challenge is 'grounding,' which involves making sure generative applications can reliably access the correct Enterprise data to produce accurate and consistent responses. The new Vertex AI APIs and improvements aim to solve recurring technical challenges, thereby lightening the developers' load and allowing them to focus on unique aspects of their use cases.\n","Q: Name at least three of the six new Vertex AI APIs or improvements discussed in the talk.\n","A: Three of the new Vertex AI APIs or improvements are: the Document Understanding API, significant improvements to the Embedding API, and enhancements to Vector Search for hybrid searches. Other mentioned APIs include the Ranking API, Grounded Generation API, and Check Grounding API.\n","Q: How does the Ranking API improve the quality of answers produced by a large language model (LLM)?\n","A: The Ranking API works by evaluating how well each retrieved search result answers a specific question. This process helps to 'bubble up' the most relevant information, which directly improves the quality of the answers that the LLM model will ultimately produce.\n","Q: According to Demitrius, what are the two main standout features that differentiate these new Vertex AI APIs?\n","A: The two main standout features are quality, ensuring the APIs produce correct results, and the embedding of Google's extensive know-how. This includes leveraging years of investment in areas like Document AI and utilizing technology from planet-scale applications such as Google Search, YouTube, and Google Ads.\n","\n","--- KEY CONCEPTS ---\n"," Vertex Artificial Intelligence (AI), generative applications for Enterprises, grounding, document understanding API, embedding API, Vector search, hybrid search, ranking API, grounded generation API\n","Row 8 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","--- SUMMARY ---\n"," The Singular Value Decomposition (SVD) is a critical tool in science and engineering, relying on unitary matrices U and V, and singular values Sigma. Unitary matrices are crucial for preserving vector lengths and angles, effectively acting as rotations or coordinate transformations, exemplified by the Fourier transform. For complex data, the complex conjugate transpose is used. Geometrically, the SVD transforms a unit sphere in one vector space into an ellipsoid in another. The singular vectors (U or V) determine the ellipsoid's orientation, while the singular values (Sigma) define the lengths of its principal axes, indicating data stretching or compression. This transformation can map between spaces of different dimensions if the input matrix X is rectangular, offering a powerful geometric understanding of how data variance is explained.\n","\n","--- TOPIC ---\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","--- Q&A ---\n"," Q: What defines a unitary matrix, and what are its primary effects on vectors?\n","A: A unitary matrix U satisfies the condition that U U transpose equals U transpose U, resulting in an identity matrix. Unitary transformations preserve the angles between any two vectors and the lengths of vectors, essentially rotating them without altering their intrinsic geometric relationships.\n","Q: Describe the geometric transformation that occurs when a matrix X multiplies a sphere of unit vectors.\n","A: When a matrix X multiplies all unit length vectors on a sphere in its row space (RM), it maps them into an ellipsoid in an N-dimensional space (the column space of X). The lengths of the principal axes of this ellipsoid are given by the singular values of X, and the orientation of the ellipsoid is determined by the left singular vectors (U) of X.\n","Q: When dealing with complex-valued data in the context of SVD, what operation replaces the standard transpose, and what does it entail?\n","A: For complex-valued data, the standard transpose is replaced by the complex conjugate transpose, denoted by X*. This operation involves both transposing the matrix and taking the complex conjugate of every single element.\n","Q: How does the transcript relate unitary transformations to the Fourier transform?\n","A: The Fourier transform is presented as a famous and widely used example of a unitary transformation. Both unitary transformations and the Fourier transform act as coordinate transformations that rotate vectors into a new representation where things might become simpler, while preserving the angles between vectors and their lengths.\n","Q: In the context of the economy SVD (X = U hat Sigma hat V transpose), why does U hat transpose U hat equal the identity, but U hat U hat transpose does not?\n","A: In the economy SVD, U hat is a rectangular matrix, specifically the first M columns of U. Due to its rectangular nature, only U hat transpose U hat equals the identity matrix. U hat U hat transpose would not be the identity because U hat is not a square matrix in this configuration.\n","\n","--- KEY CONCEPTS ---\n"," Singular Value Decomposition (SVD), unitary matrices, economy size SVD, complex conjugate transpose, Fourier transform, inner product preservation, ellipsoid mapping, singular values, left singular vectors\n","Row 9 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","--- SUMMARY ---\n"," This content introduces Google Gemini 1.5 Pro, highlighting its advanced features for building generative AI applications. A key innovation is its 1 million multimodal token context window, enabling the processing of extensive data, including text and images. The tutorial demonstrates obtaining a free API key and configuring the `google-generativeai` Python package. Gemini 1.5 Pro consolidates text and vision capabilities into a single model, simplifying multimodal application development. Demonstrations showcase its ability to analyze a 402-page transcript, extracting specific information and citing time codes, and to interpret abstract visual inputs, such as identifying events from a drawing. The `model.generate_content` method facilitates diverse text and image queries, with options for streaming responses. This model represents a substantial improvement over previous iterations, streamlining the creation of sophisticated AI solutions.\n","\n","--- TOPIC ---\n"," ['Generative AI', 'Artificial Intelligence', 'Deep Learning']\n","\n","--- Q&A ---\n"," Q: What is the main topic of the video described in the transcript?\n","A: The main topic of the video is how to build generative Artificial Intelligence (AI) powered applications using Google Gemini Pro 1.5.\n","Q: What does it mean for Google Gemini Pro 1.5 to be a 'multimodal' model?\n","A: For Google Gemini Pro 1.5 to be a 'multimodal' model means it is able to work with both text and images.\n","Q: What practical aspects will the video cover after the initial demo?\n","A: After the initial demo, the video will cover hands-on application by running code and playing with both images and text, as well as discussing how to create the API key and use it.\n","Q: What experimental feature is mentioned in relation to Gemini 1.5 at the end of the transcript?\n","A: The experimental feature mentioned in relation to Gemini 1.5 is 'long context understanding'.\n","\n","--- KEY CONCEPTS ---\n"," generative Artificial Intelligence, Google Gemini Pro 1.5, multimodal, API key, long context understanding\n","Row 10 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","--- SUMMARY ---\n"," The evaluation and testing of prompt engineering models are critical processes, relying on a comprehensive understanding of key performance metrics. Standard evaluation matrices include perplexity, which measures a language model's prediction accuracy with lower values indicating superior performance, and accuracy, assessing the correctness of generated outputs. Human evaluation is also indispensable for gauging the qualitative aspects of responses. Following initial assessment, models undergo an iterative cycle of debugging and improvement, involving error analysis and fine-tuning. Furthermore, rigorous testing across diverse datasets and tasks is essential to ensure robust generalization capabilities. This continuous refinement process, often supported by tools such as visualization and cross-validation, is fundamental for maintaining and enhancing model performance.\n","\n","--- TOPIC ---\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","--- Q&A ---\n"," Q: What are the three commonly used matrices for evaluating prompt engineering models mentioned in the transcript?\n","A: The three commonly used matrices are perplexity, accuracy, and human evaluation.\n","Q: Explain what perplexity measures in the context of evaluating language models.\n","A: Perplexity measures how well a language model predicts a sequence of words. A lower perplexity indicates a better language model.\n","Q: What is a common technique for debugging and improving prompt engineering models after they have been evaluated?\n","A: A common technique is to analyze the generated responses and identify common errors or patterns, which allows for fine-tuning the models and making improvements.\n","Q: Why is it considered an important step to test prompt engineering models on different datasets or tasks?\n","A: Testing on different data or tasks is an important step because it helps determine the model's ability to generalize on new or unseen data.\n","Q: According to the transcript, what does 'accuracy' measure when evaluating prompt engineering models?\n","A: Accuracy measures how many of the generated responses are correct.\n","\n","--- KEY CONCEPTS ---\n"," prompt engineering models, perplexity, accuracy, human evaluation, large language model, fine-tune the models, cross validation, generalize on new or unseen data\n","Row 11 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","--- SUMMARY ---\n"," This segment delineates the progression from Generative AI to AI Agents and Agentic AI, highlighting increasing complexity and capability. Generative AI, often based on Large Language Models (LLMs), creates novel content from learned data but is limited by knowledge cutoffs. AI Agents advance this by integrating external tools, such as APIs, and memory, enabling them to execute specific, narrow tasks through autonomous decision-making, thinking, and acting beyond basic question-answering. The most sophisticated stage, Agentic AI, involves one or more AI agents operating autonomously to achieve complex, multi-step objectives. These systems exhibit advanced multi-step reasoning, planning, and coordination, frequently leveraging other agents and tools to accomplish sophisticated goals like comprehensive travel planning.\n","\n","--- TOPIC ---\n"," ['Generative AI', 'Agentic AI', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is Generative Artificial Intelligence (AI) and what kind of content can it create?\n","A: Generative Artificial Intelligence (AI) is an Artificial Intelligence that can create new content, such as text, images, or video, based on patterns learned from existing data.\n","Q: How does an Artificial Intelligence (AI) agent differ from a simple Large Language Model (LLM) that has access to external tools?\n","A: While an LLM with tool access can fetch information (like flight prices), an AI agent is a program that takes input, thinks, and acts to complete a task using tools, memory, and knowledge, demonstrating autonomy and independent decision-making for that task.\n","Q: Describe a complex task that an Agentic Artificial Intelligence (AI) system can perform, as illustrated by the travel planning example in the transcript.\n","A: An Agentic AI system can handle complex, multi-step tasks like planning a 7-day trip to New Delhi in May with specific criteria (sunny weather, flight budget, no layover). It can access multiple APIs (e.g., weather, travel), perform multi-step reasoning (find sunny days, then search flights), and even coordinate with other agents (like an immigration AI agent to check visa eligibility) before booking.\n","Q: What is the relationship between Generative AI and Agentic AI systems?\n","A: Generative Artificial Intelligence (AI) is a core component of Agentic Artificial Intelligence (AI) systems. An Agentic AI system will have Generative AI, such as an LLM model, as part of its core.\n","\n","--- KEY CONCEPTS ---\n"," Generative Artificial Intelligence, Large Language Model, Knowledge cutoff date, Artificial Intelligence agent, Tool usage, Autonomous decision making, Multi-step reasoning, Multi-step planning, Agentic Artificial Intelligence\n","Row 12 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","--- SUMMARY ---\n"," Covariance is introduced as a fundamental statistical concept essential for data preprocessing and analysis, quantifying the linear relationship between two random variables, such as house size and price. The video explains the covariance formula, noting its structural similarity to the variance equation when a variable is compared to itself. A positive covariance indicates a direct relationship, meaning both variables tend to increase or decrease together. Conversely, a negative covariance signifies an inverse relationship, where one variable increases as the other decreases. While covariance effectively determines the direction of this relationship, it does not measure its strength. This limitation highlights the need for further statistical measures like the Pearson correlation coefficient, which will be discussed subsequently to provide a comprehensive understanding of relationship strength.\n","\n","--- TOPIC ---\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","--- Q&A ---\n"," Q: What is the primary purpose of covariance in data analysis, as described in the transcript?\n","A: Covariance is a very important topic in data pre-processing or data analysis because it helps to quantify the relationship between two random variables (features) in a dataset.\n","Q: According to the transcript, what does a positive covariance value indicate about the relationship between two random variables, X and Y?\n","A: A positive covariance value indicates that if variable X increases, variable Y will also increase. The transcript illustrates this with the example of house size and price.\n","Q: How does the transcript explain the relationship between the covariance of a variable with itself and its variance?\n","A: The transcript states that the covariance of a variable X with itself (Cov(X, X)) is nothing but the variance of X. This is demonstrated by showing that the covariance formula simplifies to the variance formula when both variables are the same.\n","Q: What is the mathematical formula for calculating the covariance between two random variables, X and Y, as provided in the transcript?\n","A: The formula for covariance (Cov(X, Y)) is given as: (1/n) * Σ[ (X_i - μ_X) * (Y_i - μ_Y) ] from i=1 to n, where μ_X is the mean of X and μ_Y is the mean of Y.\n","Q: What is a significant limitation of covariance that the transcript mentions, leading to the discussion of other techniques?\n","A: A significant limitation of covariance is that while it indicates whether the relationship between variables is positive or negative (e.g., X increasing, Y increasing), it does not quantify *how much* positive or negative that relationship is. This disadvantage leads to the use of techniques like Pearson correlation coefficient.\n","\n","--- KEY CONCEPTS ---\n"," Covariance, Variance, Random variables, Quantifying relationships, Covariance equation, Positive covariance, Negative covariance, Pearson correlation coefficient\n","Row 13 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","--- SUMMARY ---\n"," In reinforcement learning (RL), objectives are defined as time-constrained goals, with the primary aim for an agent to learn an optimal policy that maximizes a numerical reward signal, specifically the cumulative reward over time. Agents achieve this through trial-and-error, interacting with an environment, making decisions based on observations, and updating their policy based on feedback from reward signals. Reward assignment varies by task structure; for episodic tasks like Tic-Tac-Toe, rewards are typically provided at the task's conclusion (e.g., +1 for winning, -1 for losing). In contrast, continuous tasks, such as stock market trading, employ parameterized reward functions to reflect performance over a period, seeking to maximize the expected cumulative reward using measures like profit/loss or risk-adjusted ratios.\n","\n","--- TOPIC ---\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is the primary objective of a reinforcement learning agent?\n","A: The objective of a reinforcement learning agent is to learn the optimal policy that maximizes a numerical reward signal, specifically the cumulative reward over time.\n","Q: How does a reinforcement learning agent learn to achieve its objective?\n","A: An RL agent learns through trial and error. It explores the environment by taking actions, observes the resulting state and reward, and updates its policy accordingly based on the feedback provided by the reward signal.\n","Q: In the context of a Tic-Tac-Toe game, how are rewards defined for a reinforcement learning agent, and what is its ultimate goal?\n","A: For a Tic-Tac-Toe game, an RL agent receives a positive reward (+1) for winning, a negative reward (-1) for losing, and no reward (0) for drawing or continuing the game. The agent's ultimate goal is to win the game by placing three marks of the same kind (X or O) in a row, column, or diagonal.\n","Q: How is the objective parameterized for a continuous reinforcement learning task like stock market trading?\n","A: To parameterize the objective for continuous tasks like stock market trading, a reward function must be defined. This function is a mathematical expression that assigns a numerical value to each state or action, reflecting the trading goal. Examples include using profit or loss from each trade, or risk-adjusted measures such as the Sharpe ratio or Sortino ratio. The agent's objective is to maximize the expected cumulative reward over time.\n","Q: What are the two main types of tasks mentioned in the transcript, and how do they differ regarding their endpoints?\n","A: The two main types of tasks are episodic and continuous. Episodic tasks have a fixed end point, like a Tic-Tac-Toe game. Continuous tasks have no fixed end point, such as stock market trading.\n","\n","--- KEY CONCEPTS ---\n"," reinforcement learning, optimal policy, reward signal, agent-environment interaction, expected cumulative reward, episodic and continuous tasks, trial and error learning, reward function, value-based methods, policy-based methods\n","Row 14 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","--- SUMMARY ---\n"," Python dictionaries represent versatile data structures, consisting of immutable key-value pairs, referred to as items, which are enclosed within curly brackets. Their creation can be achieved either through direct declaration or by utilizing the `dict()` constructor, frequently involving the zipping of two distinct lists. Core operations encompass accessing specific values via their corresponding keys using square bracket notation, modifying pre-existing values, and removing particular items with the `del` keyword. Furthermore, methods such as `dict.items()`, `dict.keys()`, and `dict.values()` facilitate the retrieval of all items, keys, or values, respectively, while the `len()` function provides the dictionary's current size. Consequently, dictionaries prove exceptionally valuable for mapping diverse datasets and serve as a fundamental component in various data science applications.\n","\n","--- TOPIC ---\n"," ['Python Programming', 'Data Science']\n","\n","--- Q&A ---\n"," Q: What are the fundamental components of a Python dictionary, and how are they structured?\n","A: A Python dictionary consists of key-value pairs, with each pair referred to as an item. Items are separated by commas, and the key and value within a pair are separated by a colon. Dictionaries are declared within curly brackets.\n","Q: What is a crucial constraint regarding the data type used for keys in a Python dictionary?\n","A: The key in a Python dictionary has to be immutable, meaning it cannot change. Examples of valid key types include strings, numbers, or tuples, but not lists.\n","Q: Name three functions mentioned in the transcript that can be used to inspect the contents of a dictionary.\n","A: Three functions mentioned are `d.items()` to show all key-value pairs, `d.keys()` to get a list of keys, and `d.values()` to get a list of values.\n","Q: How can you create a new dictionary from two existing lists, one intended for keys and the other for values?\n","A: You can create a new dictionary from two lists by using the `dict()` function in combination with the `zip()` function. The `zip()` function pairs corresponding elements from the two lists, and then `dict()` converts these pairs into a dictionary.\n","Q: Explain how to access, change, and delete an entry in a dictionary using a specific key.\n","A: To access the value associated with a key, you use the dictionary name followed by the key in square brackets (e.g., `dictionary_name[key]`). To change a value, you assign a new value to the key (e.g., `dictionary_name[key] = new_value`). To delete an entry, you use the `del` function with the dictionary name and key (e.g., `del dictionary_name[key]`).\n","\n","--- KEY CONCEPTS ---\n"," Python dictionary, key-value pairs, immutable key, dictionary methods, dict() constructor, key-based value access, modifying dictionary values, deleting dictionary entries\n","Row 15 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","--- SUMMARY ---\n"," Artificial intelligence (AI) significantly enhances cybersecurity, enabling faster threat detection and improved security posture. Organizations leveraging AI and automation experienced a 108-day reduction in the average time to identify and contain data breaches, as highlighted by IBM's 2023 report. A critical application of AI is User Behavior Analytics (UBA), which utilizes machine learning to precisely detect and respond to costly insider threats, averaging $4 million per incident. UBA analyzes user patterns, identifies anomalies, and pinpoints potential risks. When integrated with Security Information and Event Management (SIEM) solutions like IBM Security QRadar, UBA capabilities are substantially amplified. The QRadar UBA app provides tools for analysts to prioritize risks, review alerts, and monitor groups, learning normal behavior over time. This system accelerates investigations by correlating events, mapping to MITRE ATT&CK tactics, and visualizing relationships, thereby reducing response times and fostering a proactive defense strategy.\n","\n","--- TOPIC ---\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","--- Q&A ---\n"," Q: According to IBM's Cost of a Data Breach Report 2023, how many fewer days did organizations extensively using AI and automation take on average to identify and contain a data breach?\n","A: Organizations extensively using AI and automation took 108 fewer days on average to identify and contain a data breach.\n","Q: What specific technology, combined with Artificial Intelligence (AI) and machine learning, will be explored for detecting and responding to Insider threats?\n","A: User Behavior Analytics (UBA) with Artificial Intelligence (AI) and machine learning will be explored for detecting and responding to Insider threats.\n","Q: What report is cited as the source for findings on AI's impact on data breach containment and the cost of Insider threats?\n","A: The IBM's Cost of a Data Breach Report 2023 is cited.\n","Q: According to the Cost of a Data Breach Report, what was the average cost of an Insider threat for an organization?\n","A: The average cost of an Insider threat for an organization was $4.\n","\n","--- KEY CONCEPTS ---\n"," Artificial Intelligence, Machine learning, User Behavior Analytics (UBA), Insider threats, Automation\n","Row 16 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","--- SUMMARY ---\n"," This presentation details Meta's Llama 3, a new open-source large language model available in 8 billion and 70 billion parameter versions, marking a significant advancement over Llama 2. Llama 3 exhibits superior performance in language nuances, contextual understanding, and complex tasks such as translation, reasoning, and code generation, having been trained on 50 trillion tokens with an 8K context length. Benchmarks indicate competitive performance, showing distinct superiority on HumanEval and GSM8K compared to GPQ, though its mathematical proficiency is less advanced than Gemini Pro 1. Meta's responsible AI development for Llama 3 includes comprehensive guidelines, defined model levels, and integrated system-level safeguards like Meta Llama Guard. Access to Llama 3 models is provided via the Meta Llama website, Hugging Face, Kaggle, and GitHub. Setup involves downloading model weights from Hugging Face and following instructions for local inference, with further practical application demonstrations anticipated.\n","\n","--- TOPIC ---\n"," ['Generative AI', 'Natural Language Processing', 'Deep Learning']\n","\n","--- Q&A ---\n"," Q: What is the name of the person speaking?\n","A: The speaker's name is Krishak.\n","Q: On what platform is Krishak welcoming viewers?\n","A: Krishak is welcoming viewers to his YouTube channel.\n","Q: What time of day is it when Krishak is recording?\n","A: It is 2 a.m. when Krishak is recording.\n","\n","--- KEY CONCEPTS ---\n"," \n","Row 17 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","--- SUMMARY ---\n"," This segment systematically revisits the Python code responsible for generating a decision boundary, providing viewers with the necessary steps for replication. A central focus is placed on the `scikit-learn` (sklearn) Python library, with practical guidance offered on effectively utilizing Google to explore its comprehensive documentation. The specific machine learning algorithm employed for the previously introduced classifier is identified as `Naive Bayes`, with particular attention given to the `Gaussian Naive Bayes` variant. The pedagogical methodology adopted prioritizes hands-on practical implementation and successful code execution as a prerequisite to delving into the theoretical underpinnings of the algorithm. This strategic sequencing ensures that learners can first achieve operational proficiency with the code, thereby establishing a concrete practical foundation before engaging with its intricate mechanical and conceptual principles.\n","\n","--- TOPIC ---\n"," ['Python Programming', 'Machine Learning']\n","\n","--- Q&A ---\n"," Q: What is the name of the Python library that the speaker will be using frequently in the lesson?\n","A: The Python library is scikit-learn, often abbreviated as sk-learn.\n","Q: What specific algorithm did the speaker use to create the decision boundary?\n","A: The speaker used the Naive Bayes algorithm, specifically Gaussian Naive Bayes.\n","Q: Why did the speaker use Google in the context of this lesson?\n","A: The speaker used Google to find and utilize the documentation of the scikit-learn library to figure out how to use some of its functions, specifically for Naive Bayes.\n","Q: What is the common abbreviation for the scikit-learn library?\n","A: The common abbreviation for the scikit-learn library is sk-learn.\n","\n","--- KEY CONCEPTS ---\n"," decision boundary, scikit-learn, Naive Bayes, Gaussian Naive Bayes, classifier\n","Row 18 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","--- SUMMARY ---\n"," The text first reviews the Gaussian (normal) distribution, noting its symmetrical bell curve and the 68-95-99.7 empirical rule. It then introduces the log-normal distribution, defined by its logarithm being normally distributed, often seen in right-skewed data like income or comment length. A central instructional point is the critical role of identifying a dataset's underlying distribution, whether Gaussian or log-normal, for effective data preprocessing in machine learning. Understanding the distribution enables appropriate transformations, such as applying a logarithm to log-normal data, to convert it into a standard normal distribution. This technique, known as log normalization, scales data to a consistent range, thereby significantly enhancing machine learning model accuracy.\n","\n","--- TOPIC ---\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","--- Q&A ---\n"," Q: What is the empirical formula for a Gaussian (normal) distribution regarding the percentage of data within standard deviations?\n","A: Within 1 standard deviation, approximately 68% of the total distribution falls. Within 2 standard deviations, it's around 95%, and within 3 standard deviations, it's about 99.7%.\n","Q: How is a random variable mathematically defined as belonging to a log normal distribution?\n","A: A random variable X belongs to a log normal distribution if the log of X (log(X)) is normally (Gaussian) distributed with some value of mean and sigma.\n","Q: Describe the key visual difference between a Gaussian distribution curve and a log normal distribution curve.\n","A: A Gaussian distribution curve is a symmetrical bell curve. A log normal distribution curve is almost similar to the Gaussian distribution, but its right side gets extended and becomes fatter, not coming down properly like a symmetrical bell curve.\n","Q: Provide two real-world examples of data that typically follow a log normal distribution, as mentioned in the transcript.\n","A: Two examples are the income of people and the length of product reviews or comments.\n","Q: Explain the practical benefit of converting log-normal data to a standard normal distribution before feeding it to a machine learning model.\n","A: Converting log-normal data to a standard normal distribution (a process called log normalization) scales the values to be in the same range as other features. This ensures all input features are on the same scale, which can lead to higher accuracy in machine learning models.\n","\n","--- KEY CONCEPTS ---\n"," Gaussian distribution, empirical formula, standard deviation, bell curve, log normal distribution, random variable, standard normal distribution, standard scaler, sentiment analysis, log normalization\n","Row 19 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","--- SUMMARY ---\n"," This comprehensive deep learning project outlines an end-to-end solution for potato disease detection, addressing significant economic losses incurred by farmers due to conditions like early and late blight. The initiative involves developing a mobile application, spearheaded by AtliQ Agriculture, which leverages convolutional neural networks (CNNs) to diagnose plant health from user-submitted images. The workflow encompasses data collection, pre-processing with `tf.data` and augmentation, CNN model building, and robust deployment. ML Ops principles are applied using TF Serving via a FastAPI server for model management, while a React JS website and a React Native mobile application provide user interfaces. Although direct on-device TF Lite model deployment is the long-term goal, initial deployment utilizes Google Cloud Functions. The technology stack includes TensorFlow, TF Serving, FastAPI, TF Lite, React Native, and Google Cloud Platform. This series, designed for aspiring machine learning engineers, emphasizes practical skill development, with foundational concepts being crucial for enhancing resumes.\n","\n","--- TOPIC ---\n"," ['Deep Learning', 'Mlops', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is the primary problem statement that the end-to-end deep learning project in agriculture aims to solve?\n","A: The project aims to solve the problem of potato farmers facing significant economic losses every year due to various diseases affecting potato plants. Early and accurate detection of these diseases is crucial to apply appropriate treatment and prevent economic loss.\n","Q: What are the two common potato plant diseases mentioned, and what causes each of them?\n","A: The two common diseases mentioned are early blight, which is caused by a fungus, and late blight, which is caused by a specific microorganism.\n","Q: How will the mobile application developed by AtliQ Agriculture assist farmers in identifying potato plant diseases?\n","A: Farmers will use the mobile application to take a picture of a potato plant. The application, using deep learning and convolutional neural networks, will then inform them whether the potato plant is healthy or if it has one of the diseases.\n","Q: Which technologies are planned for building the backend server and the mobile application in this project?\n","A: The backend server will be built using Fast API, and the mobile application will be written in React Native.\n","Q: As a data scientist working for AtliQ Agriculture on this project, what is the very first step you will take?\n","A: The very first step will be to gather in a room with colleagues and start discussing the technical architecture of the application.\n","\n","--- KEY CONCEPTS ---\n"," end-to-end deep learning project, Machine Learning Ops (MLOps), TF serving, Google Cloud deployment, Google Cloud functions, React Native, deep learning, convolutional neural network, fast API\n","Row 20 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","--- SUMMARY ---\n"," The progression of autonomy in LLM applications spans from zero autonomy to advanced agentic systems. Initially, deterministic, hard-coded rules (code) proved impractical for real-world complexity, while basic single LLM calls offered limited input-output processing. Chains enhanced capabilities by orchestrating multiple specialized LLM calls to decompose complex tasks, though they operated on rigid, human-defined sequences. Routers introduced greater autonomy, allowing an LLM to dynamically select subsequent steps or chains based on user input. The highest level discussed is the State Machine, or Agent, which integrates routers with iterative loops, memory, and adaptive learning. These agents facilitate iterative refinement, incorporate human-in-the-loop processes, and enable LLM-controlled workflow, distinguishing them from one-directional chains and routers through their dynamic, adaptive, and self-improving task execution capabilities.\n","\n","--- TOPIC ---\n"," ['Agentic AI', 'LangChain', 'Langraph']\n","\n","--- Q&A ---\n"," Q: What is the primary characteristic of 'Code' in LLM applications regarding autonomy, and what is its main disadvantage?\n","A: 'Code' has zero autonomy and is 100% deterministic, meaning everything is hard-coded. Its main disadvantage is the need to write rules for every possible scenario, making it impossible to handle real-life complexity.\n","Q: Explain the key difference in decision-making between an 'LLM Call' and a 'Router' in terms of how the application progresses.\n","A: In an 'LLM Call,' the application performs one main task, processing input to output without further internal decision-making on subsequent steps. In a 'Router,' an LLM acts as a 'smart traffic cop' that decides which specific chain or tool to route the request to next, based on the user's input, rather than following a fixed, predefined path.\n","Q: According to the transcript, what is the defining characteristic that makes a 'State Machine' an 'Agent' in the context of LLM applications?\n","A: A 'State Machine' is called an 'Agent' because the control flow within the application is controlled by an LLM. This enables features like loops, cycles, human-in-the-loop interactions, and adaptive learning, allowing for iterative refinement and intelligent decision-making.\n","Q: A user wants an LLM application to generate a LinkedIn post, a Twitter post, and a blog post from a single topic. Which level of autonomy, 'LLM Call' or 'Chains,' would be more suitable for this task and why?\n","A: 'Chains' would be more suitable. An 'LLM Call' is designed for one particular task and would likely produce confused or mixed-up responses if asked to generate multiple types of posts in one prompt. 'Chains' break down the task into steps, allowing multiple specialist LLMs to handle each post type (e.g., one for LinkedIn, one for Twitter, one for blog post), creating a much smarter system for complex tasks.\n","\n","--- KEY CONCEPTS ---\n"," levels of autonomy, single LLM call, LLM chains, LLM router, LLM agent, control flow, human in loop, adaptive learning, multi-agent systems, time travel abilities\n","Row 21 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","--- SUMMARY ---\n"," This document comprehensively examines advanced prompt engineering, moving beyond conventional text-based inputs to integrate image and audio modalities. It delves into sophisticated fine-tuning techniques for pre-trained large language models, specifically highlighting multitask learning to enhance model generalization and distillation for creating more efficient, smaller models. Crucial best practices for data pre-processing, such as tokenization and normalization, are emphasized as foundational for achieving optimal model success. The discussion also covers practical aspects of deploying prompt engineering models in production environments, leveraging frameworks like TensorFlow Serving or Flask. Finally, it addresses critical ethical considerations, including bias, fairness, and privacy, to ensure the responsible development and deployment of AI systems.\n","\n","--- TOPIC ---\n"," ['Prompt Engineering', 'Deep Learning', 'Mlops']\n","\n","--- Q&A ---\n"," Q: What are the three different types of prompts mentioned that advanced prompt engineering models can handle?\n","A: Advanced prompt engineering models can handle text-based prompts, image-based prompts, and audio-based prompts.\n","Q: If you wanted to build a prompt engineering model to identify different breeds of dogs from pictures, what type of prompt would you use, and what general steps would be involved according to the transcript?\n","A: To identify dog breeds from pictures, an image-based prompt would be used. The general steps involve training a model on a dataset of dog images labeled with their corresponding breeds, and then using a pre-trained model (like ResNet50, VGG16, Exception, Inception, or logistic regression) to identify the breed in new images.\n","Q: Explain two advanced techniques for fine-tuning pre-trained large language models mentioned in the transcript.\n","A: Two advanced techniques are: 1. **Multitask learning:** Involves training a model on multiple tasks simultaneously to help it learn more robust representations that can generalize to different users. 2. **Distillation:** Involves training a smaller model to mimic the behavior of a larger model, making the smaller model more efficient and faster.\n","Q: What are two important steps in data pre-processing and cleaning for prompt engineering models, and what is the purpose of each?\n","A: Two important steps are: 1. **Tokenization:** Breaking down text into smaller units like words or sub-words to help the model understand the meaning more accurately. 2. **Normalization:** Converting text to a standard format, such as converting all text to lowercase, to help the model avoid confusion between words spelled the same but with different meanings.\n","Q: Name two options for deploying prompt engineering models in production and one ethical consideration mentioned.\n","A: Two options for deployment are TensorFlow Serving and Flask. An ethical consideration is bias, which can occur if training data is not representative of the whole population, leading to unfair and discriminatory outcomes.\n","\n","--- KEY CONCEPTS ---\n"," prompt engineering, fine-tuning pre-trained large language models, multitask learning, distillation, data pre-processing, tokenization, deploying models in production, pre-trained models\n","Row 22 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","--- SUMMARY ---\n"," {\"generated_summary\":\"This lecture details the application of Singular Value Decomposition (SVD) for image classification, specifically through the generation of \"eigenfaces.\" The process begins with preprocessing images via cropping and alignment, followed by computing an average face. Subtracting this average from individual images forms a matrix, on which SVD is performed to derive principal components, or eigenfaces. Images are then projected into this reduced-dimensional eigenface space, facilitating clustering and classification. An initial demonstration using Arnold Schwarzenegger and Sylvester Stallone images showed effective separation and successful classification of novel images. However, a subsequent experiment with Taylor Swift revealed better distinction from Stallone but an unexpected overlap with Arnold. This overlap was attributed to shared pixel characteristics like ski\n","\n","--- TOPIC ---\n"," ['Machine Learning', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What is the primary objective of the \"eigenheroes\" example demonstrated in the lecture?\n","A: The primary objective is to find the eigenfaces (or eigenheroes) of Arnold Schwarzenegger and Sylvester Stallone and use them to cluster the two different people in eigenface space.\n","Q: How many images of each action hero (Arnold Schwarzenegger and Sylvester Stallone) were used in the initial dataset, and what was their resolution?\n","A: The initial dataset used 20 pictures of Arnold and 20 pictures of Stallone. Each image was 200 by 175 pixels.\n","Q: Before computing the SVD, what crucial step was performed on the image matrix 'A' to create matrix 'B', and what is its purpose?\n","A: Before computing the SVD, the average face of all 40 images was calculated and then subtracted from every column of the 'A' matrix to create matrix 'B'. This step is essentially performing Principal Component Analysis (PCA) by removing the mean.\n","Q: How can a new, unseen test image be classified as either Arnold or Stallone using the eigenface coordinates?\n","A: A new test image can be classified by projecting it into the eigenface coordinates (e.g., the first three principal components). Its proximity to the existing Arnold cluster or Stallone cluster in that feature space determines its classification.\n","Q: In the experiment comparing Taylor Swift and Arnold Schwarzenegger, what surprising result was observed regarding their separation in eigenface space, and what potential reason was suggested for this?\n","A: Surprisingly, Taylor Swift and Arnold Schwarzenegger were found to be more overlapping in eigenface space than Arnold and Stallone, or Taylor and Stallone. A potential reason suggested was that both are fair-skinned with blonde hair, leading to more correlation in their average pixel values, which carries a lot of information for the inner product.\n","\n","--- KEY CONCEPTS ---\n"," Singular Value Decomposition (SVD), Eigenfaces, Principal Component Analysis (PCA), Image Classification, Eigenface Space, Average Face, Linear Combinations, Training Data Set, Test Image\n","Row 23 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","--- SUMMARY ---\n"," ```json\n","{\"generated_summary\":\"Large Language Models (LLMs) like ChatGPT, despite their strong reasoning abilities, are inherently limited in their capacity to interact directly with the real world, such as performing actions or accessing external data. These LLMs are akin to \"brains\" lacking the means to communicate with external APIs or databases. LangChain addresses this fundamental limitation by serving as a crucial framework that bridges LLMs with real-world services. This connection empowers AI to extend beyond its inherent intelligence, enabling practical applications like accessing private company databases for customer inquiries, facilitating email communication, and performing web browsing on platforms such as Google and Wikipedia, including website scraping. By providing AI with the capacity for tangible actions, LangChain significantly broadens its utility and enables the deve\n","\n","--- TOPIC ---\n"," ['LangChain', 'Artificial Intelligence', 'Agentic AI']\n","\n","--- Q&A ---\n"," Q: What is a primary limitation of large language models (LLMs) when used on their own, according to the transcript?\n","A: On their own, LLMs are just the 'brains' that can reason with data but cannot interact with the real world, meaning they cannot make bookings directly or perform actions outside of their reasoning ability.\n","Q: How does LangChain address the limitations of standalone LLMs?\n","A: LangChain acts as a bridge between LLMs and the real world, providing a framework that enables LLM applications to communicate with real-world APIs, databases, and perform actions like sending emails or accessing booking APIs.\n","Q: What is one practical benefit of using LangChain if a developer wants to change the underlying LLM model in their application?\n","A: With LangChain, a developer can easily switch out one LLM (e.g., GPT-4) for another (e.g., a free Hugging Face LLM) without needing to touch the existing code written with LangChain.\n","Q: According to the transcript, what is the role of a chat application like ChatGPT in relation to the underlying LLM model?\n","A: The chat application itself is just an interface for the user, while the LLM model (like GPT-3.5 or GPT-4) is the underlying 'brain' that processes the query.\n","Q: What types of real-world interactions can an AI application built with LangChain perform?\n","A: An AI application built with LangChain can access a lot of APIs, such as flight and restaurant booking APIs, communicate with real-world databases, and send emails.\n","\n","--- KEY CONCEPTS ---\n"," LangChain, Large Language Models (LLMs), real-world APIs, reasoning ability, application framework, Hugging Face LLM\n","Row 24 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","--- SUMMARY ---\n"," Residual analysis is a critical diagnostic tool for evaluating time series model performance, distinguishing residuals (difference between fitted and observed values) from forecast errors. Effective models require residuals to exhibit no autocorrelation, signifying all relevant information has been captured, and a mean of zero to prevent forecasting bias. Autocorrelation, indicating missed patterns, can be detected using ACF/PACF plots or the Ljung-Box test, while bias is assessed via the mean residual. A demonstration using a Holt-Winters model on air passenger data in Python revealed significant autocorrelation, as indicated by Ljung-Box test p-values below 0.05, necessitating model adjustments. Conversely, the model exhibited negligible bias, with a slightly negative but small mean residual. Ultimately, analyzing residuals for autocorrelation and bias is essential for identifying model deficiencies and guiding subsequent improvements.\n","\n","--- TOPIC ---\n"," ['Time Series', 'Machine Learning', 'Python Programming']\n","\n","--- Q&A ---\n"," Q: What are residuals in time series analysis, and how do they differ from errors?\n","A: In time series analysis, residuals are simply the difference between the fitted value (y-hat) and the actual value (y) of the time series. They represent the difference between what the model has managed to fit with the data it was trained on versus the actual values. Errors, on the other hand, are for data the model has not seen before, typically used for testing performance.\n","Q: What two primary characteristics should residuals ideally exhibit for a well-performing time series model?\n","A: Ideally, residuals should have no autocorrelation or partial autocorrelation, meaning no correlation at all. Additionally, the mean of the residuals should be zero.\n","Q: How can residual analysis help improve forecasting methods, and what specific tools or tests are mentioned for this purpose?\n","A: Residual analysis allows you to diagnose model performance, detect trends or inconsistencies, and gain intuition behind what the model is doing to improve it. Tools mentioned for this purpose include plotting the autocorrelation function (ACF) and partial autocorrelation function (PACF) for residuals, and using the Ljung-Box test.\n","Q: What does it signify if the mean of the residuals is not zero, and how can this issue be addressed?\n","A: If the mean of the residuals is not zero, it signifies that the forecast is biased, meaning the model is either consistently under-forecasting or over-forecasting. This issue can be corrected by simply adding or subtracting a constant value to offset the bias in the forecast.\n","Q: What is the null hypothesis of the Ljung-Box test when applied to residuals, and what does it mean if the test results show a low significance level?\n","A: The null hypothesis of the Ljung-Box test is that the residuals are independently distributed, meaning they are random variables with no correlation. If the test results show a low significance level (e.g., many residuals are in the low significance level zone), it indicates that there is autocorrelation within the data, suggesting the residuals are not independently distributed.\n","\n","--- KEY CONCEPTS ---\n"," Residual analysis, Forecasting methods, Fitted value, Actual value, Autocorrelation, Partial autocorrelation, Young-Box test, Holt-Winters model, Exponential smoothing model, Serial correlation\n","Row 25 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","--- SUMMARY ---\n"," This project outlines the construction of an AI agent designed for SQL database interaction, integrating large language models. The architecture employs LangGraph for a ReAct agent, Next.js for the frontend, watsonx.ai for LLM deployment, and an in-memory SQLite database. The development process began with setting up the Next.js frontend, configuring dynamic UI components, and establishing environment variables for watsonx.ai model connectivity. Backend implementation utilized LangGraph and LangChain for agent logic, incorporating message serialization. A local SQLite database was established, seeded with mock data, and equipped with an `execute` function for SQL queries. A custom `GetFromDB` tool was developed to link the agent to the database, providing schema information to the LLM. The system prompt was refined to guide the LLM in generating accurate SQLite queries, demonstrated through various examples. The project underscores the critical importance of implementing guardrails for secure LLM-database interaction.\n","\n","--- TOPIC ---\n"," ['Artificial Intelligence', 'Agentic AI', 'Langraph']\n","\n","--- Q&A ---\n"," Q: What is the primary goal of the AI agent being built in this video?\n","A: The primary goal is to build an Artificial Intelligence (AI) agent that is able to talk to a database by using its SQL knowledge.\n","Q: Which specific technologies are used for building the ReAct agent, the frontend, and for running the models?\n","A: LangGraph is used to build the ReAct agent, Next.js for the frontend application, and models are run on watsonx.ai.\n","Q: How is the Next.js boilerplate project initialized, and what two specific choices are mentioned during its setup?\n","A: The Next.js boilerplate project is initialized by running `create-next-app@latest`. During setup, TypeScript is chosen, and Tailwind is selected for CSS.\n","Q: After modifying the `pages.tsx` file, what three main UI elements are added inside the `div` element of the `Home` component?\n","A: Inside the `div` element of the `Home` component, a header, some placeholder messages, and an input box with a button are added.\n","Q: What command is used to start the Next.js application after the code has been set up?\n","A: The Next.js application is started by running `npm run dev`.\n","\n","--- KEY CONCEPTS ---\n"," Artificial Intelligence agent, large language models, SQL knowledge, LangGraph, ReAct agent, watsonx. ai, in-memory database, Text2SQL agent\n","Row 26 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","--- SUMMARY ---\n"," This course introduces prompt engineering, a specialized field within Natural Language Processing (NLP) dedicated to optimizing systems like chatbots and translation software for generating accurate and coherent responses. It focuses on building models, often based on pre-trained large language models such as GPT, to produce high-quality text outputs from user prompts. A primary benefit is the generation of more accurate, coherent, and contextually appropriate text compared to traditional methods, significantly enhancing user experience in applications like content generation. However, prompt engineering faces limitations, including potential struggles with complex prompts or the generation of biased outputs. The initial section of the course will cover its fundamentals, significance in NLP, prompt analysis techniques, and an overview of its benefits and limitations.\n","\n","--- TOPIC ---\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","--- Q&A ---\n"," Q: What is prompt engineering?\n","A: Prompt engineering is a specialized field within natural language processing that focuses on building models that can generate high quality text outputs in response to prompts or inputs.\n","Q: What types of pre-trained models are prompt engineering models based on?\n","A: Prompt engineering models are based on pre-trained large language models such as OpenAI GPT, Google Bard, or Hugging Face Transformers.\n","Q: What is a key benefit of prompt engineering compared to traditional approaches?\n","A: A key benefit of prompt engineering is that it allows for generating text outputs that are more accurate, coherent, and contextually appropriate than traditional rule-based or keyword-based approaches.\n","Q: For which applications is the quality of output from prompt engineering especially important?\n","A: The quality of output from prompt engineering is especially important for applications such as chatbots, language translation, and content generation.\n","Q: What are some potential limitations of prompt engineering models mentioned in the transcript?\n","A: Prompt engineering models may struggle with complex and ambiguous prompts, or they may generate biased and inaccurate outputs due to underlying data or model architecture.\n","\n","--- KEY CONCEPTS ---\n"," prompt engineering, natural language processing, large language models, pre-trained large language models, fine-tuning, prompt analysis, rule-based approaches, keyword-based approaches, model architecture\n","Row 27 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","--- SUMMARY ---\n"," Q-learning is a prominent value-based reinforcement learning method designed to maximize a numerical reward signal by learning an optimal policy. It achieves this by estimating state-action value functions, or Q-values, which quantify the desirability of taking a specific action in a given state. These Q-values, typically stored in a Q-table, are iteratively refined as an agent interacts with its environment. The update mechanism centers on the temporal difference (TD) error, calculated as the difference between an \"observed\" Q-value (derived from the immediate reward and discounted maximum future Q-value using the Bellman equation) and the \"expected\" Q-value currently in the table. This error is then applied in a gradient-like update rule, moderated by a learning rate, to adjust the Q-value. This iterative update process unfolds across multiple time steps within an \"episode,\" concluding at a terminal state. Through numerous episodes, the Q-values converge, ultimately defining the optimal policy for the agent's action selection.\n","\n","--- TOPIC ---\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","--- Q&A ---\n"," Q: What are the three primary machine learning paradigms discussed in the transcript?\n","A: The three primary machine learning paradigms are supervised learning, unsupervised learning, and reinforcement learning.\n","Q: How does reinforcement learning define its objective, and what are the two main types of reinforcement learning algorithms?\n","A: Reinforcement learning is defined as learning what to do—how to map situations to actions—to maximize a numerical reward signal. The two main types of reinforcement learning algorithms are value-based methods and policy-based methods.\n","Q: What is the difference between a state value function (V) and a state-action value function (Q), and which one is Q-learning primarily interested in?\n","A: A state value function (V) quantifies how good it is to be in a given state, while a state-action value function (Q) quantifies how good it is to be in a state and then take a specific action in that state. Q-learning is primarily interested in learning the state-action value function (Q-value).\n","Q: In the context of Q-learning, what is the purpose of the Q-table?\n","A: The Q-table represents the state-action value function, where rows are states and columns are possible actions. Each cell value in the table is the Q-value for a given state and action, and the goal of Q-learning is to learn these Q-values to maximize total reward.\n","Q: According to the transcript, what is the Bellman equation used for in Q-learning?\n","A: The Bellman equation is used to calculate the observed Q-value, defining a recursive relationship between Q-values. It sums the reward from the next state and the maximum future Q-value for that next state, discounted by a gamma factor.\n","\n","--- KEY CONCEPTS ---\n"," Q-learning, reinforcement learning, machine learning paradigms, value-based methods, optimal policy, state-action value function, Q value, Q table, Bellman equation, discount factor\n","Row 28 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","--- SUMMARY ---\n"," The training of a logistic classifier, a fundamental linear model, involves applying a linear function—specifically, matrix multiplication—to input features (X) to generate initial prediction scores. The core machine learning objective is to optimize the model's parameters, comprising weights (W) and bias (b), through an iterative process to achieve high predictive accuracy. For multi-class classification tasks, where each input is assigned a single label, these raw prediction scores, often termed logits, are subsequently transformed into a normalized probability distribution using the softmax function. This crucial function ensures that the output probabilities for all classes sum precisely to one, with higher scores directly correlating to an increased likelihood of an input belonging to a particular class. This systematic approach underpins the effective classification of data.\n","\n","--- TOPIC ---\n"," ['Machine Learning', 'Statistics']\n","\n","--- Q&A ---\n"," Q: What type of classifier is a logistic classifier, and how does it generate its predictions?\n","A: A logistic classifier is a linear classifier. It generates predictions by applying a linear function, which is a giant matrix multiply, to its inputs.\n","Q: In the context of a logistic classifier, what do X, W, and b represent, and which of these are adjusted during the training process?\n","A: X denotes the inputs, W denotes the weights, and b denotes the bias term. The weights (W) and the bias (b) are adjusted during the training process to find values good at performing predictions.\n","Q: What is the purpose of the softmax function in a logistic classifier, and what are the characteristics of the probabilities it produces?\n","A: The softmax function is used to turn the scores generated by the linear function into proper probabilities. These probabilities sum to 1, are large when the scores are large, and small when the scores are comparatively smaller.\n","Q: What is an alternative term for 'scores' in the context of logistic regression?\n","A: In the context of logistic regression, scores are often also called logits.\n","\n","--- KEY CONCEPTS ---\n"," logistic classifier, linear classifier, linear function, matrix multiply, weights and bias, softmax function, proper probabilities, logits\n","Row 29 saved to /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","All rows processed. Final output saved to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39675b1f73214b17a3748078102dd00a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b5179e430d433fa0755a36a116342a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3a22c9d56834b35850c66446cd175ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06cbd33535b54e1aaf170a00ba00ed5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1ff85f2fd33461cb50f55bca725d995"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a33c712e0d9b4109a1730c1f56471bb1"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["\n","================== FINAL EVALUATION METRICS ==================\n","Summarisation             | ROUGE-L F1=0.361 | BLEU=0.108 | BERTScore F1=0.899\n","Topic Classification      | Subset Accuracy=0.000 | Jaccard Index=0.225 | Micro F1=0.317\n","Q&A Generation            | BLEU=0.023 | Diversity=0.690 | Answerability=0.648\n","Key Concept Extraction    | Precision@10=0.503 | Recall@10=0.201 | F1@10=0.288\n","\n","Combined Excel: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","Evaluation JSON: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/evaluation_COT.json\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jOGu39awm7S3","executionInfo":{"status":"ok","timestamp":1763205519478,"user_tz":-330,"elapsed":108160,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"89b1c467-2d90-4ec8-f721-8449812d3544"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/gemini-2.5-flash_cot_full_output.xlsx\n","\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3614\n","  - BLEU: 0.1082\n","  - BERTScore F1: 0.8991\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3589\n","  - Micro F1: 0.4892\n","  - Macro F1: 0.4775\n","  - Weighted F1: 0.4744\n","\n","Q&A Generation:\n","  - BLEU: 0.0228\n","  - Diversity: 0.6902\n","  - Answerability: 0.6483\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4900\n","  - Recall@10: 0.1960\n","  - F1@10: 0.2800\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/gemini-2.5-flash/evaluation_final.json\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0qtJuEOhm7fO"},"execution_count":null,"outputs":[]}]}