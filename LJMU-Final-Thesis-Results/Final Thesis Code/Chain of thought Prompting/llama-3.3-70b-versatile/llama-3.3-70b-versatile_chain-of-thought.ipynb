{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOK5bvjvGlg0DryeTGb+37Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2227d336c3084c7c9e81fe9cd09738b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_28576fad32a14bfba94de8ee12814f92","IPY_MODEL_8977e1c49b344141a1c6a745ec2f0dbb","IPY_MODEL_a6b54c49684a4cdc99d5d69775879f85"],"layout":"IPY_MODEL_6cb7c5d50cd748ab8eb40a565e754a69"}},"28576fad32a14bfba94de8ee12814f92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7866fb2cf2845bb8fcac0e84b4832a2","placeholder":"​","style":"IPY_MODEL_920b47c0face48c89d9401581b29524b","value":"tokenizer_config.json: 100%"}},"8977e1c49b344141a1c6a745ec2f0dbb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_00a31ba3017e4afa84e9a1ced1d69f3f","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9135da903f14d0a92b2c39eb3198b72","value":25}},"a6b54c49684a4cdc99d5d69775879f85":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b2eb7852fa9437183958cc61e9ab985","placeholder":"​","style":"IPY_MODEL_6bc3fc40e20e446f90c38d3d542316c6","value":" 25.0/25.0 [00:00&lt;00:00, 971B/s]"}},"6cb7c5d50cd748ab8eb40a565e754a69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7866fb2cf2845bb8fcac0e84b4832a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"920b47c0face48c89d9401581b29524b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00a31ba3017e4afa84e9a1ced1d69f3f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9135da903f14d0a92b2c39eb3198b72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b2eb7852fa9437183958cc61e9ab985":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bc3fc40e20e446f90c38d3d542316c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7c310f9932840baa32ab42344718a25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0177ad66e543435cb1ff5963f2939b4a","IPY_MODEL_d8ab6da16680462ab0e0565b65488c87","IPY_MODEL_414b6d3c6f85452189dd0e42bbb90a33"],"layout":"IPY_MODEL_c8b4062beb7141fcbe3e8a008cfddbdf"}},"0177ad66e543435cb1ff5963f2939b4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebd61f22690d46a58fff7cedd53b6ddd","placeholder":"​","style":"IPY_MODEL_b59c1777cdfa4971a1751065c9605159","value":"config.json: 100%"}},"d8ab6da16680462ab0e0565b65488c87":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f861ae1af78448069c710dee3871debc","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_528413d44d4f4dc2819195822fdb02b4","value":482}},"414b6d3c6f85452189dd0e42bbb90a33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7478438546694457a99e1ffc144f959e","placeholder":"​","style":"IPY_MODEL_e8809d10569a4d4493014f0ab8c75c72","value":" 482/482 [00:00&lt;00:00, 21.1kB/s]"}},"c8b4062beb7141fcbe3e8a008cfddbdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebd61f22690d46a58fff7cedd53b6ddd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b59c1777cdfa4971a1751065c9605159":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f861ae1af78448069c710dee3871debc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"528413d44d4f4dc2819195822fdb02b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7478438546694457a99e1ffc144f959e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8809d10569a4d4493014f0ab8c75c72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ab39c655aa54a13848ab295993d962b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ca6e39d5a6d4198934afa80d5aac614","IPY_MODEL_fc99fbca8eea45b0ba066c9b4b33147f","IPY_MODEL_5528c978a521497ab7940f3ecaa68023"],"layout":"IPY_MODEL_f753c46ff9614a9dbe976d2d501fe002"}},"4ca6e39d5a6d4198934afa80d5aac614":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55a83a4fb3734aed89c850c9846b9563","placeholder":"​","style":"IPY_MODEL_605ce840e0fa4a16843dc73db943c95b","value":"vocab.json: 100%"}},"fc99fbca8eea45b0ba066c9b4b33147f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_680ae4ea49e6402da465069a49287f4f","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84a289f57d044b99a1ebe46ebdbf08d1","value":898823}},"5528c978a521497ab7940f3ecaa68023":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c493e2264d8a415583617a9ddb334186","placeholder":"​","style":"IPY_MODEL_44ff10432c144ccb89e03593ee79350d","value":" 899k/899k [00:00&lt;00:00, 5.66MB/s]"}},"f753c46ff9614a9dbe976d2d501fe002":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55a83a4fb3734aed89c850c9846b9563":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"605ce840e0fa4a16843dc73db943c95b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"680ae4ea49e6402da465069a49287f4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84a289f57d044b99a1ebe46ebdbf08d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c493e2264d8a415583617a9ddb334186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44ff10432c144ccb89e03593ee79350d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fc6abe2ec56463cabe50e0127072384":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a09468e701c4e8f82ea8363aba6a75e","IPY_MODEL_6a577229d71846fe937f460ab47be791","IPY_MODEL_b121d8e8d0f942d79634a01960c916ea"],"layout":"IPY_MODEL_02be70e6cfa74d6a8327093f7e076c95"}},"7a09468e701c4e8f82ea8363aba6a75e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e895e2a10aff4888a826043f69dd36c3","placeholder":"​","style":"IPY_MODEL_b71cb1e97a6a47eca61d26f22c24f4c8","value":"merges.txt: 100%"}},"6a577229d71846fe937f460ab47be791":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c057f00781d3461f88309cb0bb4eda1d","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9f00a1448f8d4fbbad16d3ba53d585e0","value":456318}},"b121d8e8d0f942d79634a01960c916ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d30c6259c10a4a388751b8ecd2a8cf5b","placeholder":"​","style":"IPY_MODEL_b0db0c431b004854abe240a9c0864f41","value":" 456k/456k [00:00&lt;00:00, 18.4MB/s]"}},"02be70e6cfa74d6a8327093f7e076c95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e895e2a10aff4888a826043f69dd36c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b71cb1e97a6a47eca61d26f22c24f4c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c057f00781d3461f88309cb0bb4eda1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f00a1448f8d4fbbad16d3ba53d585e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d30c6259c10a4a388751b8ecd2a8cf5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0db0c431b004854abe240a9c0864f41":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aea2754a5f6b468e9f21010ae51acc55":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e929641959f440ca857da702777f02c","IPY_MODEL_422f46ddd9e64979a6313e4e2be95365","IPY_MODEL_d78c4bc57de948b2bbe2e26a8fd029b4"],"layout":"IPY_MODEL_27362f4448534869a5bac4b927da1c3a"}},"6e929641959f440ca857da702777f02c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c316df8377c64de195532bf57410b7de","placeholder":"​","style":"IPY_MODEL_5b96fed7ecb34f6aabc57c0e1ace9cf2","value":"tokenizer.json: 100%"}},"422f46ddd9e64979a6313e4e2be95365":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee3151a7159147c0a1570eed1f622ee0","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2890fb98aaa44a1bec179b84b6c5fee","value":1355863}},"d78c4bc57de948b2bbe2e26a8fd029b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f01c40b17064c969ead60fa0192d993","placeholder":"​","style":"IPY_MODEL_569675cd5ca9479e9e4a09c35dc40016","value":" 1.36M/1.36M [00:00&lt;00:00, 31.8MB/s]"}},"27362f4448534869a5bac4b927da1c3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c316df8377c64de195532bf57410b7de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b96fed7ecb34f6aabc57c0e1ace9cf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee3151a7159147c0a1570eed1f622ee0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2890fb98aaa44a1bec179b84b6c5fee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f01c40b17064c969ead60fa0192d993":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"569675cd5ca9479e9e4a09c35dc40016":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a574caa8a84e412ab5471e33d1fd23f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_758cf789ff424e20b6cca0bf344b3b80","IPY_MODEL_3b9be7fb2b0042378921a3f2c5b5ddef","IPY_MODEL_47f726698dde4572bb200f09679566bd"],"layout":"IPY_MODEL_791b9bc99bc14e31aaf79520e9f29b0d"}},"758cf789ff424e20b6cca0bf344b3b80":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40b0b6ecd5ea425796a3a9f1ef2871f5","placeholder":"​","style":"IPY_MODEL_c3e6bae5a2c942de8ada66ce466a755a","value":"model.safetensors: 100%"}},"3b9be7fb2b0042378921a3f2c5b5ddef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_efa887700eb0413a8b424335621140a3","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9eab9a98805d44e0a2e80f91b0a67e68","value":1421700479}},"47f726698dde4572bb200f09679566bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49b68465f2c94e9f904713c61255898d","placeholder":"​","style":"IPY_MODEL_e030fda3efaa49b08b4289550153efa1","value":" 1.42G/1.42G [00:18&lt;00:00, 73.3MB/s]"}},"791b9bc99bc14e31aaf79520e9f29b0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40b0b6ecd5ea425796a3a9f1ef2871f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3e6bae5a2c942de8ada66ce466a755a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"efa887700eb0413a8b424335621140a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9eab9a98805d44e0a2e80f91b0a67e68":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"49b68465f2c94e9f904713c61255898d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e030fda3efaa49b08b4289550153efa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CZx5OTRnpqx0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763297188474,"user_tz":-330,"elapsed":13729,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"f2d4d25a-d229-44f9-e71a-253dda83187f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.34.1-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.34.1-py3-none-any.whl (136 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.0/136.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=cebccbb88cff310fb14348d091a6c89e11274daffbdf763b335615930dac5e86\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.34.1 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"zGwxg5Kij8Iw","executionInfo":{"status":"ok","timestamp":1763297188486,"user_tz":-330,"elapsed":10,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"aed99291-acd9-44a3-81d1-97f8e0a81011"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"wjI6IzcuT7yQ","executionInfo":{"status":"error","timestamp":1763392311038,"user_tz":-330,"elapsed":2900,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"c2d13f2d-aba3-4319-c81b-c66c775b9030"},"execution_count":1,"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-552734336.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HhunYUNEsFtr","executionInfo":{"status":"error","timestamp":1763307782487,"user_tz":-330,"elapsed":10591047,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"b2afb0d8-c9bf-451b-9287-b57d18e49545"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Groq API key loaded ✓\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","Skipping row 7 (already processed)\n","Skipping row 8 (already processed)\n","Skipping row 9 (already processed)\n","Skipping row 10 (already processed)\n","Skipping row 11 (already processed)\n","Skipping row 12 (already processed)\n","Skipping row 13 (already processed)\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","========== OUTPUT FOR ROW 14 ==========\n","\n","SUMMARY:\n","\n","The primary goal of reinforcement learning is to determine the optimal policy that maximizes a reward signal. This is achieved by evaluating the quality of actions through feedback, with the ultimate objective of maximizing cumulative rewards over time. Through trial and error, agents can learn to make effective decisions in various tasks, including episodic and continuous problems. Real-life applications of reinforcement learning include tasks with time constraints and defined goals, such as playing games like Tic-Tac-Toe or complex tasks like stock market trading, highlighting the versatility and potential of this learning approach in diverse domains.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the objective in a reinforcement learning problem?\n","A: The objective is to learn the optimal policy that maximizes a numerical reward signal.\n","Q: How does an agent in reinforcement learning receive feedback?\n","A: The agent receives feedback through a reward signal that provides information about the quality of its actions.\n","Q: What is the ultimate goal of an agent in a reinforcement learning problem?\n","A: The ultimate goal is to maximize the cumulative reward over time by learning an optimal policy that maps states to actions.\n","Q: How is the reward defined in an episodic task like Tic-Tac-Toe?\n","A: The reward is defined as a positive reward (e.g., +1) for winning, a negative reward (e.g., -1) for losing, and no reward for drawing.\n","Q: What is the purpose of a reward function in reinforcement learning?\n","A: The reward function is a mathematical expression that assigns a numerical value to each state or action of the agent, reflecting the agent's goal and preferences.\n","\n","KEY CONCEPTS:\n","\n","reinforcement learning, optimal policy, cumulative reward, reward signal, trial and error learning, episodic task, continuous task, value-based method, policy-based method, reward function\n","\n","============================================\n","Saved row 14\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 15 ==========\n","\n","SUMMARY:\n","\n","A dictionary in Python is a fundamental data type comprising key-value pairs, defined within curly brackets, and is essential for mapping items. It offers various functions, such as items, keys, and values, to access and manipulate data. Dictionaries can be created using the dict function, zip function, or by pairing two lists together. As a crucial structure in Python, particularly in data science, understanding dictionary basics is vital for advanced learning and applications, enabling efficient data organization and analysis.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Python Programming', 'Data Science', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is a dictionary in Python and how is it declared?\n","A: A dictionary in Python is a collection of key-value pairs, declared within curly brackets.\n","Q: What is the purpose of the zip function in creating a dictionary from two lists?\n","A: The zip function pairs corresponding values from two lists together, creating a tuple of these values, which can then be used to create a dictionary.\n","Q: How can you access and change the value associated with a specific key in a dictionary?\n","A: You can access the value associated with a key by using the dictionary name followed by the key in square brackets, and you can change the value by assigning a new value to the key in a similar manner.\n","Q: What is the function of the dict() function in creating a dictionary?\n","A: The dict() function creates a dictionary from the items passed to it, such as the result of the zip function.\n","Q: How can you delete a specific key-value pair from a dictionary?\n","A: You can delete a key-value pair from a dictionary using the del function, followed by the dictionary name and the key to be deleted in square brackets.\n","\n","KEY CONCEPTS:\n","\n","key value pairs, immutable keys, dictionary declaration, zip function, dictionary items, dictionary keys, dictionary values, dict function, tuple creation, pandas integration\n","\n","============================================\n","Saved row 15\n","\n","================================================================================\n","PROCESSING ROW 16: \n","Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","========== OUTPUT FOR ROW 16 ==========\n","\n","SUMMARY:\n","\n","Artificial Intelligence (AI) can significantly enhance an organization's security posture by accelerating the identification and containment of data breaches. The integration of User Behavior Analytics (UBA) with AI and machine learning can help detect and respond to insider threats quickly and precisely, which is a major concern for organizations with an average cost of $4 million per incident. UBA uses machine learning to analyze user behavior and detect anomalies, and when combined with a Security Information and Event Management (SIEM) solution, it can assist security professionals in detecting and responding to threats more effectively. By leveraging UBA, organizations can reduce containment time and minimize the impact of insider threats, with AI and automation potentially reducing containment time by 108 days on average, according to IBM's 2023 report.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the potential benefit of using Artificial Intelligence (AI) in improving an organization's security posture?\n","A: It can reduce the time to identify and contain a data breach by 108 days on average.\n","Q: What is the average cost of an Insider threat to an organization?\n","A: $4 million\n","Q: How many organizations were surveyed for IBM's Cost of a Data Breach Report 2023?\n","A: Over 500\n","Q: What type of analytics can help detect and respond to Insider threats quickly and precisely with the help of Artificial Intelligence (AI) and machine learning?\n","A: User Behavior Analytics (UBA)\n","Q: What is the main concern for security teams in organizations of all sizes?\n","A: Insider threats\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence, User Behavior Analytics, Machine Learning, Insider Threats, Security Posture, Data Breach, Automation, Threat Detection, Cost of a Data Breach Report\n","\n","============================================\n","Saved row 16\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 17 ==========\n","\n","SUMMARY:\n","\n","Meta has released Lama 3, an open-source language model that surpasses its predecessor, Lama 2, in performance and capabilities. Available in 8 billion and 70 billion parameter variants, Lama 3 excels in language nuances, contextual understanding, and complex tasks like translation and dialog generation. Its enhanced scalability and performance enable effortless handling of multi-step tasks, with improved response alignment and delivery diversity. While its performance is competitive, with varying levels of success in human evaluation and benchmark comparisons, the model's open-source nature and transparency are notable. Users can access and utilize the model through platforms like Hugging Face, Kaggle, or Meta, with instructions and examples provided in GitHub repositories, highlighting a comprehensive approach to responsible AI development.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the name of the YouTube channel owner?\n","A: Krishak\n","Q: What is the current time according to the speaker?\n","A: 2 a.m.\n","Q: What platform is the speaker currently on?\n","A: YouTube\n","Q: What is the speaker doing at 2 a.m.?\n","A: Recording a video for their YouTube channel\n","\n","KEY CONCEPTS:\n","\n","\n","\n","============================================\n","Saved row 17\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","========== OUTPUT FOR ROW 18 ==========\n","\n","SUMMARY:\n","\n","This lesson focuses on writing Python code for a decision boundary using the scikit-learn library. The instructor begins by searching for sklearn and the Naive Bayes algorithm on Google, examining the documentation and various use cases. Specifically, Gaussian Naive Bayes is explored, which will be utilized in the code. The scikit-learn library, abbreviated as sk-learn, plays a central role in the lesson. By reviewing the steps involved in implementing the Naive Bayes algorithm, the instructor provides a foundation for understanding decision boundaries in Python. The lesson aims to provide a comprehensive understanding of the topic, leveraging the capabilities of the scikit-learn library.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Python Programming', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What Python library is being used in the lesson to write the code for the decision boundary?\n","A: scikit-learn (often abbreviated as sk-learn)\n","Q: What algorithm is being used in the code, according to the Google search results?\n","A: Naive Bayes, specifically Gaussian Naive Bayes\n","Q: Where can you find the derivation of the Naive Bayes formula and its use cases?\n","A: On the page that appears in the Google search results for 'sklearn Naive Bayes'\n","Q: What is the goal for the viewer by the end of the next video or two?\n","A: To be able to write the code themselves\n","\n","KEY CONCEPTS:\n","\n","Naive Bayes, Gaussian Naive Bayes, scikit-learn, decision boundary, Python library, algorithm derivation\n","\n","============================================\n","Saved row 18\n","\n","================================================================================\n","PROCESSING ROW 19: \n","Log Normal Distribution in Statistics\n","================================================================================\n","\n","========== OUTPUT FOR ROW 19 ==========\n","\n","SUMMARY:\n","\n","The discussion centers on Gaussian and log normal distributions, two fundamental concepts in statistics. A Gaussian distribution is characterized by a bell curve, with data evenly distributed around the mean, whereas a log normal distribution involves a variable whose log is normally distributed. These distributions are commonly observed in real-world data, such as height, income, and product reviews. Recognizing and understanding these distributions is crucial for effective data analysis and modeling, as it enables standard scaling and enhances model accuracy, ultimately leading to more reliable predictions and insights.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Statistics', 'Machine Learning', 'Data Science']\n","\n","GENERATED Q&A:\n","\n","Q: What is the percentage of the total distribution that falls within one standard deviation in a Gaussian distribution?\n","A: 68 percent\n","Q: What type of distribution does the log of a log-normal distribution follow?\n","A: Gaussian distribution or normal distribution\n","Q: Why is it important to learn about different distributions such as Gaussian and log-normal distributions?\n","A: To determine the type of distribution that data follows, which is essential for scaling and applying algorithms to achieve higher accuracy\n","Q: What is the purpose of converting a distribution to a standard normal distribution?\n","A: To scale down the values to have a mean of zero and a standard deviation of one, making it easier to compare and analyze different datasets\n","Q: What technique is used to convert a log-normal distribution to a standard normal distribution?\n","A: Log normalization, which involves finding the log of each value and then applying a formula to convert it to a standard normal distribution\n","\n","KEY CONCEPTS:\n","\n","Gaussian Distribution, Log Normal Distribution, Standard Normal Distribution, Bell Curve, Empirical Formula, Standard Deviation, Mean Value, Regression Algorithm, Classification Algorithm, Log Normalization\n","\n","============================================\n","Saved row 19\n","\n","================================================================================\n","PROCESSING ROW 20: \n","Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","========== OUTPUT FOR ROW 20 ==========\n","\n","SUMMARY:\n","\n","This video series presents an end-to-end deep learning project focused on disease detection in potato plants, specifically early and late blight diseases. The project involves developing a mobile application using convolutional neural networks, with the goal of deploying it to Google Cloud. The process begins with data collection, including gathering images of healthy and diseased plants, followed by data cleaning and pre-processing. A convolutional neural network is then used for model building, with the trained model being exported and served using TF Serving and Fast API. The project utilizes technologies such as TensorFlow, CNN, and TensorFlow Lite, and covers technical architecture, data collection, and pre-processing. The series requires basic knowledge of Python and deep learning, and is beneficial for those seeking a career as a machine learning engineer or data scientist, ultimately resulting in a fully functional mobile application for plant disease prediction.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Deep Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main goal of the end-to-end deep learning project series in the agriculture domain?\n","A: To build a mobile application that can detect diseases in potato plants using deep learning and convolutional neural networks.\n","Q: What are the two common diseases that can affect potato plants and what are their causes?\n","A: Early blight is caused by a fungus and late blight is caused by a specific microorganism.\n","Q: What is the role of the data scientist in the project and what technology will be used behind the scenes?\n","A: The data scientist will work on building the application end-to-end and behind the scenes, it will be using deep learning and convolutional neural networks.\n","Q: What is the expected outcome of the mobile application developed by AtliQ Agriculture?\n","A: The mobile application will tell the farmer whether the potato plant is healthy or has one of the diseases.\n","\n","KEY CONCEPTS:\n","\n","deep learning, Machine Learning Ops, TF serving, fast API, Google Cloud functions, React Native, convolutional neural network, end-to-end application, model building, ML Ops\n","\n","============================================\n","Saved row 20\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","========== OUTPUT FOR ROW 21 ==========\n","\n","SUMMARY:\n","\n","The levels of autonomy in Large Language Model (LLM) applications vary, ranging from zero autonomy in code to maximum autonomy in completely autonomous agents. These levels include code, single LLM calls, chains, routers, and state machines or agents, each offering increasing autonomy. Routers can make decisions based on user input but lack memory and learning capabilities. In contrast, state machines and agents enable loops, human review, and refinement, and can control task flow, make decisions, and learn from mistakes, making them crucial for advanced LLM applications. This hierarchy of autonomy levels allows for the development of complex and sophisticated LLM systems, with state machines and agents being key components.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Natural Language Processing', 'LangChain']\n","\n","GENERATED Q&A:\n","\n","Q: What is the main difference between a chain and a router in LLM applications?\n","A: A chain is a fixed sequence of steps, whereas a router can decide which steps to take next based on the input.\n","Q: What is the primary advantage of using a state machine (agent) in LLM applications?\n","A: The primary advantage is that it can make decisions, loop back, and refine its output based on user feedback, allowing for more complex and intelligent behavior.\n","Q: How does a router in LLM applications determine which chain to execute?\n","A: A router uses an LLM to decide which chain to execute based on the user input, and then directs the control flow to the corresponding chain.\n","Q: What is the key characteristic that distinguishes an agent from a chain or router in LLM applications?\n","A: The key characteristic is the ability to control the flow of execution, make decisions, and loop back, which is not present in chains and routers.\n","Q: What is the main limitation of a router in LLM applications?\n","A: The main limitation is that it cannot remember previous conversations or learn from mistakes, which is addressed by the state machine (agent) level of autonomy.\n","\n","KEY CONCEPTS:\n","\n","Levels of Autonomy, LLM Applications, Gradient Descent, Attention Mechanism, Cognitive Architecture, Artificial Intelligence Agents, State Machine, Agent Executed, Human-in-the-Loop, Multi-Agent Systems, Advanced Memory Management, Adaptive Learning, Lang Chain, Router Mechanism, Chain Architecture\n","\n","============================================\n","Saved row 21\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","========== OUTPUT FOR ROW 22 ==========\n","\n","SUMMARY:\n","\n","This study delves into advanced prompt engineering topics, encompassing the handling of diverse prompt types, including text, image, and audio-based prompts. It examines fine-tuning techniques for pre-trained large language models, such as multitask learning and distillation, and outlines best practices for data pre-processing and cleaning. Furthermore, the study addresses the deployment of prompt engineering models in production environments, emphasizing ethical considerations like fairness and privacy. The overarching objective is to equip individuals with expertise in prompt engineering, facilitating the development of efficient models that can effectively handle various prompts and promote responsible innovation.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What are the different types of prompts that can be handled in prompt engineering?\n","A: Text-based prompts, image-based prompts, and audio-based prompts.\n","Q: What is multitask learning in the context of fine-tuning pre-trained language models?\n","A: Multitask learning involves training a model on multiple tasks simultaneously to help it learn more robust representations that can generalize to different users.\n","Q: What is the purpose of tokenization in data pre-processing for prompt engineering models?\n","A: Tokenization involves breaking down the text into smaller units, such as words or subwords, to help the model understand the meaning of the text more accurately.\n","Q: What is distillation in the context of fine-tuning pre-trained language models?\n","A: Distillation involves training a smaller model to mimic the behavior of a larger model, making the smaller model more efficient and faster than the larger model.\n","Q: Why is it important to consider ethical implications in prompt engineering?\n","A: Prompt engineering models have the potential to influence decision-making in many areas, such as education, healthcare, and finance, and can lead to unfair and discriminatory outcomes if the data used to train the model is not diverse and inclusive.\n","\n","KEY CONCEPTS:\n","\n","prompt engineering, fine tuning, pre-trained language models, multitask learning, distillation, tokenization, normalization, data pre-processing, data augmentation, self-supervised learning, gradient descent, logistic regression, multinomial logistic regression, cross entropy loss, attention mechanism\n","\n","============================================\n","Saved row 22\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","========== OUTPUT FOR ROW 23 ==========\n","\n","SUMMARY:\n","\n","The lecture explores the concept of eigenfaces and singular value decomposition (SVD) in image classification, utilizing examples of celebrity images to demonstrate face clustering in a lower-dimensional space. The process involves loading and cropping images, computing the average face, and applying SVD to obtain eigenfaces. These eigenfaces are then used to project each image into a lower-dimensional space, allowing for comparison and classification. However, experiments with images of Taylor Swift and other celebrities reveal the limitations of this approach, highlighting the need to consider factors such as skin tone and hair color in image classification. The discussion underscores the importance of a nuanced understanding of image classification techniques and their potential limitations.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Deep Learning', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What is the purpose of subtracting the average face from each image in the eigenface example?\n","A: To perform principal component analysis and find the principal components of an action hero.\n","Q: How are the images of Arnold and Stallone projected into the first three eigenfaces?\n","A: By taking the image vector and multiplying it by the transposed matrix of the first three eigenfaces.\n","Q: What is the result of projecting the images of Taylor Swift and Arnold Schwarzenegger into the first three eigenfaces?\n","A: The distributions of Taylor Swift and Arnold Schwarzenegger are more overlapping than those of Arnold and Stallone or Taylor and Stallone.\n","Q: Why do Taylor Swift and Arnold Schwarzenegger have more similar eigenfaces than Arnold and Stallone?\n","A: Because Taylor Swift and Arnold Schwarzenegger both have fair skin and blonde hair, resulting in more correlation between their images.\n","Q: What technique did Facebook use to improve their face classification accuracy to match human accuracy?\n","A: They started to infer the three-dimensional geometry of the actual human head from 2D images, using information such as shadows to determine the depth of facial features.\n","\n","KEY CONCEPTS:\n","\n","Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Image Classification, Eigen Heroes, Principal Components, Image Compression, Deep Neural Network Architectures, Face Classification, Three-Dimensional Geometry\n","\n","============================================\n","Saved row 23\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","========== OUTPUT FOR ROW 24 ==========\n","\n","SUMMARY:\n","\n","LangChain is a framework that enables large language models (LLMs) to interact with the real world by acting as a bridge between the models and external resources. It allows applications to access APIs, databases, and perform tasks such as sending emails, thereby overcoming the limitations of LLMs. Through LangChain, users can receive helpful and relevant information, as demonstrated by a simple example of planning a vacation to Paris, where the framework processes a query and provides a response. By utilizing multiple models, such as LLaMA 3, LangChain provides flexibility and versatility, making it a valuable tool for building applications that leverage the capabilities of LLMs to provide effective and practical solutions.\n","\n","TOPIC CLASSIFICATION:\n","\n","['LangChain', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What is the initial step in understanding how Lang Chain works?\n","A: Starting with a simple problem, such as planning a vacation.\n","Q: What is the user asking the CHBD to do in the given example?\n","A: The user is asking CHBD to book a flight, book a hotel, and suggest good restaurants for a trip to Paris.\n","Q: What happens to the user's query when they press enter?\n","A: The query is sent to an LLM model.\n","Q: What type of application is CHBD, according to the transcript?\n","A: A CHBD application might use a lot of models, such as Charity 3.\n","\n","KEY CONCEPTS:\n","\n","LLM model, gradient descent, attention mechanism, language model, natural language processing, sequence-to-sequence, text generation, conversational AI, language understanding\n","\n","============================================\n","Saved row 24\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","========== OUTPUT FOR ROW 25 ==========\n","\n","SUMMARY:\n","\n","This video emphasizes the significance of residual analysis in time series forecasting, covering the concept of residuals, their analysis, and application in improving forecasting methods using Python. Residuals, defined as the difference between fitted and actual values, are crucial in detecting trends and inconsistencies in models. The video demonstrates the use of residual analysis to enhance model performance, highlighting the importance of residuals having no autocorrelation and a mean of zero for unbiased forecasting. It showcases the use of statistical tests, such as the Ljung-Box test, and visualizations like histograms to evaluate model bias and performance. By analyzing residuals, model weaknesses can be identified and addressed, leading to improved forecasts and better model refinement. This straightforward analysis is highly recommended for those building forecasting models, allowing for more accurate and reliable predictions.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Time Series', 'Machine Learning', 'Python Programming']\n","\n","GENERATED Q&A:\n","\n","Q: What are residuals in time series analysis?\n","A: Residuals are the difference between the fitted value and the actual value of the time series.\n","Q: What is the main difference between residuals and errors in time series analysis?\n","A: Residuals are the difference between the fitted values and the actual values for the data the model has seen, while errors refer to the performance of the model on unseen data.\n","Q: What are the two key things to look for when analyzing residuals?\n","A: The residual should have no autocorrelation or partial autocorrelation, and the mean of the residual should be zero.\n","Q: What does it indicate if the residuals have autocorrelation or partial autocorrelation?\n","A: It indicates that the model has missed some information in the data.\n","Q: How can bias in the residuals be corrected?\n","A: Bias in the residuals can be corrected by adding or subtracting a constant value to offset the bias in the forecast.\n","\n","KEY CONCEPTS:\n","\n","residual analysis, time series forecasting, gradient descent, autocorrelation, partial autocorrelation, Holt-Winters model, exponential smoothing, Ljung-Box test, serial correlation, forecasting methods\n","\n","============================================\n","Saved row 25\n","\n","================================================================================\n","PROCESSING ROW 26: \n","Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","========== OUTPUT FOR ROW 26 ==========\n","\n","SUMMARY:\n","\n","This project demonstrates the development of an Artificial Intelligence agent that interacts with a database using SQL knowledge. The agent is built using LangGraph, Next.js, and models from watsonx.ai, with an in-memory database using SQLite. The application allows users to query the database using natural language, generating SQL queries to retrieve answers. The process involves creating a custom component, setting up environment variables, and connecting the frontend and backend components. The agent is then integrated with a large language model to execute SQL queries and retrieve data from the database. The demonstration highlights the ease of building such an agent and the importance of implementing guardrails to control access to the database. The project showcases the potential of using natural language to interact with databases, making it more accessible and user-friendly.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Artificial Intelligence', 'Natural Language Processing']\n","\n","GENERATED Q&A:\n","\n","Q: What type of database will be used in the project for demonstration purposes?\n","A: SQLite\n","Q: What framework will be used for building the frontend application?\n","A: Next.js\n","Q: What tool will be used to style the application without writing CSS?\n","A: Tailwind\n","Q: What is the name of the agent being built in the video?\n","A: ReAct agent\n","Q: Where will the models used in the project be running?\n","A: watsonx.ai\n","\n","KEY CONCEPTS:\n","\n","Artificial Intelligence, Large Language Models, SQL Knowledge, LangGraph, ReAct Agent, Next.js, In-Memory Database, SQLite, TypeScript, Tailwind CSS\n","\n","============================================\n","Saved row 26\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","========== OUTPUT FOR ROW 27 ==========\n","\n","SUMMARY:\n","\n","Prompt engineering, a subset of natural language processing, involves developing models that generate high-quality text outputs in response to prompts, leveraging pre-trained large language models fine-tuned for specific tasks. This field is vital for applications such as chatbots and language translation, where output quality directly affects user experience. By providing a comprehensive introduction to prompt engineering, including its fundamentals, advantages, limitations, and advanced fine-tuning techniques, individuals can gain a deeper understanding of this crucial field and its potential to enhance various language-based technologies.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What field is responsible for enabling chatbots and virtual assistants to generate accurate and coherent responses to user prompts?\n","A: Prompt engineering\n","Q: What is the primary benefit of using prompt engineering models over traditional rule-based or keyword-based approaches?\n","A: Generating text outputs that are more accurate, coherent, and contextually appropriate\n","Q: What are some potential limitations of prompt engineering models?\n","A: Struggling with complex and ambiguous prompts, and generating biased and inaccurate outputs due to underlying data or model architecture\n","Q: What is the main focus of prompt engineering within the field of Natural Language Processing (NLP)?\n","A: Building models that can generate high-quality text outputs in response to prompts or input\n","\n","KEY CONCEPTS:\n","\n","Natural Language Processing, Prompt Engineering, Large Language Models, Pre-trained Models, Fine-tuning, Language Translation, Content Generation, Chatbots, Attention Mechanism, Pre-trained Large Language Models\n","\n","============================================\n","Saved row 27\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:LLaMA call failed (attempt 1/3): upstream connect error or disconnect/reset before headers. reset reason: connection termination\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99901, Requested 482. Please try again in 5m30.912s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99895, Requested 482. Please try again in 5m25.728s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99883, Requested 482. Please try again in 5m15.36s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99865, Requested 497. Please try again in 5m12.768s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99859, Requested 497. Please try again in 5m7.584s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99847, Requested 497. Please try again in 4m57.216s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n","WARNING:__main__:LLaMA call failed (attempt 1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99830, Requested 448. Please try again in 4m0.191999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99823, Requested 448. Please try again in 3m54.144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:LLaMA call failed (attempt 3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99811, Requested 448. Please try again in 3m43.775999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:LLaMA call failed after all retries; returning empty text.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3083117966.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;31m# 10. RUN GENERATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGeneration completed. Run separate evaluation script next.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3083117966.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mqa_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mconcepts_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_concepts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3083117966.py\u001b[0m in \u001b[0;36mclassify_topic\u001b[0;34m(transcript, summary)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0mTRANSCRIPT\u001b[0m \u001b[0mCHUNK\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;31m\\\u001b[0m\u001b[0;34m\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"predicted_topics\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3083117966.py\u001b[0m in \u001b[0;36mllm_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGLOBAL_MIN_GAP\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnow\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLAST_TS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Respecting global wait: sleeping {wait:.1f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # Groq client\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_cot.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Groq API key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2200\n","GLOBAL_MIN_GAP = 110\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","STOPWORDS = {\n","    'the','a','an','in','on','for','to','and','or','of','with','as','by','at','from',\n","    'that','this','is','are','was','were','be','been','it','its','into','about','over',\n","    'under','between','across','through','their','they','you','your','we','our'\n","}\n","\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[logging.FileHandler(logfile, encoding=\"utf-8\"), logging.StreamHandler()],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # light acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. ROBUST JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"\n","    Extract JSON object even if model mixes text + reasoning + JSON.\n","    Finds outermost {...} and tries to parse.\n","    \"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. LLaMA (Groq) CALL WITH GLOBAL WAIT & RETRIES\n","#####################################################################\n","def llm_call(prompt: str, temperature: float = 0.15, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # global wait to be nice to API\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Respecting global wait: sleeping {wait:.1f}s\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            text = resp.choices[0].message.content if resp.choices else \"\"\n","            return (text or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"LLaMA call failed (attempt {attempt}/{retries}): {e}\")\n","            time.sleep(5 * attempt)\n","\n","    logger.error(\"LLaMA call failed after all retries; returning empty text.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. TASK PROMPTS (SUM, TOPIC, QA, CONCEPTS)\n","#####################################################################\n","\n","# 8.1 Summarisation (hierarchical, CoT hidden, JSON-only)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries = []\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation – chunk {i}/{len(chunks)}\")\n","        prompt = f\"\"\"\n","You are an expert at summarising educational transcripts.\n","\n","You may reason step by step internally, but you MUST NOT include your reasoning\n","or chain-of-thought in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Requirements:\n","- Write a concise, coherent paragraph (80–120 words) for this chunk.\n","- Academic and neutral tone.\n","- Capture the main instructional ideas, not low-level details.\n","- No bullet points, no lists, no headings.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{c}\\\"\\\"\\\"\"\"\"\n","        out = llm_call(prompt, temperature=0.15)\n","        j = extract_json(out)\n","        summary_chunk = j.get(\"generated_summary\", \"\").strip()\n","        if not summary_chunk:\n","            summary_chunk = out.strip()[:600]\n","        partial_summaries.append(summary_chunk)\n","\n","    combined_text = \" \".join(p for p in partial_summaries if p)\n","\n","    final_prompt = f\"\"\"\n","You are an expert research assistant.\n","\n","You may think step by step internally to combine ideas, but DO NOT reveal your reasoning.\n","Return ONLY a single JSON object in this format:\n","{{\"generated_summary\":\"<summary>\"}}\n","\n","Take the following draft chunk-level summaries and produce ONE final global summary:\n","\n","- Length: 120–160 words\n","- Style: academic, neutral, clear\n","- Content: preserve main topics, key arguments, and flow\n","\n","DRAFT SUMMARIES:\n","\\\"\\\"\\\"{combined_text}\\\"\\\"\\\"\"\"\"\n","    out2 = llm_call(final_prompt, temperature=0.15)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 8.2 Topic classification (MULTI-LABEL, CoT hidden, JSON-only)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at MULTI-LABEL topic classification for AI/ML educational content.\n","\n","You may reason step by step internally, but DO NOT reveal your chain-of-thought.\n","\n","Return ONLY a JSON object in this exact format:\n","{{\"predicted_topics\":[\"Topic1\",\"Topic2\", ...]}}\n","\n","Rules:\n","- Select ALL topics that apply to this transcript.\n","- Choose up to 3 topics.\n","- Use the summary hint to understand high-level context.\n","- Topics MUST be chosen only from this list:\n","{\", \".join(VALID_TOPICS)}\n","- If no topic fits, return [\"Other\"].\n","\n","SUMMARY HINT:\n","\\\"\\\"\\\"{summary[:350]}\\\"\\\"\\\"\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.20)\n","    j = extract_json(out)\n","    topics = j.get(\"predicted_topics\", [])\n","\n","    if not isinstance(topics, list):\n","        topics = []\n","\n","    cleaned = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for valid in VALID_TOPICS:\n","            if t.lower() == valid.lower():\n","                cleaned.append(valid)\n","                break\n","\n","    if not cleaned:\n","        cleaned = [\"Other\"]\n","\n","    # ensure uniqueness & max 3\n","    return list(dict.fromkeys(cleaned))[:3]\n","\n","\n","# 8.3 Q&A generation (CoT hidden, JSON-only, 3–5 pairs)\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at generating educational questions and answers from transcripts.\n","\n","You may think step by step internally, but you MUST NOT include your reasoning.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"generated_questions\":[{{\"q\":\"<question>\",\"a\":\"<answer>\"}}, ...]}}\n","\n","Requirements:\n","- Generate 3–5 Q&A pairs.\n","- Questions should test understanding of key concepts in the transcript.\n","- Mix factual, conceptual, and applied questions.\n","- Answers must be correct and based ONLY on the transcript content.\n","- Questions and answers must be clear, concise, and self-contained.\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.10)\n","    j = extract_json(out)\n","    qas = j.get(\"generated_questions\", [])\n","    lines = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = (qa.get(\"q\") or \"\").strip()\n","            a = (qa.get(\"a\") or \"\").strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 8.4 Key concept extraction (CoT hidden, JSON-only)\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","\n","    prompt = f\"\"\"\n","You are an expert at extracting key technical concepts from educational AI/ML transcripts.\n","\n","You may reason step by step in your mind, but DO NOT include any reasoning in the output.\n","\n","Return ONLY a single JSON object in this exact format:\n","{{\"key_concepts\":[\"concept1\",\"concept2\", ...]}}\n","\n","Requirements:\n","- Extract 5–10 domain-specific, high-value concepts.\n","- Prefer multi-word technical phrases (e.g., \"gradient descent\", \"attention mechanism\").\n","- Focus on terms that are central for understanding the content.\n","- Avoid generic words like \"data\", \"video\", \"lesson\", \"information\".\n","\n","TRANSCRIPT CHUNK:\n","\\\"\\\"\\\"{first_chunk}\\\"\\\"\\\"\"\"\"\n","    out = llm_call(prompt, temperature=0.15)\n","    j = extract_json(out)\n","    concepts = j.get(\"key_concepts\", [])\n","    if not isinstance(concepts, list):\n","        concepts = []\n","    cleaned = [c.strip() for c in concepts if isinstance(c, str) and c.strip()]\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 9. RUN PIPELINE (NO EVALUATION HERE)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    if \"Reference Summary\" not in df.columns:\n","        raise ValueError(\"Input must contain 'Reference Summary' for evaluation later.\")\n","\n","    results = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        existing = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in existing.columns:\n","            processed = set(existing[\"row_index\"].tolist())\n","            results = existing.to_dict(orient=\"records\")\n","            logger.info(f\"Resuming – {len(processed)} rows already done.\")\n","        else:\n","            processed = set()\n","    else:\n","        processed = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in processed:\n","            print(f\"Skipping row {idx} (already processed)\")\n","            continue\n","\n","        title = str(row.get(\"title\",\"\"))\n","        transcript = str(row.get(\"transcript\",\"\"))\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary = generate_summary(transcript)\n","            topics = classify_topic(transcript, summary)\n","            qa_text = generate_qa(transcript)\n","            concepts_text = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary = \"\"\n","            topics = [\"Other\"]\n","            qa_text = \"\"\n","            concepts_text = \"\"\n","\n","        # ----------- PRINT ALL TASK OUTPUTS TO CONSOLE -----------\n","        print(\"\\n========== OUTPUT FOR ROW\", idx, \"==========\")\n","\n","        print(\"\\nSUMMARY:\\n\")\n","        print(summary)\n","\n","        print(\"\\nTOPIC CLASSIFICATION:\\n\")\n","        print(topics)\n","\n","        print(\"\\nGENERATED Q&A:\\n\")\n","        print(qa_text)\n","\n","        print(\"\\nKEY CONCEPTS:\\n\")\n","        print(concepts_text)\n","\n","        print(\"\\n============================================\")\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts_text\n","        }\n","        results.append(rec)\n","\n","        # autosave\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nAll rows processed.\")\n","    return df_out\n","\n","\n","#####################################################################\n","# 10. RUN GENERATION\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nGeneration completed. Run separate evaluation script next.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ww7DrqafWBYL","executionInfo":{"status":"ok","timestamp":1763309340727,"user_tz":-330,"elapsed":1330302,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"d8b10b63-7a04-4f54-dcef-3801c499ee20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Groq API key loaded ✓\n","Skipping row 0 (already processed)\n","Skipping row 1 (already processed)\n","Skipping row 2 (already processed)\n","Skipping row 3 (already processed)\n","Skipping row 4 (already processed)\n","Skipping row 5 (already processed)\n","Skipping row 6 (already processed)\n","Skipping row 7 (already processed)\n","Skipping row 8 (already processed)\n","Skipping row 9 (already processed)\n","Skipping row 10 (already processed)\n","Skipping row 11 (already processed)\n","Skipping row 12 (already processed)\n","Skipping row 13 (already processed)\n","Skipping row 14 (already processed)\n","Skipping row 15 (already processed)\n","Skipping row 16 (already processed)\n","Skipping row 17 (already processed)\n","Skipping row 18 (already processed)\n","Skipping row 19 (already processed)\n","Skipping row 20 (already processed)\n","Skipping row 21 (already processed)\n","Skipping row 22 (already processed)\n","Skipping row 23 (already processed)\n","Skipping row 24 (already processed)\n","Skipping row 25 (already processed)\n","Skipping row 26 (already processed)\n","Skipping row 27 (already processed)\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","========== OUTPUT FOR ROW 28 ==========\n","\n","SUMMARY:\n","\n","This discussion explores Q-learning, a value-based method in reinforcement learning, which involves mapping situations to actions to maximize a reward signal. Q-learning determines a state-action value function, Q, to quantify total reward and find an optimal policy. The process updates a Q-table by repeating episodes of choosing actions and learning their values until they become stable. Using a gradient update rule and a learning rate, the Q-values are updated based on temporal difference errors. The agent learns to make decisions based on the highest Q-value, with the goal of achieving an optimal Q-table. As an off-policy algorithm, Q-learning allows the behavior policy to be decoupled from the target policy, enabling effective learning and decision-making. This approach is a key aspect of reinforcement learning, which is one of three machine learning paradigms, alongside supervised and unsupervised learning.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","GENERATED Q&A:\n","\n","Q: What are the three primary machine learning paradigms discussed in the series on reinforcement learning?\n","A: Supervised learning, unsupervised learning, and reinforcement learning.\n","Q: What is the primary objective of reinforcement learning?\n","A: To learn how to map situations to actions in order to maximize a numerical reward signal.\n","Q: What is the difference between value-based methods and policy-based methods in reinforcement learning?\n","A: Value-based methods determine a value function that quantifies the total reward, while policy-based methods determine an optimal policy directly.\n","Q: What is the purpose of the Q-value in Q-learning?\n","A: To quantify how good it is to be in a state and take a specific action in that state.\n","Q: What is the goal of the agent in the grid world example?\n","A: To learn an optimal policy to get to the +10 reward spot in the best possible way.\n","\n","KEY CONCEPTS:\n","\n","reinforcement learning, q-learning, value-based methods, policy-based methods, state value function, state action value function, Bellman equation, exploration policy, behavior policy, discount factor\n","\n","============================================\n","Saved row 28\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","========== OUTPUT FOR ROW 29 ==========\n","\n","SUMMARY:\n","\n","A logistic classifier is a type of linear classifier that utilizes a linear function to generate predictions from input data, such as image pixels. Through a matrix multiplication process, the model makes predictions using learned weights and bias terms. The primary objective is to determine the optimal weights and bias values that yield accurate predictions. The classification process involves converting scores into probabilities using a softmax function, which normalizes the scores to ensure they sum to 1, assigning higher probabilities to high scores and lower probabilities to low scores, thus facilitating effective classification.\n","\n","TOPIC CLASSIFICATION:\n","\n","['Machine Learning', 'Artificial Intelligence', 'Deep Learning']\n","\n","GENERATED Q&A:\n","\n","Q: What type of classifier is a logistic classifier considered to be?\n","A: A linear classifier\n","Q: What is the purpose of the weights and bias in a logistic classifier?\n","A: The weights and bias are where the machine learning comes in, and their values are trained to be good at performing predictions\n","Q: How are scores turned into probabilities in a logistic classifier?\n","A: By using a softmax function\n","Q: What are the properties of proper probabilities produced by the softmax function?\n","A: They sum to 1, are large when the scores are large, and small when the scores are comparatively smaller\n","Q: What are scores in the context of logistic regression also called?\n","A: Logits\n","\n","KEY CONCEPTS:\n","\n","logistic classifier, linear classifier, linear function, matrix multiply, softmax function, logistic regression, gradient descent, machine learning, linear function, attention mechanism\n","\n","============================================\n","Saved row 29\n","\n","All rows processed.\n","\n","Generation completed. Run separate evaluation script next.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_cot.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.3-70b-versatile/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["2227d336c3084c7c9e81fe9cd09738b0","28576fad32a14bfba94de8ee12814f92","8977e1c49b344141a1c6a745ec2f0dbb","a6b54c49684a4cdc99d5d69775879f85","6cb7c5d50cd748ab8eb40a565e754a69","b7866fb2cf2845bb8fcac0e84b4832a2","920b47c0face48c89d9401581b29524b","00a31ba3017e4afa84e9a1ced1d69f3f","f9135da903f14d0a92b2c39eb3198b72","0b2eb7852fa9437183958cc61e9ab985","6bc3fc40e20e446f90c38d3d542316c6","f7c310f9932840baa32ab42344718a25","0177ad66e543435cb1ff5963f2939b4a","d8ab6da16680462ab0e0565b65488c87","414b6d3c6f85452189dd0e42bbb90a33","c8b4062beb7141fcbe3e8a008cfddbdf","ebd61f22690d46a58fff7cedd53b6ddd","b59c1777cdfa4971a1751065c9605159","f861ae1af78448069c710dee3871debc","528413d44d4f4dc2819195822fdb02b4","7478438546694457a99e1ffc144f959e","e8809d10569a4d4493014f0ab8c75c72","8ab39c655aa54a13848ab295993d962b","4ca6e39d5a6d4198934afa80d5aac614","fc99fbca8eea45b0ba066c9b4b33147f","5528c978a521497ab7940f3ecaa68023","f753c46ff9614a9dbe976d2d501fe002","55a83a4fb3734aed89c850c9846b9563","605ce840e0fa4a16843dc73db943c95b","680ae4ea49e6402da465069a49287f4f","84a289f57d044b99a1ebe46ebdbf08d1","c493e2264d8a415583617a9ddb334186","44ff10432c144ccb89e03593ee79350d","9fc6abe2ec56463cabe50e0127072384","7a09468e701c4e8f82ea8363aba6a75e","6a577229d71846fe937f460ab47be791","b121d8e8d0f942d79634a01960c916ea","02be70e6cfa74d6a8327093f7e076c95","e895e2a10aff4888a826043f69dd36c3","b71cb1e97a6a47eca61d26f22c24f4c8","c057f00781d3461f88309cb0bb4eda1d","9f00a1448f8d4fbbad16d3ba53d585e0","d30c6259c10a4a388751b8ecd2a8cf5b","b0db0c431b004854abe240a9c0864f41","aea2754a5f6b468e9f21010ae51acc55","6e929641959f440ca857da702777f02c","422f46ddd9e64979a6313e4e2be95365","d78c4bc57de948b2bbe2e26a8fd029b4","27362f4448534869a5bac4b927da1c3a","c316df8377c64de195532bf57410b7de","5b96fed7ecb34f6aabc57c0e1ace9cf2","ee3151a7159147c0a1570eed1f622ee0","c2890fb98aaa44a1bec179b84b6c5fee","7f01c40b17064c969ead60fa0192d993","569675cd5ca9479e9e4a09c35dc40016","a574caa8a84e412ab5471e33d1fd23f3","758cf789ff424e20b6cca0bf344b3b80","3b9be7fb2b0042378921a3f2c5b5ddef","47f726698dde4572bb200f09679566bd","791b9bc99bc14e31aaf79520e9f29b0d","40b0b6ecd5ea425796a3a9f1ef2871f5","c3e6bae5a2c942de8ada66ce466a755a","efa887700eb0413a8b424335621140a3","9eab9a98805d44e0a2e80f91b0a67e68","49b68465f2c94e9f904713c61255898d","e030fda3efaa49b08b4289550153efa1"]},"id":"jOfcXvcITu5G","executionInfo":{"status":"ok","timestamp":1763310276245,"user_tz":-330,"elapsed":182070,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"a7535876-fef0-4ab5-dc6e-7fb7a414d43a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_cot.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2227d336c3084c7c9e81fe9cd09738b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7c310f9932840baa32ab42344718a25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ab39c655aa54a13848ab295993d962b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fc6abe2ec56463cabe50e0127072384"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aea2754a5f6b468e9f21010ae51acc55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a574caa8a84e412ab5471e33d1fd23f3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2913\n","  - BLEU: 0.0588\n","  - BERTScore F1: 0.8896\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9333\n","  - Jaccard Index: 0.3371\n","  - Micro F1: 0.4701\n","  - Macro F1: 0.4116\n","  - Weighted F1: 0.4143\n","\n","Q&A Generation:\n","  - BLEU: 0.0357\n","  - Diversity: 0.6098\n","  - Answerability: 0.6883\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4800\n","  - Recall@10: 0.1920\n","  - F1@10: 0.2743\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Chain of Thought Prompting/llama-3.3-70b-versatile/evaluation_final.json\n"]}]}]}