{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONADc8Bx5/7dc/2eaFWS+i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"38010d9aed614a37bb10b14fff55cfe2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41d00b5e60424ccc87a98cb70daacc1f","IPY_MODEL_7bcf9d44018b41dab6d404c52e1265ec","IPY_MODEL_445a3968d9f34dd79b460e9e2952fb40"],"layout":"IPY_MODEL_8780922f98344870a10909a164aea82f"}},"41d00b5e60424ccc87a98cb70daacc1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2c8e30c6b8d48b1888aa6b0100ecdbb","placeholder":"​","style":"IPY_MODEL_37d6124f1dcc4d239a236c8769fdbdac","value":"tokenizer_config.json: 100%"}},"7bcf9d44018b41dab6d404c52e1265ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11af29b29a4245a58d1c854ee6a1d1e4","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92c9076015f44cc88610344fae29660d","value":25}},"445a3968d9f34dd79b460e9e2952fb40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6d03786559e4d5980658e011a21e6d0","placeholder":"​","style":"IPY_MODEL_c52cb1cf4d854ea39937c1e7c6cf6429","value":" 25.0/25.0 [00:00&lt;00:00, 1.88kB/s]"}},"8780922f98344870a10909a164aea82f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2c8e30c6b8d48b1888aa6b0100ecdbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37d6124f1dcc4d239a236c8769fdbdac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11af29b29a4245a58d1c854ee6a1d1e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92c9076015f44cc88610344fae29660d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e6d03786559e4d5980658e011a21e6d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c52cb1cf4d854ea39937c1e7c6cf6429":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06ae722d48fc4df086dadb472618e782":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_effc959f00804d769efe7ea6d3fc2b13","IPY_MODEL_514dd1673a574b3a9bf3c46935d19134","IPY_MODEL_88bd0c7c1c854ef6828e1b3c385f5708"],"layout":"IPY_MODEL_cdc24d6de7cb47e983f03b3b3f8165fe"}},"effc959f00804d769efe7ea6d3fc2b13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c6c55d9cef348d9b9c8a3ef7db400a0","placeholder":"​","style":"IPY_MODEL_72815cc753bb4be98fbcdcde9c63b40a","value":"config.json: 100%"}},"514dd1673a574b3a9bf3c46935d19134":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_722677a4cd7b4917b752a04f549c7e25","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eff96f26131542ee9fd1732a8a07951e","value":482}},"88bd0c7c1c854ef6828e1b3c385f5708":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba7fd30819f54fc88363299440ea5f44","placeholder":"​","style":"IPY_MODEL_6a2c402203a74dd1bba315611053f033","value":" 482/482 [00:00&lt;00:00, 45.0kB/s]"}},"cdc24d6de7cb47e983f03b3b3f8165fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c6c55d9cef348d9b9c8a3ef7db400a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72815cc753bb4be98fbcdcde9c63b40a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"722677a4cd7b4917b752a04f549c7e25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eff96f26131542ee9fd1732a8a07951e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba7fd30819f54fc88363299440ea5f44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a2c402203a74dd1bba315611053f033":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5f90c11ceac04e37813fc2161de1c6e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9da31d77c5b444608e81638514a7dfcf","IPY_MODEL_1b897ffeedb64bbd9a1984916cdd3039","IPY_MODEL_e46eea3199e742feb68936bdfa62aa00"],"layout":"IPY_MODEL_ee23e13189bc4f5ab14e65cce46ad716"}},"9da31d77c5b444608e81638514a7dfcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9310a87ca4034a2b9db99696e998f337","placeholder":"​","style":"IPY_MODEL_185278150bc845499f4d010201697f64","value":"vocab.json: 100%"}},"1b897ffeedb64bbd9a1984916cdd3039":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa6a40d399d04109946e33f43130aec6","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b14ef7255f724b6cb21493d95fc7d172","value":898823}},"e46eea3199e742feb68936bdfa62aa00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fa881068d3c428faa9e3bba6e881493","placeholder":"​","style":"IPY_MODEL_5c0f0e1e62b249d28a70e90371b43c9b","value":" 899k/899k [00:00&lt;00:00, 4.94MB/s]"}},"ee23e13189bc4f5ab14e65cce46ad716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9310a87ca4034a2b9db99696e998f337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"185278150bc845499f4d010201697f64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa6a40d399d04109946e33f43130aec6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b14ef7255f724b6cb21493d95fc7d172":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9fa881068d3c428faa9e3bba6e881493":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c0f0e1e62b249d28a70e90371b43c9b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"25232d0d3ae9497cbb867e828f6f161f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f59c2e0cb05a46329fc89673e779d24c","IPY_MODEL_4e568a03d20e4883b34904089d1a8d8c","IPY_MODEL_bc7d42b808a74f9fb04abd20961f0597"],"layout":"IPY_MODEL_9139bc34cda34ba59b96eb3f022bfde5"}},"f59c2e0cb05a46329fc89673e779d24c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1684792b54724cc589b251c272f5b376","placeholder":"​","style":"IPY_MODEL_4d21f7c6c9a2454983a982dad62a4883","value":"merges.txt: 100%"}},"4e568a03d20e4883b34904089d1a8d8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a435ba35ff6487b802c8c9b1bca37a9","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b37f6feb8bc64f12a2bd3f496b95acae","value":456318}},"bc7d42b808a74f9fb04abd20961f0597":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c19336558ba4d76b8d250af1011d384","placeholder":"​","style":"IPY_MODEL_5d9dc15fed7747aaaa5c01b7d0a89d05","value":" 456k/456k [00:00&lt;00:00, 6.77MB/s]"}},"9139bc34cda34ba59b96eb3f022bfde5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1684792b54724cc589b251c272f5b376":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d21f7c6c9a2454983a982dad62a4883":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a435ba35ff6487b802c8c9b1bca37a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b37f6feb8bc64f12a2bd3f496b95acae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6c19336558ba4d76b8d250af1011d384":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5d9dc15fed7747aaaa5c01b7d0a89d05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e2ecfa1698f44381a4ece21b1698871e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e6920c966d24f14a24458e4e0ee8a13","IPY_MODEL_e8ef60988a90458db1d07a147dfc723f","IPY_MODEL_46fff378165b4040880c88ddf5ccaade"],"layout":"IPY_MODEL_0653d78532954c83a75767e6cacf5c98"}},"9e6920c966d24f14a24458e4e0ee8a13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7557378a587a4e90abc235622686428e","placeholder":"​","style":"IPY_MODEL_f8cd8dbc5d9f43c288f7ec2d4817237c","value":"tokenizer.json: 100%"}},"e8ef60988a90458db1d07a147dfc723f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_595be49e95c34fd991ffe9f1c430922f","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75e86a76a46a41d18adffc9d203cda11","value":1355863}},"46fff378165b4040880c88ddf5ccaade":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dbafe830a744f6495d4986c5718cb73","placeholder":"​","style":"IPY_MODEL_4ee1262b32e0489b8685f6aa7d030c5b","value":" 1.36M/1.36M [00:00&lt;00:00, 12.7MB/s]"}},"0653d78532954c83a75767e6cacf5c98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7557378a587a4e90abc235622686428e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8cd8dbc5d9f43c288f7ec2d4817237c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"595be49e95c34fd991ffe9f1c430922f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75e86a76a46a41d18adffc9d203cda11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3dbafe830a744f6495d4986c5718cb73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ee1262b32e0489b8685f6aa7d030c5b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f87cf9d7850c43599ca1aa5eac241e76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6133ce410cd04799b2b64bb0f5ce1a49","IPY_MODEL_43efbf970c004a02bfdc39849846f00b","IPY_MODEL_fdf473b5655543189a7a15fee2857925"],"layout":"IPY_MODEL_dd1cfcb948cd4aa1a9d294411a4bfa74"}},"6133ce410cd04799b2b64bb0f5ce1a49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3efe4b6837f47a38b0f675832b29fea","placeholder":"​","style":"IPY_MODEL_c7dc5932164e499ca2010fe1fc815d62","value":"model.safetensors: 100%"}},"43efbf970c004a02bfdc39849846f00b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_709730e281874e428529a9c6310db934","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b71cc3241484f27a1da858f778fcb67","value":1421700479}},"fdf473b5655543189a7a15fee2857925":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e0f1cf7d29345529adf85a6f26ab3f8","placeholder":"​","style":"IPY_MODEL_f2df88604ce045b185071d3cba289505","value":" 1.42G/1.42G [00:17&lt;00:00, 238MB/s]"}},"dd1cfcb948cd4aa1a9d294411a4bfa74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3efe4b6837f47a38b0f675832b29fea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7dc5932164e499ca2010fe1fc815d62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"709730e281874e428529a9c6310db934":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b71cc3241484f27a1da858f778fcb67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e0f1cf7d29345529adf85a6f26ab3f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2df88604ce045b185071d3cba289505":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mEIt0IjCsfLp","executionInfo":{"status":"ok","timestamp":1763867846664,"user_tz":-330,"elapsed":24982,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"3184afd7-f110-44a8-b6a5-d7eaf869a071"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: google-generativeai in /usr/local/lib/python3.12/dist-packages (0.8.5)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (0.6.15)\n","Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.28.1)\n","Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.187.0)\n","Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.38.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (5.29.5)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (2.11.10)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.67.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from google-generativeai) (4.15.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai) (1.26.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai) (1.72.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.2)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.31.0)\n","Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (0.2.1)\n","Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai) (4.2.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->google-generativeai) (0.4.2)\n","Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.76.0)\n","Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.71.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=a0245ef6a483ce441d20378102dcaf7d7819f750d6c81bd8f0f1de39d5828e2c\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, bert-score\n","Successfully installed bert-score-0.3.13 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install google-generativeai rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"zu9xqlKEtLOC","executionInfo":{"status":"ok","timestamp":1763867846680,"user_tz":-330,"elapsed":12,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"5d2b66bf-c9cb-42ba-81ea-93dba59f65e8"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","import google.generativeai as genai\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gemini-2.5-pro_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/gemini_key9.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","os.environ[\"GOOGLE_API_KEY\"] = API_KEY\n","genai.configure(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Gemini key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"gemini-2.5-flash\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 70       # seconds between calls (soft global wait)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GEMINI CALL WITH GLOBAL WAIT\n","#####################################################################\n","def gemini_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    model = genai.GenerativeModel(MODEL_NAME)\n","    cfg = {\"temperature\": temperature}\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = model.generate_content(prompt, generation_config=cfg)\n","            LAST_TS = time.time()\n","            return (getattr(resp, \"text\", \"\") or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Gemini call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Gemini failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class GeminiRoleTask:\n","    \"\"\"Thin wrapper that reuses gemini_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return gemini_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(GeminiRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(GeminiRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(GeminiRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(GeminiRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (kept same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = gemini_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PqLMN2aOtLVB","executionInfo":{"status":"ok","timestamp":1763882024733,"user_tz":-330,"elapsed":14178051,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"326d1dad-2a5c-487a-fc7c-401d03c58527"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro\n","Gemini key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement Learning with Human Feedback (RLHF) is a framework that integrates human input to guide and accelerate the training of reinforcement learning algorithms, enabling more informed and human-aligned decision-making. This approach leverages human feedback to significantly improve an agent's learning efficiency. A prominent application is the fine-tuning of large language models (LLMs) such as ChatGPT. This process involves training a reward model, where human evaluators rank multiple AI-generated responses to establish quality preferences. Subsequently, this reward model, often combined with algorithms like Proximal Policy Optimization (PPO), provides iterative feedback to fine-tune the AI. This iterative refinement enhances the AI's ability to generate high-quality responses that effectively align with human preferences and values, making RLHF fundamental for developing advanced AI systems.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the primary goal for Frank in the grid world environment?\n","A: The goal for Frank is to get to this plus 10 reward spot.\n","Q: How does human feedback help Frank learn faster in the grid world?\n","A: This allows Frank to learn faster and it also allows Frank to give responses that are more human favored.\n","Q: Why is a reward model trained as the first part of applying reinforcement learning with human feedback to Chat GPT?\n","A: To be a human advisor to Chat GPT, assessing how good a given answer is to a given question.\n","Q: Who provides the initial ranking of responses to train the rewards model for Chat GPT?\n","A: We as humans then take these responses and we rank them based on which was the best response versus the worst response.\n","Q: How is the rewards model used in the second part of fine-tuning Chat GPT?\n","A: This response along with the question is passed to the rewards model to generate a score that tells us how good was this response.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning with Human Feedback (RLHF), Reinforcement Learning Algorithm, Grid World, Reward Model, Proximal Policy Optimization (PPO), Q-learning, DQ Learning, Chat GPT, GPT Architecture, Fine-tuning, Backpropagation, Loss Function\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial series comprehensively examines Support Vector Machines (SVMs), focusing on how kernel functions enable nonlinear data separation and soft margins. Kernels, including Polynomial, Linear, and Gaussian types, are crucial for transforming data into higher dimensions, facilitating linear separability. The underlying computational process involves solving a quadratic programming problem to determine Alphas, support vectors, and the intercept, with tools like CVXopt used for educational demonstration and practical implementations leveraging libraries such as libsvm or NumPy. The 'C' parameter is essential for configuring soft margins, penalizing misclassifications, while its absence yields a hard margin. Effective SVM application necessitates selecting appropriate kernels and margin strategies to address diverse data separability challenges, including adapting binary SVMs for multi-class classification and tuning scikit-learn parameters.\n","\n","TOPICS:\n"," ['Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary focus of part 32 of this machine learning tutorial?\n","A: working with CVX opt and working with kernels being applied uh to our support Vector machine\n","Q: Why is CVX opt specifically useful in this tutorial's context?\n","A: so you can see directly the impact of a kernel and where it's actually being injected and modifying the initial formal support Vector machine\n","Q: How does this tutorial primarily serve educational purposes?\n","A: for looking at the kernels how they affect and also visualizing nonlinear and also um visualiz in the soft margin\n","Q: When would you almost certainly use libsvm instead of CVX opt for writing your own Support Vector Machine?\n","A: if you wanted to write your own [Support Vector Machine] you would almost certainly be using lib svm\n","Q: Who authored the 'pattern recognition and machine learning' book referenced in the tutorial?\n","A: Christopher Bishop's pattern recognition and machine learning book\n","\n","KEY CONCEPTS:\n"," machine learning, CVX opt, kernels, support Vector machine, pattern recognition, lib svm, nonlinear, soft margin, scikit-learn, quadratic programming solver, constraints\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompts are foundational inputs in prompt engineering, providing large language models (LLMs) with the initial context and constraints necessary for generating text outputs. These inputs exhibit considerable variability in complexity and form, ranging from simple questions to intricate statements incorporating specific requirements or multiple components. The judicious selection of prompt type is paramount, directly impacting the quality and relevance of the LLM's generated output. Essential features of prompts encompass their length, the specificity of their language, and embedded constraints dictating desired tone, style, or format. Achieving accurate and efficient results fundamentally relies on clearly articulating both the expected outcome and the desired execution methodology. Effective prompt construction, alongside prompt deconstruction—the systematic breakdown of prompts into individual components to identify core requirements and constraints—is critical for fostering a deeper understanding and enabling precise model interaction. Consequently, proficiency in both prompt construction and deconstruction is vital for ensuring precise and controlled AI output generation.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Generative AI', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What are prompts in the context of prompt engineering?\n","A: Prompts are the inputs that we give to our prompt engineering models.\n","Q: Why is it important to understand prompts?\n","A: They are the starting point for generating text outputs; they provide the context and constraints for the text to large language models.\n","Q: How does understanding the key features of prompts help users?\n","A: You understand the prompts more deeply and can provide more context and information to get accurate and efficient output.\n","Q: How do 'what you expect' and 'how you want it to be done' define a prompt?\n","A: These two things will define the prompt, and how well you define these two things will get you more accurate output.\n","Q: What is the process of deconstructing a prompt?\n","A: Deconstructing a prompt is the process of breaking it down into individual components to better understand key features and constraints.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Prompts, Large Language Models (LLMs), Text Outputs, Constraints, Key Features, Prompt Deconstruction, Pre-trained Models, Specific Language, Specific Format, SEO, Python Code\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents function as autonomous problem-solvers, capable of making independent decisions and executing tasks beyond explicit instructions. They extend their capabilities by utilizing specialized 'tools,' such as calculators or search engines. A primary methodology for their construction is the React Agent Pattern, which integrates Reasoning and Acting to mimic human cognitive processes. This pattern involves a cyclical workflow where a Large Language Model (LLM) first analyzes a problem, then determines an appropriate 'action,' which can be a direct response or the invocation of a tool. If a tool is used, the LLM provides the necessary 'action input,' subsequently observing the tool's output to guide further steps or conclude the task. Essentially, an AI agent combines an LLM's reasoning abilities with external tools for sophisticated problem-solving.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Langraph']\n","\n","Q&A:\n"," Q: What are Artificial Intelligence (AI) agents?\n","A: Agents are the problem solvers of the Artificial Intelligence (AI) World, capable of thinking on their own.\n","Q: How do AI agents differ from chains and routers?\n","A: Agents can decide for themselves what steps to take on their own, unlike chains and routers which follow specific instructions.\n","Q: What is the React agent pattern?\n","A: The React agent pattern is one of the best known patterns in Artificial Intelligence (AI) today to build agents; it stands for reasoning plus acting.\n","Q: How does the React pattern mimic human thinking?\n","A: The React pattern mimics how human beings think: first think about the problem, then take action, and then observe the result.\n","Q: When does the React agent pattern cycle end?\n","A: The cycle is over if the agent gets the answer; otherwise, it repeats until the final answer is obtained.\n","\n","KEY CONCEPTS:\n"," AI agents, Tools, React agent pattern, Reasoning plus acting (ReAct), Think action observation, LLM, Action input, LangChain, Control flow, API calls, Python function, LangGraph\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," This analytical deep dive examines a reflection agent system, focusing on its operational architecture and data flow to understand its internal mechanisms for iterative refinement in digital content creation. The investigation utilizes the 'smith.chain' web-based platform, integrating Lsmith with LangChain, to meticulously trace complex AI workflows. Lsmith captures and streams comprehensive operational data, enabling detailed tracing of full application executions (\"runs\") and granular individual component actions (\"traces\"). An illustrative example showcases a reflection agent system designed to generate a viral tweet. This system employs a generation agent to create initial content and a reflect agent to critique it, providing feedback for subsequent revisions. This generate-critique-refine loop repeats multiple times, leading to a highly optimized final output. Understanding this intricate collaboration of agent systems is crucial for optimizing iterative refinement processes and demonstrates the power of reflection agents for deep, iterative problem-solving across various complex tasks.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain']\n","\n","Q&A:\n"," Q: What is the primary purpose of tracing the reflection agent system?\n","A: to understand exactly what is happening where so that we'll understand how both of these systems are working together.\n","Q: Why is it important to understand how both systems are working together?\n","A: to deliver our final refined viral tweet.\n","Q: How will the speaker initiate the tracing process?\n","A: by going to this particular website smith. chain.\n","Q: What specific system will be traced in this section?\n","A: the reflection agent system that we built.\n","Q: Where will the speaker navigate to begin tracing the system?\n","A: to this particular website smith. chain.\n","\n","KEY CONCEPTS:\n"," reflection agent system, agent, reflection, agent system, AI system, smith. chain, tweet, Natural Language Processing (NLP), Machine Learning (ML), Agentic AI, workflow, refinement\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," This summary details the integration of LangChain with OpenAI's chat models, commencing with the installation of `langchain-openai` and the import of `ChatOpenAI`. Model initialization allows selection of LLMs like `gpt-4o` for advanced performance or `gpt-3` for cost efficiency. Interaction with the LLM is facilitated via the `llm.invoke()` method. Essential for operation is robust authentication, requiring the `OPENAI_API_KEY` to be defined in a `.env` file and loaded using `python-dotenv` to prevent \"API key missing\" errors. Insufficient platform balance is identified as another potential access issue. Data extraction from API responses prioritizes isolating the `content` property. LangChain significantly simplifies API interaction and response parsing. Furthermore, the discussion extends to advanced LLM usage, advocating for transmitting entire conversation histories—rather than isolated prompts—to enhance contextual understanding and generate more relevant, coherent responses through conversational memory.\n","\n","TOPICS:\n"," ['LangChain', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What package needs to be installed to work with OpenAI chat models in LangChain?\n","A: We have to install this package L chain Das open aai.\n","Q: Why is the `gpt-4o` model chosen for the example?\n","A: Because this is kind of the latest model that's been released by open aai.\n","Q: How do you import the necessary class for OpenAI chat models?\n","A: From longchain open Artificial Intelligence (AI) ... we want to import the chat open Artificial Intelligence (AI) class.\n","Q: When might a user choose `gpt-3` instead of `gpt-4o`?\n","A: If you're a little short on cash you can always go with GPD 3.\n","Q: What class is imported from the `langchain_openai` module to initialize the model?\n","A: We want to import the chat open Artificial Intelligence (AI) class.\n","\n","KEY CONCEPTS:\n"," LangChain chat models, OpenAI APIs, ChatOpenAI, langchain-openai, VS Code, LLM, GPT-4o, GPT-3, Artificial Intelligence (AI), Terminal, keyword parameter\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," Python's `sort()` method for lists of strings exhibits specific behaviors regarding case sensitivity. When sorting strings, uppercase letters precede lowercase letters, with alphabetical order maintained within each case group. For instance, words starting with 'A' will appear before words starting with 'a'. Applying `reverse=True` inverts this order, placing lowercase strings (in reverse alphabetical order) before uppercase strings (also in reverse alphabetical order). Furthermore, the `sort()` method can handle lists containing a mix of strings and numbers, consistently sorting numbers before strings. Understanding these default sorting mechanisms, particularly case sensitivity and the precedence of numbers over strings, is crucial for developers to anticipate outcomes and potentially normalize data for specific sorting requirements.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What is the default sorting order for a Python list containing strings with both uppercase and lowercase starting letters?\n","A: Sort puts uppercase words first, sorted alphabetically, then lowercase words, also sorted alphabetically.\n","Q: How does Python's sort method handle a list that contains both strings and numbers?\n","A: Python's sort method puts the numbers first and the strings when a list contains both.\n","Q: What is the outcome when a list of mixed-case strings is sorted in reverse order?\n","A: You get lowercase letters in reverse alphabetical order, then uppercase letters at the start in reverse alphabetical order.\n","Q: When might a user need to ensure all strings in a list are the same case before sorting?\n","A: You might have to make sure they were all lowercase or all uppercase if you wanted to sort them in a particular way.\n","Q: How does the position of a number change when a mixed string/number list is sorted in reverse order?\n","A: We get the number at the end of the list after the list has been sorted in reverse order.\n","\n","KEY CONCEPTS:\n"," Python, lists, strings, sort method, uppercase letter, lowercase letter, alphabetical order, reverse order, numbers, code, data structure, method\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," Optimal decision-making allocation between humans and Artificial Intelligence (AI) is task-dependent, with AI excelling at extreme confidence levels and humans demonstrating superior judgment in areas of AI uncertainty. Augmented intelligence, which integrates human and AI capabilities, often yields the highest success rates across a broad range of confidence scores. However, its efficacy hinges on mitigating human cognitive biases, particularly automation bias, where humans unduly favor AI suggestions. This bias is exacerbated by \"forced display\" methods. Effective human-AI collaboration requires strategic presentation of AI recommendations, such as allowing individuals to form initial impressions before consulting AI or utilizing \"optional display\" mechanisms. Counterintuitively, displaying AI's accuracy, especially when acknowledging potential error, can decrease human trust. Therefore, optimizing human-AI integration necessitates careful design of AI assistance to minimize biases and leverage augmented intelligence effectively.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: Who should make a single decision when humans and AI have varying strengths?\n","A: The answer is a fascinating combination of holistic curves and human bias.\n","Q: What primary challenge do financial analysts face in the fraud detection system described?\n","A: The analysts are overwhelmed with 90 percent of those alerts being false positives.\n","Q: How is the success rate tracked on the Y-axis of the proposed graph for alert assessment?\n","A: The Y axis tracks the success rate, tracking if that prediction turned out to be right.\n","Q: When does an Artificial Intelligence (AI) typically have a lower success rate?\n","A: Lower success rate when the Artificial Intelligence (AI) is not sure.\n","Q: Why is a human likely to do a better job than an Artificial Intelligence (AI) at a 50 percent confidence level?\n","A: A human is a little better at making the right decision when the Artificial Intelligence (AI) is unsure.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), fraud detection system, fraudulent transactions, financial analysts, false positives, success rate, prediction, confidence score, AI performance curve, human performance curves, AI algorithm\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Google has significantly enhanced Vertex AI with new APIs and service improvements, aiming to accelerate and refine generative AI application development, especially for enterprise use cases. A core objective is \"grounding\" applications to ensure reliable access to accurate enterprise data, thereby mitigating common developer challenges. Key launches include a Document Understanding API for complex format processing, enhanced Embedding and Vector Search APIs supporting hybrid queries, and a Ranking API for relevance refinement. Furthermore, the Grounded Generation API leverages Gemini for cited, evidence-based responses, complemented by the Check Grounding API for evidence-based fact-checking. These high-quality APIs, integrating Google's expertise, are designed as simple primitives for seamless developer workflows and integration with frameworks such as LangChain. Collectively, these advancements empower developers to construct robust, reliable, and high-quality generative AI solutions with enhanced efficiency.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'LangChain']\n","\n","Q&A:\n"," Q: Who is Demetrius Case and what is his primary focus within Cloud AI at Google?\n","A: I am a product manager within Cloud Ai and I'm focusing mostly on search and document Artificial Intelligence (AI).\n","Q: What is the main challenge developers face when building generative applications for enterprises, according to Demetrius?\n","A: Grounding: reliably access the right Enterprise data to produce accurate and consistent responses.\n","Q: How does the Document Understanding API improve the quality of generative applications?\n","A: Using our knowhow from doi to understand the documents, understand the structure and help then improve the quality of the applications.\n","Q: Why do the new Vertex AI APIs stand out in the market?\n","A: Those apis embed a lot of our Google know how... to produce solutions that solve the problem really really well.\n","Q: When would a developer use the Check Grounding API?\n","A: To fact check a statement against a number of facts, to say if it's supported or if there's contradicting evidence.\n","\n","KEY CONCEPTS:\n"," Vertex AI, Generative applications, Grounding, APIs, Document understanding API, Embedding API, Vector search, Hybrid search, Ranking API, LLM model, Grounded generation API, Check grounding API\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," Unitary matrices, defined by U U^T = I, are fundamental to the Singular Value Decomposition (SVD) as they preserve vector lengths, angles, and inner products, geometrically representing rotations. While the SVD's U and V matrices are unitary, the general matrix X itself is not, leading to spatial deformation. The SVD reveals how X maps a unit sphere in its row space into an ellipsoid in its column space. The left singular vectors (U) determine the ellipsoid's orientation, and the singular values (Sigma) define the lengths of its principal axes, illustrating how X stretches or compresses space and potentially alters dimensionality. The economy SVD offers a compact representation for rectangular matrices where U_hat^T U_hat equals the identity. Unitary transformations maintain geometric structure, whereas the SVD elucidates how a general matrix deforms space through rotation and scaling.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What is the primary characteristic of unitary matrices regarding the properties of vectors they transform?\n","A: Unitary transformations preserve angles between vectors and lengths of vectors.\n","Q: Why is it important for unitary transformations to preserve angles and lengths of vectors?\n","A: It's important for understanding the geometry how these vectors are related in that vector space.\n","Q: How do unitary transformations geometrically affect vectors in a space?\n","A: They just rotate vectors but they don't change the length or the angles between them.\n","Q: When is the complex conjugate transpose used instead of the standard transpose?\n","A: If X is complex... then instead of transpose we use what's known as the complex conjugate transpose.\n","Q: Who is considered the most famous and widely used example of a unitary transformation?\n","A: The Fourier transform is probably the most famous and widely used unitary transformation.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Matrix, Unitary matrices, Economy SVD, Identity matrix, Vector space, Fourier transform, Complex conjugate transpose, Inner product, Ellipsoid, Singular values, Principal axes\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Google Gemini Pro 1.5 is a unified, highly capable multimodal generative AI model, proficient in processing extensive text and image data. It features a substantial 1 million multimodal token context window, a significant advancement enabling it to handle large documents and complex inputs. Demonstrations highlight its ability to accurately extract information from a 402-page transcript and interpret abstract visual details from images. Practical implementation involves obtaining API keys and programmatic interaction via the `google-generativeai` library, supporting streaming responses for enhanced user experience. This model simplifies development workflows by consolidating diverse input processing, empowering sophisticated AI applications like Retrieval-Augmented Generation systems and fostering innovative project development.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Deep Learning']\n","\n","Q&A:\n"," Q: What does it mean that Google Gemini Pro 1.5 is a multimodal model?\n","A: it will be able to work with both text and images.\n","Q: Why is Google Gemini Pro 1.5 considered amazing?\n","A: with respect to the kind of application that I've actually checked right.\n","Q: How will the speaker demonstrate the use of Google Gemini Pro 1.5?\n","A: I'll show you some Hands-On application you know by running some of the code.\n","Q: When will the demo video for Google Gemini Pro 1.5 be shown?\n","A: first of all I'm going to show you the demo video.\n","Q: Who is the speaker of this YouTube video?\n","A: my name is krishn.\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence (AI), AI-powered application, Google Gemini Pro 1.5, End-to-end projects, Code, Images, Text, Multimodel, API key, Long context understanding, Experimental feature, Newest model\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," The rigorous evaluation and testing of prompt engineering models are paramount for ensuring their performance and reliability. Key technical metrics include perplexity, which quantifies a language model's predictive capability (with lower values indicating superior performance), and accuracy, measuring the correctness of generated responses. Human evaluation further assesses the nuanced quality of responses. For example, large language models are typically evaluated by calculating their accuracy and perplexity on custom datasets comprising source and target texts. Beyond initial assessment, debugging involves systematically analyzing generated responses for errors and identifying patterns to effectively fine-tune models. Testing on diverse datasets and tasks is crucial to determine a model's generalization ability to new or unseen data, often utilizing tools like visualization and cross-validation. This iterative and continuous process of evaluation and testing is essential for maintaining optimal model performance over time.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What are matrices used for in evaluating prompt engineering models?\n","A: Matrices are used to measure the performance of the model and its ability to generate accurate and meaningful responses.\n","Q: How does perplexity measure the performance of a language model?\n","A: Perplexity measures how well a language model predicts a sequence of words; the lower the perplexity, the better the language model is.\n","Q: Why is testing prompt engineering models on different data sets or tasks important?\n","A: By testing on different data or different tasks we can determine the model's ability to generalize on new or unseen data.\n","Q: What is one common technique for debugging and improving prompt engineering models?\n","A: One common technique is to analyze the generated response and identify common errors or patterns.\n","Q: How does human evaluation contribute to assessing prompt engineering models?\n","A: Human evaluation involves having human rate the quality of the responses.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering Models, Matrices, Perplexity, Accuracy, Human Evaluation, Language Model, Large Language Model, Data Set, Debugging, Fine-tune, Cross Validation, Pre-trained Large Language Models\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," Generative Artificial Intelligence (AI) produces novel content, such as text or images, by identifying patterns within existing datasets, frequently leveraging Large Language Models (LLMs) with specific knowledge cutoffs. Building upon this, an AI Agent integrates an LLM with external tools and memory, enabling it to execute autonomous actions and accomplish narrow, defined tasks, thereby surpassing basic Q&A functionalities. For example, an agent can retrieve real-time data using an API. The more advanced Agentic AI involves one or more AI agents autonomously addressing complex, multi-step objectives. These agents make independent decisions, utilize diverse tools, and can coordinate, as seen in comprehensive travel planning systems. This progression from Generative AI to AI Agent and then to Agentic AI marks a significant advancement in the complexity and autonomy of tasks achievable by AI systems.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Langraph']\n","\n","Q&A:\n"," Q: What is Generative Artificial Intelligence (AI)?\n","A: Generative Artificial Intelligence (AI) is an AI that can create new content either text, image, or video based on patterns learned from existing data.\n","Q: Why can't a simple Large Language Model (LLM) answer questions about tomorrow's flight prices?\n","A: It won't be able to answer because it has a knowledge cutoff date.\n","Q: How does an Artificial Intelligence (AI) agent complete a task?\n","A: It will complete a task using tools, memory, and knowledge.\n","Q: When does the complexity of tasks that can be performed increase in AI systems?\n","A: As you evolve from simple generative AI to AI agent to agentic AI, you are capable of performing more and more complex tasks.\n","Q: Who defines agentic systems into five levels, according to the speaker?\n","A: The speaker's friend, who is a creator of the Agno framework, defines these agentic systems into five levels.\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence (AI), Large Language Model (LLM), API, Knowledge cutoff, Artificial Intelligence (AI) agent, Tools, Memory, Autonomy, Multi-step reasoning, Agentic Artificial Intelligence (AI), Framework\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Covariance is a fundamental statistical measure quantifying the linear relationship between two random variables, X and Y. It is computed as the average of the product of their deviations from their respective means, given by `Cov(X, Y) = (1/n) * Σ[(Xi - μX) * (Yi - μY)]`. Notably, `Cov(X, X)` equals `Var(X)`. A positive covariance indicates a direct relationship where variables increase or decrease together, while a negative value signifies an inverse relationship. However, covariance solely reveals the direction of this linear association, not its strength or magnitude, a limitation addressed by other statistical measures. Despite this, it serves as a foundational concept in data analysis for understanding directional linear dependencies.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary goal of quantifying a relation between two variables like size and price?\n","A: Quantifying a relation is basically trying to find out the relationship between size and price.\n","Q: Why is covariance considered a very important topic in data analysis?\n","A: Covariance is one of the very important topic when we consider the data pre-processing or the data analysis.\n","Q: How does covariance help quantify a relationship between variables?\n","A: Covariance will actually help you to quantify a relationship such that if the X increases my Y will increase.\n","Q: When will the covariance between two variables, X and Y, always be positive?\n","A: Whenever I have a scenario where X is increasing and Y is increasing, my covariance will be positive.\n","Q: Why is Pearson correlation coefficient introduced as a subsequent technique after covariance?\n","A: Covariance does not say how much positivity or negativity we have, so Pearson correlation coefficient is used.\n","\n","KEY CONCEPTS:\n"," Statistics, Variance, Covariance, Data Pre-processing, Data Analysis, Random Variables, Mean, Features, Equation, Data Set, Pearson Correlation Coefficient\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement Learning (RL) centers on an agent learning an optimal policy to maximize a cumulative numerical reward signal over time. This process involves the agent interacting with its environment, making decisions based on observations, and receiving feedback through reward signals that quantify action quality. Learning occurs via trial and error, where the agent explores, executes actions, observes resulting states and rewards, and iteratively updates its policy. RL objectives vary based on task type; episodic tasks, such as games, aim for specific outcomes like winning, with discrete rewards. Continuous tasks, like financial trading, seek to maximize long-term profit, utilizing rewards parameterized by profit/loss or risk-adjusted metrics. A precisely defined reward function is fundamental for effectively guiding the RL agent towards its ultimate goal of maximizing cumulative reward.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is the primary objective of reinforcement learning?\n","A: The objective of reinforcement learning is to learn the optimal policy that maximizes a numerical reward signal.\n","Q: How does a reinforcement learning agent achieve its goal of maximizing cumulative reward?\n","A: This is achieved through trial and error learning, where the agent explores the environment by taking action and observes the resulting state.\n","Q: Why is the reward signal important for a reinforcement learning agent?\n","A: The reward signal basically provides feedback to the agent about the quality of the action.\n","Q: Who is responsible for defining the rewards in a reinforcement learning problem like Tic-Tac-Toe?\n","A: You will be defining all the rewards, such as plus one for winning, minus one for losing, and no reward for drawing.\n","Q: When do continuous tasks, such as Stock Market trading, typically end?\n","A: Such continuing tasks have no fixed end point.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Agent, Environment, Objective, Optimal Policy, Reward Signal, Cumulative Reward, Episodic Task, Continuous Task, State, Action, Reward Function\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," Python dictionaries are fundamental data structures that store data as unordered collections of mutable key-value pairs, known as items, enclosed in curly brackets. Keys must be immutable types, such as strings, numbers, or tuples, while values can be of any data type. Dictionaries can be created directly or dynamically, often utilizing the `dict()` function in conjunction with `zip()` to combine key and value lists, facilitating flexible data structuring. Core operations involve accessing, modifying, or deleting values by their respective keys using square bracket notation, highlighting their dynamic capabilities. Essential methods like `.items()`, `.keys()`, and `.values()` provide comprehensive views of the dictionary's contents, while `len()` and `del` manage size and specific entries. These structures are crucial for data mapping, exemplified by associating stock prices with their open, high, and close values, and are indispensable in data science applications, particularly when integrated with libraries such as Pandas.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A dictionary is made up of items, and each item consists of a key value pair.\n","Q: Why are dictionaries useful in Python?\n","A: Dictionaries are useful for mapping one item to another.\n","Q: How are key-value pairs separated within a dictionary?\n","A: The key value pairs are separated by a colon.\n","Q: When can you change a value associated with a key in a dictionary?\n","A: By accessing the key, for example, accessing key five, and changing that to be a Z.\n","Q: Who benefits most from understanding dictionaries in Python?\n","A: Those who want to go on and do data science, because they work very well with pandas.\n","\n","KEY CONCEPTS:\n"," Dictionary, Key-value pair, Item, Immutable, String, Number, List, Tuple, Function, Zip function, Data type, Pandas\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence (AI) and automation significantly enhance organizational security and threat mitigation. IBM's 2023 report indicates that organizations leveraging AI and automation reduced data breach identification and containment times by an average of 108 days. A key application is AI-driven User Behavior Analytics (UBA), which precisely detects costly insider threats by identifying anomalous user patterns. When integrated with Security Information and Event Management (SIEM) solutions like IBM Security QRadar, UBA leverages machine learning to establish normal user baselines, prioritize risks, and generate alerts. QRadar's AI capabilities further accelerate investigations by correlating events, mapping to MITRE ATT&CK, and visualizing indicators of compromise, reducing response times from hours to minutes. This automation empowers security analysts to shift from reactive alert management to proactive defense, thereby strengthening overall cybersecurity resilience against evolving threats.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What was a key finding for organizations that extensively used AI and automation?\n","A: It took 108 fewer days on average to identify and contain a data breach.\n","Q: Why are Insider threats a major concern for organizations?\n","A: The average cost of an Insider threat for an organization was $4.93 million.\n","Q: How can User Behavior Analytics (UBA) with AI and machine learning help organizations?\n","A: Help you detect and respond to Insider threats quickly and precisely.\n","Q: When was the 'cost of a data breach report' that provided these findings published?\n","A: IBM's cost of a data breach report 2023.\n","Q: Who are Insider threats a major concern for?\n","A: Organizations of all sizes.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), automation, user Behavior analytics (UBA), machine learning, emerging threats, security posture, data breach, Insider threats, containment, detection, response\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta has released Llama 3, a significant advancement in open-source large language models, available in 8 billion and 70 billion parameter versions. This model, pre-trained and instruction-tuned, demonstrates state-of-the-art performance across diverse tasks including language understanding, contextual reasoning, translation, dialogue generation, and code generation. Llama 3 was trained on an extensive dataset of over 50 trillion tokens, supporting an 8K context length, and exhibits superior accuracy against other open-source models while competing strongly with proprietary LLMs. Evaluations of open-source models generally indicate competitive performance, often surpassing GPT in MML, human evaluation, and general science/mathematics, though some show limitations in specific mathematical reasoning tasks compared to models like Gemini Pro 1. Llama 3 integrates a comprehensive responsible AI framework and is readily accessible for deployment via platforms like Hugging Face, with detailed resources for installation and local inference, underscoring the increasing viability of high-performing open-source AI solutions.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: Who introduces themselves at the beginning of the transcript?\n","A: my name is krishak\n","Q: What is Krishak welcoming his audience to?\n","A: welcome to my YouTube channel\n","Q: When is the current time mentioned by the speaker?\n","A: right now it is 2 a. m.\n","Q: How does Krishak begin his interaction with the audience?\n","A: hello my name is krishak\n","Q: Why is Krishak extending a welcome?\n","A: welcome to my YouTube channel\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," This segment details the practical implementation of machine learning algorithms for creating decision boundaries using Python and the `scikit-learn` library. It specifically demonstrates the application of the Gaussian Naive Bayes algorithm to construct a classifier. A key instructional approach involves leveraging online documentation, such as targeted searches for \"sklearn Naive Bayes,\" to effectively understand and utilize library functions. This methodology facilitates rapid prototyping and comprehension of library capabilities. The immediate objective is to enable learners to execute foundational machine learning models, with subsequent lessons exploring the theoretical principles of algorithms like Naive Bayes. This practical introduction aims to equip students with essential skills for implementing core machine learning models.\n","\n","TOPICS:\n"," ['Python Programming', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the name of the Python library that will be used a lot in this lesson?\n","A: The name of this library is scikit-learn, which is often abbreviated sk-learn.\n","Q: Why is Google being used in this lesson?\n","A: To help us use the documentation of that library to figure out how to use some of the functions that it has.\n","Q: How did the speaker find the specific Naive Bayes algorithm used?\n","A: By searching Google for 'sklearn Naive Bayes' and finding 'Gaussian Naive Bayes' as one of the results.\n","Q: When will the students be able to write the Python code themselves?\n","A: By the end of the next video or two, you will be able to write this code yourself.\n","Q: Who will be able to write the Python code for the decision boundary by the end of the next video or two?\n","A: You will be able to write this code yourself.\n","\n","KEY CONCEPTS:\n"," Python code, Python library, documentation, functions, scikit-learn, algorithm, Naive Bayes, Naive Bayes formula, Gaussian Naive Bayes, classifier\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," This discussion differentiates between Gaussian (normal) and log-normal distributions, highlighting their distinct characteristics and implications for machine learning preprocessing. Gaussian distributions are symmetrical, bell-shaped, and adhere to the 68-95-99.7 empirical rule for data within standard deviations. In contrast, log-normal distributions are defined by their logarithm following a Gaussian distribution, resulting in a typical right-skew, as seen in examples like income or product review lengths. Effective data preprocessing necessitates understanding these distributions. For Gaussian data, direct scaling to a standard normal distribution (mean=0, SD=1) is appropriate. However, log-normal data requires a logarithmic transformation to achieve a Gaussian form before standard scaling can be applied. This technique, termed log normalization, ensures features are on a consistent scale, significantly enhancing model accuracy.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the defining characteristic of a log normal distribution?\n","A: A random variable belongs to a log normal distribution if log of X is normally distributed.\n","Q: Why is it important to learn about various data distributions like Gaussian and log normal?\n","A: Learning distributions helps scale data to the same scale, leading to higher model accuracy.\n","Q: How does one convert a log normal distribution to a standard normal distribution?\n","A: First, find the log of the numbers to make it Gaussian, then apply (X_i - mu) / sigma to convert to standard normal.\n","Q: When might you observe a log normal distribution in real-world data, according to the lecture?\n","A: Examples include income of people, product comment length, and data for sentiment analysis.\n","Q: Who uses log normal distribution in their work, according to the examples given?\n","A: Companies like Amazon and those working with sentiment analysis data use log normal distribution.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, normal distribution, standard deviation, bell curve, log normal distribution, random variable, binomial distribution, Bernoulli's distribution, sentiment analysis, standard normal distribution, standard scaler, log normalization\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This project outlines an end-to-end deep learning solution for potato plant disease detection, aiming to mitigate significant economic losses for farmers. The core technology involves Convolutional Neural Networks (CNNs) integrated into a mobile application, enabling early and accurate diagnosis of diseases like early and late blight from plant images. The comprehensive workflow begins with collecting and augmenting diverse image data, followed by CNN model building. ML Ops principles are applied via TF Serving for model management, accessed by a FastAPI backend. For multi-platform deployment, optimized TensorFlow Lite models are served through Google Cloud Functions to a React Native mobile application. This cloud-centric framework demonstrates a practical application of deep learning in agricultural technology, providing a robust pipeline from data preparation and model training to scalable, mobile-friendly deployment for real-world diagnostic challenges.\n","\n","TOPICS:\n"," ['Deep Learning', 'Mlops', 'Data Science']\n","\n","Q&A:\n"," Q: What is the primary domain of the end-to-end deep learning project series discussed in the video?\n","A: The end-to-end deep learning project series is in the agriculture domain.\n","Q: Why are farmers who grow potatoes experiencing significant economic losses annually?\n","A: Farmers who grow potatoes are facing lot of economic losses every year because of various diseases that can happen to a potato plant.\n","Q: How will the mobile application assist farmers in identifying potato plant health issues?\n","A: The mobile application will tell them whether the potato plant is healthy or it has one of these diseases after taking a picture.\n","Q: Who is the Artificial Intelligence company taking on this project to solve agriculture problems?\n","A: AtliQ Agriculture is an Artificial Intelligence (AI) company that happens to solve problems in agriculture domain.\n","Q: When starting this project as a data scientist, what is the initial step you will take with your colleagues?\n","A: First you will gather in a room along with your colleagues and you will start discussing the technical architecture.\n","\n","KEY CONCEPTS:\n"," Deep Learning, ML Ops, TF Serving, FastAPI, Google Cloud, Google Cloud Functions, React Native, Convolutional Neural Network, Model Deployment, Data Collection, Backend Server, Artificial Intelligence\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," LLM applications demonstrate a clear progression in autonomy, evolving from zero-autonomy, deterministic code and basic single LLM calls, which struggle with real-world complexity and diverse tasks. Chains enhance capability by breaking tasks into sequential, specialized LLM steps, though they remain rigid and human-defined. Routers introduce LLM-driven decision-making, directing requests to appropriate chains, yet lack memory and adaptive learning. The highest level, State Machines or Agents, integrates routers with iterative loops, advanced memory, and adaptive learning. In these systems, the LLM controls the operational flow, enabling iterative refinement, human-in-the-loop interaction, and learning from past experiences, exemplified by multi-agent systems. This evolution signifies a critical shift from human-driven, one-directional processes to truly agent-executed, intelligent systems.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain']\n","\n","Q&A:\n"," Q: What is the primary characteristic of 'code' in terms of autonomy?\n","A: Code has zero autonomy and is 100% deterministic.\n","Q: Why is a single LLM call often insufficient for complex tasks?\n","A: Trying to get everything done in one shot often leads to confused or mixed up responses.\n","Q: How do 'chains' improve upon a single LLM call's capabilities?\n","A: Chains break it down into steps where each Artificial Intelligence (AI) is really good at one thing.\n","Q: What is a key difference in decision-making between a 'router' and 'chains'?\n","A: In a router, the Artificial Intelligence (AI) itself decides what steps to take next.\n","Q: Why is a 'State machine' considered an agent, unlike chains or routers?\n","A: In a state machine, we can have cycles and the flow is controlled by the llm, hence it is called an agent.\n","\n","KEY CONCEPTS:\n"," LLM applications, Autonomy, LLM call, Chains, Router, State machine, Agent, Human in Loop, Multi-agent systems, Adaptive learning, LangGraph, Control flow\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," Advanced prompt engineering encompasses the processing of diverse prompt types, including text, image, and audio, frequently leveraging pre-trained models for tasks such as object identification. Key fine-tuning techniques involve multitask learning, which builds robust representations through simultaneous training on multiple tasks, and distillation, enhancing efficiency by enabling smaller models to mimic larger ones. Effective data pre-processing is crucial, requiring tokenization for text and normalization for various formats to ensure high-quality input. For deployment in production environments, frameworks like TensorFlow Serving or Flask facilitate model accessibility. Ethical considerations are paramount, addressing potential biases, fairness, and privacy through the advocacy and use of diverse and inclusive training data to prevent discriminatory outcomes. Mastering this field necessitates a comprehensive understanding of multi-modal data, sophisticated model optimization, rigorous data practices, and responsible deployment.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Deep Learning', 'Generative AI']\n","\n","Q&A:\n"," Q: What are two advanced techniques for fine-tuning pre-trained models mentioned in the lecture?\n","A: multitask learning and distillation.\n","Q: How can bias occur in prompt engineering models?\n","A: Bias can occur when the data used to train the model is not representative of the whole population.\n","Q: Why is tokenization an important step in data pre-processing for prompt engineering models?\n","A: It can help the model understand the meaning of the text more accurately.\n","Q: When should a prompt engineering model be deployed in production?\n","A: Once you have built a prompt engineering model, you need to deploy it in production to make it accessible to users.\n","Q: Who benefits from the distillation technique in prompt engineering?\n","A: Smaller models, as it can make them more efficient and run very faster than the larger models.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Large Language Models (LLMs), Fine-tuning, Data Pre-processing, Tokenization, Multitask Learning, Model Distillation, Deployment, Ethical Considerations, Pre-trained Models, Normalization, Cross-entropy Loss\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," This lecture introduces Singular Value Decomposition (SVD) and eigenfaces as a foundational approach to image classification. The methodology involves transforming aligned images into high-dimensional vectors, computing an average face, and applying SVD to mean-centered data to derive eigenfaces, which represent principal components. Projecting images into this reduced eigenface space facilitates effective clustering and classification. While this technique successfully separated images of Arnold Schwarzenegger and Sylvester Stallone, and Taylor Swift from Stallone, a notable overlap occurred between Taylor Swift and Schwarzenegger. This limitation highlights how naive pixel-based classification can be unduly influenced by superficial features like skin and hair color. Consequently, achieving robust, human-level classification accuracy necessitates incorporating advanced techniques, such as inferring 3D facial geometry, to leverage deeper structural information beyond two-dimensional pixel data.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What is the primary goal of the eigenfaces example shown in the lecture?\n","A: We're going to find the eigen faces of a bunch of faces and use those to cluster two different people in eigenface space.\n","Q: How were the images of Arnold Schwarzenegger and Sylvester Stallone prepared for analysis?\n","A: They were cropped and roughly aligned so that their faces were more or less in the same place and filling the same box.\n","Q: Why did the classification of Taylor Swift and Arnold Schwarzenegger show more overlap than other pairs?\n","A: Taylor Swift and Arnold Schwarzenegger are both fair-skinned, blonde hair, with lighter pixels on average, leading to more correlation.\n","Q: What do the columns of the U matrix represent after computing the SVD of the B matrix?\n","A: The columns of U are going to be my eigen heroes, which are linear combinations of the images fed in.\n","Q: How did Facebook significantly improve its face classification accuracy to match or exceed human performance?\n","A: They started to realize that you could take those images and infer what the three-dimensional geometry of the actual human head would have been.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Eigenfaces, Principal Component Analysis (PCA), Principal Components, Feature Space, Image Classification, Training Data Set, Test Image, Cluster, Deep Neural Network Architectures, Greyscale, Column Vector\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," Large Language Models (LLMs), while possessing advanced reasoning, are inherently limited by their inability to directly interact with the real world or execute external actions. LangChain addresses this by serving as a pivotal framework that connects LLMs to external systems, including APIs, databases, and web resources. This empowers LLMs to transcend their role as purely analytical engines, transforming them into active agents capable of performing practical, real-world tasks. Specific functionalities include making bookings, sending emails, accessing private company databases for customer query resolution, navigating external web resources like Google and Wikipedia for information retrieval, and advanced data acquisition through website scraping. LangChain's modular design also allows for flexible integration and swapping of different LLM models. This framework fundamentally enhances the real-world utility and application scope of LLM-powered solutions, enabling their seamless integration into operational workflows and the automation of complex tasks.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI', 'Generative AI']\n","\n","Q&A:\n"," Q: What is one of the biggest limitations of large language models (LLMs) on their own?\n","A: They are smart and can talk about travel but they cannot actually interact with the real world.\n","Q: Why is a framework like LangChain needed when building applications with LLMs?\n","A: To build an application needing LLM reasoning and the ability to communicate with real-world APIs, databases.\n","Q: How does LangChain function in relation to LLMs and the real world?\n","A: Lang chain acts as a bridge between the llms and the real world.\n","Q: When does LangChain come into the picture for building applications?\n","A: When an application needs the reasoning ability of an LLM and the ability to communicate with the real world.\n","Q: Who benefits from LangChain's ability to easily switch LLM models?\n","A: Developers can easily switch out LLMs like GP4 with others without even touching the code they wrote with LangChain.\n","\n","KEY CONCEPTS:\n"," LangChain, LLM (Large Language Model), ChatGPT, GPT-3.5, GPT-4, Chat application, APIs (Application Programming Interfaces), Databases, Framework, Artificial Intelligence (AI), Hugging Face LLM\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residuals in time series analysis, defined as the differences between a model's fitted values and actual observations, are crucial for diagnosing model performance and enhancing forecasting accuracy. Ideal residuals exhibit no autocorrelation and a zero mean, signifying an unbiased model that has captured all underlying information. Deviations, such as significant autocorrelation identified through ACF/PACF plots and statistical tests like the Ljung-Box test (e.g., p-values below 0.05), indicate missed patterns or systematic biases, necessitating model re-evaluation. For instance, analysis of a Holt-Winters model's residuals revealed significant autocorrelation, pointing to uncaptured seasonality. While a histogram confirmed largely zero-centered residuals, a minor negative mean (-0.02) suggested negligible bias. Systematically identifying and addressing these issues through residual analysis is indispensable for robust model refinement, enabling precise corrections and improving the reliability of future forecasts.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: Residuals are simply the difference between the fitted value why hat and the value of the actual value of the time series.\n","Q: Why is it important to analyze your time series?\n","A: It allows you to diagnose essentially performance.\n","Q: How can residual analysis be used to improve a model?\n","A: We can use residual analysis to basically find out what the model is doing and how we can improve it.\n","Q: When should the mean of the residuals be zero?\n","A: The mean of the residual should be zero otherwise we have bi be biased.\n","Q: Who is the speaker in this video?\n","A: Eagle, a data scientist living in London.\n","\n","KEY CONCEPTS:\n"," Time series, Residuals, Forecasting methods, Residual analysis, Fitted value, Autocorrelation, Partial Autocorrelation, Young Box test, Bias, Holt Winters model, Train set, Test set\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This project details the development of an AI agent designed for natural language querying of databases, leveraging large language models (LLMs) for Text2SQL functionality. The architecture integrates a ReAct agent built with LangGraph, a Next.js frontend, and LLMs hosted on watsonx.ai, all interacting with an in-memory SQLite database. Frontend development involved setting up a responsive user interface with input fields, message display, and state management for user queries and LLM responses. On the backend, LangChain libraries facilitate dynamic LLM interactions, with message serialization ensuring robust communication. A local SQLite database was established, complete with schema definition and seeded data, enabling the agent to perform database operations. A `GetFromDB` tool was created and integrated, allowing the LLM to execute SQL queries asynchronously. Through refined system prompts, the LLM demonstrated its capacity to translate natural language into precise SQL, retrieve data, and present results, effectively answering complex queries. The development emphasizes the importance of robust guardrails to ensure secure and controlled database access, demonstrating that efficient Text2SQL agent deployment is achievable with strategic tool integration.\n","\n","TOPICS:\n"," ['Agentic AI', 'Langraph', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the primary goal of the AI agent being built in this video?\n","A: to build an agent that's able to use that SQL knowledge to connect to your databases.\n","Q: Why are large language models suitable for connecting to databases?\n","A: Most large language models have been trained on code, including SQL.\n","Q: How will the boilerplate project for the frontend application be set up?\n","A: I'll be running create-next-app at latest, together with the name of my project.\n","Q: When can the boilerplate code be found in the Text2SQL agent directory?\n","A: After the CLI is finished, we can find some boilerplate code in our Text2SQL agent directory.\n","Q: Who will the input box be used to type a message to?\n","A: an input box that we'll use to type a message to the large language model.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI) agent, database, large language models, SQL, LangGraph, ReAct agent, Next.js, watsonx.ai, in-memory database, SQLite, TypeScript, Text2SQL agent\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a specialized field within Natural Language Processing (NLP) focused on developing models that generate high-quality, coherent text outputs from user prompts. It leverages pre-trained Large Language Models (LLMs), such as GPT or Bard, which are subsequently fine-tuned for specific tasks and inputs. This methodology offers significant advantages over traditional rule-based approaches by producing more accurate and contextually appropriate text, making it crucial for applications like chatbots, language translation, and content generation. However, prompt engineering models can struggle with complex or ambiguous prompts and may generate biased outputs due to underlying data or model architecture. Despite these limitations, prompt engineering is considered essential for developing advanced, intelligent text generation systems, requiring a comprehensive understanding of its fundamental concepts, prompt analysis, and inherent benefits and drawbacks.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: Prompt engineering is a specialized field within natural language processing that focuses on building models that can generate high quality text outputs.\n","Q: Why is prompt engineering important?\n","A: It allows us to generate text outputs that are more accurate, coherent, and contextually appropriate than traditional rule-based or keyword-based approaches.\n","Q: How are prompt engineering models typically built?\n","A: These models are based on pre-trained large language models such as OpenAI GPT or Google Bard that are fine-tuned for specific tasks and inputs.\n","Q: When is prompt engineering especially important for applications?\n","A: It is especially important for applications such as chatbots, language translation, and content generation where output quality impacts user experience and engagement.\n","Q: What is one limitation of prompt engineering models?\n","A: Prompt engineering models may struggle with complex and ambiguous prompts or generate biased and inaccurate outputs due to underlying data or model architecture.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing (NLP), Chatbots, Virtual Assistants, Translation Software, Large Language Models (LLMs), GPT, Google Bard, Hugging Face Transformers, Fine-tuning, Prompt Analysis, Model Architecture\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning (RL) enables machines to learn optimal actions by maximizing a numerical reward signal, with Q-learning being a prominent value-based method. Q-learning determines an optimal policy by estimating state-action value functions, or Q-values, which represent the desirability of taking a specific action in a given state, often stored in a Q-table. The objective is to iteratively learn and update these Q-values through exploration and exploitation to maximize cumulative reward. The Bellman equation is fundamental, recursively defining Q-values by considering immediate rewards and discounted future maximum Q-values. Q-values are updated using the Temporal Difference (TD) error, which quantifies the discrepancy between the observed Q-value (derived from the Bellman equation) and the current expected Q-value. This error drives the iterative update rule: Q(S,A) = Q(S,A) + α * TD_error, where α is the learning rate. This process refines value estimates over multiple episodes, leading to a stable \"target policy\" that guides optimal action selection. Q-learning is an off-policy algorithm, distinguishing the behavior policy used for exploration from the learned target policy.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary objective of reinforcement learning?\n","A: Reinforcement learning is learning what to do, how to map situations to actions so as to maximize a numerical reward signal.\n","Q: How do value-based methods determine an optimal policy?\n","A: Value based methods will determine a value function that quantifies total reward and using this value function it'll determine the optimal policy.\n","Q: What type of reinforcement learning method is Q-learning?\n","A: Q learning is a value-based reinforcement learning method to solve problems.\n","Q: Why are we interested in learning the state-action value function for Q-learning?\n","A: We are interested in learning the state action value function because it's a function you can think of it more easily as a table.\n","Q: How is the observed Q-value calculated for a given state and action using the Bellman equation?\n","A: The Q value is given by the sum of the reward of the next state plus the maximum future Q value for that state.\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement Learning, Supervised Learning, Unsupervised Learning, Value-based Methods, Policy-based Methods, Optimal Policy, Agent, State, Action, Q-value, Bellman Equation\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier operates as a linear model, generating predictions through a linear function derived from the matrix multiplication of input features (X) with learned weights (W) and the addition of a bias (b). The core machine learning objective involves training the model to optimize these W and b parameters, thereby enhancing prediction accuracy for classification tasks. For instances requiring a single label per input, the raw scores, termed logits, are subsequently transformed into proper probabilities. This critical conversion is performed by the softmax function, which normalizes these scores to ensure they sum precisely to one. The ultimate goal of this process is to assign a high probability to the correct class while minimizing probabilities for all other classes, thus yielding robust, probability-based classifications.\n","\n","TOPICS:\n"," ['Machine Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is what's called the linear classifier. It applies a linear function to inputs to generate its predictions.\n","Q: How does a linear function generate predictions in a logistic classifier?\n","A: A linear function is just a giant matrix multiply. It takes all the inputs as a big vector and multiplies them with a matrix.\n","Q: Why are scores turned into probabilities in a logistic classifier?\n","A: We're going to want the probability of the correct class to be very close to one and the probability for every other class to be close to zero.\n","Q: What is the primary function of the softmax function?\n","A: It can take any kind of scores and turn them into proper probabilities. Proper probabilities sum to 1.\n","Q: When do probabilities become large after being processed by a softmax function?\n","A: Proper probabilities will be large when the scores are large and small when the scores are comparatively smaller.\n","\n","KEY CONCEPTS:\n"," Logistic classifier, Linear classifier, Linear function, Matrix multiply, Weights, Bias, Machine learning, Model, Classification, Probabilities, Softmax function, Logits\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n","\n","Role-based generation pipeline completed.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["38010d9aed614a37bb10b14fff55cfe2","41d00b5e60424ccc87a98cb70daacc1f","7bcf9d44018b41dab6d404c52e1265ec","445a3968d9f34dd79b460e9e2952fb40","8780922f98344870a10909a164aea82f","e2c8e30c6b8d48b1888aa6b0100ecdbb","37d6124f1dcc4d239a236c8769fdbdac","11af29b29a4245a58d1c854ee6a1d1e4","92c9076015f44cc88610344fae29660d","e6d03786559e4d5980658e011a21e6d0","c52cb1cf4d854ea39937c1e7c6cf6429","06ae722d48fc4df086dadb472618e782","effc959f00804d769efe7ea6d3fc2b13","514dd1673a574b3a9bf3c46935d19134","88bd0c7c1c854ef6828e1b3c385f5708","cdc24d6de7cb47e983f03b3b3f8165fe","8c6c55d9cef348d9b9c8a3ef7db400a0","72815cc753bb4be98fbcdcde9c63b40a","722677a4cd7b4917b752a04f549c7e25","eff96f26131542ee9fd1732a8a07951e","ba7fd30819f54fc88363299440ea5f44","6a2c402203a74dd1bba315611053f033","5f90c11ceac04e37813fc2161de1c6e1","9da31d77c5b444608e81638514a7dfcf","1b897ffeedb64bbd9a1984916cdd3039","e46eea3199e742feb68936bdfa62aa00","ee23e13189bc4f5ab14e65cce46ad716","9310a87ca4034a2b9db99696e998f337","185278150bc845499f4d010201697f64","fa6a40d399d04109946e33f43130aec6","b14ef7255f724b6cb21493d95fc7d172","9fa881068d3c428faa9e3bba6e881493","5c0f0e1e62b249d28a70e90371b43c9b","25232d0d3ae9497cbb867e828f6f161f","f59c2e0cb05a46329fc89673e779d24c","4e568a03d20e4883b34904089d1a8d8c","bc7d42b808a74f9fb04abd20961f0597","9139bc34cda34ba59b96eb3f022bfde5","1684792b54724cc589b251c272f5b376","4d21f7c6c9a2454983a982dad62a4883","4a435ba35ff6487b802c8c9b1bca37a9","b37f6feb8bc64f12a2bd3f496b95acae","6c19336558ba4d76b8d250af1011d384","5d9dc15fed7747aaaa5c01b7d0a89d05","e2ecfa1698f44381a4ece21b1698871e","9e6920c966d24f14a24458e4e0ee8a13","e8ef60988a90458db1d07a147dfc723f","46fff378165b4040880c88ddf5ccaade","0653d78532954c83a75767e6cacf5c98","7557378a587a4e90abc235622686428e","f8cd8dbc5d9f43c288f7ec2d4817237c","595be49e95c34fd991ffe9f1c430922f","75e86a76a46a41d18adffc9d203cda11","3dbafe830a744f6495d4986c5718cb73","4ee1262b32e0489b8685f6aa7d030c5b","f87cf9d7850c43599ca1aa5eac241e76","6133ce410cd04799b2b64bb0f5ce1a49","43efbf970c004a02bfdc39849846f00b","fdf473b5655543189a7a15fee2857925","dd1cfcb948cd4aa1a9d294411a4bfa74","f3efe4b6837f47a38b0f675832b29fea","c7dc5932164e499ca2010fe1fc815d62","709730e281874e428529a9c6310db934","4b71cc3241484f27a1da858f778fcb67","0e0f1cf7d29345529adf85a6f26ab3f8","f2df88604ce045b185071d3cba289505"]},"id":"0K-NI_U2tLbx","executionInfo":{"status":"ok","timestamp":1763883145247,"user_tz":-330,"elapsed":178894,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"3533ab65-c5ea-4f36-9c0b-6ad9e48c1c83"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/gemini-2.5-pro_role_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38010d9aed614a37bb10b14fff55cfe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ae722d48fc4df086dadb472618e782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f90c11ceac04e37813fc2161de1c6e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25232d0d3ae9497cbb867e828f6f161f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ecfa1698f44381a4ece21b1698871e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f87cf9d7850c43599ca1aa5eac241e76"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3448\n","  - BLEU: 0.0885\n","  - BERTScore F1: 0.8951\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9000\n","  - Jaccard Index: 0.3665\n","  - Micro F1: 0.4894\n","  - Macro F1: 0.4594\n","  - Weighted F1: 0.4712\n","\n","Q&A Generation:\n","  - BLEU: 0.0238\n","  - Diversity: 0.7485\n","  - Answerability: 0.7600\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5200\n","  - Recall@10: 0.2080\n","  - F1@10: 0.2971\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gemini-2.5-pro/evaluation_final.json\n"]}]}]}