{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgEjH0gjY9Wo6mPLdpPMtU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"id":"Ws3wYzvkjbhz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763833854509,"user_tz":-330,"elapsed":5229,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"3f3881ce-3f4a-4f4a-9d26-f7564fba1da5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: groq in /usr/local/lib/python3.12/dist-packages (0.36.0)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n","Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":18}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"r2pSk1VtNwyn","executionInfo":{"status":"ok","timestamp":1763826067529,"user_tz":-330,"elapsed":17,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"53a97e9b-de41-4b93-a37f-925ce6fea4a6"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"openai/gpt-oss-20b\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"73er33c_Nw54","executionInfo":{"status":"error","timestamp":1763829835480,"user_tz":-330,"elapsed":3767881,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"08d5549d-ac1f-4aed-a893-78ab8589addb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning through human feedback (RLHF) augments standard RL by incorporating human judgments to shape agent behavior. In a grid‑world example, an agent named Frank learns to reach a high‑reward cell using Q‑learning or PPO, while a human mentor steers actions toward optimal paths, thereby accelerating convergence. The human component functions as a reward signal, biasing policy updates toward desirable outcomes and reducing sample complexity compared to unguided exploration. For ChatGPT, a reward model—trained on ranked human responses—provides scalar scores that are fed into a proximal policy optimization loop, enabling iterative policy refinement. The resulting fine‑tuned model demonstrates higher answer quality and alignment with human preferences, illustrating RLHF’s practical impact on large language models. Thus, RLHF provides a systematic framework for integrating subjective human insight into algorithmic learning, yielding agents that act more safely and effectively.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the primary purpose of the reward model in ChatGPT's RLHF process?\n","A: to assess and score the quality of answers generated by chat gbt\n","Q: Why does human feedback accelerate the learning process for Frank?\n","A: human feedback accelerates the learning process\n","Q: How does the reward model generate a score for an answer?\n","A: the output of this GPT network is a number, a score that says how good was this answer to this input question\n","Q: When does Frank receive human feedback during his learning?\n","A: while Frank is learning with a reinforcement learning algorithm us humans can also provide our feedback to Frank as a mentor\n","Q: Who provides feedback to Frank?\n","A: us humans\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Human Feedback, Grid World, Q-Learning, Deep Q-Learning, Proximal Policy Optimization, Reward Model, GPT, Backpropagation, Fine-tuning, Iterative Training, Agentic AI\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," {\"generated_summary\":\"During the lecture, the speaker demonstrates how CVXopt can solve the quadratic‑programming formulation of a support‑vector‑machine (SVM) and illustrates the effect of different kernel functions on the decision boundary. The discussion references Matthew Blondell’s GitHub implementation and Christopher Bishop’s Pattern Recognition and Machine Learning for theory, and presents the standard SVM objective −½‖w‖² + C∑ξᵢ subject to yᵢ(w·xᵢ+b) ≥ 1−ξᵢ, ξᵢ ≥ 0, together with kernel transformations that map data into higher‑dimensional feature spaces. Visualizations of linear, polynomial ( (1+x·y)ᵖ, p=3 ), and other kernels show how nonlinear margins arise; hard\n","\n","TOPICS:\n"," ['Machine Learning']\n","\n","Q&A:\n"," Q: What is the main purpose of using CVX opt in this tutorial?\n","A: CVX opt is used to see directly the impact of a kernel and where it's being injected and modifying the initial formal support Vector machine.\n","Q: Why might you not use CVX opt for a support vector machine in practice?\n","A: CVX opt is not something you're probably ever going to use; for a support vector machine you'd almost certainly use lib svm.\n","Q: How does the quadratic programming solver in CVX opt work?\n","A: It minimizes 12 x^t p * X plus q^t * X subject to constraints Gx <= ... and Hx = ... using quadratic programming solver.\n","Q: When can you visualize the soft margin in this tutorial?\n","A: You can visualize the soft margin in pyit learn, though it's a little harder to see the kernel's impact.\n","Q: Who provided the original code used in this tutorial?\n","A: This is not my code I grabbed this from uh Matthew blondell's GitHub.\n","\n","KEY CONCEPTS:\n"," Support Vector Machine, Kernel, CVXOPT, Quadratic Programming, Soft Margin, LibSVM, Pattern Recognition, Machine Learning, Visualization, Quadratic Programming Solver, Constraints, scikit-learn\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," No draft summaries were provided, thus no technical content could be combined into a coherent summary. In the absence of source material, it is not possible to synthesize a comprehensive academic summary that meets the specified length and technical depth. This limitation underscores the importance of providing complete draft content to enable accurate synthesis and ensures that any resulting summary accurately reflects the underlying technical concepts and conclusions. Future iterations should include the relevant draft summaries to allow for a thorough integration of key findings, methodological details, and theoretical implications, thereby producing a robust and academically rigorous synthesis. Until such content is supplied, any attempt to produce a substantive summary would be speculative and potentially misleading.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Generative AI', 'Natural Language Processing']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," Prompt engineering, Prompt, Large language model (LLM), ChatGPT, Google Bard, Prompt types (question, statement, multi-input, constraint), Prompt features (length, language, constraints, tone, style), Prompt deconstruction, Output constraints (word count, format), SEO optimization, Pre-trained models, Python code generation\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents autonomously solve problems by deciding actions without explicit instruction, contrasting with chains and routers that follow predefined commands. They employ specialized tools—search, calculators, API calls—much like a chef uses utensils, to execute tasks. The React agent pattern, denoting Reasoning + Acting, emulates human cognition by iteratively performing Think, Action, Observe cycles until a satisfactory outcome is achieved. Practically, a large language model first evaluates the prompt, selects an appropriate tool and arguments, forwards them to an orchestration framework such as LangChain, receives the tool’s output, and observes whether the objective is satisfied. This closed feedback loop allows the agent to refine its reasoning and tool usage while preserving full contextual information. Consequently, React agents integrate LLM reasoning with executable tools, enabling autonomous, context‑aware decision making.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'LangChain', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is an AI agent?\n","A: Agents are the problem solvers of the Artificial Intelligence (AI) World, capable of thinking on their own and making autonomous decisions.\n","Q: Why are tools important for agents?\n","A: Tools are the special abilities we give to Artificial Intelligence (AI) like giving a calculator tool or a search engine tool or a calendar tool.\n","Q: How does the react agent pattern work?\n","A: The react pattern is reasoning plus acting: think, action, action input, observe, loop until final answer.\n","Q: When does the agent decide to use a tool?\n","A: When the LLM decides it cannot answer by itself and should use a particular tool.\n","Q: Who is responsible for executing the tool in the react pattern?\n","A: LangChain executes the tool and returns the output back to the LLM.\n","\n","KEY CONCEPTS:\n"," AI agents, Autonomous decision making, Tool (function), React agent pattern, Reasoning + Acting, LLM (Large Language Model), LangChain, Tool execution, Observation, Think‑Action‑Observation loop, API calls, LangGraph\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The presentation examines a tracing exercise of a reflection agent system integrated within a LangChain framework, illustrating how the agent collaborates with complementary modules to produce a tweet optimized for virality. By navigating to the smith.chain web portal, the speaker demonstrates real‑time monitoring of data flow and decision points, revealing the agent’s role in content filtering, sentiment adjustment, and iterative feedback loops that enhance engagement metrics. Tracing via LSmith captures each run, recording execution time and component actions, enabling developers to inspect the workflow and adjust behavior in real time. An example of six iterative cycles shows how the reflect agent critiques initial outputs and supplies actionable feedback that the generation agent incorporates, culminating in a refined, high‑impact tweet with emojis and hashtags. The exercise underscores the importance of transparent architecture for reproducibility, ethical content generation, and debugging, and demonstrates how reflection agents transform single‑pass generation into a disciplined, multi‑step creative process.\n","\n","TOPICS:\n"," ['LangChain', 'Generative AI', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What system are we tracing in this section?\n","A: the reflection agent system that we built.\n","Q: Why are we tracing the reflection agent system?\n","A: so we can understand exactly what is happening where.\n","Q: How do the systems work together?\n","A: both of these systems are working together to deliver our final refined viral tweet.\n","Q: When do we go to the website?\n","A: to do that I'm just going ahead to this particular website smith. chain.\n","Q: Who is the speaker addressing?\n","A: guys\n","\n","KEY CONCEPTS:\n"," reflection agent, system, trace, viral tweet, refinement, website, smith.chain, agent, final, delivery, multi-agent collaboration, output generation\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The lecture demonstrates the practical integration of LangChain with the OpenAI API, beginning with the installation of the LangChain OpenAI package via pip and the correction of a syntax error. The ChatOpenAI class is imported and instantiated, typically with the gpt‑4o model for maximum capability, though gpt‑3.5 is recommended for cost‑constrained scenarios. The example illustrates invoking the model to compute the square root of 49, highlighting the use of the \"invoke\" method and the necessity of an OpenAI API key. A missing key triggers an error, prompting the creation of a .env file and the use of python‑dotenv to securely load the key into the environment. LangChain abstracts API communication, returning a response object that includes metadata, token usage, and content; developers can extract the content property to isolate results. The session also emphasizes monitoring token usage, handling insufficient balance errors, and incorporating full dialogue history to enhance contextual relevance and response coherence. Overall, LangChain streamlines initialization, authentication, error handling, and output extraction, enabling efficient deployment of OpenAI models while balancing performance and cost considerations.\n","\n","TOPICS:\n"," ['LangChain', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What command is used to install the LangChain OpenAI package?\n","A: pip install langchain-openai\n","Q: Why might the installation fail initially?\n","A: Because a percentage sign needs to be removed from the command\n","Q: How do you import the chat model from LangChain?\n","A: from langchain_openai import ChatOpenAI\n","Q: When should you use GPT-3.5 instead of GPT-4o?\n","A: If you are short on cash, you can use GPT-3.5\n","Q: Who is the class used to initialize the model?\n","A: ChatOpenAI\n","\n","KEY CONCEPTS:\n"," LangChain, OpenAI API, ChatOpenAI, LLM, GPT-4, GPT-3, VS Code, Terminal, pip, model parameter, pricing, advanced model\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," The transcript explains Python’s built‑in sort method ordering list elements containing strings with mixed capitalization. Upper‑case initial letters are positioned before lower‑case ones, each group sorted alphabetically; reversing the sort yields lower‑case words in reverse alphabetical order followed by upper‑case words similarly reversed. The speaker cautions that for consistent ordering, normalizing case (e.g., converting all to lower‑case) may be necessary. When a list mixes strings and integers, the sort places numeric values before textual ones, with the numeric element appearing at the start of the sorted list. Reversing this mixed‑type sort moves the numeric element to the end, preserving the relative order of the remaining items. Overall, Python’s default sorting algorithm groups by type and case, which must be considered when preparing data for deterministic ordering. These observations underscore the importance of explicit type handling and case normalization in data preprocessing pipelines to achieve reproducible results.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What does the sort method do with strings that have uppercase letters first?\n","A: sort puts words with capital letters first, sorting them alphabetically, then sorts lowercase words alphabetically.\n","Q: Why might you need to make all strings lowercase or uppercase before sorting?\n","A: Because uppercase and lowercase sort separately, you may need to make all strings the same case to sort them in a particular way.\n","Q: How does Python's sort method handle a list containing both strings and numbers?\n","A: Python sorts mixed lists by placing numbers first, then strings; that's how the sort method works.\n","Q: When does Python place numbers before strings in a sorted list?\n","A: When sorting a list with both numbers and strings, Python places numbers first, then strings.\n","Q: Who inserted the number 18 into the list and where?\n","A: the number 18 into the sixth place of the list.\n","\n","KEY CONCEPTS:\n"," Python, list, sort method, uppercase, lowercase, reverse, numeric sorting, mixed data types, insertion, index, string, number\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," Decision‑making performance can be quantified by curves of accuracy versus confidence. In fraud detection, AI achieves high accuracy at extreme confidence levels but drops sharply when confidence is low, whereas human analysts maintain a flatter performance curve, excelling on mid‑confidence alerts. An optimal workflow therefore delegates high‑confidence cases to AI and defers uncertain ones to human review. Augmented intelligence, combining AI recommendations with human oversight, yields the best outcomes for intermediate confidence scores. However, interface design critically influences bias: forced display of AI advice can induce automation bias, whereas optional or post‑human‑assessment presentation preserves analyst autonomy and reduces reliance on the system. Statistical analyses confirm that AI alone minimizes false positives at high or low confidence, while hybrid approaches outperform both at intermediate levels. Thus, aligning AI confidence, human judgment, and transparent interface design maximizes decision quality.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is the main challenge faced by financial analysts in the fraud detection system?\n","A: They are overwhelmed with 90 percent of those alerts being false positives.\n","Q: Why might a human perform better than AI at a 50% confidence level?\n","A: At a 50 percent confidence level, a human is likely to do a better job than an Artificial Intelligence (AI).\n","Q: How does the AI performance curve behave at low confidence scores?\n","A: A confidence score of zero percent says a prediction thinks that this is definitely not a real alert, it's a false positive.\n","Q: When does the AI algorithm say \"I don't know\"?\n","A: The AI algorithm is saying, \"I don't know.\"\n","Q: Who should decide a single decision according to the transcript?\n","A: A fascinating combination of holistic curves and human bias.\n","\n","KEY CONCEPTS:\n"," Fraud detection, False positives, Confidence score, Success rate, Performance curve, Human bias, AI algorithm, Decision-making, Predictive modeling, Risk assessment, Anomaly detection, Financial analyst\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," The presentation introduced a product manager from Google Cloud AI who outlined the challenges of developing enterprise generative applications and the recent release of six Vertex AI APIs designed to address these issues. The APIs—document understanding, embedding, vector search, ranking, grounded generation, and check grounding—provide stateless, well‑defined primitives that can be integrated into popular frameworks. Document understanding parses complex formats to enhance retrieval and answer quality, while the embedding API achieves leading performance benchmarks. Vector search delivers scalable, cost‑efficient retrieval with hybrid search capabilities, and the ranking API re‑orders results to surface the most relevant evidence for large language model responses. Grounded generation produces citation‑rich answers derived from evidence, and check grounding fact‑checks statements against supplied data, detecting contradictions. Together, these tools lower the barrier to building reliable, data‑grounded generative applications in enterprise contexts.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'LangChain']\n","\n","Q&A:\n"," Q: What is Demitrius's role at Google?\n","A: I am a product manager within Cloud AI focusing on search and document AI.\n","Q: Why do developers need new Vertex AI APIs?\n","A: Developers need reliable access to the right Enterprise data to produce accurate and consistent responses.\n","Q: How does the Document Understanding API help?\n","A: It uses our know‑how from DOI to understand document structure and improve quality of No‑text applications.\n","Q: When are the new Vertex AI APIs being launched?\n","A: We are launching a number of new Vertex AI APIs.\n","Q: Who benefits from these new APIs?\n","A: Developers building generative applications for Enterprises.\n","\n","KEY CONCEPTS:\n"," Vertex AI, Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Check Grounding API, Gemini, LlamaIndex, LangChain, Hybrid Search, LLM\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," Singular value decomposition expresses a matrix \\(X\\) as \\(X=U\\Sigma V^{\\!T}\\), where \\(U\\) and \\(V\\) are unitary (orthogonal in the real case) and \\(\\Sigma\\) contains non‑negative singular values. Unitary matrices preserve inner products, angles, and vector lengths; the Fourier transform is a familiar example of such a rotation in a different basis. Geometrically, multiplying the unit sphere in \\(\\mathbb{R}^n\\) by \\(X\\) produces an ellipsoid whose principal axes are the columns of \\(U\\) (or \\(V\\) for the transpose action) and whose semi‑axes lengths are the singular values. In the economy SVD only the first \\(m\\) columns of \\(U\\) are retained, so \\(U^{\\!T}U=I_m\\) while \\(UU^{\\!T}\\neq I_n\\); for complex data the transpose is replaced by the conjugate transpose. These properties explain why SVD reveals the dominant directions of variation and facilitates dimensionality reduction in data analysis. Thus, SVD provides a geometrically transparent decomposition that identifies principal directions and scales for efficient data representation.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is a unitary matrix?\n","A: Unitary matrices preserve the angles between any two vectors in the vector space that they're transforming.\n","Q: Why use the economy size SVD?\n","A: The economy SVD uses the first M columns of U and the first M by M sub block of Sigma.\n","Q: How does a unitary transformation affect inner products?\n","A: The inner product between x and y is unchanged if I map both vectors through the unitary transformation.\n","Q: When do we use the complex conjugate transpose?\n","A: If X is complex, we use the complex conjugate transpose (X*) instead of transpose.\n","Q: Who finds unitary matrices important?\n","A: Unitary matrices are extremely important in science and engineering.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Unitary Matrix, Fourier Transform, Inner Product, Complex Conjugate Transpose, Column Space, Row Space, Ellipsoid, Principal Axes, Left Singular Vectors, Right Singular Vectors, Economy Size SVD\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Google’s Gemini Pro 1.5 is a multimodal generative AI that processes text and images within an expanded context window of up to one million tokens, surpassing earlier versions and competitors such as GPT‑4 Turbo. The model supports a unified 1.5 Pro architecture, enabling coherent, context‑aware responses across diverse inputs. Demonstrations included uploading a 402‑page Apollo 11 transcript (~330 k tokens) to AI Studio, where the model accurately extracted quotations, identified visual scenes, and returned precise timestamps, illustrating reliable alignment and large‑context retrieval. Developers can obtain a free API key, install the `google-generativeai` library, and configure safety, streaming, and chunking options to manage latency and token limits. The session highlighted practical troubleshooting, performance trade‑offs, and the potential for PDF‑query RAG applications. Gemini Pro 1.5’s speed, contextual depth, and multimodal capabilities position it as a foundational platform for rapid prototyping of sophisticated AI tools.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the main goal of this video?\n","A: to build generative Artificial Intelligence (AI) powered application using Google Gemini Pro 1.5\n","Q: Why does the speaker plan to show a 1-minute demo video?\n","A: to give you an idea like what all Google Gman Pro can basically do 1.5 Pro can actually do\n","Q: How is Gemini Pro 1.5 described in terms of its capabilities?\n","A: it's a kind of a multi model so if we say multimodel that basically means it will be able to work with both text and images\n","Q: What will the speaker discuss after the demo?\n","A: how you can actually create the API key how you can actually use it\n","Q: Who is the speaker in this video?\n","A: my name is krishn\n","\n","KEY CONCEPTS:\n"," Generative AI, Google Gemini Pro 1.5, Gemini 1, Multimodal, API key, Long-context understanding, Text modality, Image modality, Experimental feature, End-to-end pipeline, AI-powered application, Model\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Evaluation of prompt‑engineering models relies on quantitative metrics such as perplexity, accuracy, and qualitative human assessment. Perplexity measures how well a language model predicts word sequences; lower values indicate better performance, while accuracy quantifies the proportion of correct responses and human evaluation rates overall response quality. In the transcript, a custom evaluation function computed perplexity and achieved 100 % accuracy on a small translation dataset, illustrating the utility of these metrics. Debugging proceeds by analysing generated outputs to identify recurring errors, enabling targeted fine‑tuning, and testing across diverse datasets or tasks—often via cross‑validation or visualization tools—assesses generalisation. Continuous evaluation ensures sustained model performance as usage evolves, and these practices facilitate the deployment of reliable language models in real‑world applications. Overall, systematic metric‑driven evaluation, iterative debugging, and diverse testing are essential for robust prompt‑engineering.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Deep Learning']\n","\n","Q&A:\n"," Q: What are the commonly used matrices for evaluating prompt engineering models?\n","A: perplexity, accuracy, and human evaluation.\n","Q: Why is perplexity important in evaluating language models?\n","A: perplexity measures how well a language model predicts a sequence of words; lower perplexity means better.\n","Q: How do you evaluate a large language model built in the previous video?\n","A: by building a function called evaluate translation that takes the model and dataset, checks accuracy and perplexity.\n","Q: When should you test prompt engineering models on different data sets?\n","A: to determine the model's ability to generalize on new or unseen data.\n","Q: Who is involved in human evaluation of prompt engineering models?\n","A: humans rate the quality of the responses.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Perplexity, Accuracy, Human Evaluation, Large Language Model, Fine-tuning, Visualization Tools, Cross-Validation, Generalization, Debugging, Model Evaluation, Data Sets\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," Generative AI produces new content from patterns learned by large language models such as GPT‑4 or Claude. An AI agent augments a generative model with tool access, memory, and decision‑making, enabling it to perform narrow tasks like booking a flight via APIs. Agentic AI scales this further, allowing one or more agents to execute multi‑step reasoning, coordinate with external agents, and autonomously pursue complex goals, e.g., a travel planner that checks weather, prices, visas, and hotels. The transition from generative AI to agentic AI increases task complexity, autonomy, and necessitates control mechanisms to safeguard sensitive data. Frameworks such as N8N or Agno integrate the core generative model with workflow orchestration and tool interfaces to build these systems. Overall, the evolution transforms passive Q&A into proactive, multi‑agent problem solving.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Langraph']\n","\n","Q&A:\n"," Q: What is the core component of an agentic AI system?\n","A: generative Artificial Intelligence (AI) as a core component of it\n","Q: Why does a generative AI with only LLM not answer current flight price?\n","A: because it has a knowledge cutff date\n","Q: How does an AI agent become more intelligent than a simple LLM?\n","A: by giving it access to tools like APIs\n","Q: When does an agentic AI system perform multi-step reasoning?\n","A: when it needs to check visa before booking flight\n","Q: Who can build a fully agentic AI system for onboarding employees?\n","A: you can build it using the MCP server and Clot desktop\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence, Large Language Model (LLM), GPT-4, Claude, Gemini, Xedia API, Weather API, Agentic Artificial Intelligence, AI Agent, Multi-step reasoning, Tool integration (API calls), N8N\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Covariance quantifies the linear association between two random variables by averaging the product of their deviations from their respective means. It is computed as cov(X,Y)= (1/n) Σ_{i=1}^n (X_i-μ_X)(Y_i-μ_Y), and when X equals Y the expression collapses to the variance of that variable. A positive covariance indicates that increases in one variable tend to accompany increases in the other, while a negative value signals an inverse relationship; the magnitude reflects strength but is not scale‑free. Because covariance is expressed in the product of the original units, it is sensitive to scale changes, making direct comparison across variables measured in different units unreliable. The Pearson correlation coefficient standardises covariance by dividing it by the product of the variables’ standard deviations, yielding a dimensionless measure ranging from –1 to +1. Overall, covariance captures directional association but requires normalisation to yield interpretable, comparable metrics across diverse datasets.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," Variance, Covariance, Random variable, Mean (μ), Pearson correlation coefficient, Data preprocessing, Data analysis, Positive covariance, Negative covariance, Relationship between variables, Size of house, Price\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning seeks to learn an optimal policy that maximizes cumulative reward. An agent interacts with an environment, observes states, selects actions, and receives reward signals that quantify action quality. In episodic tasks, such as Tic‑Tac‑Toe, rewards are typically assigned at episode termination (e.g., +1 for a win, –1 for a loss, 0 for a draw) to incentivize winning strategies, whereas in continuous domains like trading, rewards are defined as profit or risk‑adjusted metrics, enabling long‑term financial optimization. Reward functions translate abstract domain objectives into numerical signals that guide policy updates through value‑based, policy‑based, or hybrid algorithms. Consequently, a well‑crafted reward structure is essential for aligning learned behavior with desired outcomes; it is the linchpin that transforms high‑level goals into learnable policies. Agents improve through trial‑and‑error, exploring actions, observing resulting states and rewards, and iteratively refining their policy to enhance future decisions.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the primary objective of a reinforcement learning agent?\n","A: learn the optimal policy that maximizes a numerical reward signal\n","Q: Why does the reward signal provide feedback to the agent?\n","A: provides feedback to the agent about the quality of its action\n","Q: How does an RL agent learn the optimal action at each step?\n","A: by trial and error learning, exploring the environment, observing resulting state and reward, and updating its policy accordingly\n","Q: When is an RL task considered episodic?\n","A: when it has a fixed end point, like a game or a research task with a time constraint\n","Q: Who gives the objective in a real-world scenario?\n","A: a manager or employer sets the objective for an employee\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Agent, Environment, Policy, Reward, Cumulative reward, Value function, Policy-based method, Reward function, Sharpe ratio, Sortino ratio, Exploration\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," Python dictionaries are hash‑based mapping containers that associate immutable keys with arbitrary values. Items are separated by commas and key‑value pairs by colons. Built‑in methods such as items(), keys(), and values() provide views of the mapping, while constructors like dict(zip(keys, values)) or literal syntax {k: v} enable creation from two sequences. Access, assignment, and deletion use square‑bracket notation, and the len() function returns the number of entries. Keys must be hashable, whereas values may be any type, allowing nested dictionaries. Dictionaries support additional methods—get, setdefault, update, pop, popitem—that facilitate robust manipulation. Their average‑case O(1) lookup time and insertion‑order preservation (Python 3.7+) make them indispensable for rapid data retrieval in scientific workflows, where they underpin pandas structures such as Series and DataFrames.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: a dictionary consists of key value pairs; items are separated by commas and key/value pairs by colons.\n","Q: Why must dictionary keys be immutable?\n","A: keys must be immutable; they can be strings, numbers, or tuples, but not lists.\n","Q: How can you create a dictionary from two lists?\n","A: Use dict(zip(list1, list2)) to create a dictionary from two lists.\n","Q: When will the length of a dictionary change?\n","A: When you delete a key, the length decreases.\n","Q: Who can use dictionaries for mapping one item to another?\n","A: They are useful for mapping items, e.g., stock prices with keys open, high, close.\n","\n","KEY CONCEPTS:\n"," dictionary, key-value pair, immutable, tuple, zip function, dict function, pandas, list, range function, string, keys, values\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," IBM’s 2023 Cost of a Data Breach report indicates that AI‑driven automation shortens breach detection by an average of 108 days, while reducing the average cost of insider incidents to $4 million. User behavior analytics (UBA) systems, powered by machine learning, model baseline activity, detect anomalous insider behavior, and trigger automated containment, thereby mitigating financial risk. When integrated with IBM Security’s Curate platform, UBA delivers risk‑prioritized employee lists, real‑time offense dashboards, and event timelines, enabling analysts to focus on high‑risk accounts after a seven‑day learning period. Q Radar complements UBA by mapping alerts to MITRE ATT&CK tactics, generating natural‑language summaries, and visualizing indicator relationships; human feedback further refines analyses, cutting investigation time from days to minutes. Together, AI‑enhanced UBA and Q Radar transform threat detection into a proactive, data‑centric defense that accelerates breach response and lowers insider‑related costs.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the average reduction in days to identify and contain a data breach when extensively using AI and automation?\n","A: 108 fewer days on average to identify and contain a data breach\n","Q: Why is insider threat a major concern for organizations of all sizes?\n","A: Insider threats are a major concern for organizations of all sizes\n","Q: How can user behavior analytics with AI and machine learning help security teams?\n","A: help you detect and respond to Insider threats quickly and precisely\n","Q: When was the IBM Cost of a Data Breach report that surveyed over 500 organizations published?\n","A: IBM's cost of a data breach report 2023\n","Q: Who is the target audience for the discussion on AI and automation in security?\n","A: most Security Professionals\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Machine Learning, User Behavior Analytics (UBA), Insider Threats, Data Breach, IBM Cost of a Data Breach Report, AI-driven Automation, Threat Detection, Threat Containment, Security Posture, Security Team, Emerging Threats\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Krishak’s introductory YouTube video, recorded at 2 a.m., exemplifies a concise channel‑launch greeting that establishes personal branding through informal, direct address, setting a tone of immediacy and approachability. In parallel, Meta’s release of Llama 3—an 8 billion‑ and 70 billion‑parameter model trained on 50 trillion tokens—offers an 8 k‑token context window and surpasses many commercial systems on reasoning, code generation, and instruction following, while maintaining lower refusal rates and higher response diversity. Post‑training refinements reduce false refusals and enhance alignment, enabling robust multi‑step task execution and nuanced dialogue, as demonstrated in Meta AI’s assistant integration. Comparative benchmarks show the open‑source “cloud3 Sonet” model outperforms GPT‑4 on MMLU and HumanEval but remains below Gemini Pro 1 on advanced reasoning. Responsible deployment is supported by a structured framework, Llama Guard safeguards, and transparent documentation across Meta, Hugging Face, and Kaggle, with clear download and local‑inference instructions that emphasize proper weight acquisition and environment compatibility.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: Who is speaking in the transcript?\n","A: my name is krishak\n","Q: What is the speaker welcoming the audience to?\n","A: my YouTube channel\n","Q: When does the speaker mention the time?\n","A: right now it is 2 a.m.\n","Q: What does the speaker say at the beginning of the transcript?\n","A: hello my name is krishak\n","Q: How does the speaker address the audience?\n","A: so guys\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The instructor presents a systematic approach to constructing a decision boundary in Python, emphasizing reproducibility and clarity. Beginning with a search for scikit‑learn documentation, the lecturer highlights the library’s common abbreviation “sk‑learn” and its central role in machine‑learning workflows. The focus is on the Gaussian Naive Bayes classifier, which the instructor selects as the exemplar model. By examining the library’s Naive Bayes page, students acquire the mathematical derivation, typical applications, and practical guidance for instantiating the Gaussian variant in code. The lecture assures that, in subsequent videos, learners will be able to implement the full classifier independently, having internalized the API. Mastery of scikit‑learn’s Gaussian Naive Bayes facilitates rapid, interpretable classification model development. This instructional design supports learners in translating theoretical concepts into executable code, thereby bridging the gap between algorithmic understanding and practical implementation.\n","\n","TOPICS:\n"," ['Python Programming', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What library is used in the lesson?\n","A: scikit-learn, which is often abbreviated sk-learn.\n","Q: Why did the speaker search Google for sklearn and Naive Bayes?\n","A: to use the documentation of that library to figure out how to use some of the functions that it has.\n","Q: How does the speaker plan to help students write the code?\n","A: by the end of the next video or two, you will be able to write this code yourself.\n","Q: When did the speaker find Gaussian Naive Bayes?\n","A: I saw Gaussian Naive Bayes as one of the other results on Google.\n","Q: Who is the intended audience for the lesson?\n","A: you will be able to write this code yourself.\n","\n","KEY CONCEPTS:\n"," Python, scikit-learn, Naive Bayes, Gaussian Naive Bayes, Decision boundary, Classifier, Algorithm, Documentation, Derivation, Use cases, Google, Functions\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199898, Requested 2885. Please try again in 20m2.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199889, Requested 2885. Please try again in 19m58.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199870, Requested 2885. Please try again in 19m50.16s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199842, Requested 2890. Please try again in 19m40.224s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199832, Requested 2890. Please try again in 19m35.904s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199813, Requested 2890. Please try again in 19m27.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199785, Requested 677. Please try again in 3m19.584s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199776, Requested 677. Please try again in 3m15.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199757, Requested 677. Please try again in 3m7.488s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199729, Requested 666. Please try again in 2m50.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199719, Requested 666. Please try again in 2m46.32s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199700, Requested 666. Please try again in 2m38.112s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199672, Requested 703. Please try again in 2m42s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199662, Requested 703. Please try again in 2m37.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199644, Requested 703. Please try again in 2m29.904s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199942, Requested 287. Please try again in 1m38.928s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199933, Requested 287. Please try again in 1m35.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199914, Requested 287. Please try again in 1m26.832s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199886, Requested 759. Please try again in 4m38.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199876, Requested 759. Please try again in 4m34.32s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199857, Requested 759. Please try again in 4m26.111999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199829, Requested 648. Please try again in 3m26.064s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199820, Requested 648. Please try again in 3m22.176s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199801, Requested 648. Please try again in 3m13.968s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199773, Requested 653. Please try again in 3m4.031999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199763, Requested 653. Please try again in 2m59.712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199744, Requested 653. Please try again in 2m51.504s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199716, Requested 3392. Please try again in 22m22.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199706, Requested 3392. Please try again in 22m18.336s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199688, Requested 3392. Please try again in 22m10.56s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199921, Requested 3474. Please try again in 24m26.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199912, Requested 3474. Please try again in 24m22.752s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199893, Requested 3474. Please try again in 24m14.543999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199865, Requested 3363. Please try again in 23m14.496s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199855, Requested 3363. Please try again in 23m10.176s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199836, Requested 3363. Please try again in 23m1.967999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199808, Requested 3368. Please try again in 22m52.032s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199799, Requested 3368. Please try again in 22m48.143999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199780, Requested 3368. Please try again in 22m39.936s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199751, Requested 1736. Please try again in 10m42.384s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," {\"generated_summary\":\"\"}\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199742, Requested 1736. Please try again in 10m38.496s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199723, Requested 1736. Please try again in 10m30.288s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199942, Requested 1818. Please try again in 12m40.32s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199933, Requested 1818. Please try again in 12m36.432s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199914, Requested 1818. Please try again in 12m28.223999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199886, Requested 1707. Please try again in 11m28.175999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199876, Requested 1707. Please try again in 11m23.856s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199857, Requested 1707. Please try again in 11m15.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199829, Requested 1712. Please try again in 11m5.712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199820, Requested 1712. Please try again in 11m1.824s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199801, Requested 1712. Please try again in 10m53.616s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199773, Requested 3092. Please try again in 20m37.679999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," The submission indicated that the draft summaries were not available, preventing the synthesis of specific technical content. Consequently, no concrete technical ideas or conclusions could be extracted or combined. In an academic context, the absence of source material would necessitate a review of the original documents to ensure that all relevant findings, methodologies, and interpretations are captured before attempting integration. Typically, a coherent summary would distill key objectives, experimental designs, analytical techniques, results, and implications, while maintaining an objective tone and logical flow. Without the underlying drafts, any attempt to produce a substantive synthesis would be speculative and could misrepresent the intended message. Therefore, the lack of provided drafts precludes the creation of a meaningful, technically grounded summary at this time.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199763, Requested 3092. Please try again in 20m33.36s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199744, Requested 3092. Please try again in 20m25.152s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199942, Requested 3174. Please try again in 22m26.112s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199933, Requested 3174. Please try again in 22m22.224s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199914, Requested 3174. Please try again in 22m14.016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199886, Requested 3063. Please try again in 21m13.967999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199876, Requested 3063. Please try again in 21m9.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199857, Requested 3063. Please try again in 21m1.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199829, Requested 3068. Please try again in 20m51.504s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199820, Requested 3068. Please try again in 20m47.616s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199801, Requested 3068. Please try again in 20m39.408s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199772, Requested 662. Please try again in 3m7.488s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," {\"generated_summary\":\"\"}\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199763, Requested 662. Please try again in 3m3.6s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01jkbcxv6cfn3a9p6fgw8k4q8q` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199744, Requested 662. Please try again in 2m55.392s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3331055071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;31m# 12. RUN GENERATION ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRole-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3331055071.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0msummary\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0mtopics\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mqa_text\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3331055071.py\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;31m\\\u001b[0m\u001b[0;34m\"\\\"\\\"{combined}\\\"\\\"\\\"\"\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroq_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m     \u001b[0mj2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0mfinal_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generated_summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3331055071.py\u001b[0m in \u001b[0;36mgroq_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mwait\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGLOBAL_MIN_GAP\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnow\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mLAST_TS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Waiting {wait:.1f}s (respecting global gap)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"gpt-oss-20b_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"openai/gpt-oss-20b\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqIo-WJmc5iP","executionInfo":{"status":"ok","timestamp":1763831802370,"user_tz":-330,"elapsed":1840095,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"71d8fe52-d712-4db8-a378-a723321498cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b\n","Groq key loaded ✓\n","Resuming: 19 rows already processed.\n","Skipping row 0 (already done)\n","Skipping row 1 (already done)\n","Skipping row 2 (already done)\n","Skipping row 3 (already done)\n","Skipping row 4 (already done)\n","Skipping row 5 (already done)\n","Skipping row 6 (already done)\n","Skipping row 7 (already done)\n","Skipping row 8 (already done)\n","Skipping row 9 (already done)\n","Skipping row 10 (already done)\n","Skipping row 11 (already done)\n","Skipping row 12 (already done)\n","Skipping row 13 (already done)\n","Skipping row 14 (already done)\n","Skipping row 15 (already done)\n","Skipping row 16 (already done)\n","Skipping row 17 (already done)\n","Skipping row 18 (already done)\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Gaussian distribution is fully described by its mean μ and standard deviation σ, with 68 % of observations within one σ, 95 % within two σ, and 99.97 % within three σ, producing a symmetric bell curve that splits the data evenly around the mean. In contrast, a log‑normal distribution arises when the natural logarithm of a variable follows a normal law; a log transform therefore renders the data Gaussian. Human height follows a Gaussian pattern, whereas income levels and product review lengths exhibit log‑normal behavior with right‑skewed tails. In machine‑learning preprocessing, Gaussian features are standardized to zero mean and unit variance, while log‑normal features are first log‑transformed and then standardized to align scales. Proper scaling improves model accuracy by ensuring comparable feature magnitudes. Thus, recognizing distribution shapes and applying suitable transformations is essential for robust statistical modeling and predictive performance.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the empirical rule for a Gaussian distribution?\n","A: Within 1 standard deviation about 68%, within 2 about 95%, within 3 about 99.97% of the distribution.\n","Q: Why do we learn various distributions?\n","A: Because data often follows Gaussian or log normal distributions, and knowing them helps in scaling and improving model accuracy.\n","Q: How is a log normal distribution defined?\n","A: If log of X is normally distributed, then X follows a log normal distribution.\n","Q: When does a Gaussian distribution appear in data?\n","A: When data like height of people or iris petal length follows a bell curve with symmetry.\n","Q: Who benefits from scaling data to a standard normal distribution?\n","A: Machine learning models benefit from scaling data to standard normal distribution for higher accuracy.\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, normal distribution, empirical rule, bell curve, log-normal distribution, standard normal distribution, standard scaler, mean, standard deviation, log transformation, data normalization, regression algorithm\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," The project presents an end‑to‑end deep‑learning pipeline for detecting potato diseases, integrating data acquisition, model training, and cross‑platform deployment. Labeled images of healthy and diseased leaves are collected, cleaned, and augmented (rotation, flip, contrast) to enlarge the training set. A convolutional neural network is trained in TensorFlow, then exported and quantized to TensorFlow Lite for efficient on‑device inference. The model is served via TensorFlow Serving, wrapped by FastAPI for RESTful inference, and deployed on Google Cloud Platform using Cloud Functions, enabling serverless, scalable predictions. A React Native mobile application captures plant images, forwards them to the cloud function, and returns disease labels, while a React‑based web interface allows image uploads and real‑time predictions. The workflow demonstrates full ML‑Ops integration, model optimization, version control, and lightweight deployment, illustrating how cloud infrastructure and deep learning can mitigate crop disease losses in precision agriculture.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the main goal of the mobile application that AtliQ Agriculture is developing?\n","A: to detect early blight and late blight in potato plants so farmers can apply appropriate treatment and prevent economic loss.\n","Q: Why is early detection of potato plant diseases important?\n","A: because it can save a lot of waste and prevent economic loss for farmers.\n","Q: How will the backend server be built and deployed?\n","A: using fast API and then deploying the model to Google Cloud or GCP with Google Cloud functions.\n","Q: When will the model be served to the mobile app?\n","A: through Google Cloud functions that are called by a mobile app written in React Native.\n","Q: Who is the target user of the mobile application?\n","A: farmers who grow potatoes and need to detect diseases early.\n","\n","KEY CONCEPTS:\n"," deep learning, machine learning ops, TensorFlow Serving, FastAPI, Google Cloud Functions, React Native, convolutional neural network, potato disease detection, early blight, late blight, agriculture domain, mobile application\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The lecture delineates a progressive hierarchy of autonomy in large‑language‑model (LLM) applications, beginning with deterministic, hard‑coded code that offers no autonomy and is ill‑suited for complex real‑world scenarios. A single LLM invocation can address isolated tasks but falters on multi‑step requests. Chains partition a problem into specialist sub‑LLMs, thereby enhancing performance while remaining constrained by fixed execution paths. Routers empower an LLM to select the appropriate chain based on user intent, introducing decision‑making yet still lacking memory and iterative refinement. State‑machine agents integrate routing with loops, human approval, and persistent memory, enabling iterative drafting, adaptive learning, and a hierarchical content‑creation workflow that revises drafts prior to publication. Consequently, the degree of autonomy rises in tandem with an agent’s capacity to decide, remember, and refine its outputs.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Langraph']\n","\n","Q&A:\n"," Q: What is the main disadvantage of a single LLM call?\n","A: trying to get everything done in one shot often leads to confused or mixed up responses\n","Q: Why are chains described as having multiple Specialists instead of one generalist?\n","A: because each AI is really good at one thing\n","Q: How does a router differ from a chain in terms of decision making?\n","A: router LLM actually decides what steps to take next\n","Q: When does a state machine (agent) allow for loops and human review?\n","A: when it has loops, time travel, human review, and refinement\n","Q: Who is considered the head of the content agent in the example?\n","A: the head of content agent\n","\n","KEY CONCEPTS:\n"," LLM, Chain, Router, State machine, Agent, LangChain, LangGraph, Human-in-the-loop, Multi-agent system, Adaptive learning, Advanced memory management, Approval step\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," {\"error\":\"No draft summaries provided to combine.\"}\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Generative AI', 'Deep Learning']\n","\n","Q&A:\n"," Q: What pre-trained models are mentioned for image-based prompt fine-tuning?\n","A: ResNet50, VGG16, Inception along with logistic regression.\n","Q: Why is tokenization important in data preprocessing?\n","A: tokenization involves breaking down the text into smaller units such as words or sub words.\n","Q: How does distillation improve model efficiency?\n","A: distillation involves training a smaller model to mimic the behavior of a larger model, making it more efficient and run faster.\n","Q: When should a prompt engineering model be deployed in production?\n","A: once you have built a prompt engineering model you need to deploy it in production to make it accessible to the vast majority of the users.\n","Q: Who should consider ethical implications in prompt engineering?\n","A: prompt engineering models have potential to influence decision making in many areas such as education, healthcare, and finance therefore it is important to consider ethical applications.\n","\n","KEY CONCEPTS:\n"," Prompt engineering, Multitask learning, Model distillation, Tokenization, Normalization, Data augmentation, TensorFlow Serving, Flask, ResNet-50, VGG-16, Inception, T5 small\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The study applies singular value decomposition to a dataset of 40 aligned facial photographs—20 of Arnold Schwarzenegger and 20 of Sylvester Stallone—to derive eigenfaces. After mean-centering, the economy SVD yields principal components that serve as eigenfaces. Projection of each image onto the first three eigenfaces produces distinct clusters for the two actors, demonstrating dimensionality reduction and rudimentary classification. Extending the experiment to include Taylor Swift further clarifies the separation from Stallone, indicating that skin tone and hair color dominate the eigenface representation. The residual overlap between Arnold and Swift exposes the limitations of linear correlation‑based methods and motivates the adoption of richer models, such as the 3‑D geometry inference employed by Facebook. The findings underscore that while eigenfaces capture dominant visual traits, they can conflate unrelated identities, highlighting the necessity for more sophisticated representations.\n","\n","TOPICS:\n"," ['Machine Learning', 'Statistics', 'Data Science']\n","\n","Q&A:\n"," Q: What is the purpose of subtracting the average face from each image in the eigenfaces example?\n","A: We subtract the average face to create matrix B for PCA.\n","Q: Why does the eigenfaces method produce better separation between Taylor Swift and Stallone than between Arnold and Stallone?\n","A: Because both have fair-skinned blonde hair, giving more correlation.\n","Q: How are the eigenfaces visualized in the lecture?\n","A: By plotting the first nine eigenfaces as images.\n","Q: When does the classification of a new image into Arnold or Stallone clusters occur?\n","A: After projecting a new image into eigenface coordinates and comparing distances to clusters.\n","Q: Who is used as an example of eigenfaces in the lecture?\n","A: Arnold Schwarzenegger and Sylvester Stallone are used as examples.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Feature Space, Image Classification, Clustering, Projection, Average Face, Image Compression, 3D Geometry, Stereo Vision, Deep Neural Network Architectures\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a modular framework that connects large language models (LLMs) to external services, thereby enabling applications to perform real‑world actions such as booking flights, querying APIs, or sending emails. While LLMs can generate conversational responses, they lack direct interaction with the environment; LangChain supplies adapters for APIs, databases, and other tools, allowing developers to plug in diverse LLM back‑ends (e.g., GPT‑4, Hugging Face models) without altering application logic. This modularity supports cost‑effective experimentation and rapid deployment of AI‑powered services. By integrating external data sources and web interfaces, LangChain transforms static language models into agents capable of dynamic, context‑aware responses that reflect up‑to‑date information. The lecture illustrates concise examples and hints at broader applications such as automated customer support, data extraction, and workflow automation. Students will further explore use cases and best practices for deploying LangChain in production, underscoring its role as a versatile real‑world AI agent.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is LangChain's primary role?\n","A: Lang chain acts as a bridge between the llms and the real world.\n","Q: Why can't LLMs make bookings directly?\n","A: They cannot actually interact with the real world.\n","Q: How does LangChain enable switching LLMs without code changes?\n","A: You can easily switch models without touching the code you wrote with LangChain.\n","Q: When does the query get sent to an LLM?\n","A: When I press enter this query is sent to an llm model.\n","Q: Who uses the chat application interface?\n","A: The chat application itself is just an interface for the user like you and me.\n","\n","KEY CONCEPTS:\n"," LangChain, Large Language Model (LLM), GPT-3.5, GPT-4, Hugging Face, API integration, Real‑world interaction, Flight booking API, Restaurant booking API, Reasoning ability, Bridge architecture, Agentic AI\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residuals are the differences between a model’s fitted values and the observed data used for training, distinct from forecast errors that involve unseen data. A well‑fitted model should produce residuals with zero mean, indicating no systematic over‑ or under‑forecasting, and should exhibit no autocorrelation, meaning past residuals do not predict future ones. Diagnostic tools such as autocorrelation (ACF) and partial autocorrelation (PACF) plots, together with the Ljung‑Box test, assess these properties. In the example using a Holt‑Winters exponential smoothing model on the air‑passenger series, residuals displayed a yearly oscillation and significant autocorrelation, revealing model misspecification. Adjusting the forecast by centering the residuals can remove bias and improve accuracy. These diagnostics guide model refinement: addressing autocorrelation through additional lag terms or alternative specifications, while the negligible bias may not require adjustment. Overall, residual analysis is essential for validating and refining time‑series forecasting models.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals?\n","A: the difference between the fitted value and the actual value of the time series\n","Q: Why should the mean of residuals be zero?\n","A: otherwise we have bias; if residuals are shifted, we are over/under forecasting\n","Q: How can we detect autocorrelation in residuals?\n","A: by plotting the partial autocorrelation and autocorrelation functions, and using the Ljung-Box test\n","Q: When do we use the Ljung-Box test?\n","A: to test if residuals are independently distributed; null hypothesis: residuals are independent\n","Q: Who is the speaker in the transcript?\n","A: Eagle, a data scientist living in London\n","\n","KEY CONCEPTS:\n"," residuals, time series, forecasting, residual analysis, autocorrelation, partial autocorrelation, Box–Ljung test, exponential smoothing, Holt-Winters model, trend, seasonality, level\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," The tutorial presents a modular architecture for a conversational data‑access agent built with a ReAct framework powered by LangGraph and LangChain. A Next.js front‑end written in TypeScript and styled with Tailwind provides a chat interface that manages message history through React’s useState, distinguishing HumanMessage, AIMessage, and SystemMessage types. On the server side, an in‑memory SQLite database hosts a customer and order schema, seeded with mock data, and is accessed via a GetFromDB tool that executes SQL queries returned by a Watsonx.ai LLM. A system prompt instructs the LLM to act as a Text2SQL agent, generate correct SQLite statements, and invoke the database tool, while guardrails prevent unrestricted access. The agent is tested with joke generation, a customer‑count query, and a join query that identifies the top customer, demonstrating accurate, context‑aware SQL generation. The repository offers a reproducible, secure, and scalable solution for text‑to‑SQL applications.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Langraph', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the purpose of the AI agent described?\n","A: to talk to your database\n","Q: Why use Tailwind?\n","A: so we don't have to write any CSS\n","Q: How do you set up the Next.js project?\n","A: run create-next-app at latest, together with the name of my project\n","Q: When do you run npm run dev?\n","A: by running npm run dev, which opens a new page in the browser\n","Q: Who are the models running on?\n","A: models running on watsonx.ai\n","\n","KEY CONCEPTS:\n"," AI agent, Large Language Model, SQL, LangGraph, ReAct agent, Next.js, watsonx.ai, SQLite, VS Code, Tailwind CSS, TypeScript, create-next-app CLI\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: prompt engineering is a specialized field within NLP that builds models to generate high-quality text outputs in response to prompts.\n","Q: Why is prompt engineering important?\n","A: prompt engineering allows us to generate text outputs that are more accurate, coherent, and contextually appropriate than rule-based approaches.\n","Q: How does prompt engineering use large language models?\n","A: prompt engineering uses pre-trained large language models like GPT, Google BERT, or Hugging Face Transformers fine-tuned for specific tasks.\n","Q: When is prompt engineering applied?\n","A: applications such as chatbots language translation content generation where our quality of the output can significantly impact user experience and engagement.\n","Q: Who benefits from prompt engineering?\n","A: this course is designed to provide you with the comprehensive introduction to prompt engineering.\n","\n","KEY CONCEPTS:\n"," Prompt engineering, Natural Language Processing (NLP), Large Language Models (LLMs), GPT (OpenAI), Google BERT, Hugging Face Transformers, Fine-tuning, Rule-based approaches, Keyword-based approaches, Bias, Contextual appropriateness, Chatbots\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning']\n","\n","Q&A:\n"," Q: What are the three machine learning paradigms discussed?\n","A: supervised learning, unsupervised learning, reinforcement learning\n","Q: Why do value-based methods determine a value function?\n","A: to quantify total reward and determine optimal policy\n","Q: How is a state defined in reinforcement learning?\n","A: a snapshot of the environment\n","Q: When does the behavior policy choose actions?\n","A: based on random chance\n","Q: What is the goal of the agent in the grid world?\n","A: to get to the +10 reward spot in the best possible way\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement learning, Value-based methods, Policy-based methods, State-action value function, Bellman equation, Discount factor, Exploration policy, Behavior policy, Optimal policy, Supervised learning, Unsupervised learning\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," Logistic regression is a linear classification framework that maps an input vector X—such as the pixel intensities of an image—to a vector of class scores through a linear transformation defined by a weight matrix W and bias vector b. Each image is associated with a single discrete label, so the raw scores are converted into a probability distribution over the classes by applying the softmax function. Softmax normalises the logits into a vector that sums to one, thereby assigning a high probability to the correct class while suppressing the probabilities of the incorrect ones. The learning objective is to maximise the probability of the true class, equivalently minimising the cross‑entropy loss between the predicted distribution and the one‑hot ground‑truth vector. Gradient‑descent optimisation iteratively updates W and b to reduce this loss, yielding a model that produces calibrated probabilities for single‑label classification tasks.\n","\n","TOPICS:\n"," ['Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is what's called the linear classifier.\n","Q: Why do we use a softmax function?\n","A: The way to turn scores into probabilities is to use a softmax function.\n","Q: How does a logistic classifier generate predictions?\n","A: It takes the input, for example, the pixels in an image, and applies a linear function to them to generate its predictions.\n","Q: When do we want the probability of the correct class to be close to one?\n","A: We want the probability of the correct class to be close to one and others close to zero.\n","Q: Who denotes the inputs, weights, and bias?\n","A: Throughout, we'll denote the inputs by X, the weights by W, and the biased term by b.\n","\n","KEY CONCEPTS:\n"," logistic classifier, linear classifier, pixels, image, linear function, matrix multiplication, vector, weights, bias, softmax function, probabilities, logits\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","Role-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rGOvEpWqxSH","executionInfo":{"status":"ok","timestamp":1763833858919,"user_tz":-330,"elapsed":1860,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"397b865e-dd5f-4220-fe69-8465f805cc0a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_55tRaAdNxA1","outputId":"6a127ede-02e8-489f-e1eb-eae6678a0e11","executionInfo":{"status":"ok","timestamp":1763834034458,"user_tz":-330,"elapsed":175535,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}}},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/gpt-oss-20b_role_full_output.xlsx\n","\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2354\n","  - BLEU: 0.0322\n","  - BERTScore F1: 0.8657\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9000\n","  - Jaccard Index: 0.3877\n","  - Micro F1: 0.5064\n","  - Macro F1: 0.4888\n","  - Weighted F1: 0.4778\n","\n","Q&A Generation:\n","  - BLEU: 0.0272\n","  - Diversity: 0.7828\n","  - Answerability: 0.7267\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4833\n","  - Recall@10: 0.1933\n","  - F1@10: 0.2762\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/gpt-oss-20b/evaluation_final.json\n"]}]}]}