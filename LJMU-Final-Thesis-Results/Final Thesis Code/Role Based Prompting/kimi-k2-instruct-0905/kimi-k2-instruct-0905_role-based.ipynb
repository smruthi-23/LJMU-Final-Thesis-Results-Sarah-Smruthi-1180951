{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPFBCyqK31EFHW5IjlZ/6ch"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"52d605cf0e134600b4d8c9f3cde4ac9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_181cf712cc344ebbbc5d2a009e7abff1","IPY_MODEL_d8639b01ddc640399ab7c0e6ddd1126a","IPY_MODEL_c9b0966c6e61432a8e4ee7d77633eb43"],"layout":"IPY_MODEL_8a55efd47c374958811f297b888216dd"}},"181cf712cc344ebbbc5d2a009e7abff1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a954f05f9b64c12b2711d386f8a9f4a","placeholder":"​","style":"IPY_MODEL_63c8dbf9f436467c8a96586ac86b20c8","value":"tokenizer_config.json: 100%"}},"d8639b01ddc640399ab7c0e6ddd1126a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93e13c64b6f54901ab3c4bab4fbb5519","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e6756b7e19847e29f5d77c119bc364c","value":25}},"c9b0966c6e61432a8e4ee7d77633eb43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9687a74f011d47c8aa6b2d5a7140ee13","placeholder":"​","style":"IPY_MODEL_84aabf20f3264003bfd46ae8975d366f","value":" 25.0/25.0 [00:00&lt;00:00, 1.02kB/s]"}},"8a55efd47c374958811f297b888216dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a954f05f9b64c12b2711d386f8a9f4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63c8dbf9f436467c8a96586ac86b20c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93e13c64b6f54901ab3c4bab4fbb5519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e6756b7e19847e29f5d77c119bc364c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9687a74f011d47c8aa6b2d5a7140ee13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84aabf20f3264003bfd46ae8975d366f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"45fbc21444c7479192e091ee422532e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55f38698220c44eaafe6bcfa81dbe3f1","IPY_MODEL_86a6a778c59044e6a6b518b4a7df391a","IPY_MODEL_b8670e8ec672484a9b213e9e30edf4f7"],"layout":"IPY_MODEL_bb5d1b71b6cb4969a0155ad7c959c625"}},"55f38698220c44eaafe6bcfa81dbe3f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b86f70059ba64e6f833bf18d8589a130","placeholder":"​","style":"IPY_MODEL_9e3618e691094f09afa489cc2c738a6c","value":"config.json: 100%"}},"86a6a778c59044e6a6b518b4a7df391a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_38a4fc4f6355422a8b3e5a1972c94822","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d41ba3da4da48328e238a4b7ed222cb","value":482}},"b8670e8ec672484a9b213e9e30edf4f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f454331630b9435c947158b854be18b2","placeholder":"​","style":"IPY_MODEL_410fa44907d04006befdf38c1bff75b8","value":" 482/482 [00:00&lt;00:00, 16.8kB/s]"}},"bb5d1b71b6cb4969a0155ad7c959c625":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b86f70059ba64e6f833bf18d8589a130":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e3618e691094f09afa489cc2c738a6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38a4fc4f6355422a8b3e5a1972c94822":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d41ba3da4da48328e238a4b7ed222cb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f454331630b9435c947158b854be18b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"410fa44907d04006befdf38c1bff75b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce37398ae983478f98ca2774e6dedc54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b88015d12234ea2878c5e09f9ca58c7","IPY_MODEL_b2886d352ff24a3daf73f7efee325f05","IPY_MODEL_116d9810df684b92b17ede2bd4b0af52"],"layout":"IPY_MODEL_a76ee7a326134472aa9eab1ca1ddd9af"}},"6b88015d12234ea2878c5e09f9ca58c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce29f3e9ffea44f98924951ba8b81b48","placeholder":"​","style":"IPY_MODEL_033212c4f4f24e9a8f3bc61cfe1f04c1","value":"vocab.json: 100%"}},"b2886d352ff24a3daf73f7efee325f05":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec454271c9484891a6c5abd1d5ece515","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_845073f66a984ee9ae8f770378085f90","value":898823}},"116d9810df684b92b17ede2bd4b0af52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ee3108d868a42e3b6ac988946a4a64d","placeholder":"​","style":"IPY_MODEL_a8fadb5dc4f8444f88d7b6d4558e9d24","value":" 899k/899k [00:00&lt;00:00, 11.1MB/s]"}},"a76ee7a326134472aa9eab1ca1ddd9af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce29f3e9ffea44f98924951ba8b81b48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"033212c4f4f24e9a8f3bc61cfe1f04c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ec454271c9484891a6c5abd1d5ece515":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"845073f66a984ee9ae8f770378085f90":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2ee3108d868a42e3b6ac988946a4a64d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8fadb5dc4f8444f88d7b6d4558e9d24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f6b4a801b2a484ba3ce245d20c6d46b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfb211c926be43a8a4b8a816f61c3119","IPY_MODEL_a62ad976e3ec4bc985604a8a663c2cdc","IPY_MODEL_868e7b66f2fd4f04a6dbdcb702826251"],"layout":"IPY_MODEL_e14b3d6cdc0647daa44ae018068e93e7"}},"bfb211c926be43a8a4b8a816f61c3119":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4da4913135214df0bc96dd15b2776d30","placeholder":"​","style":"IPY_MODEL_d0a28bd50b114621880c9694fde2cc75","value":"merges.txt: 100%"}},"a62ad976e3ec4bc985604a8a663c2cdc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebaec1c26aee412eb36233e0815e18aa","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7fe12732d3e7496da345b7931431d7da","value":456318}},"868e7b66f2fd4f04a6dbdcb702826251":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57e3d54ee4a041a69199967afd4cc93d","placeholder":"​","style":"IPY_MODEL_8b36b5be27c04df2901ce6e3747e5a94","value":" 456k/456k [00:00&lt;00:00, 10.5MB/s]"}},"e14b3d6cdc0647daa44ae018068e93e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4da4913135214df0bc96dd15b2776d30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0a28bd50b114621880c9694fde2cc75":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebaec1c26aee412eb36233e0815e18aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fe12732d3e7496da345b7931431d7da":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57e3d54ee4a041a69199967afd4cc93d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b36b5be27c04df2901ce6e3747e5a94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f569db1ced414578aed91a70089ea38a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aeceed819baf433ebe58a5424444a27c","IPY_MODEL_248b0644bb43429a902096bdcbbd7256","IPY_MODEL_779bce1c3dc34deab989da03e10653cf"],"layout":"IPY_MODEL_104c159e46f942fcb295d8155d510c3e"}},"aeceed819baf433ebe58a5424444a27c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40e020246d864c49b635fae4ea92fd72","placeholder":"​","style":"IPY_MODEL_57255c4f14b549fc95d037e7e2739ae2","value":"tokenizer.json: 100%"}},"248b0644bb43429a902096bdcbbd7256":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_10325e3a955c4713bebd56eb8ff59cc0","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_688c73b2d46e4134953d435c79fb7a64","value":1355863}},"779bce1c3dc34deab989da03e10653cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d2168244444452a8296413e712044db","placeholder":"​","style":"IPY_MODEL_4adfc9225eb045e497904386a79823e6","value":" 1.36M/1.36M [00:00&lt;00:00, 15.7MB/s]"}},"104c159e46f942fcb295d8155d510c3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40e020246d864c49b635fae4ea92fd72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57255c4f14b549fc95d037e7e2739ae2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"10325e3a955c4713bebd56eb8ff59cc0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"688c73b2d46e4134953d435c79fb7a64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d2168244444452a8296413e712044db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4adfc9225eb045e497904386a79823e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"171d288a81d748ecbbd49163350788e1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9a6e3f7a1bc34f7fbb9573850f9d5d0c","IPY_MODEL_c3ec8ac420ff4a59878f90e231367c9c","IPY_MODEL_0543ea4acfa242cb8235abfee806e765"],"layout":"IPY_MODEL_9ab4e36aa4964f00a1bdfc034a0a7a8f"}},"9a6e3f7a1bc34f7fbb9573850f9d5d0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a115a8ab5c8943f3bb420b624c6a7741","placeholder":"​","style":"IPY_MODEL_cd3f922e25264cc09facd7f780950cd6","value":"model.safetensors: 100%"}},"c3ec8ac420ff4a59878f90e231367c9c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cba195634a584860bed4f38229b98709","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e34a63fa75924d678cc3990467fdc64f","value":1421700479}},"0543ea4acfa242cb8235abfee806e765":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8ce5bac2d8847d385f6e06838eba8f3","placeholder":"​","style":"IPY_MODEL_c62257f724ce40e4a4111c5927aa30cc","value":" 1.42G/1.42G [00:24&lt;00:00, 49.2MB/s]"}},"9ab4e36aa4964f00a1bdfc034a0a7a8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a115a8ab5c8943f3bb420b624c6a7741":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd3f922e25264cc09facd7f780950cd6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cba195634a584860bed4f38229b98709":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e34a63fa75924d678cc3990467fdc64f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8ce5bac2d8847d385f6e06838eba8f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c62257f724ce40e4a4111c5927aa30cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_QzvW2cUBVm1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763950946418,"user_tz":-330,"elapsed":26645,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"7ec5c192-149b-4bb2-bfd0-7c1f335bee19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c8ab7be0289f1aae558c59d2d318fec4bce689a568923f6d862d337bdf44d27c\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"hfN340i_t91q","executionInfo":{"status":"ok","timestamp":1763950946459,"user_tz":-330,"elapsed":29,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"7e2387ed-9556-4f8f-f5bb-73c35325fd7e"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"kimi-k2-instruct-0905_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"moonshotai/kimi-k2-instruct-0905\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9H2lMMpt_Xa","executionInfo":{"status":"ok","timestamp":1763955830901,"user_tz":-330,"elapsed":451405,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"958d0810-d3c8-4614-b85d-87a5003e6bd5"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement Learning from Human Feedback (RLHF) integrates human evaluative signals into reinforcement learning to accelerate convergence toward policies aligned with human preferences. In grid-world navigation, an agent employing Q-learning, DQN, or PPO receives intermittent human guidance that directs action selection, lowering sample complexity and aligning trajectories with user intent. This framework extends to language models: a GPT-based reward model trained on human-ranked outputs predicts response quality, and Proximal Policy Optimization fine-tunes ChatGPT by maximizing predicted reward through iterative back-propagation updates. By replacing dense environmental rewards with human rankings, RLHF enables alignment with subtle human values, transforming reinforcement learning from pure exploration into a human-guided search that yields sample-efficient, value-aligned policies.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What is the grid-world goal for Frank?\n","A: get to this plus 10 reward spot\n","Q: Why is human feedback added while Frank learns?\n","A: allows Frank to learn faster and give responses that are more human favored\n","Q: How does human feedback affect RL as shown with Frank?\n","A: accelerates the learning process\n","Q: When training ChatGPT, who ranks the multiple answers?\n","A: we as humans\n","Q: Who uses proximal policy optimization during ChatGPT fine-tuning?\n","A: chat GPT generates a response using the reinforcement learning algorithm called proximal policy optimization\n","\n","KEY CONCEPTS:\n"," reinforcement learning, human feedback, reward model, proximal policy optimization, Q-learning, DQ-learning, grid world, backpropagation, loss function, GPT architecture, fine-tuning, policy gradient\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," CVXOPT is used pedagogically to demonstrate how kernels are injected into the dual quadratic-programming formulation of a support-vector machine, enabling students to visualise the transition from linear to non-linear decision boundaries and the soft-margin effect. Solving min ½xᵀPx+qᵀx s.t. Gx≼h, Ax=b, the tutorial explicitly replaces the inner-product matrix with the kernel matrix K; Gaussian and polynomial kernels illustrate margin curvature and support-vector redistribution, while a linear kernel reproduces the hard-margin hyperplane. Although CVXOPT is numerically robust, it scales poorly, so production workflows delegate to LIBSVM or scikit-learn. Kernels alone allow SVMs to approximate arbitrary functions while preserving convex optimisation guarantees. Quadratic programming solvers minimise a quadratic objective under linear constraints; a minimal NumPy implementation and MIT tutorial expose solver internals, emphasising rapid prototyping with standard linear-algebra routines. Hard-margin SVMs arise when penalty C is absent; setting C>0 yields soft-margin variants. Kernels map data to higher-dimensional spaces where linear separation becomes feasible: linear K(x,y)=x·y, polynomial K(x,y)=(1+x·y)^p, Gaussian K(x,y)=exp(−||x−y||²/2σ²). The QP solver optimises Lagrange multipliers α, producing support vectors, bias b, and weight vector w for linear models; otherwise kernel evaluations replace explicit weights. Code generates linearly separable, non-linearly separable, and overlapping linear cases, visualising 2-D data lifted into 3-D, separated by a hyperplane, then projected back as a circular decision boundary. Kernel choice and C control model capacity and generalisation. The session concludes the SVM implementation walkthrough and announces the next tutorial: a systematic review of scikit-learn’s SVC hyperparameters and multiclass strategies such as one-vs-rest, equipping practitioners to deploy robust SVMs on real-world data.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What solver is used in the tutorial to demonstrate kernel impact on SVM?\n","A: CVX opt\n","Q: Why is CVX opt employed here instead of lib svm?\n","A: purely so you can see directly the impact of a kernel\n","Q: How does CVX opt accept the optimization problem?\n","A: you stuff in whatever values for these variables you want\n","Q: When would you realistically choose lib svm over CVX opt?\n","A: if you wanted to write your own support Vector machine\n","Q: Who originally authored the example code shown in the tutorial?\n","A: Matthew blondell's GitHub\n","\n","KEY CONCEPTS:\n"," CVXopt, kernels, support vector machine, quadratic programming solver, libSVM, soft margin, nonlinear visualization, Christopher Bishop, pattern recognition, machine learning book, Matthew Blondell\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompts are structured inputs that govern text generation in large language models, prescribing both desired outputs and operational constraints. Seven canonical types—question, statement, multi-input, and constraint-bound—modulate complexity and fidelity. Effective prompts explicitly encode task objectives and execution rules, specifying tone, style, length, and format to suppress extraneous content. Deconstruction dissects extant prompts into objective, constraint, and linguistic components, surfacing implicit directives. Empirical refinement, such as converting “What is the capital of France?” to “One-word answer: capital of France?”, shortens responses without sacrificing accuracy. Precise prompt design constitutes the principal determinant of LLM behaviour and output quality.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Python Programming']\n","\n","Q&A:\n"," Q: What are prompts in prompt engineering?\n","A: Prompts are the inputs that we give to our prompt engineering models.\n","Q: Why is choosing the right type of prompt important?\n","A: It can impact the complexity and the quality of your output.\n","Q: How do constraints within a prompt improve output accuracy?\n","A: These two things will define the prompt and … will get you more accurate output.\n","Q: When should you deconstruct an existing prompt?\n","A: When you want to know what are the elements and key features and constraints in this prompt.\n","Q: Who determines the expected output and constraints of a prompt?\n","A: You … define these two things: what do you expect and how you want it to be done.\n","\n","KEY CONCEPTS:\n"," prompt, large language model, prompt engineering, question prompt, statement prompt, constraint, context, tone, style, deconstruction, pre-trained model, output format\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," AI agents are autonomous systems that dynamically determine their own problem-solving steps, contrasting with rigid chains or routers. By integrating task-specific tools—such as search, calculator, or calendar—they extend large language models beyond static knowledge, analogous to a chef selecting utensils. The ReAct pattern operationalises this capability: the agent alternates internal reasoning with external tool invocation, preserving full conversational context across iterative think-act-observe cycles until the query is resolved. LangChain mediates tool execution, returning observations that inform subsequent reasoning. Consequently, an LLM equipped with tools and this control loop becomes an agent capable of multi-step, tool-augmented reasoning, delivering accurate, contextually grounded answers without predefined workflows.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What are AI agents described as in the transcript?\n","A: the problem solvers of the Artificial Intelligence World\n","Q: Why are tools compared to a chef’s kitchen utensils?\n","A: tools are specific functions agents use to complete tasks, like knife for cutting oven for baking\n","Q: How does the ReAct pattern mimic human thinking?\n","A: first we think about the problem, then take action, then observe the result\n","Q: When does the ReAct cycle terminate?\n","A: if it has found the answer then it is going to end it right here\n","Q: Who executes the tool in the ReAct loop?\n","A: Lang chain executes the tool and then Returns the output back to the llm\n","\n","KEY CONCEPTS:\n"," AI agents, autonomous decisions, tools, ReAct agent pattern, reasoning, action, observation, LLM, LangChain, control flow, multi-step problem solving, LangGraph\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," A reflective-agent pipeline iteratively refines viral tweets by coupling a generator that drafts candidates with a critic scoring virality, tone and novelty, then rewriting the highest-scoring text. State persistence across loops is maintained via LangSmith, storing prompts, scores and metadata for reproducibility. Decomposing evaluation into numeric rubrics and chain-of-thought rationales exposes revision logic, systematically lifting engagement while conserving tokens. LangSmith tracing streams every graph operation to a hosted project; six loops in the demo enrich an initial tweet with emojis, hashtags and threading. Granular timing and token-level traces enable debugging and auditing, generalising the pattern to complex reasoning tasks where reflective iteration enhances output quality and operationalises self-improvement within observable, production-grade pipelines.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain']\n","\n","Q&A:\n"," Q: What will we trace in this section?\n","A: the reflection agent system that we built\n","Q: Why are we tracing the reflection agent system?\n","A: so we can understand exactly what is happening where\n","Q: How do the two systems cooperate?\n","A: working together to deliver our final refined viral tweet\n","Q: When will the tracing begin?\n","A: so in this section we will actually trace\n","Q: Who is directing the tracing activity?\n","A: I'm just going to go ahead\n","\n","KEY CONCEPTS:\n"," reflection agent system, viral tweet, smith.chain\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," LangChain’s OpenAI integration is initialised by pip-installing langchain-openai, importing ChatOpenAI, and instantiating it with model=\"gpt-4o\" (or the economical gpt-3.5-turbo). The constructor routes every query through llm.invoke(\"query\"), returning an object whose result.content field isolates the generated text. Secure operation requires an OPENAI_API_KEY exported via python-dotenv; absence or insufficient credit triggers an exception, remedied by a ≥$5 top-up. Token usage metadata accompany responses but may be ignored for pure content retrieval. Stateful dialogue is achieved by concatenating the full conversational history into each prompt, furnishing temporal context that suppresses contradiction and repetition without architectural model change. Mastering this credential-loading and invoke-content extraction pattern equips downstream LangChain workflows with a standardised, generative backend.\n","\n","TOPICS:\n"," ['LangChain', 'Prompt Engineering', 'Python Programming']\n","\n","Q&A:\n"," Q: What package must be installed to use LangChain’s OpenAI chat model?\n","A: pip install langchain-openai\n","Q: Why did the installation initially fail in the demo?\n","A: because we have to remove this particular percentage sign\n","Q: How do you import the chat model class after installation?\n","A: from langchain_openai import ChatOpenAI\n","Q: When initializing ChatOpenAI, which keyword parameter sets the model?\n","A: model_name=\"gpt-4o\"\n","Q: Who might prefer GPT-3 over GPT-4o according to the transcript?\n","A: if you're a little short on cash\n","\n","KEY CONCEPTS:\n"," LangChain, ChatOpenAI, OpenAI API, GPT-4o, GPT-3, Large Language Model, Chat Model, Python Package Installation, VS Code Terminal, Import Statement, Model Initialization\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," Python’s list.sort() arranges strings lexicographically according to ASCII values, placing uppercase letters before lowercase; reverse=True inverts this sequence. Mixed-type lists are permitted: numerics precede strings in ascending order and follow them when reversed. Because the collation is case-sensitive, uniform case normalization is required to obtain a purely alphabetical order. The behaviour is deterministic but depends on both character case and type, so the default ordering is ASCII-based and type-hierarchical rather than strictly alphabetic.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What happens when Python sorts a list of strings with mixed capitalisation?\n","A: sort puts the words that have a capital uppercase letter first... then it sorts the ones with the lowercase\n","Q: Why does Python place numbers before strings during mixed-type list sorting?\n","A: that's just how it works\n","Q: How can you prevent the uppercase-first sorting behaviour in Python?\n","A: make sure they were all lowercase or all uppercase\n","Q: When reversing a sorted mixed-type list, where does the number appear?\n","A: we get the number at the end of the list\n","Q: Who is advised to ensure uniform case before sorting strings?\n","A: if you didn't want this to happen and you were doing it for real\n","\n","KEY CONCEPTS:\n"," Python list, sort method, reverse sorting, case-sensitive ordering, mixed-type sorting, string sorting, alphabetical ordering, uppercase precedence, lowercase precedence, numeric precedence\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," Optimal fraud detection is achieved through a hybrid model in which AI adjudicates high-confidence alerts (>70 %), where its accuracy peaks, and humans handle mid-range confidence (30–70 %), outperforming the algorithm in zones of uncertainty, cutting false-positive load by 90 %. The delegation threshold is set at the intersection of the two performance curves and must be recalibrated as data drift. Augmented intelligence—human judgment selectively enhanced by AI—surpasses either agent alone, but interface design is critical: displaying AI advice only on demand or after analysts form an initial opinion mitigates automation bias, preserves autonomous scrutiny, and maximizes predictive accuracy. Explicit uncertainty metrics reduce adherence; therefore, transparency must be sequenced to curtail cognitive biases. Objective, evidence-based allocation of decision authority converts subjective choices into calibrated human–AI collaboration, yielding superior joint detection efficacy.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What does the Y-axis of the fraud-detection graph track?\n","A: The Y axis tracks the success rate of predictions.\n","Q: Why are financial analysts overwhelmed in the fraud-detection example?\n","A: 90 percent of the alerts are false positives.\n","Q: How does a 50 % confidence score affect AI versus human performance?\n","A: At a 50 percent confidence level, a human is likely to do a better job.\n","Q: When does the AI algorithm effectively say, \"I don't know\"?\n","A: When the AI is not sure about a given prediction.\n","Q: Who typically has a flatter performance curve in fraud detection?\n","A: Human performance curves are typically a little bit flatter.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Fraud Detection, False Positives, Confidence Score, Performance Curve, Human Bias, Prediction Accuracy, Machine Learning, Decision Boundary, Human-in-the-loop, AI Uncertainty, Alert Prioritization\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Vertex AI introduces six stateless APIs that convert enterprise corpora into verifiable generative outputs. A Document Understanding model extracts tables and diagrams to improve retrieval, while upgraded text embeddings reach MTEB leadership. Vector Search unifies sparse-dense retrieval and a Ranking API reorders passages by answer relevance. The Grounded Generation API fine-tunes Gemini to cite evidence, and Check Grounding validates every sentence against provided facts, flagging contradictions. Exposing Google-scale infrastructure used in YouTube and Ads, the services integrate through LangChain and LlamaIndex, enabling developers to assemble high-accuracy retrieval-augmented generation pipelines without internal complexity and yielding hallucination-resistant, reliably cited responses.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the purpose of the Document Understanding API?\n","A: Understand document structure to improve retrieval and answer quality for generative apps.\n","Q: Why is the new embedding API considered market-leading?\n","A: Improvements make Geo models some of the most performant on leaderboards.\n","Q: How does the Ranking API enhance answer quality?\n","A: Checks how well each retrieved result answers the question to bubble up the most relevant.\n","Q: When would a developer use the Check Grounding API?\n","A: To fact-check every sentence of a statement against provided evidence.\n","Q: Who benefits from these six Vertex AI APIs?\n","A: Developers building enterprise generative applications who face repeated technical challenges.\n","\n","KEY CONCEPTS:\n"," Vertex AI, Document Understanding API, Embedding API, Vector Search, Hybrid Search, Ranking API, Grounded Generation API, Check Grounding API, Gemini, Retrieval-Augmented Generation, Enterprise grounding, Generative applications\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," Singular Value Decomposition X = UΣVᵀ decomposes a matrix into unitary matrices U, V and a diagonal Σ; economy SVD retains only the first m columns of U and the leading Σ block. Unitary matrices satisfy UᵀU = I, VᵀV = I, acting as rigid rotations that preserve vector lengths, angles, and inner products ⟨x, y⟩ = ⟨Ux, Uy⟩. Geometrically, X transforms the unit sphere in ℝⁿ into an ellipsoid in ℝᵐ whose principal axes are the left singular vectors scaled by singular values, encoding how X stretches or compresses directions. Even for rectangular X, the result is a lower-dimensional ellipsoid, extending the Fourier transform’s coordinate rotation to data-driven bases that maximize variance along principal directions.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What defining property makes U and V unitary matrices in the SVD?\n","A: u u transpose equals u transpose u equals an identity matrix\n","Q: Why does the lecturer say unitary transformations preserve vector geometry?\n","A: they preserve angles between any two vectors in the vector space\n","Q: How does multiplying a sphere of unit vectors by X geometrically deform it?\n","A: it maps into an ellipsoid with principal axes lengths given by singular values\n","Q: When data are complex rather than real, what replaces the transpose in SVD?\n","A: complex conjugate transpose\n","Q: Who—or what analogy—does the lecturer invoke to explain fixed angles under rotation?\n","A: constellations in the sky\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Unitary matrix, Economy-size SVD, Complex conjugate transpose, Fourier transform, Inner product preservation, Vector space rotation, Ellipsoid mapping, Singular values, Left/right singular vectors, Geometric deformation, Rectangular matrix mapping\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Google Gemini Pro 1.5 is a unified multimodal generative model that processes text and images within a single inference call, offering a 1-million-token context window—an order-of-magnitude advance over Gemini 1.0 Pro’s 32 k and GPT-4-Turbo’s 128 k—enabling end-to-end reasoning over hour-long videos, entire books or 30 k-line codebases without chunking. Developers obtain a no-credit-card API key, pip-install google-generativeai, instantiate gemini-1.5-pro-latest, and invoke generate_content() with interleaved image-text prompts; streaming (stream=True) mitigates latency. Demonstrations include verbatim retrieval from 330 k-token Apollo 11 transcripts, visual Q&A on crude sketches, document analysis, captioning and blog generation, all within one scalable endpoint that collapses previously separate vision and language pipelines. The consolidated interface reduces round-trips, simplifies prototyping and is already deployed enterprise-wide, redefining retrieval, summarisation and creative tasks across multimodal corpora.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What is the main goal of this video?\n","A: Build generative AI-powered application using Google Gemini Pro 1.5.\n","Q: Why does the speaker mention a 1-minute demo?\n","A: Give you an idea what all Google Gemini Pro 1.5 Pro can do.\n","Q: How will the speaker demonstrate capabilities?\n","A: Running code, playing with images and text.\n","Q: When will viewers create an API key?\n","A: After the demo, during the hands-on implementation.\n","Q: Who is the presenter of the video?\n","A: Krishn, host of the YouTube channel.\n","\n","KEY CONCEPTS:\n"," Google Gemini Pro 1.5, generative Artificial Intelligence, multimodal model, API key, long context understanding, text processing, image processing, Hands-On application, end-to-end projects, experimental feature\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Prompt-engineered models are assessed through perplexity, accuracy, and human ratings, with a bespoke evaluate_translation function achieving 100 % accuracy and minimal perplexity on a small parallel corpus. Systematic inspection of outputs identifies recurrent errors, directing targeted fine-tuning, while cross-validation and diverse test sets evaluate generalisation, aided by visualisations that reveal failure modes. Because model performance drifts with evolving data and tasks, continuous re-evaluation is obligatory. Metrics-driven iterative refinement underpins the development of dependable, deployment-ready prompt-based systems.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What matrices are commonly used to evaluate prompt-engineering models?\n","A: Perplexity, accuracy, and human evaluation.\n","Q: Why is low perplexity desirable when evaluating a language model?\n","A: The lower the perplexity, the better the language model.\n","Q: How can you debug and improve a prompt-engineering model after evaluation?\n","A: Analyze generated responses to identify common errors or patterns.\n","Q: When should you re-evaluate your prompt-engineering model?\n","A: As you continue to use your model and generate responses.\n","Q: Who ultimately rates quality in human evaluation of prompt-engineering models?\n","A: Having human rate the quality of the responses.\n","\n","KEY CONCEPTS:\n"," prompt engineering, perplexity, accuracy, human evaluation, evaluate_translation, fine-tuning, cross-validation, generalization, visualization tools, large language model, model debugging\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," Generative AI synthesizes novel content by learning statistical patterns, with large language models such as GPT-4 forming its computational core. Augmenting an LLM with external tools, persistent memory, and curated knowledge yields an AI agent capable of autonomous, goal-oriented action beyond passive question answering, exemplified by autonomously booking the cheapest flight through a travel API. Agentic AI extends this paradigm by orchestrating multiple specialized agents to execute multi-step, long-horizon tasks: a flight agent queries weather and budget APIs, then delegates to an immigration agent that verifies visa validity before ticketing. Autonomy increases along this continuum, yet meaningful human oversight remains indispensable. Frameworks including LangGraph, N8N, and Agno embed generative AI as a pluggable component within larger agentic architectures, enabling scalable progression from single-tool agents to coordinated multi-agent ecosystems that dynamically negotiate complex real-world constraints.\n","\n","TOPICS:\n"," ['Agentic AI', 'Generative AI', 'Langraph']\n","\n","Q&A:\n"," Q: What is generative AI?\n","A: AI that can create new content—text, image, or video—based on patterns learned from existing data.\n","Q: Why can’t an LLM alone give tomorrow’s flight price?\n","A: It has a knowledge cutoff date.\n","Q: How does an AI agent differ from a generative AI chatbot?\n","A: It completes tasks using tools, memory, and knowledge, not just Q&A.\n","Q: When does a system qualify as agentic AI?\n","A: When one or more agents work autonomously over long, complex tasks using tools and planning.\n","Q: Who checks visa eligibility in the multi-agent travel example?\n","A: The immigration AI agent.\n","\n","KEY CONCEPTS:\n"," Generative AI, Large Language Model (LLM), Knowledge Cutoff, API Integration, AI Agent, Tool Use, Autonomy, Agentic AI, Multi-step Reasoning, Multi-agent System, Planning, N8N\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Covariance measures the directional linear relationship between two random variables via Cov(X,Y)=1n∑i=1n(Xi−μX)(Yi−μY). Positive values indicate that Y tends to increase with X, whereas negative values imply Y decreases as X increases. When X=Y the expression collapses to Var(X), illustrating that variance is the self-covariance of a variable. Because the magnitude of covariance is unscaled, it conveys only the direction, not the strength, of association, thereby motivating the introduction of the normalized Pearson correlation coefficient in subsequent discussion. Consequently, covariance functions as a directional but not standardized indicator of joint variability.\n","\n","TOPICS:\n"," ['Statistics']\n","\n","Q&A:\n"," Q: What does covariance quantify between two random variables?\n","A: quantify a relationship such that if the X increases my Y will increase\n","Q: Why does covariance become positive when both variables increase together?\n","A: X of I minus mu of X... positive value multiplied by Y... also a positive value\n","Q: How is covariance related to variance when X equals Y?\n","A: covariance of X with X is nothing but variance of X\n","Q: When does covariance yield a negative value?\n","A: when X is increasing Y is decreasing my covariance will be negative\n","Q: Who uses covariance in data preprocessing and why?\n","A: covariance is one of the very important topic when we consider the data pre-processing\n","\n","KEY CONCEPTS:\n"," covariance, variance, random variables, mean, Pearson correlation coefficient, data preprocessing, data analysis, positive covariance, negative covariance, quantifying relationship, machine learning, statistics\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning seeks an optimal policy π*(s)→a that maximises the expected cumulative reward 𝔼[∑tγtrt], where the scalar reward rt constitutes the sole feedback on action quality. Positive values signal goal attainment, as in Tic-Tac-Toe (+1 win, −1 loss, 0 draw), while continuous domains such as trading employ profit or risk-adjusted metrics. The agent iteratively explores and exploits, refining state–action value estimates until convergence. Episodic settings rely on sparse terminal rewards, whereas continuing tasks provide dense reward streams without a fixed horizon. Effective parameterisation demands explicit reward functions that map states and actions to scalars aligned with user objectives, ensuring that the emergent behaviour optimally balances exploration and exploitation to maximise long-term return.\n","\n","TOPICS:\n"," ['Reinforcement Learning']\n","\n","Q&A:\n"," Q: What is the objective of any reinforcement-learning problem?\n","A: to learn the optimal policy that maximize a numerical reward signal\n","Q: Why does an RL agent need a reward signal?\n","A: provides feedback to the agent about the quality of the action\n","Q: How does an agent learn the optimal policy?\n","A: by trial and error learning where it explores the environment by taking action\n","Q: When does an episodic task like Tic-Tac-Toe give its final reward?\n","A: at the end of the game either you win or lose\n","Q: Who defines the reward function for continuous tasks such as stock-market trading?\n","A: we need to define the reward function that reflect your trading goal and preferences\n","\n","KEY CONCEPTS:\n"," reinforcement learning, agent, environment, reward signal, optimal policy, cumulative reward, trial-and-error learning, value-based method, policy-based method, reward function, episodic task, continuous task\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," Python dictionaries are unordered collections of mutable key–value pairs enclosed in curly braces, where immutable objects (strings, integers, tuples) serve as keys. Pairs are comma-separated and linked by colons; the built-in functions items(), keys(), and values() expose dictionary components, while len() reports cardinality. Dictionaries can be constructed from two equal-length iterables using zip() combined with dict(). Element retrieval employs bracket notation, and content is modified through assignment or deletion; list() converts keys or values into lists. Proficiency with these structures is foundational for subsequent data manipulation in pandas workflows.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What separates individual key-value pairs inside a Python dictionary?\n","A: items are separated by commas\n","Q: Why must a dictionary key be immutable?\n","A: the key has to be immutable so it can't change\n","Q: How can you create a dictionary from two separate lists?\n","A: dict and zip function\n","Q: When you run D.items() on a dictionary D, what does it return?\n","A: it will show that we have three items three key value pairs\n","Q: Who (which built-in function) deletes a key-value pair from a dictionary?\n","A: de el\n","\n","KEY CONCEPTS:\n"," dictionary, key-value pair, immutable key, curly brackets, dict() constructor, zip() function, tuple, list conversion, item access, item deletion, length, keys(), values(), items()\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," AI-driven User and Entity Behavior Analytics (UEBA) reduces breach lifecycles by 108 days versus manual methods, cutting average containment cost below IBM’s 2023 benchmark of $4.45 M. Unsupervised machine-learning models baseline legitimate user activity within seven days, then flag deviations such as abnormal file access, off-hours logins, or credential sharing that indicate insider threats. Real-time anomaly scoring enables proactive containment, shrinking dwell time and data exfiltration. IBM QRadar SIEM embeds UEBA to rank employee risk, surface correlated offenses, and map them to MITRE ATT&CK tactics with confidence scores; analysts interrogate dashboards for identities, session-level IoCs, high-value assets, and historical trends, reducing investigation time from hours to minutes. Continuous telemetry ingestion from identity providers, endpoints, and SaaS, coupled with reinforcement learning from analyst feedback, suppresses false positives and refines detection accuracy. Natural-language insights and relationship graphs visualize internal and external IoCs, converting behavioral noise into prioritized, actionable intelligence that materially lowers organizational risk.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What did IBM’s 2023 Cost of a Data Breach Report find about AI-enabled breach containment?\n","A: It took 108 fewer days on average to identify and contain a data breach.\n","Q: Why is faster containment significant for organizations using AI and automation?\n","A: Faster containment reduces the overall impact and cost of a data breach.\n","Q: How can UBA with AI and machine learning assist security teams?\n","A: It helps detect and respond to Insider threats quickly and precisely.\n","Q: When did IBM survey over 500 organizations for its 2023 report?\n","A: During the 2023 Cost of a Data Breach Report study period.\n","Q: Who faces major concerns from Insider threats according to the transcript?\n","A: Organizations of all sizes.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Machine Learning, User Behavior Analytics, Insider Threat Detection, Automation, Data Breach Containment, Security Posture, Threat Intelligence, Anomaly Detection, Predictive Analytics, Security Information and Event Management\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta’s open-source Llama 3, released in 8 B and 70 B parameter variants pretrained on 50 T tokens with 8 k-token context, advances reasoning, code generation, translation, and dialogue while halving false refusals. Benchmarks show the 8 B model surpassing Gemma-7B and Mistral-7B, and the 70 B model rivaling Gemini-Pro-1; an associated MML implementation matches GPT-4 on HumanEval and GSM8K, confirming open-weight parity with proprietary LLMs outside high-end mathematics. Weights are distributed under a gated license: users request access via Meta, Hugging Face, or Kaggle, receive a signed URL, clone the repository, install dependencies, and run the provided inference snippet locally. Comprehensive documentation, GitHub recipes, and the bundled Llama Guard safety toolkit ensure reproducible, accountable deployment, establishing Llama 3 as a cost-effective, state-of-the-art foundation for research and industry.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the speaker's name?\n","A: krishak\n","Q: Why is the speaker recording at 2 a.m.?\n","A: welcome to my YouTube channel\n","Q: How does the speaker greet the audience?\n","A: hello my name is krishak\n","Q: When is the video being recorded?\n","A: right now it is 2 a.m.\n","Q: Who is hosting the YouTube channel?\n","A: krishak\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," scikit-learn’s GaussianNB realises a generative classifier that models each class as an independent univariate Gaussian, yielding closed-form parameter estimates and linear-quadratic decision boundaries. After importing GaussianNB, users invoke fit to estimate per-feature means and variances, then predict to allocate new observations via posterior maximisation. The compact API supports rapid prototyping, cross-validated performance assessment, and boundary visualisation on continuous-valued datasets, while the underlying Bayes derivation remains accessible through searchable documentation. The resulting estimator scales linearly with sample size, requires no hyper-parameter tuning, and distils probabilistic classification into a single, reusable object suitable for educational and production contexts alike.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What Python library does the instructor rely on for the Naive Bayes classifier?\n","A: scikit-learn, which is often abbreviated sk-learn.\n","Q: Why does the instructor suggest starting with Google?\n","A: to help us use the documentation of that library to figure out how to use some of the functions.\n","Q: How did the instructor write the classifier you just saw?\n","A: Gaussian Naive Bayes, this is what the way that I actually wrote classifier.\n","Q: When will students be able to write the decision-boundary code themselves?\n","A: by the end of the next video or two.\n","Q: Who is the target algorithm the instructor used in the lesson?\n","A: the name of the algorithm that I just used, which happens to be Naive Bayes.\n","\n","KEY CONCEPTS:\n"," scikit-learn, sklearn, Naive Bayes, Gaussian Naive Bayes, decision boundary, classifier, Python library, algorithm, documentation, derivation, use cases\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Gaussian distributions are symmetric bell curves centred at μ with 68 %, 95 % and 99.7 % of probability mass within ±1σ, ±2σ, ±3σ. A variable X is log-normal if ln X ~ 𝒩(μ,σ²), yielding right-skewed, heavy-tailed densities commonly observed in income, product-review length or comment-size data. Recognising the underlying distribution dictates preprocessing: Gaussian variables are standardised via (x–μ)/σ, whereas log-normal variables are first log-transformed to Gaussian before identical rescaling. Aligning scales across features such as R&D and marketing spend stabilises gradients and improves downstream model accuracy. Mastery of distributional assumptions therefore underpins effective normalisation and feature-engineering pipelines. Correct distributional diagnosis enables principled scaling that converts heterogeneous raw inputs into comparably scaled, model-ready features.\n","\n","TOPICS:\n"," ['Statistics']\n","\n","Q&A:\n"," Q: What percentage of a Gaussian distribution falls within one standard deviation?\n","A: around 68 percent\n","Q: Why is the Gaussian curve called a bell curve?\n","A: the right side is symmetrical to the left side\n","Q: How is a log-normal distribution formally defined?\n","A: if log of X is normally distributed\n","Q: When should log normalization be applied to a feature?\n","A: when the feature follows a log-normal distribution\n","Q: Who benefits from converting features to the same scale?\n","A: my model accuracy will be increased\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, normal distribution, empirical rule, bell curve, log normal distribution, standard normal distribution, standard scaler, log normalization, mean, standard deviation, scaling, domain knowledge\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," AtliQ Agriculture presents an end-to-end CNN-based mobile system that enables potato growers to distinguish early from late blight, two diseases whose fungicide treatments differ. A TensorFlow CNN trained on augmented leaf images is exported to TensorFlow Lite for edge efficiency while the full SavedModel is served via TensorFlow Serving and FastAPI on Google Cloud Functions, exposing a serverless REST endpoint. A React-Native app captures leaf photos, queries the endpoint, and returns instantaneous healthy/disease predictions. The modular pipeline—data ingestion, augmentation, CNN training, TF Lite quantization, TF Serving versioning, FastAPI routing, and cross-platform front-end—exemplifies a reproducible MLOps workflow balancing latency, scalability, and maintainability. The project offers a portfolio-ready template for computer-vision crop protection, readily extensible to other crops and deployable with basic Python and CNN knowledge.\n","\n","TOPICS:\n"," ['Deep Learning', 'Mlops']\n","\n","Q&A:\n"," Q: What two potato diseases will the deep-learning model identify?\n","A: early blight and late blight\n","Q: Why is early detection of these diseases important for farmers?\n","A: it can save lot of waste and prevent the economic loss\n","Q: How will the farmer use the mobile app to check a plant?\n","A: take a picture of the plant and the mobile application will tell\n","Q: When the model is built, where will it be deployed?\n","A: deploy the model to Google Cloud or GCP\n","Q: Who has taken the project to build this AI mobile application?\n","A: AtliQ Agriculture\n","\n","KEY CONCEPTS:\n"," deep learning, convolutional neural network, TF serving, ML Ops, FastAPI, Google Cloud Platform, Google Cloud Functions, React Native, early blight, late blight, computer vision, image classification\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," Large language model autonomy evolves through six stages: deterministic code, single-call I/O, fixed chains of specialist LLMs, router-based selection among predefined chains, state-machine agents, and fully autonomous agents. While chains surpass monolithic calls via task decomposition, they remain rigid; routers introduce conditional routing yet lack memory. State machines, exemplified by LangGraph, confer genuine agency by supporting cyclic control flow, memory, tool use, human-in-the-loop approval, and iterative refinement. Users delegate to a single head agent that hierarchically assigns, reviews, and publishes tasks. Agenthood is thus defined by LLM-directed loops enabling self-correction, distinguishing it from unidirectional chains and routers; full autonomy remains impractical.\n","\n","TOPICS:\n"," ['Agentic AI', 'Langraph', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What disadvantage of code with zero autonomy is highlighted?\n","A: You would need to write rules for every possible scenario\n","Q: Why does a single LLM call often give confused responses?\n","A: Trying to get everything done in one shot often leads to confused or mixed up responses\n","Q: How do chains improve on a single LLM call?\n","A: Multiple specialists instead of one generalist\n","Q: When is an LLM-based system first called an agent?\n","A: Whenever the control flow is controlled by an LLM\n","Q: Who ultimately approves the draft in the State-machine example?\n","A: The user\n","\n","KEY CONCEPTS:\n"," LLM call, Chains, Router, State machine, Agent, LangGraph, Human-in-the-loop, Multi-agent systems, Memory management, Tools, Autonomy levels, Deterministic code\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," Advanced prompt engineering now spans text, image, and audio modalities, exemplified by a dog-breed classifier fine-tuned from CNN backbones (ResNet50, VGG16, Xception, Inception) with logistic regression. Multitask learning optimizes shared representations across objectives via cross-entropy and Adam, while knowledge distillation compresses a large teacher such as T5 into a faster student by matching logits. Robust pipelines mandate tokenization, lowercase normalization, and augmentation to mitigate bias. Deployment leverages TensorFlow Serving or Flask APIs, and ethical compliance requires representative data to safeguard fairness and privacy. Mastery entails integrating multimodal preprocessing, parameter-efficient fine-tuning, and continuous responsible monitoring for production-grade systems.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Deep Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is multitask learning in fine-tuning LLMs?\n","A: Training a model on multiple tasks simultaneously to learn robust representations that generalize to different users.\n","Q: Why use distillation when deploying LLMs?\n","A: To train a smaller model to mimic a larger one, making it more efficient and faster.\n","Q: How does tokenization aid prompt engineering?\n","A: Breaking text into smaller units helps the model understand meaning more accurately.\n","Q: When should TensorFlow Serving be chosen for deployment?\n","A: When you need a framework for serving machine-learning models in production environments.\n","Q: Who is affected by bias in prompt-engineering datasets?\n","A: Whole populations, leading to unfair and discriminatory outcomes if data is not representative.\n","\n","KEY CONCEPTS:\n"," multitask learning, distillation, tokenization, normalization, data augmentation, self-supervised learning, TensorFlow Serving, Flask, ResNet50, VGG16, Xception, Inception\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," Eigenfaces compress 35 k-pixel aligned greyscale portraits by applying SVD to the mean-centred 35 000×40 data matrix B, producing orthogonal basis vectors whose first nine columns encode salient identity cues such as Terminator glasses or Stallone eyes. Projecting each image onto the leading three principal components yields 3-D coordinates that cluster by subject; Arnold and Stallone clouds separate imperfectly, whereas replacing Arnold with Taylor Swift enlarges the margin, yet Arnold versus Swift overlap remains high, revealing pixel-correlation bias toward shared fair skin and hair. New images are classified by nearest-cluster assignment in eigenface space. The experiment demonstrates both the efficacy and the colour-bounded limitations of linear PCA face recognition prior to deep 3-D modelling.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What is the size of each cropped action-hero image used to build the data matrix?\n","A: 200 by 175 pixels\n","Q: Why are the columns of U from the SVD called the eigenfaces?\n","A: They are the principal components of the mean-subtracted image matrix\n","Q: How is a 35,000-dimensional image vector projected into 3-D eigenface space?\n","A: Multiply the image vector by the transpose of the first three eigenfaces\n","Q: When classifying a new face, which cluster proximity rule is used?\n","A: Whichever cluster centroid is nearest in eigenface coordinates\n","Q: Who exhibits better separation in the first three eigenfaces: Arnold vs Stallone or Taylor vs Stallone?\n","A: Taylor Swift and Stallone show better separation than Arnold and Stallone\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Image Classification, Eigen Heroes, Face Clustering, Projection into Eigenface Space, Average Face Subtraction, Economy SVD, Reshaped Image Vectors, Skin-Tone Bias in Classification, 3-D Face Geometry Inference\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is an open-source orchestration framework that converts large language models from isolated text generators into production-grade, context-aware agents capable of autonomous, multi-step action. By abstracting model interfaces, it enables seamless swapping between proprietary and open-source LLMs without code changes. Its compositional architecture chains prompts, tools, and memory, securely integrating live APIs, corporate databases, web search, and structured scraping into deterministic workflows. Applications span customer-support bots querying CRM records, marketing agents personalizing outreach via aggregated web and wiki intelligence, and autonomous booking or email dispatch. LangChain thus operationalises LLMs, shifting AI from passive prediction to embodied, auditable, real-world task execution.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is LangChain described as in the transcript?\n","A: Lang chain is by far the most popular framework that helps build applications using llms\n","Q: Why do LLMs alone fail to book real-world services?\n","A: They cannot actually interact with the real world\n","Q: How does LangChain let developers swap models without code changes?\n","A: You can easily do so without even touching the code that you wrote with L chain\n","Q: When planning a vacation, what limitation did ChatGPT show?\n","A: It says I cannot make bookings directly but I can help you with planning\n","Q: Who (or what) acts as the bridge between LLMs and real-world APIs?\n","A: Lang chain acts as a bridge between the llms and the real world\n","\n","KEY CONCEPTS:\n"," LangChain, large language models, LLM, API integration, agentic AI, real-world interaction, framework, reasoning ability, model swapping, Hugging Face, GPT-4, booking APIs\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residuals, the discrepancies between fitted training values and actual observations, must display zero mean and absence of serial correlation to ensure unbiased time-series models; errors pertain to unseen test data. Autocorrelation (ACF) and partial autocorrelation (PACF) plots, supplemented by the Ljung-Box test, diagnose residual structure, with significant seasonal-lag correlation indicating unmodelled seasonality. A seasonal Holt-Winters exponential smoothing model applied to the AirPassengers dataset yielded residuals with negligible mean bias (−0.02) but exhibited 12-period ACF/PACF spikes and Ljung-Box p-values below 0.05, confirming residual correlation. Re-specifying the model to incorporate the remaining yearly pattern is expected to enhance forecast accuracy. Histograms reveal approximately symmetric residuals around zero, underscoring minimal systematic deviation. Joint assessment of autocorrelation and bias exposes departures from white-noise assumptions, guiding iterative model refinement. Because this diagnostic process relies only on residual plots and standard tests, it is lightweight yet indispensable for converting opaque errors into actionable improvements.\n","\n","TOPICS:\n"," ['Time Series', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What are residuals in time-series analysis?\n","A: residuals are simply the difference between the fitted value y-hat and the actual value\n","Q: Why must residuals have zero autocorrelation?\n","A: if it has some form of correlation the model has missed some information\n","Q: How can a non-zero mean residual bias forecasts?\n","A: we have biased so if residuals are shifted left or right we’re under- or over-forecasting\n","Q: When should the Ljung-Box test be applied to residuals?\n","A: quantifying if there is indeed correlation in residuals\n","Q: Who uses fitted values when computing residuals?\n","A: the fitted values are the data the model has seen\n","\n","KEY CONCEPTS:\n"," residuals, fitted values, forecast errors, autocorrelation, partial autocorrelation, Ljung-Box test, bias, zero-mean residuals, Holt-Winters model, exponential smoothing, seasonality, serial correlation\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," A ReAct agent implemented with LangGraph and watsonx.ai Mistral Large synthesises natural-language queries into SQL, enabling conversational access to an in-memory SQLite database. The TypeScript Next.js frontend, styled with Tailwind, uses React state hooks to manage messages, input, and loading status, rendering HumanMessage and AIMessage while excluding SystemMessage. Server-side actions instantiate the agent, serialize chat history, and return LLM responses. A GetFromDB LangChain tool encapsulates query execution and embeds the full schema, guiding the LLM to generate syntactically correct SQL with double-quoted identifiers. On component mount, seed() creates and populates customer and order tables with foreign-key relationships. Precise system prompts mandate tool usage and SQLite dialect, converting the LLM into a reliable SQL generator verified through logged queries. Guardrails, credential isolation, and ephemeral storage ensure secure, reproducible NLIDB deployment; the open-source stack demonstrates rapid prototyping of declarative, AI-driven database interfaces.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Langraph', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is the goal of the AI agent being built?\n","A: An agent that's able to use SQL knowledge to connect to your databases.\n","Q: Why is LangGraph used in the project?\n","A: To build a ReAct agent.\n","Q: How is the Next.js boilerplate project created?\n","A: Running create-next-app at latest with the project name.\n","Q: When should you move into the Text2SQL agent directory?\n","A: Before you try to start the application.\n","Q: Who decides whether to run components client-side or server-side in Next.js?\n","A: You can run components either client-side or run code server-side.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence agent, large language models, SQL, LangGraph, ReAct agent, Next.js, watsonx.ai, SQLite, TypeScript, Tailwind, client-side component, server-side rendering\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering, the NLP discipline of crafting inputs that elicit accurate, context-sensitive outputs from large pre-trained language models such as GPT, PaLM, or T5, systematically outperforms rule-based or keyword approaches in chatbots, translation, and content generation by leveraging prompt decomposition, constraint identification, and task-specific fine-tuning. Despite limitations in resolving ambiguous queries and mitigating dataset or architectural bias, a structured curriculum covering foundational concepts, evaluation metrics, iterative testing, and advanced fine-tuning strategies equips practitioners to build reliable prompt pipelines. Mastery converts raw model potential into deployable, user-aligned language services, establishing prompt engineering as the critical interface that transforms foundation models into trustworthy applications.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: A specialized field within NLP that focuses on building models generating high-quality text outputs in response to prompts.\n","Q: Why is prompt engineering important?\n","A: It allows us to generate text outputs that are more accurate, coherent, and contextually appropriate than traditional rule-based approaches.\n","Q: How do prompt engineering models work?\n","A: Based on pre-trained large language models such as OpenAI GPT, Google BERT, or Hugging Face Transformers fine-tuned for specific tasks.\n","Q: When may prompt engineering models struggle?\n","A: Models may struggle with complex and ambiguous prompts or generate biased and inaccurate outputs due to underlying data.\n","Q: Who provides the core technologies underlying prompt engineering?\n","A: OpenAI GPT, Google BERT, and Hugging Face Transformers.\n","\n","KEY CONCEPTS:\n"," prompt engineering, natural language processing, large language models, GPT, Google BERT, Hugging Face Transformers, fine-tuning, contextual appropriateness, rule-based approaches, keyword-based approaches, bias in language models, prompt analysis\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is an off-policy, model-free reinforcement-learning algorithm that recursively estimates the optimal state-action value function Q(s,a) by iteratively applying the Bellman update Qt(s,a)=r+γmaxa′Qt(s′,a′), where r is the immediate reward, γ the discount factor, and s′ the successor state. Exploration is performed by an ε-greedy behavior policy μ, whereas the target policy π*(s)=argmaxₐ Q(s,a) is derived greedily from the converging Q-table. Temporal-difference learning propagates value estimates online: the TD error δ=r+γmaxₐ′Q(s′,a′)−Q(s,a) quantifies the prediction mismatch and drives gradient-style updates with step-size α, exemplified by revisions from 1→0.985 and 1.5→1.267. Repeated interaction through episodes, such as navigating a grid-world toward a +10 reward while avoiding a −10 poison square, guarantees convergence to the optimal action-value function under sufficient state-action visitation, enabling the agent to maximize cumulative reward without environmental models.\n","\n","TOPICS:\n"," ['Reinforcement Learning']\n","\n","Q&A:\n"," Q: What are the three primary machine learning paradigms mentioned?\n","A: supervised learning, unsupervised learning, and reinforcement learning\n","Q: Why is Q-learning classified as a value-based method?\n","A: it learns a value function that quantifies total reward to determine the optimal policy\n","Q: How does a state-action value function differ from a state value function?\n","A: state-action value function takes state and action and outputs a q value; state value function takes only state\n","Q: When initializing the Q-table for Q-learning, what values can be used?\n","A: arbitrary values or values loaded by some other agent that explored the environment previously\n","Q: Who defines the behavior policy that selects actions during Q-learning exploration?\n","A: the behavior policy can be just about anything; actions based on random chance\n","\n","KEY CONCEPTS:\n"," Q-learning, reinforcement learning, value-based methods, policy-based methods, state value function, state-action value function, Q value, Bellman equation, discount factor, behavior policy, target policy, optimal policy\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," Logistic regression is a linear multiclass model that maps input features X to class predictions through a single affine transformation with learnable weights W and bias b. Training maximises the raw scores (logits) corresponding to the correct class. To enforce a single-label constraint, the logits are transformed by a softmax function into a probability distribution summing to one, exponentially favouring larger scores while preserving their order. These calibrated probabilities are fed into a cross-entropy loss, enabling gradient-based optimisation of the decision boundaries. By integrating linear transformation with probabilistic normalisation, the classifier converts pixel data into reliable class probabilities and achieves effective multiclass discrimination.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is what's called the linear classifier.\n","Q: How does a logistic classifier generate its predictions?\n","A: It applies a linear function to them... a giant matrix multiply.\n","Q: Why do we turn scores into probabilities?\n","A: We want the probability of the correct class to be very close to one.\n","Q: When are proper probabilities large?\n","A: They will be large when the scores are large.\n","Q: Who denotes the softmax function?\n","A: I'll denote here by S.\n","\n","KEY CONCEPTS:\n"," logistic classifier, linear classifier, linear function, matrix multiply, weights, bias, softmax function, scores, logits, probabilities, training, output class\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n","\n","Role-based generation pipeline completed.\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["52d605cf0e134600b4d8c9f3cde4ac9f","181cf712cc344ebbbc5d2a009e7abff1","d8639b01ddc640399ab7c0e6ddd1126a","c9b0966c6e61432a8e4ee7d77633eb43","8a55efd47c374958811f297b888216dd","6a954f05f9b64c12b2711d386f8a9f4a","63c8dbf9f436467c8a96586ac86b20c8","93e13c64b6f54901ab3c4bab4fbb5519","1e6756b7e19847e29f5d77c119bc364c","9687a74f011d47c8aa6b2d5a7140ee13","84aabf20f3264003bfd46ae8975d366f","45fbc21444c7479192e091ee422532e5","55f38698220c44eaafe6bcfa81dbe3f1","86a6a778c59044e6a6b518b4a7df391a","b8670e8ec672484a9b213e9e30edf4f7","bb5d1b71b6cb4969a0155ad7c959c625","b86f70059ba64e6f833bf18d8589a130","9e3618e691094f09afa489cc2c738a6c","38a4fc4f6355422a8b3e5a1972c94822","9d41ba3da4da48328e238a4b7ed222cb","f454331630b9435c947158b854be18b2","410fa44907d04006befdf38c1bff75b8","ce37398ae983478f98ca2774e6dedc54","6b88015d12234ea2878c5e09f9ca58c7","b2886d352ff24a3daf73f7efee325f05","116d9810df684b92b17ede2bd4b0af52","a76ee7a326134472aa9eab1ca1ddd9af","ce29f3e9ffea44f98924951ba8b81b48","033212c4f4f24e9a8f3bc61cfe1f04c1","ec454271c9484891a6c5abd1d5ece515","845073f66a984ee9ae8f770378085f90","2ee3108d868a42e3b6ac988946a4a64d","a8fadb5dc4f8444f88d7b6d4558e9d24","6f6b4a801b2a484ba3ce245d20c6d46b","bfb211c926be43a8a4b8a816f61c3119","a62ad976e3ec4bc985604a8a663c2cdc","868e7b66f2fd4f04a6dbdcb702826251","e14b3d6cdc0647daa44ae018068e93e7","4da4913135214df0bc96dd15b2776d30","d0a28bd50b114621880c9694fde2cc75","ebaec1c26aee412eb36233e0815e18aa","7fe12732d3e7496da345b7931431d7da","57e3d54ee4a041a69199967afd4cc93d","8b36b5be27c04df2901ce6e3747e5a94","f569db1ced414578aed91a70089ea38a","aeceed819baf433ebe58a5424444a27c","248b0644bb43429a902096bdcbbd7256","779bce1c3dc34deab989da03e10653cf","104c159e46f942fcb295d8155d510c3e","40e020246d864c49b635fae4ea92fd72","57255c4f14b549fc95d037e7e2739ae2","10325e3a955c4713bebd56eb8ff59cc0","688c73b2d46e4134953d435c79fb7a64","3d2168244444452a8296413e712044db","4adfc9225eb045e497904386a79823e6","171d288a81d748ecbbd49163350788e1","9a6e3f7a1bc34f7fbb9573850f9d5d0c","c3ec8ac420ff4a59878f90e231367c9c","0543ea4acfa242cb8235abfee806e765","9ab4e36aa4964f00a1bdfc034a0a7a8f","a115a8ab5c8943f3bb420b624c6a7741","cd3f922e25264cc09facd7f780950cd6","cba195634a584860bed4f38229b98709","e34a63fa75924d678cc3990467fdc64f","f8ce5bac2d8847d385f6e06838eba8f3","c62257f724ce40e4a4111c5927aa30cc"]},"id":"yYdeX5BeuEQT","executionInfo":{"status":"ok","timestamp":1763956145207,"user_tz":-330,"elapsed":202146,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"a0e93f23-1731-4d96-d6c5-46eae4a9003d"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/kimi-k2-instruct-0905_role_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52d605cf0e134600b4d8c9f3cde4ac9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45fbc21444c7479192e091ee422532e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce37398ae983478f98ca2774e6dedc54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f6b4a801b2a484ba3ce245d20c6d46b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f569db1ced414578aed91a70089ea38a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171d288a81d748ecbbd49163350788e1"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.1943\n","  - BLEU: 0.0148\n","  - BERTScore F1: 0.8558\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.9000\n","  - Jaccard Index: 0.4067\n","  - Micro F1: 0.5357\n","  - Macro F1: 0.4813\n","  - Weighted F1: 0.5014\n","\n","Q&A Generation:\n","  - BLEU: 0.0223\n","  - Diversity: 0.8306\n","  - Answerability: 0.6200\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4933\n","  - Recall@10: 0.1973\n","  - F1@10: 0.2819\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/kimi-k2-instruct-0905/evaluation_final.json\n"]}]}]}