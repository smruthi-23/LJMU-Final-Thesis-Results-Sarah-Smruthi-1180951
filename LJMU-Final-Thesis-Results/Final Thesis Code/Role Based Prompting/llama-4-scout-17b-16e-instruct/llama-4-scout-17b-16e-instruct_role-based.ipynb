{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOni8On6cjzLEaQ5HiKdBvp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f8c98ff15c144ca7aa417a84bf9ccdaa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9ea2ad51582d483690a0d34ce106618d","IPY_MODEL_caf071a01d68488c86f304182e0e5384","IPY_MODEL_db15d0a527304208b594ce856b20b812"],"layout":"IPY_MODEL_3fcd83da31ad4cd3ac5592f8eeeff6be"}},"9ea2ad51582d483690a0d34ce106618d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df89c698450b4358bac58aba301b1d2d","placeholder":"​","style":"IPY_MODEL_9a25dbdf47a745be8b502ef4613cb804","value":"tokenizer_config.json: 100%"}},"caf071a01d68488c86f304182e0e5384":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f68cd6d98cb944409cb026d7f8994cb5","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_50e4a497542a4e38b649e3e144cd7e6c","value":25}},"db15d0a527304208b594ce856b20b812":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cc658169c7842d4bba7c1507f7f3a23","placeholder":"​","style":"IPY_MODEL_d08941c1b46c4398966076f0012b03e4","value":" 25.0/25.0 [00:00&lt;00:00, 2.26kB/s]"}},"3fcd83da31ad4cd3ac5592f8eeeff6be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df89c698450b4358bac58aba301b1d2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a25dbdf47a745be8b502ef4613cb804":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f68cd6d98cb944409cb026d7f8994cb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50e4a497542a4e38b649e3e144cd7e6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0cc658169c7842d4bba7c1507f7f3a23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d08941c1b46c4398966076f0012b03e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f1f0d4378d8742408f0c3c88ccb21f1a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0c8d9cbaea46446eaf7261ad84c808e3","IPY_MODEL_6f42716967284b74a747a2ebeeee626c","IPY_MODEL_51b961555cb34a8fb1c4d8fd9a261fc8"],"layout":"IPY_MODEL_f73a73fe056242c49d9c122daca509fe"}},"0c8d9cbaea46446eaf7261ad84c808e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_adc5e3f8cf8049aca534ee7f3dfbda2f","placeholder":"​","style":"IPY_MODEL_35dc5ba7c94f4055b3a32bee083fdba5","value":"config.json: 100%"}},"6f42716967284b74a747a2ebeeee626c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_47da88c2049f4289b4420f33754a132d","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e820a81973a40be9d00e192df204be0","value":482}},"51b961555cb34a8fb1c4d8fd9a261fc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1daf2015c62b4edca6f59fafb910d9d3","placeholder":"​","style":"IPY_MODEL_359bf54ffb3a4655aecf3d0c3e299259","value":" 482/482 [00:00&lt;00:00, 50.1kB/s]"}},"f73a73fe056242c49d9c122daca509fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adc5e3f8cf8049aca534ee7f3dfbda2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35dc5ba7c94f4055b3a32bee083fdba5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47da88c2049f4289b4420f33754a132d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e820a81973a40be9d00e192df204be0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1daf2015c62b4edca6f59fafb910d9d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"359bf54ffb3a4655aecf3d0c3e299259":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0b92809b45e4c8d8958e60b9e23f7d0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9036f55b58d94097a404155da58fefef","IPY_MODEL_c5c64ff10a8b4a09965d43bbe92074f9","IPY_MODEL_db67db4bb54b4b8da67167bb66b54624"],"layout":"IPY_MODEL_f10255c8237a4efc92bbbddd74d62dcb"}},"9036f55b58d94097a404155da58fefef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5b275b0c95442d8be8f0c75c6a61c61","placeholder":"​","style":"IPY_MODEL_50c40aace4464aadba5526ac83dc4862","value":"vocab.json: 100%"}},"c5c64ff10a8b4a09965d43bbe92074f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f676310a3124c4a93229ab8779f9895","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e86d70f26bf4b578cb710db416f5227","value":898823}},"db67db4bb54b4b8da67167bb66b54624":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_edbcea073c9e4da7b6a3896d544d0b33","placeholder":"​","style":"IPY_MODEL_2a733aba23fb4a379c9d861da8b01ec3","value":" 899k/899k [00:00&lt;00:00, 7.70MB/s]"}},"f10255c8237a4efc92bbbddd74d62dcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5b275b0c95442d8be8f0c75c6a61c61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50c40aace4464aadba5526ac83dc4862":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f676310a3124c4a93229ab8779f9895":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e86d70f26bf4b578cb710db416f5227":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"edbcea073c9e4da7b6a3896d544d0b33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a733aba23fb4a379c9d861da8b01ec3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89b4685de74a47ce8290552485a0e4d8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d9514a39d5df44b78d45b2946dbe29a8","IPY_MODEL_9b07aa00f3ab4835b1339303fafa51cb","IPY_MODEL_7e8559cdc7da43a6af62b1240c40583b"],"layout":"IPY_MODEL_04addd9f644b479bb300f0db5553d010"}},"d9514a39d5df44b78d45b2946dbe29a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae50cc99480042c784cc1ef27dfbf220","placeholder":"​","style":"IPY_MODEL_bd003f4cb48b4e628f2b1f6d3815d701","value":"merges.txt: 100%"}},"9b07aa00f3ab4835b1339303fafa51cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2de2fb8eb73543cdbf41231db20b6f4e","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7dba41e415d411d82330421410cc633","value":456318}},"7e8559cdc7da43a6af62b1240c40583b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47233f390c3e4948a85c3e77ab6989e0","placeholder":"​","style":"IPY_MODEL_bc70d7f57ac3492ea965da0baa005c78","value":" 456k/456k [00:00&lt;00:00, 5.92MB/s]"}},"04addd9f644b479bb300f0db5553d010":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae50cc99480042c784cc1ef27dfbf220":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd003f4cb48b4e628f2b1f6d3815d701":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2de2fb8eb73543cdbf41231db20b6f4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7dba41e415d411d82330421410cc633":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47233f390c3e4948a85c3e77ab6989e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc70d7f57ac3492ea965da0baa005c78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9af50a0344a249a8bb60608e7467dabb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_019b2374bbe14bdaa861a5b163c258af","IPY_MODEL_81eedc9d6eda4da088b2e8d5ff7e975e","IPY_MODEL_5a257ff7c8ee49729321d4183e7ba32b"],"layout":"IPY_MODEL_42add648368448ab803691d37bdac5b3"}},"019b2374bbe14bdaa861a5b163c258af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21612435a81345ddbd37ab15689b7378","placeholder":"​","style":"IPY_MODEL_ff20b3ae05f547fa8e6f0ebfb5ff7224","value":"tokenizer.json: 100%"}},"81eedc9d6eda4da088b2e8d5ff7e975e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75ad49e4f3e6432f93c97ec9331ce403","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_48bf9a0d1c73466bb2dca2273f479185","value":1355863}},"5a257ff7c8ee49729321d4183e7ba32b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b1f6ea287204968976841e7c46b5769","placeholder":"​","style":"IPY_MODEL_5cdb13d0211d418c9f041ed9efb31148","value":" 1.36M/1.36M [00:00&lt;00:00, 6.75MB/s]"}},"42add648368448ab803691d37bdac5b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21612435a81345ddbd37ab15689b7378":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff20b3ae05f547fa8e6f0ebfb5ff7224":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75ad49e4f3e6432f93c97ec9331ce403":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48bf9a0d1c73466bb2dca2273f479185":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5b1f6ea287204968976841e7c46b5769":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cdb13d0211d418c9f041ed9efb31148":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bccab9079fb54a78a9fd1f5bfe1d8bf9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_07770be3a4a84999ac376c94bd55b7f4","IPY_MODEL_a51ba776106245b9a4371d997dbe60cc","IPY_MODEL_5a857da4e3ba4f8da889911160cf86ee"],"layout":"IPY_MODEL_78e0837cc9b347109b7e45570ed96c0f"}},"07770be3a4a84999ac376c94bd55b7f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_944fc180420d47df8a26d2f4cb2961d5","placeholder":"​","style":"IPY_MODEL_b838bbf4909f4d7fa4981fd1d629cad2","value":"model.safetensors: 100%"}},"a51ba776106245b9a4371d997dbe60cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9db71d225f224b1aa180866aa07a5ab9","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4b7a668c06b04069a677255a442f6470","value":1421700479}},"5a857da4e3ba4f8da889911160cf86ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffbe9ba6a7384cedb4190b280a0b3e4e","placeholder":"​","style":"IPY_MODEL_6eb367bf4787417ab5c2cba15cba8c13","value":" 1.42G/1.42G [00:12&lt;00:00, 220MB/s]"}},"78e0837cc9b347109b7e45570ed96c0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"944fc180420d47df8a26d2f4cb2961d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b838bbf4909f4d7fa4981fd1d629cad2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9db71d225f224b1aa180866aa07a5ab9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b7a668c06b04069a677255a442f6470":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ffbe9ba6a7384cedb4190b280a0b3e4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6eb367bf4787417ab5c2cba15cba8c13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_QzvW2cUBVm1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763817658583,"user_tz":-330,"elapsed":21152,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"4f742993-ffc8-4718-a120-af6fa9503c53"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=e2a7700ce0569c4a3b6e74b2b2f4a24f9a776677eb02f8551033158284d3bb5a\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"hfN340i_t91q","executionInfo":{"status":"ok","timestamp":1763817658604,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"9066ae3d-66df-4861-ba7c-c4a52e6becf3"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-4-scout-17b-16e-instruct_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"meta-llama/llama-4-maverick-17b-128e-instruct\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9H2lMMpt_Xa","executionInfo":{"status":"ok","timestamp":1763822458885,"user_tz":-330,"elapsed":4800271,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"94ebbdea-4c55-4a79-de02-0ca5bd254afd"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning with human feedback is a framework that integrates human guidance into the training process of reinforcement learning algorithms, enhancing decision-making and response quality. This approach is exemplified in a grid world example, where an agent learns faster and makes more informed decisions with human feedback. Similarly, ChatGPT employs a rewards model, trained using human feedback, to assess answer quality and fine-tune its responses through proximal policy optimization. The iterative training process improves ChatGPT's capabilities, enabling it to generate high-quality responses. By leveraging human feedback, reinforcement learning algorithms become more efficient and effective, leading to improved AI decision-making. Overall, this framework significantly enhances the quality of AI responses, demonstrating the value of human feedback in AI development.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is Frank's goal in the grid world?\n","A: To get to the +10 reward spot.\n","Q: Why is human feedback used in Frank's learning process?\n","A: To guide and accelerate the learning process.\n","Q: How does human feedback contribute to reinforcement learning as illustrated with Frank's grid world adventure?\n","A: It accelerates the learning process.\n","Q: When is the rewards model used in chat GPT?\n","A: To assess and score the quality of answers generated by chat GPT.\n","Q: Who provides feedback to Frank during his learning process?\n","A: Us humans as mentors.\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Human Feedback, Grid World, Q Learning, Deep Q Learning, Proximal Policy Optimization, Reward Model, Chat GPT, Back Propagation, Loss Function\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial explores the application of kernels to Support Vector Machines (SVMs) using CVXopt, a Python library for convex optimization. The code demonstrates the impact of kernels on SVMs, visualizing nonlinear and soft margin effects. The quadratic programming solver in CVXopt minimizes a function subject to constraints, illustrating kernel effects and nonlinear visualization. Kernels significantly affect SVM performance and visualization, enabling SVMs to classify complex datasets by transforming them into linearly separable spaces. The kernel is used in SVMs, with a linear kernel being the dot product of two values, while non-linear kernels have a 'None' weight vector. Understanding kernel usage is crucial for effective SVM implementation, and its parameters will be reviewed to enhance understanding of SVMs, including handling multi-class classification.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Other']\n","\n","Q&A:\n"," Q: What is being covered in part 32 of the machine learning tutorial?\n","A: some example code working with CVX opt and kernels applied to support Vector machine\n","Q: Why is CVX opt used in this tutorial?\n","A: so you can see directly the impact of a kernel and where it's actually being injected\n","Q: How is the impact of a kernel visualized?\n","A: by working with CVX opt and visualizing nonlinear and soft margin\n","Q: When would you use lib svm instead of CVX opt?\n","A: if you wanted to write your own support Vector machine\n","Q: Who is the original author of the code being referenced?\n","A: Matthew blondell\n","\n","KEY CONCEPTS:\n"," \n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompts are inputs to large language models that provide context and constraints for generating text outputs. They have various features, including length, language, context, tone, style, and specific requirements, which are crucial for crafting effective prompts. There are different types of prompts, such as question prompts and statements with multiple inputs or constraints. Understanding the characteristics of prompts is essential for achieving accurate outputs. Modifying prompts can significantly impact the output, as demonstrated by examples that specify a one-word answer or generate code. Deconstructing a prompt into its individual components enables better prompt engineering, allowing for the identification of key features and constraints, which is vital for obtaining desired outputs from large language models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are prompts in prompt engineering?\n","A: inputs given to prompt engineering models\n","Q: Why is understanding different types of prompts important?\n","A: to choose the right prompt for desired output\n","Q: How do key features of prompts impact output?\n","A: provide more context and information for accurate output\n","Q: When do you need to deconstruct a prompt?\n","A: when you already have a prompt and want to know its elements\n","Q: Who is involved in using prompts?\n","A: users of large language models like chat GPT or Google bard\n","\n","KEY CONCEPTS:\n"," Prompts, Prompt Engineering, Large Language Models, Constraints, Context, Deconstruction, Pre-trained Models, NLP, Text Outputs, Agentic AI\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," The ReAct agent pattern is a prominent method for developing AI agents that can autonomously solve complex problems. By mimicking human thought processes, this pattern involves a cycle of thinking, acting, and observing, where a large language model (LLM) contemplates a problem, decides on an action, and utilizes tools to execute it. The output is then observed, and the cycle repeats until a solution is found. This enables AI agents to reason and act autonomously, making the ReAct pattern a fundamental concept in AI. Consequently, AI agents can be created to think and act like humans, leveraging the ReAct pattern to address intricate problems.\n","\n","TOPICS:\n"," ['Agentic AI', 'LangChain']\n","\n","Q&A:\n"," Q: What are AI agents capable of?\n","A: thinking on their own and making autonomous decisions\n","Q: Why are tools given to AI agents?\n","A: to complete tasks like a calculator tool or a search engine tool\n","Q: How can we create an AI agent?\n","A: using the react agent pattern which stands for reasoning plus acting\n","Q: When does the cycle of think, action, and observe repeat?\n","A: if the answer is not found and the problem is complex or multi-step\n","Q: Who or what observes the output of the tool?\n","A: the llm observes the output of the tool\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI) agents, Autonomous decisions, React agent pattern, Reasoning, Acting, LLM, Lang chain, Tools, Action input, Observation, Agent creation\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The reflection agent system is analyzed to understand its functionality and interaction with other systems in generating a viral tweet. LangChain integrates with LangSmith (LSmith) for tracing and monitoring AI applications, capturing detailed information on each component's actions and outputs. To utilize LSmith, environment variables are set, and API keys are generated, allowing for the tracing of the application's execution. In a tweet generation application, a generation agent and a reflection agent iteratively refine a tweet, with LSmith capturing the entire process. The reflection agent system plays a crucial role in refining the output, and the final output is a refined tweet with improved virality. LSmith provides a high-level overview of the application's execution, enabling understanding of complex AI workflows. The analysis highlights the collaborative effort between systems to achieve the desired outcome, revealing key insights into the system's mechanics.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI']\n","\n","Q&A:\n"," Q: What will be traced in this section?\n","A: the reflection agent system\n","Q: Why are we tracing the reflection agent system?\n","A: so we can understand exactly what is happening where\n","Q: How are the two systems working together?\n","A: to deliver our final refined viral tweet\n","Q: When will the tracing happen?\n","A: in this section\n","Q: Who is explaining the tracing process?\n","A: the lecturer/I'm just going to go ahead\n","\n","KEY CONCEPTS:\n"," Reflection Agent, Agent System, NLP, ML, Agentic AI, Tweet Generation, Chain, Smith, Viral Tweet, Refined Tweet\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," To integrate LangChain's chat models with OpenAI's API, the `langchain-openai` package is installed and imported into the project. The `ChatOpenAI` class is used to initialize a model, such as `gpt-4.0`, which is known for its advanced capabilities. To authenticate API calls, an OpenAI API key is stored in an .env file and loaded using the `python-dotenv` package. The `invoke` method is used to make API calls by passing a string query, and the response contains the desired content along with additional metadata. To access the content, the 'result.content' property can be used. Passing conversation history to the Large Language Model (LLM) enables it to provide more informed and contextually relevant responses. Overall, the LangChain framework provides a simple interface to interact with OpenAI's API, facilitating various applications and enhancing the LLM's response accuracy.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What are we going to work with in this section?\n","A: Lang chains chat models\n","Q: Why is the latest model expensive?\n","A: because they are the most advanced model\n","Q: How do we install the required package?\n","A: using the command to install L chain Das open aai\n","Q: When can we initialize our model?\n","A: after importing the chat open Artificial Intelligence (AI) class\n","Q: Who released the gbt 40 model?\n","A: open aai\n","\n","KEY CONCEPTS:\n"," LangChain, Chat Models, OpenAI, APIs, LChain, ChatOpenAI, GBT 4.0, GPT 3, Model Initialization, LLM\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," Python's sort() method sorts lists containing strings alphabetically, but distinguishes between uppercase and lowercase letters, with uppercase letters sorted first. When lists contain both strings and numbers, the sort() method prioritizes numbers over strings, regardless of the sort order. Reversing the sort order maintains this distinction. To achieve a uniform sort, it is essential to ensure consistent case, either all uppercase or all lowercase. The sort() method handles mixed data types by placing numbers before strings. Understanding the sort() method's behavior is crucial for effective list manipulation in Python, particularly when dealing with diverse data types and case sensitivity.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What happens when you sort a list containing strings with different cases?\n","A: puts the words with a capital uppercase letter first and then the ones with lowercase\n","Q: Why does the list sort strings with uppercase letters first?\n","A: because that's just how sort works, it does two sorts: uppercase and then lowercase\n","Q: How can you sort a list of strings in a particular case?\n","A: make sure they were all lowercase or all uppercase\n","Q: When does Python put numbers first in a sorted list?\n","A: when the list contains both strings and numbers\n","Q: Who is the intended audience for this video?\n","A: students learning Python\n","\n","KEY CONCEPTS:\n"," Python, sort, lists, strings, numbers, uppercase, lowercase, alphabetical, reverse, method, sorting\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," Human-AI collaboration is effective in decision-making by leveraging the strengths of both parties. AI excels at high-confidence tasks, such as fraud detection, while humans handle uncertain cases, performing relatively better when AI is uncertain, particularly at a 50% confidence level. By dividing tasks between AI and humans, overall accuracy can be improved. Augmented intelligence, combining human decision-making with AI assistance, achieves the highest success rate for moderate confidence scores. However, human cognitive bias affects its effectiveness, and the presentation of AI information to humans significantly influences its usage. To mitigate bias, AI assistance should be presented carefully, allowing humans to form their own initial impressions. Quantifying the decision-making process can determine the most effective decision-maker, and human-AI collaboration can lead to improved decision-making outcomes when AI recommendations are presented in a way that minimizes cognitive bias.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: Who should make a decision, a human or Artificial Intelligence (AI)?\n","A: A fascinating combination of holistic curves and human bias.\n","Q: What is the task of the fraud detection system?\n","A: Generates alerts of potentially fraudulent transactions.\n","Q: Why are financial analysts overwhelmed?\n","A: 90 percent of alerts being false positives.\n","Q: How does the success rate relate to the confidence score?\n","A: Very low and very high confidence scores correlated to a high success rate.\n","Q: When is a human likely to do a better job than Artificial Intelligence (AI)?\n","A: At a 50 percent confidence level.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Fraud Detection, Confidence Score, False Positives, Performance Curve, Prediction, Success Rate, Human Bias, Alerts, Financial Analysts\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Google's Vertex AI has introduced six new APIs to enhance generative AI applications, addressing key challenges in building reliable and accurate enterprise applications. The new APIs include Document Understanding API, improved Embedding API, enhanced Vector Search, Ranking API, Grounded Generation API, and Check Grounding API. Leveraging Google's expertise and technology used in products like Google Search and YouTube, these APIs provide high-quality and scalable solutions for processing complex documents and improving search results. Designed to be simple and stateless, the APIs enable easy integration into developer workflows, allowing them to focus on building unique application features. Overall, Vertex AI's new APIs simplify the development of generative AI applications, enabling more accurate and reliable outcomes.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is Demitrius' role at Google?\n","A: Product manager within Cloud AI focusing on search and document Artificial Intelligence (AI)\n","Q: Why are the new Vertex AI APIs being launched?\n","A: To solve technical challenges faced by developers building generative applications for Enterprises\n","Q: How do the new Vertex AI APIs help developers?\n","A: By lightening the load and allowing them to focus on building unique aspects of their use case\n","Q: When are the new Vertex AI APIs being introduced?\n","A: In the talk being discussed (no specific date mentioned)\n","Q: Who is the target user for the new Vertex AI APIs?\n","A: Developers building generative applications for Enterprises\n","\n","KEY CONCEPTS:\n"," Vertex AI, Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Check Grounding API, Generative Applications, LLM Model, Gemini\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The Singular Value Decomposition (SVD) utilizes unitary matrices U and V to preserve vector lengths and angles, effectively rotating vectors without altering their geometric properties, akin to the Fourier transform. The economy SVD offers a compact representation of this decomposition. Unitary transformations play a crucial role in various scientific and engineering applications, facilitating the preservation of vector relationships. Through SVD, a sphere of unit vectors is mapped to an ellipsoid, with the lengths of the principal axes determined by singular values. Overall, SVD provides a potent geometric interpretation of data transformation via unitary matrices, underlining its significance in data analysis and processing.\n","\n","TOPICS:\n"," ['Machine Learning', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What are unitary matrices?\n","A: Matrices where u*u^T = u^T*u = I and V*V^T = V^T*V = I.\n","Q: Why are unitary matrices important?\n","A: They preserve angles and lengths of vectors, and are used in science and engineering.\n","Q: How do unitary matrices affect vectors?\n","A: They rotate vectors without changing their lengths or angles.\n","Q: When is the complex conjugate transpose used?\n","A: When dealing with complex-valued data.\n","Q: Who can benefit from understanding unitary transformations?\n","A: Those working with data that needs to preserve geometric structure.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, SVD, Unitary Matrices, Fourier Transform, Complex Conjugate Transpose, Vector Space, Ellipsoid, Singular Values, Left Singular Vectors, Inner Product\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Google Gemini Pro 1.5 is a multimodal AI model that processes both text and images, showcasing significant advancements in AI capabilities. It has a large context window of up to 1 million multimodal tokens, enabling the development of generative AI-powered applications handling 1 hour of video, 11 hours of audio, 30k lines of code, or 700k words. The model's capabilities were demonstrated through a demo, accurately extracting information from a 402-page PDF and identifying a scene from a simple drawing. Gemini Pro 1.5 offers enhanced capabilities compared to its predecessor, with a single model handling both text and images. To utilize the model, an API key is required, which can be obtained for free from ai.google.com. The model's performance is affected by token size, and it can generate relevant responses to diverse queries, making it suitable for complex applications.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: Who is the speaker in the video?\n","A: Krishn\n","Q: What is the topic of the video?\n","A: Building generative AI powered application using Google Gemini Pro 1.5\n","Q: What is Google Gemini Pro 1.5?\n","A: A multi-model that works with both text and images\n","Q: How will the speaker demonstrate the capabilities of Google Gemini Pro 1.5?\n","A: By showing a demo video and running some code\n","Q: What is the demo video about?\n","A: Long context understanding, an experimental feature in Gemini 1.5\n","\n","KEY CONCEPTS:\n"," Generative AI, Google Gemini Pro 1.5, Multimodal, Artificial Intelligence, API Key, Long Context Understanding, Gemini Pro, NLP, ML, Agentic AI\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Evaluating prompt engineering models involves using metrics such as perplexity and accuracy to assess their performance. Perplexity measures a language model's ability to predict a sequence of words, whereas accuracy evaluates the correctness of generated responses. Techniques for debugging and improving models include analyzing generated responses, identifying common errors, and fine-tuning the model. Testing on diverse datasets and tasks is essential to determine the model's ability to generalize. The evaluation and testing of prompt engineering models is an ongoing process to ensure continued performance, highlighting the importance of effective evaluation and testing for reliable models. This process enables the identification of areas for improvement, ultimately contributing to the development of more robust prompt engineering models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the purpose of the 'evaluate_translation' function?\n","A: To check the accuracy and perplexity of the large language model.\n","Q: Why is human evaluation used in assessing prompt engineering models?\n","A: To have humans rate the quality of the responses.\n","Q: How is perplexity used to evaluate language models?\n","A: To measure how well a language model predicts a sequence of words.\n","Q: When is it necessary to continue evaluating and testing prompt engineering models?\n","A: As you continue to use your model and generate responses.\n","Q: Who is involved in rating the quality of responses in human evaluation?\n","A: Humans\n","\n","KEY CONCEPTS:\n"," Perplexity, Accuracy, Human Evaluation, Prompt Engineering, Language Model, Model Evaluation, Debugging, Cross Validation, Fine-Tune, Large Language Models, Pre-Trained Models\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," Agentic AI systems, powered by large language models, represent a significant advancement in AI capabilities, enabling complex task automation with increased autonomy. These systems comprise one or more AI agents that work together to achieve complex goals, utilizing tools, memory, and knowledge. As AI evolves from generative AI to AI agents to agentic AI, task complexity increases and autonomy improves. AI agents can access external tools and APIs, enabling autonomous task performance, such as booking flights. Agentic AI can handle multi-step planning, coordination, and tool usage, making it more sophisticated. Overall, agentic AI marks a notable progression in AI, facilitating the automation of intricate tasks with enhanced autonomy and capability.\n","\n","TOPICS:\n"," ['Agentic AI', 'Artificial Intelligence', 'Generative AI']\n","\n","Q&A:\n"," Q: What is Generative Artificial Intelligence (AI)?\n","A: An AI that creates new content based on patterns learned from existing data.\n","Q: Why can't Generative AI answer questions about future events?\n","A: It has a knowledge cutoff date.\n","Q: How does an AI agent become more intelligent than a simple LLM?\n","A: By giving it access to tools like APIs.\n","Q: When does an AI system become agentic?\n","A: When one or more AI agents work autonomously to reach a goal.\n","Q: Who defines agentic systems differently?\n","A: Different people, like the creator of Agno framework.\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence (AI), Large Language Model (LLM), Artificial Intelligence (AI) Agent, Agentic Artificial Intelligence (AI), Autonomous Decision Making, Multi-Step Reasoning, Multi-Step Planning, Tool Usage, N8N, LLM, GPT4, API Integration\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Covariance is a measure of the relationship between two random variables, calculated as 1/n Σ(Xi - μX)(Yi - μY), where Xi and Yi are individual data points and μX and μY are the means of the variables. A positive covariance indicates that as one variable increases, the other also increases, while a negative covariance indicates that as one variable increases, the other decreases. Although covariance quantifies the relationship between variables, it does not measure the strength of the relationship. The Pearson correlation coefficient is used to address this limitation, providing a more detailed understanding of the relationship. As a fundamental concept in data analysis and preprocessing, covariance helps to understand the relationships between different variables in a dataset, facilitating further analysis.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science']\n","\n","Q&A:\n"," Q: What is the topic discussed after status statistics?\n","A: variance covariance\n","Q: Why is covariance an important topic?\n","A: one of the very important topic when we consider the data pre-processing or the data analysis\n","Q: How is the relation between size and price quantified?\n","A: with the help of covariance\n","Q: When is covariance positive?\n","A: whenever we have a scenario where X is increasing and Y is increasing\n","Q: Who can benefit from understanding covariance?\n","A: people who wants to actually learn statistics want to learn status things with respect to machine learning\n","\n","KEY CONCEPTS:\n"," Covariance, Variance, Random Variables, Mean, Data Pre-processing, Data Analysis, Pearson Correlation Coefficient, Relationship Quantification, Machine Learning, Statistics\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," In reinforcement learning, the primary objective is to learn an optimal policy that maximizes a numerical reward signal. An agent interacts with its environment, making decisions based on observations to maximize cumulative rewards over time. The reward signal provides feedback on the quality of the agent's actions, and can be defined for both episodic tasks, such as winning a game, and continuous tasks, like maximizing profit in stock market trading. The reward function assigns a numerical value to each state or action, with the ultimate goal being to maximize expected cumulative rewards. Defining this objective is crucial as it guides the agent's learning process, enabling it to make informed decisions and achieve the desired outcome.\n","\n","TOPICS:\n"," ['Reinforcement Learning']\n","\n","Q&A:\n"," Q: What is the objective in any reinforcement learning problem?\n","A: to learn the optimal policy that maximize a numerical reward signal\n","Q: Why does the agent interact with the environment?\n","A: to make decisions based on the observation it received from the environment\n","Q: How does the agent learn in reinforcement learning?\n","A: through trial and error learning\n","Q: When does the agent receive a reward in a Tic-Tac-Toe game?\n","A: at the end of the game either you win or lose\n","Q: Who or what learns from its environment in reinforcement learning?\n","A: an agent\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Objective, Agent, Environment, Reward Signal, Optimal Policy, Episodic Task, Continuous Task, Value-based Method, Policy-based Method, Reward Function, Cumulative Reward\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," In Python, a dictionary is a data type comprising key-value pairs enclosed within curly brackets, where keys are immutable, such as strings, numbers, or tuples. Dictionaries are particularly useful for mapping items, like stock prices, and offer key functions including `.items()`, `.keys()`, and `.values()`. They can be created using the `dict()` function and paired with the `zip()` function to combine two lists. Values can be accessed, edited, and deleted using square brackets `[]` and the `del` function. As a fundamental data structure, dictionaries are essential in data science, integrating well with pandas, and providing flexibility for various applications.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: A data type consisting of key-value pairs.\n","Q: Why are dictionaries useful?\n","A: For mapping one item to another, like stock prices.\n","Q: How are key-value pairs separated in a dictionary?\n","A: By a colon.\n","Q: When are dictionaries declared?\n","A: Within curly brackets.\n","Q: Who can use dictionaries?\n","A: Anyone, especially those doing data science with pandas.\n","\n","KEY CONCEPTS:\n"," Dictionary, Key-value pairs, Immutable, Tuples, Zip function, Dict function, Pandas, Data science, Items, Keys, Values\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," IBM's 2023 Cost of a Data Breach Report found that organizations using AI and automation extensively identified and contained data breaches 108 days faster on average than those that didn't. The report highlighted the average cost of an insider threat to be $4 million. User Behavior Analytics (UBA) combined with AI and machine learning can enhance detection and response to insider threats by analyzing user behavior and identifying anomalies. When integrated with a Security Information and Event Management (SIM) solution, UBA helps security professionals detect and respond to threats more effectively. IBM Security's QRadar SIM, for instance, harnesses AI and automation to streamline security operations, accelerating investigations and providing actionable insights. By leveraging AI, security teams can improve their security posture and respond more precisely to threats, significantly improving threat detection and response capabilities. AI-driven SIM solutions like QRadar enable security analysts to view key observables and offense relationships to identify insider threats.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What did IBM's cost of a data breach report 2023 find regarding AI and breach containment?\n","A: Faster containment for those that extensively used AI and automation.\n","Q: How many organizations were surveyed in IBM's cost of a data breach report 2023?\n","A: Over 500 organizations\n","Q: What is User Behavior analytics (UBA) used for with AI and machine learning?\n","A: Detect and respond to Insider threats quickly and precisely\n","Q: Why are Insider threats a major concern according to the cost of a data breach report?\n","A: Average cost of an Insider threat was $4\n","Q: Who is the report 'Cost of a data breach' by?\n","A: IBM\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Machine Learning, User Behavior Analytics, UBA, Insider Threats, Data Breach, Automation, IBM, Security Posture, Threat Detection\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta has released Llama 3, an open-source large language model available in 8 billion and 70 billion parameter versions, which outperforms its predecessor, Llama 2, and competes with paid models. Trained on 50 trillion tokens, it supports an 8K context length and excels in language nuances, contextual understanding, and complex tasks. Llama 3's performance metrics show high accuracy, surpassing other open-source models, and it elevates capabilities like reasoning, code generation, and instruction following. The model's open-source nature is a significant advantage, although it shows a trade-off between performance in specific tasks. To access Llama 3, users can visit the Meta Llama site, Hugging Face, or Kaggle, and follow the provided guidelines for responsible and transparent use, including downloading model weights and tokenizer from GitHub repositories.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the host's name?\n","A: Krishak\n","Q: When is the host uploading the video?\n","A: 2 a. m.\n","Q: Who is welcoming the audience?\n","A: Krishak\n","Q: What is the host doing?\n","A: welcoming to my YouTube channel\n","Q: Where is the host's content being published?\n","A: YouTube channel\n","\n","KEY CONCEPTS:\n"," YouTube, NLP, ML, Agentic AI, Transcript, Channel, AI, Nouns, Terminology, Ontology\n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The implementation of a decision boundary using Python is facilitated by the scikit-learn (sklearn) library, which provides the necessary tools for utilizing the Naive Bayes algorithm. By referencing the relevant documentation, including the page on Gaussian Naive Bayes, the instructor is able to derive the Naive Bayes formula and understand its various use cases. The sklearn library enables the effective implementation of the Naive Bayes classifier, demonstrating the importance of leveraging documentation and libraries in executing complex algorithms. The process involves understanding the algorithm and its implementation, highlighting the utility of sklearn in simplifying the development process. The Naive Bayes algorithm is successfully implemented using sklearn, showcasing its efficacy in machine learning tasks.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What library is being used in the lesson?\n","A: scikit-learn\n","Q: Why is Google being used?\n","A: to help us use the documentation of the library\n","Q: How will the code be written by the end of the next video or two?\n","A: by walking through the steps\n","Q: When will the viewer be able to write the code?\n","A: by the end of the next video or two\n","Q: Who is the library often abbreviated as?\n","A: sk-learn\n","\n","KEY CONCEPTS:\n"," scikit-learn, Naive Bayes, Gaussian Naive Bayes, Python, sklearn, algorithm, classifier, documentation, library, functions\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The log normal distribution is a probability distribution where the logarithm of a random variable follows a Gaussian distribution, denoted as X belonging to log normal distribution if log(X) is normally distributed. It is characterized by a curve with a longer tail on the right side, similar to the Gaussian distribution. Log normal distribution is commonly observed in variables such as income, product reviews, and feedback comments. Understanding this distribution is crucial for data preprocessing, as taking the log of the values can convert it to a standard normal distribution, enabling scaling and improving model accuracy. The log normal distribution is useful for modeling skewed data, and its properties can be leveraged for data normalization, making it a valuable concept in data analysis.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What distribution does the height of people usually follow?\n","A: Gaussian distribution\n","Q: Why are we learning various distributions?\n","A: To understand how to scale data for modeling and improve accuracy\n","Q: How is log normal distribution denoted?\n","A: A random variable belongs to log normal distribution if log of X is normally distributed\n","Q: When does a random variable belong to log normal distribution?\n","A: If log of X is normally distributed\n","Q: Who can benefit from understanding different distributions like Gaussian and log normal?\n","A: Data analysts and machine learning practitioners\n","\n","KEY CONCEPTS:\n"," Gaussian distribution, Log normal distribution, Standard normal distribution, Empirical formula, Bell curve, Mean, Standard deviation, Normalization, Scaling, Distribution\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," AtliQ Agriculture's AI-powered mobile application detects potato plant diseases using deep learning. The project involves building an end-to-end application, including data collection, model building using convolutional neural networks, and deployment on Google Cloud. The application identifies early blight and late blight diseases, enabling farmers to apply treatments and prevent economic losses. The technology stack includes TensorFlow, CNN, data augmentation, TF Serving, Fast API, and React Native. The model is optimized using quantization and TensorFlow Lite. The project demonstrates a comprehensive tech stack for a mobile-based plant health prediction application, providing hands-on experience valuable for a career in machine learning or data science. The application is accessible via a React Native mobile app, utilizing a serverless architecture on Google Cloud. Overall, the project showcases AI's potential in agriculture to reduce waste and improve crop yields, mitigating economic losses through early disease detection.\n","\n","TOPICS:\n"," ['Deep Learning', 'Artificial Intelligence', 'Mlops']\n","\n","Q&A:\n"," Q: What domain is the deep learning project series focused on?\n","A: agriculture domain\n","Q: Why is it important to accurately identify the disease in a potato plant?\n","A: The treatments for early blight and late blight are little different\n","Q: How will the farmer use the mobile application?\n","A: take a picture of the plant\n","Q: When will the Google Cloud functions be called?\n","A: by a mobile app written in React Native\n","Q: Who is the company that has taken this project?\n","A: AtliQ Agriculture\n","\n","KEY CONCEPTS:\n"," Deep Learning, Machine Learning (ML), TF Serving, Fast API, Google Cloud (GCP), Convolutional Neural Network, Artificial Intelligence (AI), React Native, ML Ops, Data Collection\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," LLM applications exhibit varying levels of autonomy, ranging from zero autonomy in code to maximum autonomy in agents. The autonomy levels include LLM call, chains, routers, state machines, and autonomous agents, with the latter enabling features like human-in-loop approval and adaptive learning. State machines, also referred to as agents, allow AI to control the flow, incorporate loops, and refine tasks. The primary distinction between human-driven systems, such as chains and routers, and agent-executed systems, like state machines, lies in the presence of cycles and AI-controlled flow. The autonomy in LLM applications facilitates the execution of complex tasks and enhances overall performance, with state machines and autonomous agents representing the most advanced forms of autonomy.\n","\n","TOPICS:\n"," ['LangChain', 'Agentic AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What has zero autonomy in LLM applications?\n","A: Code\n","Q: Why are chains considered to have less autonomy?\n","A: They follow the same steps defined by humans\n","Q: How does a router decide what steps to take next?\n","A: The AI itself decides what steps to take next\n","Q: When is a state machine called an agent?\n","A: Whenever the control flow is controlled by an LLM\n","Q: Who controls the flow in a state machine?\n","A: The LLM\n","\n","KEY CONCEPTS:\n"," LLM, Autonomy, Chains, Router, State Machine, Agent, LLM Call, NLP, Multi-Agent Systems, Lang Graph, Cognitive Architecture\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," Advanced prompt engineering encompasses handling diverse prompts, including text, images, and audio, through techniques such as fine-tuning pre-trained models using multitask learning and distillation. Effective data pre-processing and cleaning practices, including tokenization and normalization, are also essential. Deploying models in production environments is facilitated by frameworks like TensorFlow Serving or Flask. Addressing ethical considerations, including bias, fairness, and privacy, is crucial. Mastering these advanced topics enables the development of accurate and efficient prompt engineering models, which can be applied in various real-world applications, transforming decision-making in areas like education, healthcare, and finance, and ultimately enabling the creation of robust models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Deep Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What will be explored in this section?\n","A: More complex topics and techniques to become an expert in prompt engineering.\n","Q: Why is data quality crucial for training models?\n","A: The quality of data used to train models is crucial to the success.\n","Q: How is multitask learning implemented?\n","A: By training a model on multiple tasks simultaneously using cross entropy loss and Adam Optimizer.\n","Q: When is tokenization used in data pre-processing?\n","A: Tokenization involves breaking down text into smaller units such as words or subwords.\n","Q: Who can be affected by biased outcomes in prompt engineering?\n","A: The whole population can be affected when data used to train models is not representative.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Multitask Learning, Distillation, Tokenization, Normalization, Pre-trained Models, Fine-tuning, Data Pre-processing, TensorFlow Serving, Flask, Self-supervised Learning, Ethical Considerations\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," Eigenfaces are utilized to cluster images of different individuals, specifically Arnold Schwarzenegger, Sylvester Stallone, and Taylor Swift. The process involves loading, cropping, and aligning 20 images of each person, computing the average face, and applying Singular Value Decomposition to obtain eigenfaces that capture dominant facial features. By projecting images into the first three eigenfaces, clustering and classification are achieved. Testing with new images demonstrates correct classification. Experiments with different pairs of individuals reveal varying degrees of overlap in their distributions, with the Taylor Swift and Schwarzenegger pair exhibiting more overlap, likely due to similarities in skin tone and hair color. This highlights the limitations of image classification based on correlations, underscoring the need for more nuanced approaches.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the purpose of cropping and aligning the images of Arnold and Stallone?\n","A: So that their faces are more or less in the same place and filling the same box.\n","Q: Why are the images converted to greyscale?\n","A: To check if they're already greyscale or if they're color and then convert them to greyscale.\n","Q: How are the eigenfaces computed?\n","A: By computing the SVD of the matrix B after subtracting the average face.\n","Q: When are the images projected into the eigenface coordinates?\n","A: After computing the SVD and obtaining the eigenfaces.\n","Q: Who are the two action heroes used in the example?\n","A: Arnold Schwarzenegger and Sylvester Stallone.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Eigenvectors, Image Classification, Clustering, Face Recognition, SVD, Eigenvalues, Dimensionality Reduction\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," LangChain is a framework that enables large language models (LLMs) to interact with the real world by accessing external data and services, such as private databases, emails, and websites. This bridges the gap between LLMs' cognitive abilities and real-world applications, empowering AI to take actions beyond just processing and generating human-like language. LangChain provides a layer of abstraction, allowing developers to switch between different LLMs without modifying the underlying code. By leveraging LangChain, developers can build more powerful and interactive AI applications that can perform various tasks, such as answering customer queries and automating processes. LangChain unlocks new possibilities for AI-driven solutions, revolutionizing the utility and potential impact of AI. Its vast potential applications make it a significant development in the field of AI.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'LangChain']\n","\n","Q&A:\n"," Q: What is the limitation of large language models?\n","A: They cannot interact with the real world.\n","Q: Why is Lang chain needed?\n","A: To act as a bridge between LLMs and the real world.\n","Q: How does Lang chain help in building applications?\n","A: By helping build applications using LLMs and enabling communication with real world APIs.\n","Q: When can you switch out GP4 with another LLM using Lang chain?\n","A: In the future, without touching the code written with Lang chain.\n","Q: Who can benefit from using Lang chain?\n","A: Developers building applications using LLMs.\n","\n","KEY CONCEPTS:\n"," LangChain, LLM, Large Language Models, Agentic AI, Chat Application, APIs, Hugging Face, GP4, Frameworks, NLP, Reasoning Ability\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residuals in time series analysis represent the differences between fitted and actual values, facilitating the diagnosis of model performance and detection of trends or inconsistencies. Key characteristics of residuals include no autocorrelation, a mean of zero, and independence. The Ljung-Box test is employed to check for independence of residuals, while a histogram of residuals is used to check for bias. In an example using the Air Passenger dataset and a Holt-Winters model, residual analysis revealed some correlation in the residuals and a slight negative bias with a mean of -0.02. Although the bias is minimal, residual analysis is crucial for identifying model weaknesses, such as autocorrelation and bias, allowing for improvements in subsequent iterations and refining forecasting models. Overall, residual analysis is a vital step in improving forecasting models by identifying areas for improvement.\n","\n","TOPICS:\n"," ['Time Series', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: the difference between the fitted value and the actual value of the time series\n","Q: Why is residual analysis important?\n","A: to diagnose performance and detect trends or inconsistencies in the model\n","Q: How do we check for autocorrelation in residuals?\n","A: by plotting the autocorrelation function and partial autocorrelation function\n","Q: When is a residual considered to have no bias?\n","A: when the mean of the residual is zero\n","Q: Who can benefit from understanding residuals?\n","A: data scientists working with time series data\n","\n","KEY CONCEPTS:\n"," Residuals, Time Series Analysis, Holt-Winters Model, Exponential Smoothing, Autocorrelation, Partial Autocorrelation, Ljung-Box Test, Forecasting, Model Diagnosis, Residual Analysis\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," A tutorial on building an AI agent that interacts with a database using LangGraph, Next.js, and watsonx.ai is presented. The agent utilizes SQL knowledge to connect to databases, with an in-memory database using SQLite. The integration enables users to query databases using natural language inputs. The ReAct agent is created using functions from LangChain, and the necessary libraries are installed. State variables are created using the useState hook from React to manage user input and message history. The application is made more user-friendly by implementing a loading state and dynamic message rendering. SQLite3 is used to create a database, and a foreign key relation is established between 'customer' and 'order' tables. The large language model is informed of the tool's usage via the system prompt, enabling it to effectively utilize the database to retrieve data. The outcome demonstrates the model's improved performance with the new guidelines, and the integration enables accurate and complex query execution.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Artificial Intelligence', 'LangChain']\n","\n","Q&A:\n"," Q: What will be built in the video?\n","A: An AI agent that can talk to a database\n","Q: Why is LangGraph being used?\n","A: To build a ReAct agent\n","Q: How will the frontend application be set up?\n","A: Using Next.js CLI to set up a boilerplate project\n","Q: When will the Next.js application be started?\n","A: After putting in the code for the messages\n","Q: Who will be using the input box?\n","A: The user to type a message to the large language model\n","\n","KEY CONCEPTS:\n"," \n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a specialized field within natural language processing that focuses on building models generating high-quality text outputs in response to prompts. It utilizes pre-trained large language models fine-tuned for specific tasks, producing more accurate and contextually appropriate outputs. Key applications include chatbots, language translation, and content generation, where output quality significantly impacts user experience. However, these models may struggle with complex prompts or generate biased outputs due to underlying data or model architecture. The field enables the creation of more accurate and coherent text outputs, revolutionizing NLP applications by fine-tuning pre-trained models for specific tasks, thus enhancing overall output quality and user experience.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: a specialized field within natural language processing that focuses on building models that can generate high quality text outputs\n","Q: Why is prompt engineering important?\n","A: it allows us to generate text outputs that are more accurate coherent and contextually appropriate\n","Q: How are prompt engineering models built?\n","A: based on pre-trained large language models such as open Artificial Intelligence (AI) GPT Google bird or hugging face Transformers\n","Q: When may prompt engineering models struggle?\n","A: with complex and ambiguous prompts\n","Q: Who can benefit from prompt engineering?\n","A: applications such as chatbots language translation content generation\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Large Language Models, GPT, Transformers, Fine-Tuning, Text Generation, Chatbots, Language Translation, Pre-Trained Models\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is a value-based reinforcement learning method that determines a value function to maximize total reward by learning a state-action value function, represented by Q-values. The Q-values are stored in a table and updated using the Bellman equation based on the reward received and the maximum future Q-value. The process involves an exploration policy to choose actions and learn an optimal policy to maximize total reward. The Q-value update process involves calculating the temporal difference error between observed and expected Q-values, and updating the Q-value using a gradient update rule with a learning rate. The Q-value is updated until the end of an episode, and multiple episodes are performed to learn the Q-table values, which become stable over time. Q-learning is an off-policy algorithm, decoupling the behavior policy from the target policy, enabling agents to learn optimal policies in complex environments. The learned Q-values dictate the policy to achieve the target reward.\n","\n","TOPICS:\n"," ['Machine Learning', 'Reinforcement Learning']\n","\n","Q&A:\n"," Q: What are the three machine learning paradigms?\n","A: supervised learning, unsupervised learning, and reinforcement learning\n","Q: Why is reinforcement learning used?\n","A: to maximize a numerical reward signal\n","Q: How are reinforcement learning algorithms categorized?\n","A: into value-based methods and policy-based methods\n","Q: When is a state value function used?\n","A: to quantify how good it is to be in a given state\n","Q: Who determines the behavior policy in Q-learning?\n","A: the agent\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement Learning, Supervised Learning, Unsupervised Learning, Value-based Methods, Policy-based Methods, State Value Function, State-action Value Function, Bellman Equation, Optimal Policy, Exploration Policy, Discount Factor\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier, a type of linear classifier, generates predictions from input data using a linear function involving matrix multiplication of the input vector X with a weight matrix W and adding a bias term b. The model's training involves finding optimal values for W and b. The softmax function S is used to convert scores into probabilities, ensuring the correct class has a probability close to 1 and others close to 0. Softmax transforms scores into probabilities that sum to 1, with larger scores resulting in larger probabilities. Logistic regression scores are referred to as logits. The logistic classifier effectively maps inputs to probabilities for classification, utilizing linear functions and softmax to achieve this mapping, thereby enabling the classification of inputs into distinct classes based on their probabilities.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is a logistic classifier considered as?\n","A: a linear classifier\n","Q: How does a logistic classifier generate its predictions?\n","A: by applying a linear function to the inputs\n","Q: What is used to turn scores into probabilities?\n","A: a softmax function\n","Q: Why are probabilities used in classification?\n","A: to have the probability of the correct class be very close to one\n","Q: What do proper probabilities sum to?\n","A: 1\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Classifier, Linear Function, Softmax Function, Logits, Weights, Bias, Matrix Multiply, Probabilities, Classification\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n","\n","Role-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["f8c98ff15c144ca7aa417a84bf9ccdaa","9ea2ad51582d483690a0d34ce106618d","caf071a01d68488c86f304182e0e5384","db15d0a527304208b594ce856b20b812","3fcd83da31ad4cd3ac5592f8eeeff6be","df89c698450b4358bac58aba301b1d2d","9a25dbdf47a745be8b502ef4613cb804","f68cd6d98cb944409cb026d7f8994cb5","50e4a497542a4e38b649e3e144cd7e6c","0cc658169c7842d4bba7c1507f7f3a23","d08941c1b46c4398966076f0012b03e4","f1f0d4378d8742408f0c3c88ccb21f1a","0c8d9cbaea46446eaf7261ad84c808e3","6f42716967284b74a747a2ebeeee626c","51b961555cb34a8fb1c4d8fd9a261fc8","f73a73fe056242c49d9c122daca509fe","adc5e3f8cf8049aca534ee7f3dfbda2f","35dc5ba7c94f4055b3a32bee083fdba5","47da88c2049f4289b4420f33754a132d","9e820a81973a40be9d00e192df204be0","1daf2015c62b4edca6f59fafb910d9d3","359bf54ffb3a4655aecf3d0c3e299259","a0b92809b45e4c8d8958e60b9e23f7d0","9036f55b58d94097a404155da58fefef","c5c64ff10a8b4a09965d43bbe92074f9","db67db4bb54b4b8da67167bb66b54624","f10255c8237a4efc92bbbddd74d62dcb","f5b275b0c95442d8be8f0c75c6a61c61","50c40aace4464aadba5526ac83dc4862","8f676310a3124c4a93229ab8779f9895","6e86d70f26bf4b578cb710db416f5227","edbcea073c9e4da7b6a3896d544d0b33","2a733aba23fb4a379c9d861da8b01ec3","89b4685de74a47ce8290552485a0e4d8","d9514a39d5df44b78d45b2946dbe29a8","9b07aa00f3ab4835b1339303fafa51cb","7e8559cdc7da43a6af62b1240c40583b","04addd9f644b479bb300f0db5553d010","ae50cc99480042c784cc1ef27dfbf220","bd003f4cb48b4e628f2b1f6d3815d701","2de2fb8eb73543cdbf41231db20b6f4e","f7dba41e415d411d82330421410cc633","47233f390c3e4948a85c3e77ab6989e0","bc70d7f57ac3492ea965da0baa005c78","9af50a0344a249a8bb60608e7467dabb","019b2374bbe14bdaa861a5b163c258af","81eedc9d6eda4da088b2e8d5ff7e975e","5a257ff7c8ee49729321d4183e7ba32b","42add648368448ab803691d37bdac5b3","21612435a81345ddbd37ab15689b7378","ff20b3ae05f547fa8e6f0ebfb5ff7224","75ad49e4f3e6432f93c97ec9331ce403","48bf9a0d1c73466bb2dca2273f479185","5b1f6ea287204968976841e7c46b5769","5cdb13d0211d418c9f041ed9efb31148","bccab9079fb54a78a9fd1f5bfe1d8bf9","07770be3a4a84999ac376c94bd55b7f4","a51ba776106245b9a4371d997dbe60cc","5a857da4e3ba4f8da889911160cf86ee","78e0837cc9b347109b7e45570ed96c0f","944fc180420d47df8a26d2f4cb2961d5","b838bbf4909f4d7fa4981fd1d629cad2","9db71d225f224b1aa180866aa07a5ab9","4b7a668c06b04069a677255a442f6470","ffbe9ba6a7384cedb4190b280a0b3e4e","6eb367bf4787417ab5c2cba15cba8c13"]},"id":"yYdeX5BeuEQT","executionInfo":{"status":"ok","timestamp":1763822681736,"user_tz":-330,"elapsed":188447,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"346ae7ce-e517-47b3-ce0b-f2af1a2b7798"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/llama-4-scout-17b-16e-instruct_role_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c98ff15c144ca7aa417a84bf9ccdaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1f0d4378d8742408f0c3c88ccb21f1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b92809b45e4c8d8958e60b9e23f7d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b4685de74a47ce8290552485a0e4d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af50a0344a249a8bb60608e7467dabb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bccab9079fb54a78a9fd1f5bfe1d8bf9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2997\n","  - BLEU: 0.0817\n","  - BERTScore F1: 0.8909\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.8667\n","  - Jaccard Index: 0.3353\n","  - Micro F1: 0.4640\n","  - Macro F1: 0.4364\n","  - Weighted F1: 0.4265\n","\n","Q&A Generation:\n","  - BLEU: 0.0286\n","  - Diversity: 0.7580\n","  - Answerability: 0.7000\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4667\n","  - Recall@10: 0.1867\n","  - F1@10: 0.2667\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-4-scout-17b-16e-instruct/evaluation_final.json\n"]}]}]}