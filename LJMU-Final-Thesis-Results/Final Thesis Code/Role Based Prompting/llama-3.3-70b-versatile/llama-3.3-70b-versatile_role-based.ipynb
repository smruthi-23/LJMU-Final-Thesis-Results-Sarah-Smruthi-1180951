{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM+/65YxmZqFtCtVyZ5xCXI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8a413b4c0eb34a9ab4a2e29bb6fda4ad":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c92b7296dd48452096eb5e0939e6d0fb","IPY_MODEL_e18150160a824d0599ca8aec7aa6b4c3","IPY_MODEL_487aad575c924aa0ab92534aa38fb59a"],"layout":"IPY_MODEL_0fd42d98219f455ea4b7465b0285cce9"}},"c92b7296dd48452096eb5e0939e6d0fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_794bb3273d034fdca0b8aa81b68cd6fd","placeholder":"​","style":"IPY_MODEL_67bc50dd84ac4d0592df5c4507d955f2","value":"tokenizer_config.json: 100%"}},"e18150160a824d0599ca8aec7aa6b4c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7534d038a2fb4c32ae26f79cd35c1009","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e0d0ca49cb64603ad9df492e8acc034","value":25}},"487aad575c924aa0ab92534aa38fb59a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_926be3fe276d405d83548c7a3a100aa0","placeholder":"​","style":"IPY_MODEL_5baa6726939145809acc72dd2b942172","value":" 25.0/25.0 [00:00&lt;00:00, 496B/s]"}},"0fd42d98219f455ea4b7465b0285cce9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"794bb3273d034fdca0b8aa81b68cd6fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67bc50dd84ac4d0592df5c4507d955f2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7534d038a2fb4c32ae26f79cd35c1009":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e0d0ca49cb64603ad9df492e8acc034":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"926be3fe276d405d83548c7a3a100aa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5baa6726939145809acc72dd2b942172":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c768f479e2542b1bcbe4f34261172ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd18067a78e14b88b320105f14dd10c1","IPY_MODEL_8a1209986f454717b388d391f8df356e","IPY_MODEL_a4e786ebd73e4a11a108365de40dbc5c"],"layout":"IPY_MODEL_f4b4ee8479ee40f7977d535f1791e9cf"}},"cd18067a78e14b88b320105f14dd10c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f541932967294b50818a18842e5ee715","placeholder":"​","style":"IPY_MODEL_480337eb0d2549209d9433b3f70a442d","value":"config.json: 100%"}},"8a1209986f454717b388d391f8df356e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb0baa90fda4bf19e1adf82d29ce07c","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb96afa7b98348438581253d3f194cbc","value":482}},"a4e786ebd73e4a11a108365de40dbc5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b4843f4de424064a84ce2abfdde3497","placeholder":"​","style":"IPY_MODEL_7931e29190924b4b9bcc53e2501d367a","value":" 482/482 [00:00&lt;00:00, 6.09kB/s]"}},"f4b4ee8479ee40f7977d535f1791e9cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f541932967294b50818a18842e5ee715":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"480337eb0d2549209d9433b3f70a442d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cb0baa90fda4bf19e1adf82d29ce07c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb96afa7b98348438581253d3f194cbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b4843f4de424064a84ce2abfdde3497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7931e29190924b4b9bcc53e2501d367a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f43bc4528ad4fb6ade504b6795b24f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7ec3d84a0eb544eba1b3f32188f4ae9a","IPY_MODEL_6f924c1bca5f44be931baf5e0769ec74","IPY_MODEL_e4b3fed759a34a0e8f143a8742453cc9"],"layout":"IPY_MODEL_d93ce64ec0ce4410836d95e0d120ae8b"}},"7ec3d84a0eb544eba1b3f32188f4ae9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c1f238c271e409b84d8bdcc10bfc186","placeholder":"​","style":"IPY_MODEL_42635e399522464d8e126a823288e812","value":"vocab.json: 100%"}},"6f924c1bca5f44be931baf5e0769ec74":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8762bb6dd8c44d25ad81ad7cbda2b1f5","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b94e509925ab457299af5b9a4bdd4eba","value":898823}},"e4b3fed759a34a0e8f143a8742453cc9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2121cc013c56488eb03defb3a62f5096","placeholder":"​","style":"IPY_MODEL_d1305f9aac2a43f7af9d281cc38040a8","value":" 899k/899k [00:00&lt;00:00, 2.10MB/s]"}},"d93ce64ec0ce4410836d95e0d120ae8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c1f238c271e409b84d8bdcc10bfc186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42635e399522464d8e126a823288e812":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8762bb6dd8c44d25ad81ad7cbda2b1f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b94e509925ab457299af5b9a4bdd4eba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2121cc013c56488eb03defb3a62f5096":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1305f9aac2a43f7af9d281cc38040a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5ecd7dd0ff94da08cd38f3d057fd9ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1c71f6d0a5242d4ab6f3a408b19fa39","IPY_MODEL_36ee4d5f68394aaf975ce030aa953be8","IPY_MODEL_afecdf6f9480462682d8a629bd796c73"],"layout":"IPY_MODEL_4a349f2a29c3425abd0a7ea8e0d235f7"}},"a1c71f6d0a5242d4ab6f3a408b19fa39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_536280d49ac94b89b14d31bb8b22385a","placeholder":"​","style":"IPY_MODEL_c9dbd4a3f38747c79273f49e3681198b","value":"merges.txt: 100%"}},"36ee4d5f68394aaf975ce030aa953be8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26560f54c6d94ad08ccc2779d57ca231","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72eeae881e8a45208d6427081168700b","value":456318}},"afecdf6f9480462682d8a629bd796c73":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa6d3d9f689e40739ef209ef14401c4a","placeholder":"​","style":"IPY_MODEL_301dc0ee577d4fe9bb532c7f89d1dd29","value":" 456k/456k [00:00&lt;00:00, 2.10MB/s]"}},"4a349f2a29c3425abd0a7ea8e0d235f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"536280d49ac94b89b14d31bb8b22385a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9dbd4a3f38747c79273f49e3681198b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"26560f54c6d94ad08ccc2779d57ca231":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72eeae881e8a45208d6427081168700b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa6d3d9f689e40739ef209ef14401c4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"301dc0ee577d4fe9bb532c7f89d1dd29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05499cd947cd4090aef8bc1f93614fe1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b56c8bd157f949c6939c1b12fde71605","IPY_MODEL_44b820420c0342a48c0eb7809c50190f","IPY_MODEL_46ca22ac7c794f7894d5cf866a71fcf5"],"layout":"IPY_MODEL_ea7644191e23410680a2c072fb6db0e8"}},"b56c8bd157f949c6939c1b12fde71605":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1432e0f6b84146468769a36af6682597","placeholder":"​","style":"IPY_MODEL_0a44ac81049b4d609f356727a4d1f56f","value":"tokenizer.json: 100%"}},"44b820420c0342a48c0eb7809c50190f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e02fce0148654460802be1e215002554","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a5514ec2eb54c4592a517d0375c643b","value":1355863}},"46ca22ac7c794f7894d5cf866a71fcf5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47e6dd62b91845ae86cb132126c64f47","placeholder":"​","style":"IPY_MODEL_9bf68c2f8cc04ec78d834663ccc3ba59","value":" 1.36M/1.36M [00:00&lt;00:00, 2.14MB/s]"}},"ea7644191e23410680a2c072fb6db0e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1432e0f6b84146468769a36af6682597":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a44ac81049b4d609f356727a4d1f56f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e02fce0148654460802be1e215002554":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a5514ec2eb54c4592a517d0375c643b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47e6dd62b91845ae86cb132126c64f47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bf68c2f8cc04ec78d834663ccc3ba59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ba6c171597540f9847a270da8f1146c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b74ffb86b3ef469593afd7bdf87cfe6e","IPY_MODEL_426faa421a6647d294854b707c524256","IPY_MODEL_36afc0bf6c304db1b582f6ffdb312080"],"layout":"IPY_MODEL_758f8f82024040adac5887e9eefab015"}},"b74ffb86b3ef469593afd7bdf87cfe6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_795eb2b358304d83921c0ed7ce9c7cf7","placeholder":"​","style":"IPY_MODEL_e0adb13e524041b7a7088c285b4042a6","value":"model.safetensors: 100%"}},"426faa421a6647d294854b707c524256":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1a04d9f5d424563ba362f2d5e4a288c","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3c1026f237994c90a3b0d60a2aa21c50","value":1421700479}},"36afc0bf6c304db1b582f6ffdb312080":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53642f6f20ab46cdb7773f75c0d649f7","placeholder":"​","style":"IPY_MODEL_2a24e315f56e4ada942421b7df562fc3","value":" 1.42G/1.42G [00:15&lt;00:00, 132MB/s]"}},"758f8f82024040adac5887e9eefab015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"795eb2b358304d83921c0ed7ce9c7cf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0adb13e524041b7a7088c285b4042a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1a04d9f5d424563ba362f2d5e4a288c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c1026f237994c90a3b0d60a2aa21c50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53642f6f20ab46cdb7773f75c0d649f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a24e315f56e4ada942421b7df562fc3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"h65-Imy3BUm0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763742583065,"user_tz":-330,"elapsed":22775,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"7b66f07c-21b2-4ec1-a2d3-2ad38ab8a129"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=38f7c8e9bd935546eec9e1c6bf00d82766c60eedc5b2156c802740b92fe9ad10\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"SmUFiMfBFuaE","executionInfo":{"status":"ok","timestamp":1763742583094,"user_tz":-330,"elapsed":23,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"19b78e19-8634-46fd-c602-cc1f87ece629"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key1.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4Kb__lGCFuiW","executionInfo":{"status":"error","timestamp":1763707433449,"user_tz":-330,"elapsed":377967,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"546f5619-6e8d-480e-d780-48fbc44a9814"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning with human feedback integrates human input into the training process, guiding and accelerating the algorithm's learning, and can be applied to various algorithms. Human feedback enhances the algorithm's decision-making capabilities, as seen in examples such as Frank's grid world and chat GPT, where a rewards model assesses the quality of generated answers. The iterative training process with human feedback improves performance, making it a powerful tool for generating high-quality responses. This framework improves AI decision-making, and its applications can be seen in various areas, ultimately leading to more effective and efficient AI systems through human feedback and reinforcement learning.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What type of learning does the video introduce?\n","A: reinforcement learning with human feedback\n","Q: Why is human feedback used in reinforcement learning?\n","A: to guide and accelerate the learning process\n","Q: How does Frank learn in the grid world?\n","A: by using a reinforcement learning algorithm\n","Q: When is human feedback provided to Frank?\n","A: while Frank is learning with a reinforcement learning algorithm\n","Q: Who helps Frank learn in the grid world?\n","A: humans can also provide our feedback to Frank as a mentor\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Human Feedback, Q-Learning, DQ Learning, Proximal Policy Optimization, Grid World, Reward Model, GPT Architecture, Chat GPT, Back Propagation, Proximal Policy Optimization\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial explores the application of CVX opt and kernels in Support Vector Machines (SVMs), demonstrating the impact of kernels on SVMs and providing visualization of nonlinear and soft margin effects. The quadratic programming solver minimizes a quadratic equation subject to constraints, with key kernels including linear, polynomial, and Gaussian. The tutorial discusses the implementation and application of SVMs with different kernels, covering initialization, fit, and prediction methods. Understanding kernel functions and SVM parameters is essential for effective machine learning, and the tutorial aims to illustrate the effects of kernels on SVMs using CVX opt and numerical methods like numpy.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is CVX opt useful for in this case?\n","A: so you can see directly the impact of a kernel\n","Q: Why is CVX opt used here?\n","A: just purely\n","Q: How does CVX opt affect the support Vector machine?\n","A: change and modifying the initial formal support Vector machine\n","Q: When would you use lib svm?\n","A: if you wanted to write your own support Vector machine\n","Q: Who wrote the code used in this tutorial?\n","A: Matthew blondell\n","\n","KEY CONCEPTS:\n"," Machine Learning, CVX opt, Kernels, Support Vector Machine, Quadratic Programming, LibSVM, Nonlinear, Soft Margin, PyLearn, Pattern Recognition, Quadratic Programming Solver\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k5knw813e26rfgrs0dt4z54f` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99790, Requested 1102. Please try again in 12m50.688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k5knw813e26rfgrs0dt4z54f` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99786, Requested 1102. Please try again in 12m47.232s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k5knw813e26rfgrs0dt4z54f` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99776, Requested 1102. Please try again in 12m38.592s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-374773835.py\u001b[0m in \u001b[0;36mgroq_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             resp = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01k5knw813e26rfgrs0dt4z54f` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99776, Requested 1102. Please try again in 12m38.592s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-374773835.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;31m# 12. RUN GENERATION ONLY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;31m#####################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m \u001b[0mdf_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nRole-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-374773835.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0msummary\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0mtopics\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mclassify_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mqa_text\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgenerate_qa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-374773835.py\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(transcript)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Summarisation (role-based) – chunk {i}/{len(chunks)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mchunk_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummariser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk_summary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mchunk_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-374773835.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"ERROR\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-374773835.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgroq_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-374773835.py\u001b[0m in \u001b[0;36mgroq_call\u001b[0;34m(prompt, temperature, retries)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Groq call failed ({attempt}/{retries}): {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Groq failed after all retries — returning empty string.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key2.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fq7FFd9FJ_CJ","executionInfo":{"status":"ok","timestamp":1763711839338,"user_tz":-330,"elapsed":1211582,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"35d4413f-72d2-46f3-a93e-a35e9a77ef07"},"execution_count":4,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile\n","Groq key loaded ✓\n","Resuming: 2 rows already processed.\n","Skipping row 0 (already done)\n","Skipping row 1 (already done)\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompts are inputs given to prompt engineering models, providing context and constraints for generating text outputs. There are various types of prompts, including question and statement prompts, with key features such as length, language, and context or constraints. Constraints can include tone, style, and specific requirements. Deconstructing a prompt involves breaking it down into individual components to understand key features and constraints. Effective prompt engineering requires understanding the different types of prompts and their key features to generate accurate and efficient outputs, ultimately helping choose the right prompt for desired outcomes.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What are prompts in prompt engineering?\n","A: inputs that we give to our prompt engineering models\n","Q: Why is it important to understand prompts?\n","A: they are the starting point for generating the text outputs\n","Q: How do key features of prompts impact output?\n","A: they can help you choose the right prompt for your desired output\n","Q: When do we need to deconstruct a prompt?\n","A: when you want to know that what are the elements and key features and constraints in this prompt\n","Q: Who can benefit from understanding prompts?\n","A: you\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Large Language Models, GPT, Google Bard, Natural Language Processing, Machine Learning, Agentic AI, Prompt Deconstruction, Language Models, Pre-trained Models, SEO Optimization\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence agents are autonomous problem solvers that make decisions and use specific functions to complete tasks. The react agent pattern, which mimics human thinking, involves identifying a problem, taking action, and observing the result to find a solution. This pattern is essential for creating AI agents that can solve complex problems. The combination of reasoning ability with tools gives rise to an agent, and understanding AI agents and the react pattern is crucial for building effective AI systems that can think autonomously and make decisions.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'LangChain', 'Agentic AI']\n","\n","Q&A:\n"," Q: What are agents in AI World?\n","A: problem solvers of the Artificial Intelligence (AI) World\n","Q: Why do agents take steps on their own?\n","A: they can decide for themselves what steps to take\n","Q: How do we create an AI agent?\n","A: using react agent pattern\n","Q: When does the observe step happen?\n","A: after we take some action\n","Q: Who is mimicked by the react pattern?\n","A: human beings\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Agents, React Agent Pattern, Reasoning, Acting, Observation, LLM, Lang Chain, Tools, API Calls, Python Functions\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The reflection agent system is examined to understand its inner workings in delivering refined viral tweets. Analyzing the system's architecture and functionality provides insight into how components interact to produce the desired outcome. Langchain supports LSmith, enabling tracing and recording of operations by setting up environment variables and generating an API key. This setup allows for recording runs, threads, and monitors, with runs being the execution of an entire application. The reflection agent generates and critiques tweets through multiple iterations, refining them. The integration of Langchain and LSmith facilitates the development of powerful reflection agents that can generate refined outputs, making understanding the reflection agent system essential for refining viral tweets.\n","\n","TOPICS:\n"," ['LangChain']\n","\n","Q&A:\n"," Q: What will be traced in this section?\n","A: the reflection agent system\n","Q: Why is the system being traced?\n","A: to understand exactly what is happening\n","Q: How will the tracing be done?\n","A: by going to a particular website\n","Q: When will the tracing be done?\n","A: in this section\n","Q: Who will trace the system?\n","A: I\n","\n","KEY CONCEPTS:\n"," Agent, System, Reflection, Tweet, Website, Chain, AI, NLP, ML, Agentic AI, Refinement\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," The LangChain chat model is installed using the command `llm-chain open-ai` and imported with `from langchain.open_ai import ChatOpenAI`. The model is initialized with the desired model, such as `gpt-4`, and set up to interact with Open AI's APIs using a variable and API key stored in an EnV file. The `python-dotenv` package is used to load the EnV file, allowing access to the API key. The Large Language Model processes simple prompts, and providing conversation history enhances its ability to generate accurate responses. The result object contains extensive data, but only the content property is needed, and specifying `result.content` yields the desired output. Overall, using LangChain chat models simplifies communication with APIs like Open AI, enabling the LLM to provide relevant and informed responses.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What are we working with in this section?\n","A: Lang chains chat models\n","Q: Why are we using open Artificial Intelligence (AI) APIs?\n","A: because we are going to be predominantly working with open Artificial Intelligence (AI) apis\n","Q: How do we install the package?\n","A: by using the command to install this particular package\n","Q: When can we use GPD 3?\n","A: if you're a little short on cash\n","Q: Who released the latest model?\n","A: open aai\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, AI, Langchain, Chat Models, Open AI, APIs, LLM, GBT, GPD, NLP, Agentic AI\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," In Python, the sort method can be applied to lists containing strings, with uppercase letters sorted before lowercase letters, resulting in two separate alphabetical sorts. Lists containing both strings and numbers are sorted with numbers first, followed by strings, and reversing the sort order reverses their placement. The sort method handles lists with mixed data types, but may require normalization to uppercase or lowercase for consistent sorting. Python's sort method can handle complex lists, but requires careful consideration of data types and case sensitivity, with default sorting behavior prioritizing numbers over strings.\n","\n","TOPICS:\n"," ['Python Programming']\n","\n","Q&A:\n"," Q: What happens to lists with uppercase and lowercase letters when sorted?\n","A: sort puts the words that have a capital uppercase letter first\n","Q: Why do lists with mixed case sort in a particular way?\n","A: you get uppercase and then lowercase\n","Q: How does Python handle lists with both strings and numbers?\n","A: it puts the numbers first and the strings\n","Q: When does Python sort numbers before strings?\n","A: that's just how it works\n","Q: Who should ensure all letters are the same case for sorting?\n","A: you\n","\n","KEY CONCEPTS:\n"," Python, Lists, Strings, Sort, Method, Alphabetical Order, Uppercase, Lowercase, Numbers, Reverse\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," Decision-making can be performed by humans or Artificial Intelligence (AI), with each having strengths in different tasks. AI can help alleviate the workload of financial analysts reviewing alerts, and a graph with confidence scores and success rates can determine which alerts AI or humans should handle. The optimal approach combines AI and human strengths, with humans excelling when AI is uncertain. AI outperforms humans when certain, but humans excel when AI is unsure, particularly in complex cases. Augmented intelligence, combining human and AI decision-making, often yields the highest success rate, but human cognitive bias can hinder its effectiveness. Combining human and AI decision-making strategically can lead to more effective outcomes, and presenting AI information to humans in a way that minimizes automation bias is crucial for optimal performance.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What needs to be made?\n","A: A decision\n","Q: Why use Artificial Intelligence (AI)?\n","A: make a better job of deciding\n","Q: How do humans perform?\n","A: a little bit flatter\n","Q: When does human outperform AI?\n","A: when the Artificial Intelligence (AI) is unsure\n","Q: Who should decide?\n","A: a fascinating combination of holistic curves and human bias\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Fraud Detection, False Positives, Confidence Score, Success Rate, Prediction, Algorithm, Human Bias, Machine Learning, Financial Analysts, Performance Curve\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Dimitrius, a product manager at Google, discusses building generative applications with Vertex Artificial Intelligence (AI), highlighting technical challenges and introducing six new APIs to address these issues. The APIs include document understanding, embedding, vector search, ranking, grounded generation, and fact-checking, aiming to improve the quality and efficiency of generative applications. Key features include high-quality results and embedded Google expertise, with simple, standalone, and stateless designs for seamless integration. The new Vertex AI APIs simplify the development of generative applications, enabling developers to focus on unique aspects of their projects, with the goal of improving overall quality and efficiency.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: Who is the product manager within Cloud Ai?\n","A: Demitrius case\n","Q: What is the talk about?\n","A: building gen abs faster and better with vertex Artificial Intelligence (AI)\n","Q: How many new vertex Artificial Intelligence (AI) apis are launched?\n","A: six\n","Q: When can developers use these apis?\n","A: now\n","Q: Why are these apis unique?\n","A: they embed a lot of our Google know how\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Generative Applications, Vertex AI, Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Fact Checking API, Gemini, LLM Model, Geo Models\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The Singular Value Decomposition (SVD) of a matrix X is given by X = UΣV^T, where U and V are unitary matrices that preserve angles and lengths of vectors. The SVD has a geometric interpretation, where the matrix X maps a sphere of unit vectors to an ellipsoid, with the singular values determining the lengths of the principal axes. Unitary transformations preserve geometric structure, and the SVD provides a powerful tool for understanding the geometric structure of matrices and their transformations, with variants like the economy SVD using a reduced version of the U matrix to achieve this goal.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is the SVD of a matrix X?\n","A: x equals u Sigma V transpose\n","Q: Why are unitary matrices important?\n","A: unitary matrices are extremely important in science and engineering\n","Q: How do unitary transformations work?\n","A: unitary transformations preserve angles and lengths of vectors\n","Q: When is a matrix considered unitary?\n","A: u u transpose equals u transpose u equals an identity matrix\n","Q: Who uses unitary transformations?\n","A: everyone in science and engineering\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Unitary Matrices, Economy SVD, Fourier Transform, Complex Conjugate Transpose, Vector Space, Inner Product, Matrix Transpose, Singular Vectors, Principal Axes\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Google Gemini Pro 1.5 is a multi-model AI application that enables the creation of generative AI-powered applications, working with both text and images. It has a context window of up to 1 million multimodal tokens, surpassing previous versions. The model accurately extracts information and identifies scenes, and its response to finding specific moments is mostly accurate. With a capacity of around 1 million tokens, it can process large amounts of data, including videos, audios, and code. The model offers significant capabilities for developing innovative applications, and its API key can be created for free, allowing for a limited number of requests. Overall, Google Gemini Pro 1.5 demonstrates improved performance in processing and understanding large amounts of data, making it a valuable tool for content creation and generation.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the topic of this video?\n","A: build generative Artificial Intelligence (AI) powered application\n","Q: Why is Google Gemini Pro 1.5 important?\n","A: it's quite amazing with respect to the kind of application\n","Q: How will the video proceed?\n","A: first 1 minute will be the demo\n","Q: When will hands-on application be shown?\n","A: after that\n","Q: Who is presenting the video?\n","A: my name is krishn\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Google Gemini Pro, Generative AI, Multi-model, API key, Machine Learning, Natural Language Processing, Agentic AI, Deep Learning, Neural Networks, Computer Vision\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Evaluating prompt engineering models involves using matrices such as perplexity, accuracy, and human evaluation to measure performance. Perplexity assesses a language model's ability to predict a sequence of words, while accuracy measures the correctness of generated responses. Human evaluation involves rating the quality of responses. Models can be improved by analyzing generated responses, identifying common errors, and testing on different data sets to determine generalizability. This ongoing process ensures optimal performance, as debugging and refinement are crucial for achieving desired outcomes in prompt engineering models, ultimately leading to enhanced language understanding and generation capabilities.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is perplexity in prompt engineering models?\n","A: perplexity Myers how well a language model predicts a sequence of words\n","Q: Why is evaluating prompt engineering models important?\n","A: to have a good understanding of the matrices used to evaluate\n","Q: How do we evaluate the performance of prompt engineering models?\n","A: using matrices such as perplexity, accuracy, and human evaluation\n","Q: When is it important to test prompt engineering models?\n","A: on different data sets or different tasks\n","Q: Who rates the quality of responses in human evaluation?\n","A: human rate the quality of the responses\n","\n","KEY CONCEPTS:\n"," Perplexity, Accuracy, Human Evaluation, Language Model, Prompt Engineering, Debugging, Cross Validation, Pre-trained Models, Large Language Models, Model Evaluation, Fine-tuning\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," Generative Artificial Intelligence creates new content based on patterns learned from existing data, but has limitations in providing real-time information due to its knowledge cutoff date. In contrast, AI agents can perform tasks using tools and memory, and make autonomous decisions. Agentic AI involves multiple agents working together to achieve complex goals through coordination and tool usage, enabling multi-step planning and decision-making. The complexity of tasks increases from generative AI to AI agents to agentic AI, with the latter allowing for the creation of sophisticated systems that can autonomously complete complex tasks, thereby enhancing overall capabilities.\n","\n","TOPICS:\n"," ['Generative AI', 'Agentic AI', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is generative Artificial Intelligence (AI)?\n","A: Artificial Intelligence (AI) that can create new content\n","Q: Why can't generative AI answer some questions?\n","A: it has a knowledge cutoff date\n","Q: How does LLM work with tools?\n","A: LLM is smart enough to call this API and fetch the latest price\n","Q: When is an Artificial Intelligence (AI) agent used?\n","A: when performing actions, not just simple Q&A\n","Q: Who defines the levels of agentic systems?\n","A: different people, like the creator of Agno framework\n","\n","KEY CONCEPTS:\n"," Generative AI, LLM, Artificial Intelligence Agent, Agentic AI, NLP, API, Autonomy, Multi-step Reasoning, Multi-step Planning, N8N, GPT4\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Covariance measures the relationship between two random variables, such as size and price of houses, quantifying how much they change together. It is calculated using the equation 1/n * Σ(Xi - μX)(Yi - μY), where Xi and Yi are individual data points and μX and μY are the means of the variables. Positive covariance indicates an increase in both variables, while negative covariance indicates an increase in one variable and a decrease in the other. Covariance determines the direction of the relationship between variables, but not the strength, making it essential in data analysis and machine learning to understand relationships between variables.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What topic is being discussed?\n","A: variance covariance\n","Q: Why is covariance important?\n","A: to quantify the relationship between variables\n","Q: How is covariance calculated?\n","A: 1 by n summation of I is equal to 1 to N\n","Q: When is covariance positive?\n","A: when X is increasing and Y is increasing\n","Q: Who should watch this video?\n","A: people who want to learn statistics\n","\n","KEY CONCEPTS:\n"," Variance, Covariance, Random Variables, Mean, Machine Learning, Data Pre-processing, Data Analysis, Pearson Correlation Coefficient, Standard Deviation, Correlation, Regression\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," In reinforcement learning, an objective is a goal that an agent strives to achieve, defined by a reward function providing feedback based on its actions. The agent learns an optimal policy to maximize cumulative reward over time through trial and error, exploring the environment and updating its policy based on the resulting state and reward. Objectives can be defined for episodic tasks, such as playing Tic-Tac-Toe, or continuous tasks, like stock market trading. Defining the objective is crucial as it guides the agent's learning and decision-making process, ultimately determining its ability to achieve the desired outcome in various tasks.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the objective in reinforcement learning?\n","A: to learn the optimal policy that maximize a numerical reverse signal\n","Q: Why do we need to define the objective?\n","A: to give a goal to the agent\n","Q: How does the agent learn in reinforcement learning?\n","A: through trial and error learning\n","Q: When is the reward given in an episodic task?\n","A: at the end of the game\n","Q: Who gives feedback to the agent?\n","A: the manager\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Objective, Agent, Environment, Reward Signal, Policy, Episodic Task, Continuous Task, Value-based Method, Policy-based Method, Reward Function\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," A dictionary in Python comprises key-value pairs, where each pair is an item separated by commas, with the key being immutable, such as a string or number. Dictionaries are useful for mapping items and are created using the dict() function or the zip() function to pair two lists. Functions like items(), keys(), and values() are associated with dictionaries. Values can be accessed, changed, and deleted using square brackets and the del function, with the len() function determining the dictionary's length. As a crucial structure in Python, dictionaries are particularly significant in data science applications, especially with pandas, facilitating efficient data manipulation and analysis.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is a dictionary in Python?\n","A: the dictionary consists of key value pairs\n","Q: Why must a key be immutable?\n","A: it can't change\n","Q: How are items separated in a dictionary?\n","A: by commas\n","Q: When are key value pairs separated?\n","A: by colons\n","Q: Who might use dictionaries?\n","A: those who want to go on and do data science\n","\n","KEY CONCEPTS:\n"," Dictionary, Key-Value Pairs, Immutable, Tuples, Lists, Zip Function, Dict Function, Pandas, Data Science, Python, Items\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence (AI) can significantly improve an organization's security posture by enhancing threat detection and response. The use of AI and automation can reduce the average time to identify and contain a data breach, with IBM's 2023 report indicating a reduction of 108 days. User Behavior Analytics (UBA) with AI and machine learning can detect and respond to insider threats, which have an average cost of $4 million per incident. UBA uses machine learning to identify anomalies and potential threats by analyzing user behavior, and can be integrated with a Security Information and Event Management solution to detect and respond to insider threats, enabling security professionals to respond more effectively and stay ahead of emerging threats.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What are Security Professionals looking for?\n","A: ways to stay ahead of emerging threats\n","Q: Why is AI important?\n","A: to improve your organization security posture\n","Q: How many fewer days to identify a breach?\n","A: 108 fewer days\n","Q: When was the cost of a data breach report?\n","A: 2023\n","Q: Who conducted the survey?\n","A: IBM\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, Machine Learning, User Behavior Analytics, Insider Threats, Security Posture, Data Breach, Automation, IBM, Security Team, Threats, Containment\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," Meta has released Lama 3, an open-source large language model with 8 billion and 70 billion pre-trained and instruction-tuned versions, excelling at language nuances and complex tasks. It has been trained on a dataset 7x larger than Lama 2, with 50 trillion tokens, and supports an 8K context length. The model's performance metrics show high accuracy, outperforming other open-source models. Lama 3 is available on various platforms, including Meta, Hugging Face, and Kaggle, and can be accessed by downloading the model and following installation instructions on GitHub. The model demonstrates promising performance, beating some models while being slightly outperformed by others, and its transparency and accessibility contribute to responsible AI development.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: What is the speaker's name?\n","A: krishak\n","Q: Why are they speaking?\n","A: welcome to my YouTube channel\n","Q: How is the speaker addressing the audience?\n","A: so guys\n","Q: When is the speaker recording?\n","A: 2 a. m.\n","Q: Who is speaking?\n","A: krishak\n","\n","KEY CONCEPTS:\n"," \n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The lesson introduces the scikit-learn library, used for machine learning tasks including the Naive Bayes algorithm, a classification technique. To utilize the Naive Bayes function in scikit-learn, the instructor references the library's documentation, which includes a derivation of the Naive Bayes formula and various use cases, such as Gaussian Naive Bayes. The instructor will use Gaussian Naive Bayes to write a classifier, aiming to equip students with the skills to write Python code using scikit-learn and Naive Bayes by the end of the lesson, enabling them to perform machine learning tasks.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Statistics']\n","\n","Q&A:\n"," Q: What library is used in this lesson?\n","A: scikit-learn\n","Q: Why is scikit-learn often abbreviated?\n","A: sk-learn\n","Q: How will the students learn about Naive Bayes?\n","A: using Google\n","Q: When will students be able to write the code themselves?\n","A: by the end of the next video or two\n","Q: Who will be able to write the code themselves?\n","A: you\n","\n","KEY CONCEPTS:\n"," Python, scikit-learn, sklearn, Naive Bayes, Gaussian Naive Bayes, algorithm, classifier, decision boundary, library, documentation, Google\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97642, Requested 2897. Please try again in 7m45.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97638, Requested 2897. Please try again in 7m42.239999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97628, Requested 2897. Please try again in 7m33.6s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97706, Requested 2977. Please try again in 9m50.112s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97702, Requested 2977. Please try again in 9m46.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97692, Requested 2977. Please try again in 9m38.016s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97678, Requested 2868. Please try again in 7m51.744s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97673, Requested 2868. Please try again in 7m47.424s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97664, Requested 2868. Please try again in 7m39.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97650, Requested 2873. Please try again in 7m31.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97645, Requested 2873. Please try again in 7m27.551999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 97635, Requested 2873. Please try again in 7m18.912s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","SUMMARY:\n"," No draft summaries were provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99910, Requested 357. Please try again in 3m50.688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99906, Requested 357. Please try again in 3m47.232s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99896, Requested 357. Please try again in 3m38.592s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99882, Requested 485. Please try again in 5m17.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99877, Requested 485. Please try again in 5m12.768s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99868, Requested 485. Please try again in 5m4.992s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99854, Requested 732. Please try again in 8m26.304s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99849, Requested 732. Please try again in 8m21.984s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99840, Requested 732. Please try again in 8m14.208s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99825, Requested 623. Please try again in 6m27.071999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99821, Requested 623. Please try again in 6m23.616s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99811, Requested 623. Please try again in 6m14.976s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99797, Requested 628. Please try again in 6m7.2s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99792, Requested 628. Please try again in 6m2.88s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99783, Requested 628. Please try again in 5m55.104s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99769, Requested 3362. Please try again in 45m5.183999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99764, Requested 3362. Please try again in 45m0.864s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99755, Requested 3362. Please try again in 44m53.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99832, Requested 3442. Please try again in 47m8.736s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99828, Requested 3442. Please try again in 47m5.28s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99818, Requested 3442. Please try again in 46m56.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99804, Requested 3333. Please try again in 45m10.368s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99799, Requested 3333. Please try again in 45m6.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99790, Requested 3333. Please try again in 44m58.272s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99776, Requested 3338. Please try again in 44m50.496s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99771, Requested 3338. Please try again in 44m46.176s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99762, Requested 3338. Please try again in 44m38.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","SUMMARY:\n"," No draft summaries were provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99747, Requested 1713. Please try again in 21m1.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99743, Requested 1713. Please try again in 20m57.984s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99733, Requested 1713. Please try again in 20m49.344s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99811, Requested 1793. Please try again in 23m5.856s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99807, Requested 1793. Please try again in 23m2.4s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99797, Requested 1793. Please try again in 22m53.76s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99783, Requested 1684. Please try again in 21m7.488s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99778, Requested 1684. Please try again in 21m3.168s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99769, Requested 1684. Please try again in 20m55.392s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99755, Requested 1689. Please try again in 20m47.616s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99750, Requested 1689. Please try again in 20m43.296s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99740, Requested 1689. Please try again in 20m34.656s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," No draft summaries were provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99726, Requested 3172. Please try again in 41m43.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99722, Requested 3172. Please try again in 41m40.416s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99712, Requested 3172. Please try again in 41m31.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99790, Requested 3252. Please try again in 43m48.288s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99786, Requested 3252. Please try again in 43m44.832s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99776, Requested 3252. Please try again in 43m36.192s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99762, Requested 3143. Please try again in 41m49.92s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99757, Requested 3143. Please try again in 41m45.6s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 3143. Please try again in 41m37.824s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99734, Requested 3148. Please try again in 41m30.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99729, Requested 3148. Please try again in 41m25.728s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99719, Requested 3148. Please try again in 41m17.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," No draft summaries were provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99705, Requested 637. Please try again in 4m55.488s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99700, Requested 637. Please try again in 4m51.168s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99691, Requested 637. Please try again in 4m43.392s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99971, Requested 191. Please try again in 2m19.968s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99967, Requested 191. Please try again in 2m16.512s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99957, Requested 191. Please try again in 2m7.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99943, Requested 717. Please try again in 9m30.24s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99938, Requested 717. Please try again in 9m25.92s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99929, Requested 717. Please try again in 9m18.144s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99915, Requested 608. Please try again in 7m31.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99910, Requested 608. Please try again in 7m27.551999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99901, Requested 608. Please try again in 7m19.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99886, Requested 613. Please try again in 7m11.135999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99882, Requested 613. Please try again in 7m7.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99872, Requested 613. Please try again in 6m59.04s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99858, Requested 1852. Please try again in 24m37.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99853, Requested 1852. Please try again in 24m33.12s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99844, Requested 1852. Please try again in 24m25.344s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99830, Requested 644. Please try again in 6m49.536s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99825, Requested 644. Please try again in 6m45.215999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99816, Requested 644. Please try again in 6m37.44s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99892, Requested 1932. Please try again in 26m15.936s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99888, Requested 1932. Please try again in 26m12.48s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99878, Requested 1932. Please try again in 26m3.84s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99864, Requested 1823. Please try again in 24m17.568s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99859, Requested 1823. Please try again in 24m13.248s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99850, Requested 1823. Please try again in 24m5.471999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99836, Requested 1828. Please try again in 23m57.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99831, Requested 1828. Please try again in 23m53.376s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99821, Requested 1828. Please try again in 23m44.736s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99807, Requested 740. Please try again in 7m52.608s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," There are no draft summaries provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99803, Requested 740. Please try again in 7m49.152s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99793, Requested 740. Please try again in 7m40.511999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99779, Requested 721. Please try again in 7m12s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99774, Requested 721. Please try again in 7m7.68s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99765, Requested 721. Please try again in 6m59.904s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99751, Requested 740. Please try again in 7m4.224s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99746, Requested 740. Please try again in 6m59.904s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99737, Requested 740. Please try again in 6m52.128s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99722, Requested 740. Please try again in 6m39.168s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99718, Requested 740. Please try again in 6m35.712s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99708, Requested 740. Please try again in 6m27.071999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99694, Requested 697. Please try again in 5m37.824s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99689, Requested 697. Please try again in 5m33.504s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99680, Requested 697. Please try again in 5m25.728s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99666, Requested 740. Please try again in 5m50.784s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99661, Requested 740. Please try again in 5m46.464s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99652, Requested 740. Please try again in 5m38.688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99638, Requested 740. Please try again in 5m26.592s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99633, Requested 740. Please try again in 5m22.272s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99623, Requested 740. Please try again in 5m13.632s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99609, Requested 758. Please try again in 5m17.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99604, Requested 758. Please try again in 5m12.768s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99595, Requested 758. Please try again in 5m4.992s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99581, Requested 705. Please try again in 4m7.104s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99576, Requested 705. Please try again in 4m2.784s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99567, Requested 705. Please try again in 3m55.008s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99890, Requested 198. Please try again in 1m16.032s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99886, Requested 198. Please try again in 1m12.576s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99876, Requested 198. Please try again in 1m3.936s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99862, Requested 820. Please try again in 9m49.247999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99857, Requested 820. Please try again in 9m44.928s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99848, Requested 820. Please try again in 9m37.152s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99834, Requested 711. Please try again in 7m50.88s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99829, Requested 711. Please try again in 7m46.56s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99820, Requested 711. Please try again in 7m38.784s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99806, Requested 716. Please try again in 7m31.008s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99801, Requested 716. Please try again in 7m26.688s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99791, Requested 716. Please try again in 7m18.048s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99777, Requested 583. Please try again in 5m11.039999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," \n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99772, Requested 583. Please try again in 5m6.72s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99763, Requested 583. Please try again in 4m58.944s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99841, Requested 663. Please try again in 7m15.456s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99837, Requested 663. Please try again in 7m12s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99827, Requested 663. Please try again in 7m3.36s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99813, Requested 554. Please try again in 5m17.088s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99808, Requested 554. Please try again in 5m12.768s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99799, Requested 554. Please try again in 5m4.992s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99785, Requested 559. Please try again in 4m57.216s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99780, Requested 559. Please try again in 4m52.895999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99770, Requested 559. Please try again in 4m44.256s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," No draft summaries were provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99756, Requested 1207. Please try again in 13m52.031999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99752, Requested 1207. Please try again in 13m48.575999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99742, Requested 1207. Please try again in 13m39.936s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99728, Requested 743. Please try again in 6m46.943999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99723, Requested 743. Please try again in 6m42.624s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99714, Requested 743. Please try again in 6m34.848s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99700, Requested 560. Please try again in 3m44.64s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99695, Requested 560. Please try again in 3m40.32s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99686, Requested 560. Please try again in 3m32.543999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99762, Requested 1287. Please try again in 15m6.335999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99758, Requested 1287. Please try again in 15m2.879999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99748, Requested 1287. Please try again in 14m54.24s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99734, Requested 1178. Please try again in 13m7.968s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99729, Requested 1178. Please try again in 13m3.648s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99720, Requested 1178. Please try again in 12m55.872s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99706, Requested 1183. Please try again in 12m48.096s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99701, Requested 1183. Please try again in 12m43.776s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99691, Requested 1183. Please try again in 12m35.136s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," There are no draft summaries provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99677, Requested 491. Please try again in 2m25.152s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99673, Requested 491. Please try again in 2m21.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99663, Requested 491. Please try again in 2m13.055999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99741, Requested 571. Please try again in 4m29.567999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99737, Requested 571. Please try again in 4m26.111999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99727, Requested 571. Please try again in 4m17.471999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99713, Requested 462. Please try again in 2m31.2s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99708, Requested 462. Please try again in 2m26.88s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99699, Requested 462. Please try again in 2m19.104s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n","WARNING:__main__:Groq call failed (1/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99685, Requested 467. Please try again in 2m11.328s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (2/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99680, Requested 467. Please try again in 2m7.008s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","WARNING:__main__:Groq call failed (3/3): Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01ka5q2fh8e34tx0s6xrjw780q` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99671, Requested 467. Please try again in 1m59.232s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n","ERROR:__main__:Groq failed after all retries — returning empty string.\n"]},{"output_type":"stream","name":"stdout","text":["\n","SUMMARY:\n"," No draft summaries were provided to combine into a coherent summary.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," \n","\n","KEY CONCEPTS:\n"," \n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","Role-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.3-70b-versatile_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.3-70b-versatile\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C7QnPmBkPt-R","executionInfo":{"status":"ok","timestamp":1763744480144,"user_tz":-330,"elapsed":1818080,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"7ed10c7b-9e73-40b4-b849-b2f59c3b1de4"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile\n","Groq key loaded ✓\n","Resuming: 19 rows already processed.\n","Skipping row 0 (already done)\n","Skipping row 1 (already done)\n","Skipping row 2 (already done)\n","Skipping row 3 (already done)\n","Skipping row 4 (already done)\n","Skipping row 5 (already done)\n","Skipping row 6 (already done)\n","Skipping row 7 (already done)\n","Skipping row 8 (already done)\n","Skipping row 9 (already done)\n","Skipping row 10 (already done)\n","Skipping row 11 (already done)\n","Skipping row 12 (already done)\n","Skipping row 13 (already done)\n","Skipping row 14 (already done)\n","Skipping row 15 (already done)\n","Skipping row 16 (already done)\n","Skipping row 17 (already done)\n","Skipping row 18 (already done)\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The Gaussian distribution, also known as the normal distribution, is characterized by a bell curve with 50% of data on each side of the mean. In contrast, the log normal distribution is a type of distribution where the log of the variable is normally distributed, with examples including income of people and product reviews. Converting data to a standard normal distribution, such as through log normalization, can improve model accuracy by finding the log of the values and applying a formula, which is essential for achieving higher accuracy in models and is crucial for data analysis and modeling.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What type of distribution is denoted by X belonging to a Gaussian distribution?\n","A: Normal distribution\n","Q: Why are we learning various distributions?\n","A: Because data usually follows Gaussian or log normal distribution\n","Q: How is a log normal distribution denoted?\n","A: If log of X is normally distributed\n","Q: When does a random variable belong to a log normal distribution?\n","A: If log of X belongs to a Gaussian distribution\n","Q: Who uses log normal distribution?\n","A: Companies like Amazon\n","\n","KEY CONCEPTS:\n"," Gaussian Distribution, Normal Distribution, Log Normal Distribution, Standard Normal Distribution, Standard Scaler, Regression Algorithm, Classification Algorithm, Mean, Sigma, Standard Deviation, Log Normalization\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This project develops an end-to-end deep learning application for detecting potato diseases using convolutional neural networks. The application, built by AtliQ Agriculture, utilizes a mobile app for farmers to take pictures of potato plants and receive disease diagnoses. The technical architecture involves data collection, model building using TF serving, Fast API, and React Native, and deployment to Google Cloud. The approach includes data cleaning, pre-processing, and image classification using CNNs. The trained model is exported, deployed, and adapted for mobile app development using TF Lite and quantization. The project demonstrates a machine learning model deployment using a hybrid approach, combining edge devices and cloud services, and enhances skills and job prospects for machine learning engineers and data scientists.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What domain is the deep learning project series about?\n","A: agriculture domain\n","Q: Why are farmers facing economic losses?\n","A: because of various diseases\n","Q: How will the mobile application help farmers?\n","A: tell them whether the potato plant is healthy\n","Q: When can farmers detect diseases using the application?\n","A: early\n","Q: Who is working on this project?\n","A: AtliQ Agriculture\n","\n","KEY CONCEPTS:\n"," Deep Learning, Machine Learning, ML Ops, TF Serving, Fast API, Google Cloud, GCP, Google Cloud Functions, React Native, Convolutional Neural Network, Artificial Intelligence, Data Analytics\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The levels of autonomy in LLM applications vary, with code having zero autonomy and completely autonomous agents having maximum autonomy. LLM calls have limited autonomy, whereas chains and routers offer more autonomy with limitations. Chains comprise multiple specialists and routers direct tasks to the right tool or chain. State machines, or agents, combine routers with loops, enabling human-in-the-loop approval and iterative refinement, thus increasing the system's ability to make decisions and learn from mistakes, which in turn improves autonomy.\n","\n","TOPICS:\n"," ['LangChain', 'Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What has zero autonomy?\n","A: code\n","Q: Why is code not considered a cognitive architecture?\n","A: it is 100% deterministic\n","Q: How does a single LLM call work?\n","A: it gives an output based on an input\n","Q: When does a router decide what steps to take next?\n","A: based on user input\n","Q: Who decides the output of each step in a chain?\n","A: human\n","\n","KEY CONCEPTS:\n"," LLM, Autonomy, Chains, Router, State Machine, Agent, LangChain, Artificial Intelligence, NLP, Multi-Agent Systems, Cognitive Architecture\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This section delves into advanced prompt engineering topics, including handling various prompt types, such as text, image, and audio-based prompts, and fine-tuning pre-trained large language models through multitask learning and distillation. Data pre-processing and cleaning, including tokenization and normalization, are crucial steps in building effective models. Deployment using frameworks like TensorFlow Serving or Flask is also covered, along with essential ethical considerations, including fairness and privacy. Mastering prompt engineering necessitates a comprehensive understanding of these techniques, enabling the development of sophisticated models that can be effectively deployed in production, while addressing key ethical concerns.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Deep Learning']\n","\n","Q&A:\n"," Q: What type of prompts can be handled in advanced prompt engineering?\n","A: text-based prompt, image based prompts, and audio based prompts\n","Q: Why is data pre-processing crucial in prompt engineering?\n","A: to make the model more efficient and run faster\n","Q: How is multitasking lighting used in fine-tuning pre-trained models?\n","A: training a model on multiple tasks simultaneously\n","Q: When is distillation used in prompt engineering?\n","A: to train a smaller model to mimic a larger model\n","Q: Who should consider the ethical implications of prompt engineering?\n","A: everyone involved in prompt engineering models\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Pre-trained Models, Fine-tuning, Multitask Learning, Distillation, Tokenization, Normalization, Data Pre-processing, Data Augmentation, TensorFlow Serving, Flask\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The application of Singular Value Decomposition to image classification is demonstrated, using eigenfaces to distinguish between images of Arnold Schwarzenegger and Sylvester Stallone. The process involves loading and clustering images, computing the average face, and subtracting it to create a matrix for SVD. The resulting eigenfaces capture dominant features and allow for classification, with test images being correctly classified. Experiments with Taylor Swift and Arnold Schwarzenegger images reveal unexpected similarities in eigenface space, likely due to similar skin tone and hair color, showcasing the power and limitations of eigenfaces in image classification and highlighting their potential applications.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is the topic of this lecture\n","A: singular value decomposition and eigenfaces\n","Q: Why are the images cropped and aligned\n","A: so that their faces were more or less in the same place\n","Q: How many pictures of Arnold and Stallone are used\n","A: 20 pictures of Arnold, 20 pictures of Stallone\n","Q: When is the average face computed\n","A: at the end of loading all images\n","Q: Who are the action heroes used in the example\n","A: Arnold Schwarzenegger and Sylvester Stallone\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition, Eigenfaces, Principal Component Analysis, Image Classification, Deep Neural Network Architectures, Face Classification, Three-Dimensional Geometry, Stereo Vision, Linear Combination, Principal Components, Eigenvalues, Eigenvectors\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," Lang chain is a framework that bridges the gap between large language models and the real world, enabling applications to interact with external APIs, databases, and services. It acts as an interface, allowing language models to access real-world data and services, and supports multiple models. This framework expands the capabilities of AI applications by connecting language models to real-world services and data, enabling access to private databases, sending emails, and browsing the internet. Lang chain's capabilities are vast, with numerous use cases, and its integration enhances AI's functionality, allowing it to operate effectively in real-world scenarios, ultimately bridging the gap between intelligence and action.\n","\n","TOPICS:\n"," ['LangChain', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is Lang chain used for?\n","A: build applications using llms\n","Q: Why are large language models limited?\n","A: cannot interact with the real world\n","Q: How does Lang chain work?\n","A: acts as a bridge between llms and real world\n","Q: When is Lang chain useful?\n","A: if you want to switch out gp4\n","Q: Who can use Lang chain?\n","A: you and me\n","\n","KEY CONCEPTS:\n"," LLM, Lang Chain, APIs, Databases, Agentic AI, Large Language Models, Hugging Face, NLP, Artificial Intelligence, Machine Learning, Chat Application\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residual analysis is a crucial step in improving forecasting methods, particularly in time series analysis, where residuals represent the difference between fitted and actual values. The absence of autocorrelation, a mean of zero, and symmetrical distribution around zero are key indicators of a well-performing model. Autocorrelation tests, such as the Ljung-Box test, can diagnose correlation in residuals, while a histogram of residuals can reveal bias. Analyzing residuals helps diagnose model performance, identify areas for correction, and refine forecasting models, ensuring accurate predictions. By examining residuals, model performance can be improved, and overall, residual analysis is essential for refining forecasting models and making precise predictions.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: the difference between the fitted value and the actual value\n","Q: Why are residuals important?\n","A: to diagnose performance and improve forecasting methods\n","Q: How can residual analysis be used?\n","A: to detect Trends or inconsistencies in the model\n","Q: When should residuals have no autocorrelation?\n","A: always, because if it has some form of correlation the model has missed some information\n","Q: Who can use residual analysis?\n","A: data scientists, like Eagle\n","\n","KEY CONCEPTS:\n"," Residuals, Residual Analysis, Time Series, Forecasting, Autocorrelation, Partial Autocorrelation, Ljung-Box Test, Holt-Winters Model, Exponential Smoothing, Trend, Seasonality, Bias\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This project involves building an AI agent that interacts with a database using SQL knowledge and large language models. The agent is built using LangGraph, Next.js, and models running on Watsonx.ai, with an in-memory database using SQLite. The application consists of a client-side component with a header, input box, and placeholder messages, and is set up using the Next.js CLI, with TypeScript and Tailwind for styling. The overall insight is that AI agents can be built to interact with databases using SQL knowledge and large language models, enabling the creation of a joke-telling agent using ReAct and Watsonx.ai models, and the rendering of human and AI-generated messages with a corresponding database structure, including a foreign key relation between customer and order tables.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What will we build in this video?\n","A: an agent that's able to use that SQL knowledge to connect to your databases\n","Q: Why have large language models been useful?\n","A: have been trained on code, including SQL\n","Q: How will we set up the project?\n","A: using the Next. js CLI to set up my boilerplate project\n","Q: When should you move into the project directory?\n","A: before you try to start the application\n","Q: Who or what will be used for the frontend application?\n","A: Next. js\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence, LangGraph, ReAct, SQL, Next.js, SQLite, TypeScript, Tailwind, Text2SQL, Large Language Model, VS Code\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering, a specialized field within natural language processing, focuses on generating high-quality text outputs using pre-trained large language models like GPT and Transformers. It involves fine-tuning models for specific tasks and inputs to produce accurate and coherent text. The field is crucial for applications such as chatbots and language translation, where output quality significantly impacts user experience. Prompt engineering can generate contextually appropriate text outputs, but may struggle with complex or ambiguous prompts and produce biased outputs. This area of study provides a comprehensive introduction to the basics, prompt analysis, and fine-tuning pre-trained models, ultimately enhancing the quality of text outputs in various applications.\n","\n","TOPICS:\n"," ['Natural Language Processing', 'Prompt Engineering', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: a specialized field within natural language processing\n","Q: Why is prompt engineering important?\n","A: it allows us to generate text outputs that are more accurate\n","Q: How do prompt engineering models work?\n","A: based on pre-trained large language models\n","Q: When is prompt engineering especially important?\n","A: for applications such as chatbots language translation\n","Q: Who can benefit from this course?\n","A: you\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing, Large Language Models, Pre-trained Models, Fine-tuning, Chatbots, Language Translation, Content Generation, Model Architecture, Artificial Intelligence, Transformers\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning involves training machines to map situations to actions that maximize a numerical reward signal, with Q-learning being a key value-based method. Q-learning determines a state-action value function, represented as a Q-table, to maximize total reward. The Q-value is calculated and updated using a gradient update rule, with the temporal difference error driving the learning process. The Q-value update is based on the temporal difference error, which is used to learn the optimal policy. As an off-policy algorithm, Q-learning decoupling the behavior policy from the target policy, allowing for effective exploration and learning, and enabling an agent to learn an optimal policy through trial and error, with the learned Q values becoming more stable over time.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Machine Learning', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is Q-learning?\n","A: a value-based reinforcement learning method\n","Q: Why use value-based methods?\n","A: to maximize total reward\n","Q: How do policy-based methods work?\n","A: determine an optimal policy directly\n","Q: When is a state-action value function used?\n","A: to quantify how good it is to be in a state s and take an action a\n","Q: Who determines the optimal policy?\n","A: the policy based methods\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Q-Learning, Supervised Learning, Unsupervised Learning, Value-Based Methods, Policy-Based Methods, State Value Function, State Action Value Function, Bellman Equation, Discount Factor, Optimal Policy\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier applies a linear function to input data, such as image pixels, to generate predictions. This involves a matrix multiply of the input vector X and a weight matrix W, with an added bias term b. The goal is to train the model by finding optimal values for W and b. The model's scores are converted to probabilities using a softmax function, which ensures proper probabilities that sum to 1. The softmax function maps large scores to high probabilities and small scores to low probabilities, ultimately producing probabilities for each output class. Logistic classification trains a linear model to achieve this, enabling effective prediction and classification.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: linear classifier\n","Q: Why is a linear function used?\n","A: to generate its predictions\n","Q: How are scores turned into probabilities?\n","A: use a softmax function\n","Q: When are proper probabilities achieved?\n","A: sum to 1\n","Q: Who denotes the inputs, weights, and bias?\n","A: we'll denote the inputs by X\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Classifier, Linear Function, Matrix Multiply, Machine Learning, Weights, Bias, Classification, Softmax Function, Logits, Probabilities\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n","\n","Role-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["8a413b4c0eb34a9ab4a2e29bb6fda4ad","c92b7296dd48452096eb5e0939e6d0fb","e18150160a824d0599ca8aec7aa6b4c3","487aad575c924aa0ab92534aa38fb59a","0fd42d98219f455ea4b7465b0285cce9","794bb3273d034fdca0b8aa81b68cd6fd","67bc50dd84ac4d0592df5c4507d955f2","7534d038a2fb4c32ae26f79cd35c1009","9e0d0ca49cb64603ad9df492e8acc034","926be3fe276d405d83548c7a3a100aa0","5baa6726939145809acc72dd2b942172","8c768f479e2542b1bcbe4f34261172ff","cd18067a78e14b88b320105f14dd10c1","8a1209986f454717b388d391f8df356e","a4e786ebd73e4a11a108365de40dbc5c","f4b4ee8479ee40f7977d535f1791e9cf","f541932967294b50818a18842e5ee715","480337eb0d2549209d9433b3f70a442d","4cb0baa90fda4bf19e1adf82d29ce07c","fb96afa7b98348438581253d3f194cbc","7b4843f4de424064a84ce2abfdde3497","7931e29190924b4b9bcc53e2501d367a","3f43bc4528ad4fb6ade504b6795b24f9","7ec3d84a0eb544eba1b3f32188f4ae9a","6f924c1bca5f44be931baf5e0769ec74","e4b3fed759a34a0e8f143a8742453cc9","d93ce64ec0ce4410836d95e0d120ae8b","1c1f238c271e409b84d8bdcc10bfc186","42635e399522464d8e126a823288e812","8762bb6dd8c44d25ad81ad7cbda2b1f5","b94e509925ab457299af5b9a4bdd4eba","2121cc013c56488eb03defb3a62f5096","d1305f9aac2a43f7af9d281cc38040a8","f5ecd7dd0ff94da08cd38f3d057fd9ac","a1c71f6d0a5242d4ab6f3a408b19fa39","36ee4d5f68394aaf975ce030aa953be8","afecdf6f9480462682d8a629bd796c73","4a349f2a29c3425abd0a7ea8e0d235f7","536280d49ac94b89b14d31bb8b22385a","c9dbd4a3f38747c79273f49e3681198b","26560f54c6d94ad08ccc2779d57ca231","72eeae881e8a45208d6427081168700b","fa6d3d9f689e40739ef209ef14401c4a","301dc0ee577d4fe9bb532c7f89d1dd29","05499cd947cd4090aef8bc1f93614fe1","b56c8bd157f949c6939c1b12fde71605","44b820420c0342a48c0eb7809c50190f","46ca22ac7c794f7894d5cf866a71fcf5","ea7644191e23410680a2c072fb6db0e8","1432e0f6b84146468769a36af6682597","0a44ac81049b4d609f356727a4d1f56f","e02fce0148654460802be1e215002554","9a5514ec2eb54c4592a517d0375c643b","47e6dd62b91845ae86cb132126c64f47","9bf68c2f8cc04ec78d834663ccc3ba59","6ba6c171597540f9847a270da8f1146c","b74ffb86b3ef469593afd7bdf87cfe6e","426faa421a6647d294854b707c524256","36afc0bf6c304db1b582f6ffdb312080","758f8f82024040adac5887e9eefab015","795eb2b358304d83921c0ed7ce9c7cf7","e0adb13e524041b7a7088c285b4042a6","d1a04d9f5d424563ba362f2d5e4a288c","3c1026f237994c90a3b0d60a2aa21c50","53642f6f20ab46cdb7773f75c0d649f7","2a24e315f56e4ada942421b7df562fc3"]},"id":"-HxN3H6PFuq9","executionInfo":{"status":"ok","timestamp":1763744809941,"user_tz":-330,"elapsed":193594,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"1a86f2b5-661a-441d-aea9-655d213369c0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/llama-3.3-70b-versatile_role_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a413b4c0eb34a9ab4a2e29bb6fda4ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c768f479e2542b1bcbe4f34261172ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f43bc4528ad4fb6ade504b6795b24f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ecd7dd0ff94da08cd38f3d057fd9ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05499cd947cd4090aef8bc1f93614fe1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba6c171597540f9847a270da8f1146c"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.3017\n","  - BLEU: 0.0575\n","  - BERTScore F1: 0.8895\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.8667\n","  - Jaccard Index: 0.3249\n","  - Micro F1: 0.4489\n","  - Macro F1: 0.4001\n","  - Weighted F1: 0.4144\n","\n","Q&A Generation:\n","  - BLEU: 0.0258\n","  - Diversity: 0.8053\n","  - Answerability: 0.6733\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.4600\n","  - Recall@10: 0.1840\n","  - F1@10: 0.2629\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.3-70b-versatile/evaluation_final.json\n"]}]}]}