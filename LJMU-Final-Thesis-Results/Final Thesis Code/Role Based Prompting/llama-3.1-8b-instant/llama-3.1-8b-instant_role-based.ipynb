{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVeEeF7wVJmmIxj5EW0WDL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ccce3e56390042bcad2151d453f195e7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2297fcb9a93a4d708fdb6408d47ab762","IPY_MODEL_18f2c61341f3436991409b6d373aab18","IPY_MODEL_9228b99eafd34ebb8ca7cde9116aeadc"],"layout":"IPY_MODEL_ecaa2f752f8c4418b782d71e0d6070ec"}},"2297fcb9a93a4d708fdb6408d47ab762":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_96ed93e1eddc45b18f184a1b1edeffdb","placeholder":"​","style":"IPY_MODEL_b2c4dbf0015147e2b725dc625db98532","value":"tokenizer_config.json: 100%"}},"18f2c61341f3436991409b6d373aab18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbc3736d61a5447b88d64bb611bb2823","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4381fd49987c4b35a1b33b7af241527d","value":25}},"9228b99eafd34ebb8ca7cde9116aeadc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d21de52000c49e2af8a94cabaf1daab","placeholder":"​","style":"IPY_MODEL_ad10470bd8e74c51958d1084eede6516","value":" 25.0/25.0 [00:00&lt;00:00, 1.34kB/s]"}},"ecaa2f752f8c4418b782d71e0d6070ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96ed93e1eddc45b18f184a1b1edeffdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2c4dbf0015147e2b725dc625db98532":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbc3736d61a5447b88d64bb611bb2823":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4381fd49987c4b35a1b33b7af241527d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9d21de52000c49e2af8a94cabaf1daab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad10470bd8e74c51958d1084eede6516":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3dc6b52d41b14a97821e8e655303febe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e3c6752f08d42bcb3b78c2d52b42bd8","IPY_MODEL_9959d33b9d224ee1821fbe62094af5cb","IPY_MODEL_835d051169dd4d8e9f750239a73469a8"],"layout":"IPY_MODEL_ea15d67520104c5ea0402bebea415c1c"}},"6e3c6752f08d42bcb3b78c2d52b42bd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d4a772bfc3648578eb34589104c02c1","placeholder":"​","style":"IPY_MODEL_cc92916b320246dd92f3ac34d5369019","value":"config.json: 100%"}},"9959d33b9d224ee1821fbe62094af5cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_480fdbbc51334f61aa5c270471128d48","max":482,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c91866b550ed495aab462222bfaa1c12","value":482}},"835d051169dd4d8e9f750239a73469a8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_576f8b11680e49c999e14321ff860082","placeholder":"​","style":"IPY_MODEL_35062c0465794ad7957b78e9f2abb90d","value":" 482/482 [00:00&lt;00:00, 31.1kB/s]"}},"ea15d67520104c5ea0402bebea415c1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d4a772bfc3648578eb34589104c02c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc92916b320246dd92f3ac34d5369019":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"480fdbbc51334f61aa5c270471128d48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c91866b550ed495aab462222bfaa1c12":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"576f8b11680e49c999e14321ff860082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35062c0465794ad7957b78e9f2abb90d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33e1a386f92e44daae40a720beab9996":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2385f49497174a20b1b4d59d7bcabeb7","IPY_MODEL_e10cf33b7c5f455296ade032a87a80bd","IPY_MODEL_4ff29752e848492b8988b13380238adf"],"layout":"IPY_MODEL_9fe4e9c169a04259a46ce8238c415cfd"}},"2385f49497174a20b1b4d59d7bcabeb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6dde1a699864d7eb76ee37202803f90","placeholder":"​","style":"IPY_MODEL_b8eadf912e794dad8c47534f0d71fc9a","value":"vocab.json: 100%"}},"e10cf33b7c5f455296ade032a87a80bd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_479b607a3df945d1bd65a7562520abb3","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eed8510ca0924d00b7e58d17bed297e4","value":898823}},"4ff29752e848492b8988b13380238adf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e36acc9979a24f6b908a6b1a5f5aed66","placeholder":"​","style":"IPY_MODEL_ea0a6641a088415b810ea48fc610f68c","value":" 899k/899k [00:00&lt;00:00, 5.90MB/s]"}},"9fe4e9c169a04259a46ce8238c415cfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6dde1a699864d7eb76ee37202803f90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8eadf912e794dad8c47534f0d71fc9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"479b607a3df945d1bd65a7562520abb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eed8510ca0924d00b7e58d17bed297e4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e36acc9979a24f6b908a6b1a5f5aed66":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea0a6641a088415b810ea48fc610f68c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b02f7a94f748463f9f6117be71b727a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a5614002710483fa2a6375251c23202","IPY_MODEL_879ac44acd8a4980b2bf806f656982f1","IPY_MODEL_fe6860de73354fa3a9a8e136dbc90e59"],"layout":"IPY_MODEL_092b3730fbd7470ebe5d8acbda428aad"}},"2a5614002710483fa2a6375251c23202":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c714c9b865ad452baf6c6287d4f11498","placeholder":"​","style":"IPY_MODEL_06c3de536d204dfba3145973ec36f2fa","value":"merges.txt: 100%"}},"879ac44acd8a4980b2bf806f656982f1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4017180746874b9fb99319f842d32d11","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_134a11ec686c4291a9aef7e63c50a48f","value":456318}},"fe6860de73354fa3a9a8e136dbc90e59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9818ce7424dd4742a8c0a93b51d99a3c","placeholder":"​","style":"IPY_MODEL_1500de5f528643eab3c57d07db63720b","value":" 456k/456k [00:00&lt;00:00, 29.9MB/s]"}},"092b3730fbd7470ebe5d8acbda428aad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c714c9b865ad452baf6c6287d4f11498":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06c3de536d204dfba3145973ec36f2fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4017180746874b9fb99319f842d32d11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"134a11ec686c4291a9aef7e63c50a48f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9818ce7424dd4742a8c0a93b51d99a3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1500de5f528643eab3c57d07db63720b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"734fb28f406c4b9d9d8acfb748400c43":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6fa223292d92431993d8c07da9dbcaee","IPY_MODEL_a68b18899db445e297e6eb6f5af8055c","IPY_MODEL_802f51647daf4da6a4a2ab82441df731"],"layout":"IPY_MODEL_c66912e61dec4b099b139f32c01b8992"}},"6fa223292d92431993d8c07da9dbcaee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7696b65d056b401980f7d0485c34d034","placeholder":"​","style":"IPY_MODEL_056a933926f145338cc17fdbd78d8e51","value":"tokenizer.json: 100%"}},"a68b18899db445e297e6eb6f5af8055c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b642940d9e94eb9a08340682b4cc368","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8ef989cf672640cdaa256f08a91d7e87","value":1355863}},"802f51647daf4da6a4a2ab82441df731":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d04f729b90e44a9ba564355874952e0","placeholder":"​","style":"IPY_MODEL_6499f041cd1b458ea63ffdfba05fe6e1","value":" 1.36M/1.36M [00:00&lt;00:00, 15.2MB/s]"}},"c66912e61dec4b099b139f32c01b8992":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7696b65d056b401980f7d0485c34d034":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"056a933926f145338cc17fdbd78d8e51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b642940d9e94eb9a08340682b4cc368":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ef989cf672640cdaa256f08a91d7e87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d04f729b90e44a9ba564355874952e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6499f041cd1b458ea63ffdfba05fe6e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2cb43ed848fa41fd9bc070edebd44265":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1aba318813764469ac4fb83dbc78860f","IPY_MODEL_bef1429818b8458196478a57d1f0ddb3","IPY_MODEL_f6c81bf961f54015b83f7f98b23fefb2"],"layout":"IPY_MODEL_19b8bfd84b7543c7b2a5e79904b9f984"}},"1aba318813764469ac4fb83dbc78860f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_52c5751c30c34bb5b387af03883ead9e","placeholder":"​","style":"IPY_MODEL_e2be8869f2904bc8a662203d0753aed5","value":"model.safetensors: 100%"}},"bef1429818b8458196478a57d1f0ddb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9ed84c751e64f6bb3d695844dfde89b","max":1421700479,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d78b4b63ae624f86800bca9d0e785967","value":1421700479}},"f6c81bf961f54015b83f7f98b23fefb2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1c6c71c72db487cbaf3933d39139d80","placeholder":"​","style":"IPY_MODEL_4ef4ad1ebdb7442da83c79b387f4f17d","value":" 1.42G/1.42G [00:16&lt;00:00, 102MB/s]"}},"19b8bfd84b7543c7b2a5e79904b9f984":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52c5751c30c34bb5b387af03883ead9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2be8869f2904bc8a662203d0753aed5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f9ed84c751e64f6bb3d695844dfde89b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d78b4b63ae624f86800bca9d0e785967":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1c6c71c72db487cbaf3933d39139d80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ef4ad1ebdb7442da83c79b387f4f17d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"bBTqvBRWBVYO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763744659161,"user_tz":-330,"elapsed":25767,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"0526895e-a69d-4ee8-c7e8-ddad3b2f9b70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting groq\n","  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert-score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.10)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.57.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.20.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.36.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.3)\n","Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n","Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=2e923e05a64b001548aefeac87a58909d0f17be96f890e22f54ba54100566155\n","  Stored in directory: /root/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n","Successfully built rouge-score\n","Installing collected packages: rouge-score, groq, bert-score\n","Successfully installed bert-score-0.3.13 groq-0.36.0 rouge-score-0.1.2\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}],"source":["!pip install groq rouge-score bert-score nltk\n","import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","source":["%%javascript\n","function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"WfOlnHxpIecj","executionInfo":{"status":"ok","timestamp":1763744659199,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"ecf7cb79-b190-4a20-b253-aa115add5279"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["function ClickConnect(){\n","  console.log(\"Clicking\");\n","  document.querySelector(\"colab-toolbar-button#connect\").click();\n","}\n","setInterval(ClickConnect, 60000)\n"]},"metadata":{}}]},{"cell_type":"code","source":["#####################################################################\n","# 1. MOUNT DRIVE & IMPORTS\n","#####################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os, re, json, time, logging\n","from datetime import datetime\n","from pathlib import Path\n","from typing import List, Dict, Any\n","\n","import pandas as pd\n","import numpy as np\n","from groq import Groq   # <-- CHANGED: using Groq instead of Gemini\n","\n","\n","#####################################################################\n","# 2. PATHS & API KEY (GROQ)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","\n","BASE_OUT = Path(\"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/\")\n","BASE_OUT.mkdir(parents=True, exist_ok=True)\n","\n","FINAL_OUTPUT_FILE = BASE_OUT / \"llama-3.1-8b-instant_role_full_output.xlsx\"\n","\n","API_KEY_PATH = \"/content/drive/MyDrive/Final Thesis Code/api_keys/groq_key3.txt\"\n","\n","def load_key(path: str) -> str:\n","    with open(path, \"r\") as f:\n","        return f.read().strip()\n","\n","API_KEY = load_key(API_KEY_PATH)\n","client = Groq(api_key=API_KEY)\n","\n","print(\"Input file:\", INPUT_FILE)\n","print(\"Output folder:\", BASE_OUT)\n","print(\"Groq key loaded ✓\")\n","\n","\n","#####################################################################\n","# 3. GLOBAL CONFIG\n","#####################################################################\n","MODEL_NAME     = \"llama-3.1-8b-instant\"\n","MAX_CHARS      = 2600\n","GLOBAL_MIN_GAP = 25       # soft global wait between calls (seconds)\n","LAST_TS        = 0.0\n","\n","VALID_TOPICS = [\n","    \"Natural Language Processing\",\"Artificial Intelligence\",\"Prompt Engineering\",\n","    \"Machine Learning\",\"Deep Learning\",\"Reinforcement Learning\",\"Generative AI\",\n","    \"Data Science\",\"Time Series\",\"Statistics\",\"LangChain\",\"Langraph\",\n","    \"Python Programming\",\"Mlops\",\"Agentic AI\",\"Other\"\n","]\n","\n","\n","#####################################################################\n","# 4. LOGGING\n","#####################################################################\n","def setup_logging():\n","    logs = Path(\"/content/logs\")\n","    logs.mkdir(exist_ok=True)\n","    logfile = logs / f\"log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n","\n","    logging.basicConfig(\n","        level=logging.INFO,\n","        handlers=[\n","            logging.FileHandler(logfile, encoding=\"utf-8\"),\n","            logging.StreamHandler()\n","        ],\n","        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n","    )\n","    return logging.getLogger(__name__)\n","\n","logger = setup_logging()\n","\n","\n","#####################################################################\n","# 5. CLEANING, SANITISE & CHUNKING\n","#####################################################################\n","def deep_clean(text: str) -> str:\n","    t = str(text)\n","    t = re.sub(r'https?://\\S+|www\\.\\S+', ' ', t)\n","    t = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', ' ', t)\n","    t = re.sub(r'\\[.*?\\]|\\(.*?\\)', ' ', t)\n","    t = re.sub(r'\\s+', ' ', t)\n","    # acronym expansion\n","    t = re.sub(r'\\bNLP\\b', 'Natural Language Processing (NLP)', t)\n","    t = re.sub(r'\\bML\\b', 'Machine Learning (ML)', t)\n","    t = re.sub(r'\\bAI\\b', 'Artificial Intelligence (AI)', t)\n","    return t.strip()\n","\n","def sanitize(text: str) -> str:\n","    \"\"\"Light sanitisation for prompts (keeps content, flattens whitespace).\"\"\"\n","    return re.sub(r'\\s+', ' ', str(text)).strip()\n","\n","def chunk_text(text: str, max_chars: int = MAX_CHARS) -> List[str]:\n","    clean = deep_clean(text)\n","    if len(clean) <= max_chars:\n","        return [clean] if clean else [\"\"]\n","    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', clean) if s.strip()]\n","    chunks, cur = [], \"\"\n","    for s in sents:\n","        if len(cur) + len(s) + 1 <= max_chars:\n","            cur = (cur + \" \" + s).strip()\n","        else:\n","            if cur:\n","                chunks.append(cur)\n","            cur = s\n","    if cur:\n","        chunks.append(cur)\n","    return chunks or [\"\"]\n","\n","\n","#####################################################################\n","# 6. JSON EXTRACTION\n","#####################################################################\n","def extract_json(text: str) -> Dict[str, Any]:\n","    \"\"\"Extract outermost {...} and parse as JSON.\"\"\"\n","    if not text:\n","        return {}\n","    start = text.find(\"{\")\n","    end = text.rfind(\"}\")\n","    if start == -1 or end == -1 or end <= start:\n","        return {}\n","    candidate = text[start:end+1]\n","    try:\n","        return json.loads(candidate)\n","    except Exception:\n","        return {}\n","\n","\n","#####################################################################\n","# 7. GROQ CALL WITH GLOBAL WAIT\n","#####################################################################\n","def groq_call(prompt: str, temperature: float = 0.2, retries: int = 3) -> str:\n","    global LAST_TS\n","    now = time.time()\n","\n","    # soft global rate limiting\n","    if LAST_TS > 0 and now - LAST_TS < GLOBAL_MIN_GAP:\n","        wait = GLOBAL_MIN_GAP - (now - LAST_TS)\n","        logger.info(f\"Waiting {wait:.1f}s (respecting global gap)\")\n","        time.sleep(wait)\n","\n","    for attempt in range(1, retries+1):\n","        try:\n","            resp = client.chat.completions.create(\n","                model=MODEL_NAME,\n","                messages=[{\"role\": \"user\", \"content\": prompt}],\n","                temperature=temperature,\n","                max_tokens=2048,\n","            )\n","            LAST_TS = time.time()\n","            return (resp.choices[0].message.content or \"\").strip()\n","        except Exception as e:\n","            logger.warning(f\"Groq call failed ({attempt}/{retries}): {e}\")\n","            time.sleep(4 * attempt)\n","\n","    logger.error(\"Groq failed after all retries — returning empty string.\")\n","    return \"\"\n","\n","\n","#####################################################################\n","# 8. ROLE-BASED TASK BASE CLASS\n","#####################################################################\n","class LlamaRoleTask:\n","    \"\"\"Thin wrapper that reuses groq_call + global wait.\"\"\"\n","    def __init__(self, temperature: float = 0.2):\n","        self.temperature = temperature\n","\n","    def _run(self, prompt: str) -> str:\n","        return groq_call(prompt, temperature=self.temperature)\n","\n","\n","#####################################################################\n","# 9. ROLE-BASED TASK CLASSES\n","#####################################################################\n","\n","class SummarisationRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Educational Content Summariser** working for a university’s online learning platform.\n","\n","Your role:\n","- Summarise lectures and AI/ML transcripts into concise, academic English.\n","- Highlight only key insights, definitions, examples, and outcomes.\n","\n","Guidelines:\n","- Write 120–150 words (5–6 sentences).\n","- Avoid filler phrases like “the video discusses”.\n","- Use direct, objective tone suitable for academic notes.\n","- End with one line capturing the overall insight.\n","\n","Return JSON only:\n","{{\"generated_summary\": \"<summary text>\"}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> str:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return \"ERROR\"\n","        try:\n","            return json.loads(txt).get(\"generated_summary\", \"\").strip()\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_summary\", \"\").strip()\n","                except Exception:\n","                    pass\n","        return txt.strip()[:2000]\n","\n","\n","class TopicRoleMulti(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        topics = \", \".join(VALID_TOPICS)\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are a **Research Domain Classifier** for AI and ML educational transcripts.\n","\n","Role Description:\n","You are an academic expert responsible for categorising transcripts into precise research areas based on terminology, algorithms, or learning paradigms mentioned.\n","\n","Decision Guidelines:\n","- Identify the technical scope and key methods discussed.\n","- Select up to THREE relevant topics from the list below.\n","- Prefer domain-specific categories (e.g., \"Reinforcement Learning\") over broad ones (e.g., \"Machine Learning\").\n","- Avoid guessing: only include topics clearly supported by evidence in the text.\n","\n","AVAILABLE TOPICS:\n","{topics}\n","\n","Output Format:\n","Return valid one-line JSON only:\n","{{\"predicted_topics\":[\"<TOPIC1>\",\"<TOPIC2>\",...]}}\n","\n","Transcript:\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        raw = self._run(self.build_prompt(t))\n","        if not raw:\n","            return [\"Other\"]\n","        try:\n","            obj = json.loads(raw)\n","            tp = obj.get(\"predicted_topics\", [])\n","            if isinstance(tp, str):\n","                tp = [tp]\n","        except Exception:\n","            tp = [v for v in VALID_TOPICS if v.lower() in raw.lower()]\n","        tp = [x for x in tp if x in VALID_TOPICS]\n","        return tp or [\"Other\"]\n","\n","\n","class QARole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **Academic Question Author** who designs comprehension questions from lecture transcripts.\n","\n","Role Objectives:\n","- Draft 5 short-answer questions to test conceptual understanding.\n","- Use “What, Why, How, When, Who” formats.\n","- Each answer must come verbatim or closely from transcript phrases (≤25 words).\n","\n","Return JSON only:\n","{{\"generated_questions\":[{{\"q\":\"...\",\"a\":\"...\"}},...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[Dict[str, str]]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"generated_questions\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"generated_questions\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","class ConceptRole(LlamaRoleTask):\n","    def build_prompt(self, t: str) -> str:\n","        t = sanitize(t)\n","        return f\"\"\"\n","You are an **AI Glossary Curator** creating concise glossaries for students.\n","\n","Your Role:\n","- Extract exactly 10–12 distinct key technical terms, tools, or concepts.\n","- Focus on ML, NLP, Agentic AI, and related domain-specific nouns.\n","- Remove words like “video”, “lesson”, “concept”, “people”.\n","\n","Return JSON only:\n","{{\"key_concepts\":[\"concept1\",\"concept2\",...]}}\n","\n","Transcript :\n","\\\"\\\"\\\"{t}\\\"\\\"\\\"\"\"\"\n","\n","    def run(self, t: str) -> List[str]:\n","        txt = self._run(self.build_prompt(t))\n","        if not txt:\n","            return []\n","        try:\n","            return json.loads(txt).get(\"key_concepts\", [])\n","        except Exception:\n","            s, e = txt.find(\"{\"), txt.rfind(\"}\")\n","            if s != -1 and e != -1:\n","                try:\n","                    return json.loads(txt[s:e+1]).get(\"key_concepts\", [])\n","                except Exception:\n","                    pass\n","        return []\n","\n","\n","#####################################################################\n","# 10. ROLE-BASED TASK WRAPPERS (MATCH OLD FUNCTION SIGNATURES)\n","#####################################################################\n","\n","# 10.1 ROLE-BASED SUMMARISATION (still hierarchical: per-chunk + final combine)\n","def generate_summary(transcript: str) -> str:\n","    chunks = chunk_text(transcript)\n","    partial_summaries: List[str] = []\n","\n","    summariser = SummarisationRole(temperature=0.18)\n","\n","    for i, c in enumerate(chunks, start=1):\n","        logger.info(f\"Summarisation (role-based) – chunk {i}/{len(chunks)}\")\n","        chunk_summary = summariser.run(c)\n","        if not chunk_summary:\n","            chunk_summary = \"\"\n","        partial_summaries.append(chunk_summary)\n","\n","    combined = \" \".join(p for p in partial_summaries if p)\n","\n","    # Final combine using a simple JSON instruction (same style)\n","    final_prompt = f\"\"\"\n","Combine the following draft summaries into ONE coherent summary.\n","\n","Requirements:\n","- Length: 120–160 words.\n","- Academic tone.\n","- Preserve main technical ideas and conclusions.\n","- No bullet points, no headings, no reasoning.\n","\n","Return ONLY JSON:\n","{{\"generated_summary\":\"<summary text>\"}}\n","\n","Draft Summaries:\n","\\\"\\\"\\\"{combined}\\\"\\\"\\\"\"\"\"\n","\n","    out2 = groq_call(final_prompt, temperature=0.18)\n","    j2 = extract_json(out2)\n","    final_summary = j2.get(\"generated_summary\", \"\").strip()\n","    if not final_summary:\n","        final_summary = out2.strip()[:900]\n","    return final_summary\n","\n","\n","# 10.2 ROLE-BASED TOPIC CLASSIFICATION (multi-label)\n","def classify_topic(transcript: str, summary: str) -> List[str]:\n","    # Use first chunk to keep tokens bounded (same as before)\n","    first_chunk = chunk_text(transcript)[0]\n","    classifier = TopicRoleMulti(temperature=0.22)\n","    topics = classifier.run(first_chunk)\n","\n","    # Normalise + max 3 + fallback already handled in TopicRoleMulti\n","    cleaned: List[str] = []\n","    for t in topics:\n","        t = str(t).strip()\n","        for v in VALID_TOPICS:\n","            if t.lower() == v.lower():\n","                cleaned.append(v)\n","                break\n","    cleaned = list(dict.fromkeys(cleaned))[:3]\n","    return cleaned or [\"Other\"]\n","\n","\n","# 10.3 ROLE-BASED Q&A GENERATION\n","def generate_qa(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    qa_task = QARole(temperature=0.15)\n","    qas = qa_task.run(first_chunk)\n","\n","    lines: List[str] = []\n","    if isinstance(qas, list):\n","        for qa in qas:\n","            q = str(qa.get(\"q\", \"\")).strip()\n","            a = str(qa.get(\"a\", \"\")).strip()\n","            if q:\n","                lines.append(f\"Q: {q}\")\n","            if a:\n","                lines.append(f\"A: {a}\")\n","    return \"\\n\".join(lines).strip()\n","\n","\n","# 10.4 ROLE-BASED KEY CONCEPT EXTRACTION\n","def generate_concepts(transcript: str) -> str:\n","    first_chunk = chunk_text(transcript)[0]\n","    concept_task = ConceptRole(temperature=0.22)\n","    concepts = concept_task.run(first_chunk)\n","\n","    cleaned: List[str] = []\n","    if isinstance(concepts, list):\n","        for c in concepts:\n","            c2 = str(c).strip()\n","            if c2:\n","                cleaned.append(c2)\n","    return \", \".join(cleaned)\n","\n","\n","#####################################################################\n","# 11. MAIN PIPELINE — GENERATION ONLY (NO EVALUATION)\n","#####################################################################\n","def run_pipeline() -> pd.DataFrame:\n","    df = pd.read_excel(INPUT_FILE)\n","    results: List[Dict[str, Any]] = []\n","\n","    if FINAL_OUTPUT_FILE.exists():\n","        old = pd.read_excel(FINAL_OUTPUT_FILE)\n","        if \"row_index\" in old.columns:\n","            done = set(old[\"row_index\"])\n","            results = old.to_dict(orient=\"records\")\n","            print(f\"Resuming: {len(done)} rows already processed.\")\n","        else:\n","            done = set()\n","    else:\n","        done = set()\n","\n","    for idx, row in df.iterrows():\n","        if idx in done:\n","            print(f\"Skipping row {idx} (already done)\")\n","            continue\n","\n","        title = str(row.get(\"title\", \"\")).strip()\n","        transcript = str(row.get(\"transcript\", \"\")).strip()\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(f\"PROCESSING ROW {idx}: {title}\")\n","        print(\"=\"*80)\n","\n","        try:\n","            summary  = generate_summary(transcript)\n","            topics   = classify_topic(transcript, summary)\n","            qa_text  = generate_qa(transcript)\n","            concepts = generate_concepts(transcript)\n","        except Exception as e:\n","            logger.error(f\"Error row {idx}: {e}\")\n","            summary, topics, qa_text, concepts = \"\", [\"Other\"], \"\", \"\"\n","\n","        print(\"\\nSUMMARY:\\n\", summary)\n","        print(\"\\nTOPICS:\\n\", topics)\n","        print(\"\\nQ&A:\\n\", qa_text)\n","        print(\"\\nKEY CONCEPTS:\\n\", concepts)\n","\n","        rec = {\n","            \"row_index\": idx,\n","            \"title\": title,\n","            \"summary\": summary,\n","            \"topic_classification\": \", \".join(topics),\n","            \"Q_and_A\": qa_text,\n","            \"key_concepts\": concepts\n","        }\n","        results.append(rec)\n","\n","        pd.DataFrame(results).to_excel(FINAL_OUTPUT_FILE, index=False)\n","        print(f\"Saved row {idx} → {FINAL_OUTPUT_FILE}\")\n","\n","    df_out = pd.DataFrame(results)\n","    df_out.to_excel(FINAL_OUTPUT_FILE, index=False)\n","    print(\"\\nDONE. Final file saved:\", FINAL_OUTPUT_FILE)\n","    return df_out\n","\n","\n","#####################################################################\n","# 12. RUN GENERATION ONLY\n","#####################################################################\n","df_out = run_pipeline()\n","print(\"\\nRole-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfOIviuyIekh","executionInfo":{"status":"ok","timestamp":1763749382288,"user_tz":-330,"elapsed":1919840,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"994a6c1c-6287-4ce2-b4bb-24fb31fd202f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Input file: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Output folder: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant\n","Groq key loaded ✓\n","\n","================================================================================\n","PROCESSING ROW 0: Reinforcement Learning through Human Feedback - EXPLAINED! | RLHF\n","================================================================================\n","\n","SUMMARY:\n"," Reinforcement learning with human feedback is a framework that integrates human guidance into the training process of a reinforcement learning algorithm, accelerating the learning process and enabling more informed decisions. In the context of Chat GPT, human feedback is provided via the rewards model, which assesses the quality of generated responses. This iterative training process enhances Chat GPT's capabilities, making it a powerful tool for generating high-quality responses. The rewards model can be applied to various reinforcement learning algorithms, including DQ learning and proximal policy optimization, to assess and score the quality of answers generated by Chat GPT, ultimately improving its performance.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Generative AI', 'Deep Learning']\n","\n","Q&A:\n"," Q: When has feedback from others made a noticeable impact on your decision-making or learning?\n","A: When learning something new when has feedback from others made a noticeable impact on your decision-making or learning this could be any experience that you had in your life\n","Q: How does human feedback contribute to reinforcement learning as illustrated with Frank's grid world adventure?\n","A: it accelerates the learning process\n","Q: What is the primary purpose of the rewards model in chat GPT?\n","A: to assess and score the quality of answers generated by chat GPT\n","Q: When is the rewards model used in chat GPT's training process?\n","A: once the rewards model is trained it should be able to assess how good a given answer is to a given question\n","Q: What is the name of the algorithm used to fine-tune chat GPT in the second part of the rewards model?\n","A: proximal policy optimization\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Human Feedback, Q-Learning, Deep Q-Networks, Proximal Policy Optimization, Chat GPT, Reward Model, GPT Architecture, Proximal Policy Optimization Algorithm, Grid World, Reinforcement Learning Algorithm, Loss Function\n","Saved row 0 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 1: Soft Margin SVM and Kernels with CVXOPT - Practical Machine Learning Tutorial with Python p.32\n","================================================================================\n","\n","SUMMARY:\n"," This tutorial explores the use of CVX opt and kernels in Support Vector Machines (SVMs), focusing on visualization and the impact of kernel injection. CVX opt is a quadratic programming solver that minimizes the equation 1/2 * x^T * P * x + q^T * x, subject to constraints G * x <= H and a * x = b. The tutorial provides example code and references to further learning resources, including Christopher Bishop's book on pattern recognition and machine learning. It covers a simple quadratic programming problem, illustrating the solution process, and discusses the implementation of SVMs using the CVXOPT library. Key concepts include quadratic programming, solver mechanics, and numpy module usage. The lecture demonstrates the use of different types of data and how the choice of kernel affects the classification results. A kernel in machine learning is a function that takes data as input and returns a feature space representation, crucial in SVMs for classification and regression tasks. SVMs can be extended to multi-class classification by using techniques such as one-vs-one or one-vs-all. The tutorial provides a comprehensive overview of the key components and processes involved in implementing SVMs using CVXOPT, with additional resources available for further learning.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What is the main topic of this tutorial?\n","A: working with CVX opt and working with kernels being applied to our support Vector machine\n","Q: Why is CVX opt useful in this case?\n","A: so you can see directly the impact of a kernel and where it's actually being injected and change and modifying the initial formal support Vector machine\n","Q: How do kernels affect the support Vector machine?\n","A: how they affect and also visualizing nonlinear and also um visualizing in the soft margin\n","Q: When is CVX opt not useful?\n","A: beyond that honestly CVX opt is not something you're probably ever going to use\n","Q: Who wrote the book 'Pattern Recognition and Machine Learning'?\n","A: Christopher Bishop\n","\n","KEY CONCEPTS:\n"," Machine Learning, Support Vector Machine, CVX Opt, Kernels, Quadratic Programming, Lib SVM, PyTorch, Soft Margin, Nonlinear, Optimization, Quadratic Equation, Support Vector\n","Saved row 1 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 2: How to create high-quality outputs with ChatGPT Prompt Engineering | Unlocking the Power of Prompts\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is the foundation of generating accurate text outputs from large language models. It involves designing inputs, known as prompts, that provide context and constraints for the models to produce desired outputs. Prompts can take various forms, including question prompts, statement prompts, and those with multiple inputs or constraints. The key features of prompts, such as length, language, context, and constraints, define what is expected and how it should be done. By understanding these features and constraints, users can create effective prompts that achieve accurate outputs. This requires deconstructing prompts into individual components, allowing users to tailor their prompts to specific needs and applications.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Generative AI']\n","\n","Q&A:\n"," Q: What are the inputs given to our prompt engineering models?\n","A: They can take many forms and can vary in the complexity and the context, called prompts.\n","Q: Why are prompts important in prompt engineering?\n","A: They are the starting point for generating the text outputs and provide the context and constraints for the large language models.\n","Q: How can understanding the key features of prompts help in prompt engineering?\n","A: It can help you choose the right prompt for your desired output and provide more context and information in the prompt to get the accurate and efficient output.\n","Q: What are the two key things that define a prompt?\n","A: The what you expect and how you want it to be done.\n","Q: What is the process of breaking down a prompt into individual components called?\n","A: Deconstruction of the prompt.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Prompts, Large Language Models, GPT, Google Bard, Island of Large Language Model, Question Prompts, Statement Prompts, Multiple Inputs, Constraints, Prompt Deconstruction, Prompt Components\n","Saved row 2 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 3: LangGraph Crash Course #3 - Agents & Tools - Intro\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence (AI) agents are problem solvers that can think and act autonomously, using tools to complete tasks. The React Agent Pattern, also known as Reasoning + Acting, mimics human thinking by combining thinking, action, and observation. This pattern enables Large Language Models (LLMs) to reason, act, and observe, using tools such as API calls or Python functions to solve problems. By equipping the LLM with tools, an agent is created, enabling it to make decisions and take actions autonomously. The React Agent Pattern is a fundamental concept in AI, crucial for building intelligent systems that can think and act independently.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Prompt Engineering', 'LangChain']\n","\n","Q&A:\n"," Q: What are AI agents in the context of Artificial Intelligence?\n","A: Agents are the problem solvers of the Artificial Intelligence (AI) World, capable of thinking on their own.\n","Q: What is the purpose of tools in the context of AI agents?\n","A: Tools are specific functions that agents can use to complete tasks, like a chef's kitchen tool or a calculator tool.\n","Q: What is the React Agent Pattern, and what does it stand for?\n","A: The React Agent Pattern stands for Reasoning Plus Acting, mimicking how human beings think.\n","Q: What is the purpose of the 'think' component in the React Agent Pattern?\n","A: The 'think' component is where the LLM first thinks about the user's prompt or problem.\n","Q: What is the purpose of the 'observe' component in the React Agent Pattern?\n","A: The 'observe' component is where the LLM observes the output of the tool, to determine if the problem is solved or not.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI) Agents, React Agent Pattern, Reasoning + Acting, LLM (Large Language Model), Tools, Control Flow, Lang Chain, Lang Graph, API Calls, Python Functions, Agent, Autonomous Decisions\n","Saved row 3 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 4: LangGraph Crash Course #9 - Reflection Agent - LangSmith Tracing\n","================================================================================\n","\n","SUMMARY:\n"," The reflection agent system is analyzed to understand its functionality and integration with other systems. By dissecting the system, its strengths and weaknesses can be identified, leading to improvements in the tweet generation process. The lecturer demonstrates how to use LangChain with LSmith, highlighting the importance of understanding the workflow and feedback loops between agents. A reflection agent is used to critique and refine a generated tweet, with multiple iterations of generation and reflection resulting in a final, viral tweet. This process illustrates the power of reflection agents in complex tasks, and the lecturer will explore another type of reflection agent, the reflexion agent, in the next section. The analysis focuses on the system's architecture and how it contributes to the final output, enabling the development of a comprehensive system that effectively generates a viral tweet.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Prompt Engineering', 'LangChain']\n","\n","Q&A:\n"," Q: What is the purpose of tracing the reflection agent system?\n","A: to understand exactly what is happening where\n","Q: Why is it necessary to understand the reflection agent system?\n","A: so that we can understand how both of these systems are working together\n","Q: How will tracing the reflection agent system help in delivering the final refined viral tweet?\n","A: to deliver our final refined viral tweet\n","Q: Who is the author referring to when mentioning the website 'smith.chain'?\n","A: no specific person is mentioned, but rather a website 'smith.chain'\n","Q: When will the tracing of the reflection agent system take place?\n","A: no specific time is mentioned, but rather a general statement about tracing the system\n","\n","KEY CONCEPTS:\n"," Reflection Agent System, ML, NLP, Agent System, Viral Tweet, Smith Chain, Agent, System, Agentic AI, Machine Learning, Natural Language Processing, Refined Tweet\n","Saved row 4 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 5: LangChain Crash Course #7 - Chat Models - Setup\n","================================================================================\n","\n","SUMMARY:\n"," To work with LangChain chat models, install the 'langchain' package and import the 'ChatOpenAI' class from the 'langchain.openai' module. Initialize the model with a chosen AI model, such as 'gpt-40' or 'gpt-3', and use the 'invoke' keyword to make API calls to OpenAI's chat models. The 'invoke' keyword is a magic keyword used throughout the course. When making API calls, the API key is required and must be provided. LangChain chat models provide a simple interface for accessing APIs and retrieving specific data. A simple prompt can be passed to the Large Language Model (LLM) to calculate the square root of 49. To improve response accuracy, the LLM can be sent an entire conversation history, including human and AI interactions, to leverage context from previous interactions. This technique is crucial for developing more sophisticated AI models with human-like conversation capabilities.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Prompt Engineering', 'LangChain']\n","\n","Q&A:\n"," Q: What is the primary focus of this section?\n","A: working with Lang chains chat models\n","Q: Why are we predominantly working with open Artificial Intelligence (AI) APIs?\n","A: no specific reason is mentioned in the transcript\n","Q: How do you install the L chain Das open aai package?\n","A: you have to install this package L chain Das open aai by copying the command and running it in the terminal\n","Q: When is the latest model released by open aai?\n","A: no specific time is mentioned in the transcript\n","Q: Who is the target audience for this section?\n","A: no specific audience is mentioned in the transcript\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), LangChain, Chat Models, Open AI APIs, Llama, LLM, GBT-40, GPT-3, Chat Open AI Class, Model Initialization, API Installation\n","Saved row 5 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 6: Python Training Course - Python Sort List\n","================================================================================\n","\n","SUMMARY:\n"," In Python, the sort method can handle lists containing strings and numbers, but it prioritizes numbers over strings due to its lexicographic comparison. When sorting a list with both string and number elements, Python places numbers first and strings second. This behavior also applies to mixed-case strings, where uppercase strings are placed before lowercase strings. To control this behavior, all elements can be converted to either lowercase or uppercase. The sort method can be reversed to sort in descending order, which affects the placement of numbers and strings accordingly.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science', 'Statistics']\n","\n","Q&A:\n"," Q: What happens when you sort a list containing strings with both uppercase and lowercase letters?\n","A: Sort puts the words that have a capital uppercase letter first and it sorts them alphabetically and then it sorts the ones with the lowercase first letter alphabetically.\n","Q: Why might you need to make sure all strings are either all lowercase or all uppercase when sorting?\n","A: If you didn't want this to happen and you were doing it for real you might have to make sure they were all lowercase or all uppercase if you wanted to sort them in a particular way.\n","Q: How does Python handle sorting lists that contain both strings and numbers?\n","A: Python can handle the sort method can handle lists that contain both strings and numbers but it puts the numbers first and the strings that's just how it works.\n","Q: When you insert a number into a list containing strings, what happens to the list?\n","A: What we have is this list with 18 here now let's sort that and see what we get.\n","Q: Who might benefit from watching this video?\n","A: I hope that was helpful if it was please click like subscribe to the channel and I'll see you in the next one\n","\n","KEY CONCEPTS:\n"," List, Strings, Sort, Method, Function, Code, Python, Insert, Index, Reverse, Alphabetical Order\n","Saved row 6 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 7: Humans vs. AI: Who should make the decision?\n","================================================================================\n","\n","SUMMARY:\n"," The optimal decision-maker for a task depends on the confidence level of the Artificial Intelligence (AI) system. Humans perform better when the AI is unsure, as their performance curves are flatter, indicating they can make better decisions in uncertain cases. AI excels in high-confidence cases but falters in uncertain situations. Augmented intelligence combines human decision-making with AI, achieving higher success rates than either approach alone. However, its effectiveness relies on mitigating human cognitive bias, particularly automation bias. The integration of human and AI decision-making can lead to more effective outcomes by minimizing cognitive bias. This collaboration between humans and AI can improve decision-making outcomes by leveraging the strengths of both parties. Ultimately, understanding who to ask for a decision can lead to better outcomes, with a combination of human and AI being referred to as augmented intelligence.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: Who should make a decision, a human or an artificial intelligence?\n","A: A fascinating combination of holistic curves and human bias.\n","Q: What is the purpose of a fraud detection system?\n","A: The system generates the alerts of potentially fraudulent transactions.\n","Q: How does an Artificial Intelligence (AI) performance curve typically look?\n","A: Very low confidence scores, this is not a real alert, and very high confidence scores, this is a real alert.\n","Q: When is a human likely to do a better job than an Artificial Intelligence (AI)?\n","A: At a 50 percent confidence level.\n","Q: Why is a human likely to do a better job than an Artificial Intelligence (AI) at a 50 percent confidence level?\n","A: Often not quite as accurate as a very confident Artificial Intelligence (AI) algorithm, but a little better at making the right decision when the Artificial Intelligence (AI) is unsure.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Decision Making, Fraud Detection, Success Rate, Confidence Score, Performance Curve, Human Bias, Holistic Curves, False Positives, Machine Learning, Natural Language Processing, Agentic AI\n","Saved row 7 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 8: Build generative apps faster with Vertex AI\n","================================================================================\n","\n","SUMMARY:\n"," Dimitrius, a product manager at Google Cloud AI, presented six new Vertex AI APIs to address technical challenges in building generative applications for enterprises. The APIs, including document understanding, improved embedding, vector search, ranking, grounded generation, and fact-checking, leverage Google's expertise to provide high-quality, unique solutions to common problems. These APIs offer clear interfaces, allowing developers to seamlessly integrate them into their workflow and combine them with other APIs to build custom solutions, thereby facilitating the development of innovative generative applications.\n","\n","TOPICS:\n"," ['Generative AI', 'Natural Language Processing', 'Deep Learning']\n","\n","Q&A:\n"," Q: What is the main challenge developers face when building generative applications for Enterprises?\n","A: how do you make those applications reliably access the right Enterprise data in order to produce responses which are accurate and consistent\n","Q: Why did Google develop new APIs for Vertex AI?\n","A: to effectively solve technical challenges that developers have to face\n","Q: How do the new Vertex AI APIs help developers?\n","A: to lighten the load and focus on building what is really unique for their own use case\n","Q: When were some of the Geo models in Vertex AI launched?\n","A: some time\n","Q: Who is responsible for the development of the new Vertex AI APIs?\n","A: Dimitrius Case, a product manager within Cloud AI\n","\n","KEY CONCEPTS:\n"," Document Understanding API, Embedding API, Vector Search, Ranking API, Grounded Generation API, Check Grounding API, Vertex AI, Generative Applications, Enterprise Data, LLM Model, Fine-Tuned Model, Google Knowledge Graph\n","Saved row 8 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 9: Unitary Transformations\n","================================================================================\n","\n","SUMMARY:\n"," The Singular Value Decomposition (SVD) of a matrix X is decomposed into U, Σ, and V^T, where U and V are unitary matrices that preserve angles and lengths of vectors. The economy SVD is a reduced version of SVD, considering only the first M columns of U and the first M by M sub-block of Σ. Unitary transformations, such as the Fourier transform, rotate vectors without changing their angles and lengths. Geometrically, the SVD can be interpreted as a transformation that maps a sphere of unit vectors into an ellipsoid, with singular values determining elongation and left singular vectors determining orientation, highlighting the importance of unitary matrices in data-driven transformations.\n","\n","TOPICS:\n"," ['Statistics', 'Deep Learning', 'Data Science']\n","\n","Q&A:\n"," Q: What is the property of unitary matrices?\n","A: They preserve the angles between any two vectors in the vector space that they're transforming.\n","Q: What is the effect of unitary transformations on the lengths of vectors?\n","A: They preserve the lengths of vectors.\n","Q: What is the geometric interpretation of the matrix X?\n","A: It can multiply vectors on the right or on the left, corresponding to a change or rotation of space.\n","Q: What is the role of singular values in the geometric interpretation of the SVD?\n","A: They determine the elongation or squishing of the ellipsoid in the vector space.\n","Q: What is the property of unitary matrices in complex-valued vector spaces?\n","A: They preserve the angles between any two vectors in the vector space that they're transforming, and also preserve the lengths of vectors.\n","\n","KEY CONCEPTS:\n"," Singular Value Decomposition (SVD), Unitary Matrices, Economy Size SVD, Inner Product, Complex Conjugate Transpose, Real Valued Data, Complex Valued Data, Geometric Interpretation, Principal Axes, Ellipsoid, Row Space, Column Space\n","Saved row 9 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 10: Hands On With Google Gemini 1.5 Pro- Is this the Best LLM Model?\n","================================================================================\n","\n","SUMMARY:\n"," Google Gemini Pro 1.5 is a multi-model AI application that enables multimodal processing with a context window of up to 1 million tokens. This allows for accurate extraction of comedic moments and identification of scenes from abstract details. The model's responses can be imperfect, but it can correctly identify time codes in transcripts. Google's Generative AI has a massive token capacity, enabling the development of complex applications. The API key can be created for free on ai.google.com, with a limited number of requests. The API is used to provide an amazing experience to end-users, as seen in clients like Mercedes. The model can process 1 million tokens, generate 1 hour of video, 11 hours of audio, and 30k lines of code. The model can respond to text-based queries, including philosophical questions, and process image-based queries, displaying responses in markdown format. The lecture discusses setting up prompt feedback and streaming options for AI responses, showcasing the capabilities of the Gemini Pro model in generating text from images and combining text and images. Google G Pro 1.5 has enhanced features, including the ability to write responses along with prompts and combine them with images, offering improved performance compared to its predecessor. The model is part of a larger competition in the field of language models, but it remains a strong contender. The video showcases the potential of Google G Pro 1.5 as a powerful tool for generating engaging content.\n","\n","TOPICS:\n"," ['Generative AI', 'Artificial Intelligence', 'Deep Learning']\n","\n","Q&A:\n"," Q: Who is the speaker in this video?\n","A: my name is krishn\n","Q: What is the topic of this specific video?\n","A: how you can actually build generative Artificial Intelligence (AI) powered application\n","Q: What is Google Gemini Pro 1.5?\n","A: a kind of a multi model\n","Q: What will be discussed in the first 1 minute of the video?\n","A: a demo video that Google has probably come up with\n","Q: What is the experimental feature in the newest model Gemini 1?\n","A: a demo of long context understanding\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Generative AI, Google Gemini Pro, Multi-model, API key, Long context understanding, Experimental feature, Model, Application, Agentic AI, Natural Language Processing (NLP), Machine Learning (ML)\n","Saved row 10 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 11: How to evaluate large language models using Prompt Engineering | Testing and Improving with PyTorch\n","================================================================================\n","\n","SUMMARY:\n"," Evaluating prompt engineering models involves using matrices such as perplexity, accuracy, and human evaluation to measure performance and identify areas for improvement. Techniques for debugging and improving models include analyzing generated responses, identifying common errors, and fine-tuning the models. Testing models on different data sets and tasks is crucial to determine their ability to generalize and perform well on new or unseen data. This process is ongoing, requiring continuous evaluation and testing to ensure the model's performance. Ongoing evaluation and refinement are key to achieving optimal performance in prompt engineering models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the purpose of reevaluating and testing prompt engineering models?\n","A: to test them in different matrices for evaluate prompt engineering models\n","Q: Why is it important to have a good understanding of the matrices used to evaluate prompt engineering models?\n","A: to measure the performance of the model and its ability to generate accurate and meaningful responses\n","Q: How do you measure the performance of a language model?\n","A: using matrices such as perplexity and accuracy\n","Q: When is it important to evaluate and test prompt engineering models?\n","A: as you continue to use your model and generate responses\n","Q: Who evaluates the quality of the responses in human evaluation?\n","A: human\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Perplexity, Accuracy, Human Evaluation, Language Model, Cross Validation, Model Evaluation, Model Testing, Generalization, Model Fine-Tuning, Visualization Tools, Cross Validation\n","Saved row 11 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 12: Generative AI vs AI agents vs Agentic AI\n","================================================================================\n","\n","SUMMARY:\n"," Generative Artificial Intelligence creates new content based on patterns learned from existing data, whereas Artificial Intelligence agents take input, think, and act to complete tasks. Agentic Artificial Intelligence involves multiple agents working autonomously to achieve complex goals, utilizing tools, memory, and other agents. This complexity increases with each level, from generative AI to AI agents to agentic AI, enabling the performance of more sophisticated tasks.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Generative AI', 'Agentic AI']\n","\n","Q&A:\n"," Q: What is the primary difference between generative Artificial Intelligence (AI) and Artificial Intelligence (AI) agents?\n","A: Generative Artificial Intelligence (AI) is an Artificial Intelligence (AI) that can create new content, whereas an Artificial Intelligence (AI) agent is a program that takes input, thinks, and acts to complete a task.\n","Q: How does an Artificial Intelligence (AI) agent make decisions?\n","A: An Artificial Intelligence (AI) agent makes decisions using tools, memory, and other agents to reach a goal.\n","Q: What is the key characteristic of agentic Artificial Intelligence (AI) systems?\n","A: Agentic Artificial Intelligence (AI) systems have one or more Artificial Intelligence (AI) agents that work autonomously, often over long, complex tasks, making decisions using tools and other agents to reach a goal.\n","Q: What is the primary function of a generative Artificial Intelligence (AI) in an agentic Artificial Intelligence (AI) system?\n","A: The primary function of a generative Artificial Intelligence (AI) in an agentic Artificial Intelligence (AI) system is to provide the core component of the system.\n","Q: What is the primary goal of an agentic Artificial Intelligence (AI) system?\n","A: The primary goal of an agentic Artificial Intelligence (AI) system is to perform complex tasks autonomously, often involving multi-step reasoning, planning, and coordination.\n","\n","KEY CONCEPTS:\n"," Generative Artificial Intelligence (AI), Large Language Model (LLM), Artificial Intelligence (AI) Agent, Agentic Artificial Intelligence (AI), Multi-Step Reasoning, Multi-Step Planning, Autonomy, Tool Usage, Knowledge Graph, Natural Language Processing (NLP), N8N, Agno Framework\n","Saved row 12 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 13: Covariance in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," Covariance measures the relationship between two random variables, X and Y, by quantifying how their values change together. It is calculated as the sum of the product of the deviations of each data point from its mean, divided by the number of data points. A positive covariance indicates that as X increases, Y also tends to increase, while a negative covariance indicates that as X increases, Y tends to decrease. However, covariance does not provide information on the strength or direction of the relationship between X and Y, highlighting the need for further analysis. The Pearson correlation coefficient is used to address this limitation, providing a more comprehensive understanding of the relationship between random variables.\n","\n","TOPICS:\n"," ['Statistics', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the topic that will be discussed next in the lecture?\n","A: Variance covariance\n","Q: Why is variance covariance an important topic in data pre-processing or data analysis?\n","A: One of the very important topic altogether\n","Q: How is covariance measured?\n","A: Cobb\n","Q: When is the covariance formula used?\n","A: To quantify a relationship between two random variables\n","Q: Who can benefit from learning statistics and machine learning?\n","A: People who wants to actually learn statistics want to learn status things with respect to machine learning\n","\n","KEY CONCEPTS:\n"," Variance, Covariance, Random Variables, Data Pre-processing, Data Analysis, Machine Learning, Natural Language Processing, Agentic AI, Pearson Correlation Coefficient, Mean, Standard Deviation\n","Saved row 13 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 14: 3. Objective || End to End AI Tutorial\n","================================================================================\n","\n","SUMMARY:\n"," In reinforcement learning, the objective is to learn the optimal policy that maximizes a numerical reward signal. The agent interacts with an environment, making decisions based on observations, and aims to learn the optimal action to take in each step, which gives the maximum reward or cumulative reward over time. The reward signal provides feedback on the quality of the action, achieved through trial and error learning. The ultimate goal is to maximize the cumulative reward over time, which can be achieved through various methods, including value-based, policy-based, or a combination of both. This objective can be defined in different ways, such as in episodic tasks like Tic-Tac-Toe or continuous tasks like Stock Market trading, where the reward is based on the outcome or profit earned. The reward function is a mathematical expression that reflects the trading goal and preferences, and the agent's objective is to maximize the expected cumulative reward over time.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the ultimate goal of reinforcement learning?\n","A: to learn the optimal policy that maximizes a numerical reward signal\n","Q: How does an agent in reinforcement learning interact with its environment?\n","A: by making decisions based on the observation it received from the environment\n","Q: What is the role of the reward signal in reinforcement learning?\n","A: it provides feedback to the agent about the quality of its action\n","Q: What is the objective of defining rewards in reinforcement learning?\n","A: to help the agent learn to reach its goal\n","Q: How is the reward function parameterized in continuous tasks?\n","A: by defining a mathematical expression that reflects the agent's trading goal and preferences\n","\n","KEY CONCEPTS:\n"," Reinforcement Learning, Agent, Environment, Policy, Reward Signal, Cumulative Reward, Episodic Task, Continuous Task, Value Function, Policy-Based Method, Value-Based Method, Q-Learning, Deep Q-Networks\n","Saved row 14 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 15: Python Training - Python Dictionary Basics\n","================================================================================\n","\n","SUMMARY:\n"," In Python, a dictionary is a data type consisting of key-value pairs, referred to as items, which are separated by colons and items by commas. Dictionaries are declared within curly brackets, with keys being immutable. They are useful for mapping one item to another, such as stock prices, and are essential in Python, particularly in data science. Key functions associated with dictionaries include items, keys, and values. Dictionaries can be created using the dict function, paired lists, and the zip function. Values can be accessed and changed using square brackets, and deleted using the del function. Dictionaries work well with pandas, making them a fundamental structure in Python for data manipulation and analysis.\n","\n","TOPICS:\n"," ['Python Programming', 'Data Science']\n","\n","Q&A:\n"," Q: What is another data type in Python?\n","A: the dictionary\n","Q: Why are keys in a dictionary immutable?\n","A: it has to be something that's immutable so it can't change\n","Q: How are key value pairs separated in a dictionary?\n","A: the key value pairs are separated by a colon and the items are separated by a comma\n","Q: What is the purpose of the zip function in creating a dictionary?\n","A: it takes two lists and it pairs them together and it creates a tuple of the corresponding values in each list\n","Q: How do you access a value associated with a key in a dictionary?\n","A: we look at what we've got here and we've said okay the dictionary called D I see we want to access the key that is five\n","\n","KEY CONCEPTS:\n"," Dictionary, Key-Value Pair, Immutable, Tuple, Zip Function, List, Pandas, Data Science, Data Type, Python, Function, Variable\n","Saved row 15 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 16: Fight Insider Threats with AI-infused SIEM\n","================================================================================\n","\n","SUMMARY:\n"," Artificial Intelligence (AI) can significantly enhance an organization's security posture by expediting the identification and containment of data breaches. According to IBM's 2023 report, organizations utilizing AI and automation extensively contained breaches 108 days faster than those without. AI-powered User Behavior Analytics (UBA) can effectively detect and respond to Insider threats, a major concern with an average cost of $4 million per incident. UBA integrates with Security Information and Event Management (SIEM) solutions to streamline processes, enhance skills, and provide actionable insights, enabling organizations to stay ahead of emerging threats. By integrating AI and automation, organizations can accelerate investigations, freeing security analysts to focus on proactive defense efforts and mitigate the risk of Insider threats.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is a potential benefit of using Artificial Intelligence (AI) in security?\n","A: it took 108 fewer days on average to identify and contain a data breach\n","Q: Why is Insider threat a major concern for organizations?\n","A: Insider threats are a major concern for organizations of all sizes\n","Q: How can User Behavior Analytics (UBA) with AI and machine learning help in security?\n","A: detect and respond to Insider threats quickly and precisely\n","Q: When is the report based on a survey of over 500 organizations?\n","A: 2023\n","Q: Who is the source of the cost of a data breach report?\n","A: IBM\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Machine Learning, User Behavior Analytics (UBA), Insider Threats, Data Breach, Security Posture, Emerging Threats, Automation, IBM's Cost of a Data Breach Report, Security Team, Organizational Security\n","Saved row 16 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 17: Meta Llama 3 Is Here- And It Will Rule the Open Source LLM Models\n","================================================================================\n","\n","SUMMARY:\n"," This video introduces the YouTube channel 'Krishak' and discusses the Meta Llama 3, an open-source large language model developed by Meta. Llama 3 surpasses its predecessor in performance metrics, featuring 8 billion and 70 billion pre-trained and instruction-tuned versions, supporting various applications. It excels in language nuances, contextual understanding, and complex tasks like translation and dialog generation, with enhanced scalability and performance. The transcript also highlights the performance of MML, an open-source model, in comparison to GPQ and other models, demonstrating high accuracy and outperforming GPQ in certain aspects. Additionally, the video discusses the comprehensive responsibility approach taken by companies and provides transparency through the model card available on Meta, Hugging Face, and Kaggle. The key to accessing Meta Llama 3 lies in following the provided instructions and obtaining approval for access, which will enable local inference and allow users to try out the models.\n","\n","TOPICS:\n"," ['Other']\n","\n","Q&A:\n"," Q: Who is the host of this YouTube channel?\n","A: My name is Krishak.\n","Q: What time is it currently?\n","A: It is 2 a.m.\n","Q: Why is this video being recorded?\n","A: No reason is given in the transcript.\n","Q: How is the host greeting the audience?\n","A: Hello, my name is Krishak.\n","Q: When is the video being recorded?\n","A: Right now.\n","\n","KEY CONCEPTS:\n"," Natural Language Processing (NLP), Machine Learning (ML), Agentic AI, Named Entity Recognition (NER), Part-of-Speech (POS) Tagging, Text Preprocessing, Tokenization, Sentiment Analysis, Deep Learning, Reinforcement Learning, Chatbots, Language Model\n","Saved row 17 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 18: Getting Started With sklearn\n","================================================================================\n","\n","SUMMARY:\n"," The lecture focuses on the Python library scikit-learn and its application in machine learning. The instructor utilizes the library's documentation to explore the Naive Bayes algorithm, specifically Gaussian Naive Bayes, which is used to create a decision boundary in a classification problem. This algorithm is part of a range of machine learning algorithms provided by scikit-learn, enabling classification and decision-making tasks. By the end of the next video or two, students will be able to write the code themselves, demonstrating their understanding of the library and its capabilities.\n","\n","TOPICS:\n"," ['Machine Learning', 'Python Programming', 'Deep Learning']\n","\n","Q&A:\n"," Q: What library will be used in this lesson?\n","A: scikit-learn, which is often abbreviated sk-learn.\n","Q: Why is Naive Bayes used in this lesson?\n","A: But first I want you to have you running the code. So sklearn Naive Bayes.\n","Q: How will the code be written?\n","A: And by the end of the next video or two, you will be able to write this code yourself.\n","Q: When will the code be written?\n","A: And by the end of the next video or two,\n","Q: Who will be using the code?\n","A: You're going to see all the steps that I went through when I wrote the Python code that just made that decision boundary.\n","\n","KEY CONCEPTS:\n"," Naive Bayes, scikit-learn, sk-learn, Google, Python library, decision boundary, algorithm, function, documentation, Gaussian Naive Bayes, derivation, formula, use cases\n","Saved row 18 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 19: Log Normal Distribution in Statistics\n","================================================================================\n","\n","SUMMARY:\n"," The lecture emphasized the significance of understanding various statistical distributions, including Gaussian and log normal distributions. A Gaussian distribution follows a bell curve, with 50% of data on either side of the mean, commonly found in datasets such as height and iris data. In contrast, log normal distributions exhibit a skewed curve with a long right tail, often observed in datasets like income and product reviews. Standard scaling, a crucial technique in machine learning, involves converting data to a standard normal distribution to enhance model accuracy. This is achieved by applying a formula to log-normal distributed data to convert it to a Gaussian distribution, which can then be scaled to a standard normal distribution, particularly useful in regression and classification algorithms.\n","\n","TOPICS:\n"," ['Statistics', 'Data Science', 'Machine Learning']\n","\n","Q&A:\n"," Q: What is the name of the distribution that is denoted by X belonging to a Gaussian distribution with a mean and Sigma?\n","A: Gaussian distribution\n","Q: Why is it important to learn various distributions in statistics?\n","A: To understand how data follows different patterns and to apply the right algorithms for analysis\n","Q: What is the property of a log normal distribution?\n","A: If the log of X is normally distributed, then X is log normally distributed\n","Q: What is the purpose of standard scaling in machine learning?\n","A: To scale down values to the same scale so that they can be analyzed using the same algorithm\n","Q: What is the formula to convert a Gaussian distribution to a standard normal distribution?\n","A: X = (X - μ) / σ\n","\n","KEY CONCEPTS:\n"," Gaussian Distribution, Normal Distribution, Log Normal Distribution, Standard Normal Distribution, StandardScaler, Empirical Formula, Bell Curve, Log Normal Deviate, Machine Learning, Sentiment Analysis, Regression Algorithm, Classification Algorithm, Domain Knowledge, Log Normal Distribution Property, Standard Deviation, Mean Value, Sigma Value, Gaussian Distribution Property, Standard Normal Deviate, Normalization, Log Normalization\n","Saved row 19 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 20: Deep learning project end to end | Potato Disease Classification Using CNN - 1 : Problem Statement\n","================================================================================\n","\n","SUMMARY:\n"," This video initiates an end-to-end deep learning project series in agriculture, focusing on disease detection in potato plants. The project aims to develop a mobile application that uses convolutional neural networks to identify early blight and late blight diseases. The application will enable farmers to take a picture of the plant, and the AI system will provide a diagnosis. The project involves data collection, model building, and deployment on Google Cloud, utilizing TF serving, FastAPI, and React Native. The goal is to prevent economic losses by early disease detection and treatment. The project requires a data scientist to work on the technical architecture and build the application end-to-end. A supervised machine learning project begins with data collection, specifically images of healthy potato plant leaves and those with early or late blight disease. Data cleaning and pre-processing involve using tf dataset and data augmentation to create diverse training samples. A CNN is used for image classification, and the trained model is exported to disk. The project also involves building a website in React JS to interact with the model and a mobile app development phase, where the model is converted to a TF lite model using quantization to reduce its size. The technology stack includes TensorFlow, CNN, data augmentation, and TF Dataset. The project showcases the application of machine learning concepts in a real-world scenario and provides a comprehensive learning experience for those looking to develop practical skills in machine learning and deep learning. By completing this series, viewers can gain hands-on experience and enhance their resume with a real-world project, which can be adapted to various applications and customized to suit individual interests.\n","\n","TOPICS:\n"," ['Deep Learning', 'Machine Learning', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the main problem faced by potato farmers?\n","A: Farmers who grow potatoes are facing lot of economic losses every year because of various diseases that can happen to a potato plant.\n","Q: Why is it important to accurately identify the disease in a potato plant?\n","A: The treatments for early blight and late blight are little different so it's important that you accurately identify what kind of disease is there in that potato plant.\n","Q: How will the mobile application help farmers?\n","A: the mobile application will tell them whether the potato plant is healthy or it has one of these diseases\n","Q: Who is working on the project to build the mobile application?\n","A: you are a data scientist working for AtliQ Agriculture\n","Q: What will be used behind the scenes in the mobile application to identify diseases?\n","A: it will be using deep learning and convolutional neural network\n","\n","KEY CONCEPTS:\n"," Machine Learning (ML), TF Serving, Fast API, Google Cloud (GCP), Google Cloud Functions, React Native, Convolutional Neural Network (CNN), Deep Learning, Artificial Intelligence (AI), Aggregation, Natural Language Processing (NLP), Agentic AI\n","Saved row 20 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 21: LangGraph Crash Course #2 - Levels of Autonomy in LLM applications\n","================================================================================\n","\n","SUMMARY:\n"," The levels of autonomy in Large Language Model (LLM) applications range from Code to Agent, with each level increasing in complexity and decision-making capabilities. Code has zero autonomy and is 100% deterministic, while Agent can make decisions, learn from mistakes, and have human review and approval. The key differences between Chain/Routers and Agents lie in the presence of cycles, time travel, and human review, which enable Agents to make intelligent decisions and refine tasks. The autonomy levels, from lowest to highest, are Code, LLM Call, Chains, Router, State Machine, and Completely Autonomous Agents, with Agent being the most advanced and capable level.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Prompt Engineering', 'Reinforcement Learning']\n","\n","Q&A:\n"," Q: What is the first level of autonomy in LLM applications?\n","A: Code has zero autonomy and is 100% deterministic.\n","Q: Why is it difficult to handle real-life complexity with code?\n","A: You would need to write rules for every possible scenario.\n","Q: How does a single LLM call work?\n","A: Your app basically does one main thing, it takes an input, processes it, and gives you back an output.\n","Q: When does a chain occur in LLM applications?\n","A: When we break down a task into steps where each AI is really good at one thing.\n","Q: Who makes decisions in a router LLM?\n","A: The Artificial Intelligence (AI) itself decides what steps to take next.\n","\n","KEY CONCEPTS:\n"," LLM, Autonomy, Code, LLM Call, Chains, Router, State Machine, Agent, Land Graph, Tools, Artificial Intelligence (AI), Machine Learning (ML), Natural Language Processing (NLP)\n","Saved row 21 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 22: Introduction to Advanced Topics in Prompt Engineering using Pre-Trained Large Language Models\n","================================================================================\n","\n","SUMMARY:\n"," This section delves into advanced topics in prompt engineering, encompassing the handling of diverse prompt types, fine-tuning pre-trained large language models through techniques like multitask learning and distillation, and data preprocessing and cleaning. Key concepts include tokenization, normalization, and data augmentation, as well as deploying prompt engineering models in production using frameworks such as TensorFlow Serving. Additionally, ethical considerations, including bias and fairness, are addressed. The aim is to equip users with the skills necessary to master prompt engineering and generate accurate outputs from various types of prompts, thereby enhancing the reliability and effectiveness of language models.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Deep Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What are some advanced topics in prompt engineering?\n","A: We'll explore more complex topics and techniques that will help you become an expert in prompt and generic.\n","Q: Why is data pre-processing and cleaning crucial in prompt engineering?\n","A: The quality of the data used to train from preserving models is crucial to the success.\n","Q: How can we handle different types of prompts in prompt engineering?\n","A: We'll learn how to preprocess these prompts and fine-tune pre-trained models to generate appropriate output.\n","Q: When is it necessary to deploy prompt engineering models in production?\n","A: Once you have built a prompt between model you need to deploy it in production to make it accessible to the vast majority of the users.\n","Q: Who should consider the ethical implications in prompt engineering?\n","A: It is important to consider the ethical applications of prompt engineering so in this section we will see what are the ethical implications in problem engineering like buyers fairness and privacy.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Multitasking Learning, Distillation, Tokenization, Normalization, Data Pre-processing, Data Augmentation, TensorFlow Serving, Flask App, Artificial Intelligence (AI) API, Fairness, Privacy, Pre-trained Large Language Models, Self-supervised Learning, Multitask Learning, Cross Entropy Loss, Adam Optimizer, Keras Library, ResNet 50, BERT 16, Inception, Logistic Regression, Multinomial Regression\n","Saved row 22 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 23: SVD: Eigen Action Heros [Matlab]\n","================================================================================\n","\n","SUMMARY:\n"," The lecturer employs Singular Value Decomposition (SVD) and eigenfaces to cluster action heroes Arnold Schwarzenegger and Sylvester Stallone. By computing the SVD of a matrix B, representing the difference between original images and their average face, the lecturer obtains eigenfaces as linear combinations of the original images. The lecturer projects the images onto the first three eigenfaces and plots the results, demonstrating good separation between Arnold and Stallone. This technique is also applied to Taylor Swift and Stallone, yielding better separation than with Arnold and Stallone. Although the method may be shallow and limited in capturing human face complexity, it remains a useful tool for image classification.\n","\n","TOPICS:\n"," ['Deep Learning', 'Natural Language Processing']\n","\n","Q&A:\n"," Q: What is the purpose of finding the eigenfaces of a bunch of faces?\n","A: to cluster two different people in face space\n","Q: Why is it necessary to subtract off the average face from all of the faces?\n","A: to compute the principal components\n","Q: How do you project an image into the principal component space?\n","A: by taking the image vector times the library matrix\n","Q: When is it possible to classify a new image as belonging to a particular class?\n","A: when the new image projects closer to the cluster of images in the principal component space\n","Q: Who are the two action heroes used in the example of eigenfaces?\n","A: Arnold Schwarzenegger and Sylvester Stallone\n","\n","KEY CONCEPTS:\n"," Eigenfaces, Singular Value Decomposition (SVD), Principal Component Analysis (PCA), Image Classification, Eigenvalues, Eigenvectors, Face Space, Image Compression, Deep Neural Network Architectures, Convolutional Neural Networks (CNNs), Image Recognition, Object Detection, Natural Language Processing (NLP), Agentic AI, Machine Learning (ML)\n","Saved row 23 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 24: LangChain Crash Course #3 - What is LangChain?\n","================================================================================\n","\n","SUMMARY:\n"," Lang chain is a framework that enables applications to reason with large language models (LLMs) while interacting with external APIs, databases, and other systems. It allows developers to build applications that can perform tasks such as booking flights and restaurants, and can be easily switched between different LLMs without modifying the code. Lang chain provides a way to integrate the reasoning ability of LLMs with real-world interactions, making it a powerful tool for building applications that require both AI and external connectivity. This framework has significant implications for industries and businesses, enabling AI to access private company databases, send emails, browse the internet, and scrape websites, ultimately allowing it to interact with the physical world and revolutionize the way companies operate and interact with customers.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'LangChain', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is the main limitation of large language models?\n","A: They are smart and can talk about travel but they cannot actually interact with the real world.\n","Q: Why do we need a framework like Lang chain?\n","A: To have the brains of an LLM and the ability to communicate with the real world.\n","Q: How does Lang chain act as a bridge between LLMs and the real world?\n","A: It acts as a bridge between the LLMs and the real world so to put it simply Lang chain is by far the most popular framework that helps build applications using LLMs.\n","Q: Who can access a lot of APIs with Lang chain?\n","A: The Artificial Intelligence (AI) that we're working with can access a lot of APIs.\n","Q: When can you easily switch out GP4 with a free Hugging Face LLM using Lang chain?\n","A: In the future if you want to switch out GP4 with let's say a free Hugging Face LLM if you're shot on cash let's say you can easily do so without even touching the code that you wrote with L chain.\n","\n","KEY CONCEPTS:\n"," LLM (Large Language Model), Lang Chain, APIs (Application Programming Interfaces), Real-world Interaction, Reasoning Ability, Training Data, Real-world Communication, Frameworks, GP4, Hugging Face LLM, Booking APIs, Database Integration\n","Saved row 24 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 25: How To Use Residuals For Time Series Forecasting\n","================================================================================\n","\n","SUMMARY:\n"," Residuals in time series analysis are the difference between the fitted value and the actual value, used to diagnose model performance and detect trends or inconsistencies. Key points to look for include no autocorrelation or partial autocorrelation, and a mean of zero. Residuals can be used to improve the model by adding or subtracting shifting data to offset bias in the forecast. The Young Box test can quantify correlation in residuals, and if residuals are not independently distributed, it indicates serial correlation. A histogram of residuals can indicate bias, with a mean close to zero indicating unbiased forecasting. Residual analysis is crucial for understanding model performance and improving forecasting models, allowing for the identification of issues and correction in subsequent iterations.\n","\n","TOPICS:\n"," ['Time Series', 'Statistics', 'Python Programming']\n","\n","Q&A:\n"," Q: What are residuals in time series analysis?\n","A: The difference between the fitted value and the actual value of the time series.\n","Q: Why is residual analysis important in forecasting methods?\n","A: To diagnose performance and improve the model.\n","Q: How can we use residual analysis to improve our forecasting methods?\n","A: To detect trends or inconsistencies in the model and make corrections.\n","Q: When should the mean of the residual be zero?\n","A: When the residuals are not biased, indicating equal over- and under-forecasting.\n","Q: Who can use residual analysis to improve their forecasting methods?\n","A: Data scientists, such as myself, Eagle, who can use Python to analyze residuals and improve forecasting methods.\n","\n","KEY CONCEPTS:\n"," Residuals, Residual Analysis, Time Series, Autocorrelation, Partial Autocorrelation, Young Box Test, Holt Winters Model, Exponential Smoothing, Trend, Seasonality, Level, Fitted Values, Error\n","Saved row 25 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 26: Build a Text-to-SQL Agent for Smarter Database Queries\n","================================================================================\n","\n","SUMMARY:\n"," This lecture demonstrates building an Artificial Intelligence (AI) agent that connects to a database using SQL knowledge. The agent is built using LangGraph, Next.js, and models running on WatsonX AI, with an in-memory database using SQLite. The agent is designed to interact with a large language model, allowing users to submit questions and receive responses. A ReAct agent is implemented in VS Code to generate jokes about SQL, connecting to models on WatsonX.ai using LangChain and LangGraph libraries. The agent is set up to deserialize messages from the frontend, set up the chat interface, and return the latest message. A WatsonX AI project is created with environment variables and state variables, enabling sending a message to the large language model. The agent is enhanced with a loading state to prevent confusion when sending messages to the large language model. A database is created using SQLite 3 and defined server-side functions for interacting with the database, including connecting to SQLite, creating tables, and seeding data. The key takeaway is that a well-structured database setup is crucial for efficient data retrieval and manipulation. A text-to-SQL agent requires guardrails to prevent the Data Loss Mitigation (DLM) from having unlimited control over the database, and LangGraph can be used to create ReAct agent models available on WatsonX.ai. The agent is tested with complex system prompts and queries, demonstrating its ability to generate SQL queries and retrieve data from a database.\n","\n","TOPICS:\n"," ['Artificial Intelligence', 'Natural Language Processing', 'Prompt Engineering']\n","\n","Q&A:\n"," Q: What AI agent will be built in this video?\n","A: An Artificial Intelligence (AI) agent that's able to talk to your database.\n","Q: Why have large language models been trained on code?\n","A: Including SQL.\n","Q: How will the ReAct agent be built?\n","A: We'll be using LangGraph to build a ReAct agent.\n","Q: What will be used for the frontend application?\n","A: Next.js.\n","Q: What will be used as the in-memory database?\n","A: SQLite.\n","\n","KEY CONCEPTS:\n"," Artificial Intelligence (AI), Agent, Large Language Models, SQL, LangGraph, ReAct, Next.js, WatsonX AI, SQLite, In-Memory Database, Tailwind, Client-Side Components, Server-Side Code\n","Saved row 26 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 27: Learn Prompt Engineering from Scratch: Master the Art of Text Generation - Introduction\n","================================================================================\n","\n","SUMMARY:\n"," Prompt engineering is a specialized field in Natural Language Processing (NLP) that focuses on building models to generate high-quality text outputs in response to prompts or input. Leveraging pre-trained large language models, such as GPT or Transformers, fine-tuned for specific tasks and inputs, it enables the generation of accurate, coherent, and contextually appropriate text outputs. This is essential for applications like chatbots, language translation, and content generation. However, prompt engineering models may struggle with complex prompts or generate biased outputs due to underlying data or model architecture. This comprehensive introduction covers basics, advanced techniques, and fine-tuning pre-trained models, highlighting the importance of prompt engineering and its applications in NLP.\n","\n","TOPICS:\n"," ['Prompt Engineering', 'Natural Language Processing', 'Artificial Intelligence']\n","\n","Q&A:\n"," Q: What is prompt engineering?\n","A: Prompt engineering is a specialized field within natural language processing that focuses on building models that can generate high quality text outputs in response to the prompts or our input.\n","Q: Why is prompt engineering important?\n","A: The key benefit of prompt engineering is that it allows us to generate text outputs that are more accurate, coherent and contextually appropriate than traditional rule based or keyword based approaches.\n","Q: How are prompt engineering models based?\n","A: These models are basically based on pre-trained large language models such as Open AI (AI) GPT, Google BERT or Hugging Face Transformers that are fine-tuned for specific tasks and inputs.\n","Q: What can prompt engineering models struggle with?\n","A: Prompt engineering models may struggle with the complex and ambiguous prompts or they may generate outputs that are biased and inaccurate due to underlying data or model architecture.\n","Q: What can you expect to learn in the first part of this course?\n","A: We'll introduce you to the prompt engineering and explain why it is important field for Natural Language Processing (NLP) and cover the basics of prompt analysis.\n","\n","KEY CONCEPTS:\n"," Prompt Engineering, Natural Language Processing (NLP), Large Language Models, Pre-trained Models, Fine-tuning, Chatbots, Virtual Assistants, Translation Software, Text Generation, Bias and Inaccuracy, Ambiguous Prompts, Agentic AI\n","Saved row 27 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 28: Q-learning - Explained!\n","================================================================================\n","\n","SUMMARY:\n"," Q-learning is a value-based reinforcement learning method that determines an optimal policy by learning a value function, known as the Q-value, which quantifies the total reward. The Q-value is a real number that represents the expected return when taking an action in a given state. Q-learning iteratively updates the Q-values using the Bellman equation, which defines a recursive relationship between Q-values. The agent starts in an initial state, takes an action based on an exploration policy, transitions to a new state, and receives a reward. The observed Q-value is calculated using the Bellman equation, and the Q-values are updated accordingly. The process is repeated until the agent learns an optimal policy that maximizes the total reward. Q-learning is an off-policy reinforcement learning algorithm that learns the optimal policy by updating a Q-table based on the temporal difference error. The update rule involves calculating the expected Q-value and adding a learning rate times the temporal difference error. This process is repeated over multiple episodes, allowing the agent to take actions that maximize the Q-value and achieve the target policy. The Q-values in the table become more stable over time, enabling effective learning and decoupling the behavior policy from the target policy.\n","\n","TOPICS:\n"," ['Reinforcement Learning', 'Deep Learning', 'Python Programming']\n","\n","Q&A:\n"," Q: What are the three primary machine learning paradigms?\n","A: The three primary machine learning paradigms are supervised learning, unsupervised learning, and reinforcement learning.\n","Q: What is the primary objective of reinforcement learning?\n","A: Reinforcement learning is learning what to do, that is, how to map situations to actions so as to maximize a numerical reward signal.\n","Q: What are the two types of value-based reinforcement learning methods?\n","A: The two types of value-based reinforcement learning methods are value-based methods and policy-based methods.\n","Q: What is the goal of Q-learning?\n","A: The goal of Q-learning is to effectively learn these Q values such that the total reward is maximized.\n","Q: What is the Bellman equation used for in Q-learning?\n","A: The Bellman equation defines a recursive relationship between Q values.\n","\n","KEY CONCEPTS:\n"," Q-learning, Reinforcement Learning, Value-Based Methods, Policy-Based Methods, Value Function, State Value Function (V), State-Action Value Function (Q), Q-Value, State, Action, Bellman Equation, Exploration Policy, Behavior Policy\n","Saved row 28 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","================================================================================\n","PROCESSING ROW 29: Training Your Logistic Classifier\n","================================================================================\n","\n","SUMMARY:\n"," A logistic classifier is a linear classifier that applies a linear function to input data to generate predictions. This function is a matrix multiply of the inputs (X) and weights (W), with a biased term (b). The weights and bias are trained to optimize predictions. To perform classification, scores are transformed into probabilities using a softmax function (S), which ensures probabilities sum to 1 and are large for high scores and small for low scores. This process is crucial in logistic regression, where scores are often referred to as logits, with the goal of obtaining probabilities close to 1 for the correct class and close to 0 for other classes.\n","\n","TOPICS:\n"," ['Machine Learning', 'Deep Learning', 'Statistics']\n","\n","Q&A:\n"," Q: What is a logistic classifier?\n","A: A logistic classifier is what's called the linear classifier.\n","Q: Why do we use a linear function in logistic regression?\n","A: It takes the input, for example, the pixels in an image, and applies a linear function to them to generate its predictions.\n","Q: How do we generate predictions in logistic regression?\n","A: It take all the inputs as a big vector, that will denote X, and multiplies them with a matrix to generate its predictions, one per output class.\n","Q: When do we use the softmax function in logistic regression?\n","A: We're going to want the probability of the correct class to be very close to one and the probability for every other class to be close to zero.\n","Q: What is the purpose of the softmax function in logistic regression?\n","A: The way to turn scores into probabilities is to use a softmax function, which I'll denote here by S.\n","\n","KEY CONCEPTS:\n"," Logistic Classifier, Linear Classifier, Matrix Multiply, Logits, Softmax Function, Probabilities, Machine Learning, Weights, Bias, Vector, Matrix, Logistic Regression\n","Saved row 29 → /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","DONE. Final file saved: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n","\n","Role-based generation pipeline (llama-3.3-70b-versatile, Groq) completed (NO evaluation).\n"]}]},{"cell_type":"code","source":["#####################################################################\n","# 1. IMPORTS\n","#####################################################################\n","import os, re, json, warnings\n","import pandas as pd\n","import numpy as np\n","\n","from rouge_score import rouge_scorer\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from bert_score import score as bert_score\n","\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","\n","#####################################################################\n","# 2. SUPPRESS WARNINGS (BERTScore spam)\n","#####################################################################\n","warnings.filterwarnings(\"ignore\")\n","import logging\n","logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n","logging.getLogger(\"absl\").setLevel(logging.ERROR)\n","\n","\n","#####################################################################\n","# 3. PATHS (EDIT THESE)\n","#####################################################################\n","INPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\"\n","OUTPUT_FILE = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\"\n","FINAL_EVAL_JSON = \"/content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/evaluation_final.json\"\n","\n","print(\"Loaded input:\", INPUT_FILE)\n","print(\"Loaded model output:\", OUTPUT_FILE)\n","\n","\n","#####################################################################\n","# 4. GOLD TOPIC EXTRACTION (KEYWORD-BASED — FINAL VERSION)\n","#####################################################################\n","def gold_topics_from_ref_summary(ref_sum: str):\n","    text = (ref_sum or \"\").lower()\n","    matched = []\n","\n","    rules = [\n","        (\"Natural Language Processing\", [\n","            \"nlp\", \"bert\", \"transformer\", \"language model\", \"token\",\n","            \"text processing\", \"semantic\", \"embedding\"\n","        ]),\n","        (\"Artificial Intelligence\", [\n","            \"artificial intelligence\", \"ai system\", \"symbolic ai\",\n","            \"reasoning\", \"planning\", \"search\"\n","        ]),\n","        (\"Prompt Engineering\", [\n","            \"prompt\", \"few-shot\", \"zero-shot\", \"instruction\",\n","            \"cot\", \"chain-of-thought\", \"in-context learning\"\n","        ]),\n","        (\"Machine Learning\", [\n","            \"machine learning\", \"supervised\", \"unsupervised\", \"regression\",\n","            \"classification\", \"clustering\", \"features\"\n","        ]),\n","        (\"Deep Learning\", [\n","            \"deep learning\", \"neural network\", \"cnn\", \"rnn\",\n","            \"lstm\", \"gan\", \"transformer model\", \"backpropagation\"\n","        ]),\n","        (\"Reinforcement Learning\", [\n","            \"reinforcement\", \"policy gradient\", \"q-learning\",\n","            \"reward\", \"actor-critic\", \"rlhf\"\n","        ]),\n","        (\"Generative AI\", [\n","            \"genai\", \"text generation\", \"image generation\",\n","            \"diffusion\", \"sampling\", \"generation model\", \"llm\"\n","        ]),\n","        (\"Data Science\", [\n","            \"data science\", \"visualization\", \"feature\", \"pandas\",\n","            \"analysis\", \"data preprocessing\", \"eda\"\n","        ]),\n","        (\"Time Series\", [\n","            \"time series\", \"forecasting\", \"temporal\", \"trend\",\n","            \"seasonality\", \"arima\", \"prophet\", \"lag\"\n","        ]),\n","        (\"Statistics\", [\n","            \"statistics\", \"probability\", \"distribution\", \"variance\",\n","            \"hypothesis\", \"confidence interval\", \"p-value\"\n","        ]),\n","        (\"LangChain\", [\n","            \"langchain\", \"chain\", \"memory\", \"retriever\",\n","            \"agent executor\", \"llmchain\", \"prompt template\"\n","        ]),\n","        (\"Langraph\", [\n","            \"langraph\", \"workflow\", \"graph\", \"multi-agent orchestration\",\n","            \"node\", \"edge\", \"state graph\"\n","        ]),\n","        (\"Python Programming\", [\n","            \"python\", \"numpy\", \"matplotlib\", \"function\",\n","            \"loop\", \"list comprehension\", \"script\"\n","        ]),\n","        (\"Mlops\", [\n","            \"mlops\", \"deployment\", \"monitoring\", \"pipeline\",\n","            \"model registry\", \"cicd\", \"serving\"\n","        ]),\n","        (\"Agentic AI\", [\n","            \"agentic\", \"tool calling\", \"multi-agent\",\n","            \"planner\", \"agent\", \"reasoning agent\", \"autonomous\"\n","        ])\n","    ]\n","\n","    for label, keywords in rules:\n","        if any(kw in text for kw in keywords):\n","            matched.append(label)\n","\n","    return matched or [\"Other\"]\n","\n","\n","#####################################################################\n","# 5. TOKENIZER FOR QA & CONCEPTS\n","#####################################################################\n","STOPWORDS = set([\n","    \"the\",\"a\",\"an\",\"in\",\"on\",\"for\",\"to\",\"and\",\"or\",\"of\",\"with\",\"as\",\n","    \"by\",\"at\",\"from\",\"that\",\"this\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\n","    \"it\",\"its\",\"into\",\"about\",\"over\",\"under\",\"between\",\"across\",\n","    \"through\",\"their\",\"they\",\"you\",\"your\",\"we\",\"our\"\n","])\n","\n","def tokenize(text: str):\n","    return [\n","        t for t in re.findall(r\"[A-Za-z][A-Za-z0-9\\-_\\’']+\", text.lower())\n","        if t not in STOPWORDS\n","    ]\n","\n","\n","#####################################################################\n","# 6. FINAL EVALUATION FUNCTION  (FULL AND CORRECT)\n","#####################################################################\n","def evaluate(df_out: pd.DataFrame, df_ref: pd.DataFrame):\n","\n","    rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n","    smooth = SmoothingFunction().method1\n","\n","    sum_r, sum_b, sum_bert = [], [], []\n","    overlap_acc_list, jaccard_list, micro_f1_list = [], [], []\n","    macro_f1_list, weighted_f1_list = [], []\n","    qa_bleu, qa_div, qa_ans = [], [], []\n","    kc_p, kc_r, kc_f = [], [], []\n","\n","    VALID_TOPICS = [\n","        \"Natural Language Processing\", \"Artificial Intelligence\", \"Prompt Engineering\",\n","        \"Machine Learning\", \"Deep Learning\", \"Reinforcement Learning\", \"Generative AI\",\n","        \"Data Science\", \"Time Series\", \"Statistics\", \"LangChain\", \"Langraph\",\n","        \"Python Programming\", \"Mlops\", \"Agentic AI\", \"Other\"\n","    ]\n","\n","    # for macro/weighted F1\n","    all_true, all_pred = [], []\n","\n","    for _, row in df_out.iterrows():\n","        idx = int(row[\"row_index\"])\n","        ref_summary = df_ref.loc[idx, \"Reference Summary\"] or \"\"\n","\n","        # -------------------- Summarisation --------------------\n","        gen_sum = row[\"summary\"] or \"\"\n","        r = rouge.score(ref_summary, gen_sum)['rougeL'].fmeasure\n","        b = sentence_bleu([ref_summary.split()], gen_sum.split(), smoothing_function=smooth)\n","\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            P, R, F1 = bert_score([gen_sum], [ref_summary], lang='en', verbose=False)\n","\n","        sum_r.append(r)\n","        sum_b.append(b)\n","        sum_bert.append(float(F1.mean()))\n","\n","        # -------------------- Topic Classification --------------------\n","        gold = gold_topics_from_ref_summary(ref_summary)\n","        pred = [x.strip() for x in (row[\"topic_classification\"] or \"\").split(\",\") if x.strip()]\n","\n","        set_pred = set(pred)\n","        set_gold = set(gold)\n","\n","        # Overlap Accuracy (your metric)\n","        overlap_acc = 1.0 if len(set_pred & set_gold) > 0 else 0.0\n","\n","        # Jaccard\n","        inter = len(set_pred & set_gold)\n","        union = len(set_pred | set_gold)\n","        jaccard = inter / union if union > 0 else 0.0\n","\n","        # Micro-F1\n","        tp = inter\n","        fp = len([p for p in pred if p not in gold])\n","        fn = len([g for g in gold if g not in pred])\n","\n","        prec = tp / (tp + fp) if (tp + fp) else 0.0\n","        rec  = tp / (tp + fn) if (tp + fn) else 0.0\n","        micro_f1 = (2 * prec * rec / (prec + rec)) if (prec + rec) else 0.0\n","\n","        overlap_acc_list.append(overlap_acc)\n","        jaccard_list.append(jaccard)\n","        micro_f1_list.append(micro_f1)\n","\n","        # Macro/Weighted F1 prep\n","        true_bin = [1 if t in gold else 0 for t in VALID_TOPICS]\n","        pred_bin = [1 if t in pred else 0 for t in VALID_TOPICS]\n","\n","        all_true.append(true_bin)\n","        all_pred.append(pred_bin)\n","\n","        # -------------------- Q&A --------------------\n","        qa_text = row[\"Q_and_A\"] or \"\"\n","        qs = [l[2:].strip() for l in qa_text.splitlines() if l.lower().startswith(\"q:\")]\n","\n","        gold_qs = [\n","            \"What is the main topic discussed in the video?\",\n","            \"Why is this topic important?\",\n","            \"How is the core concept explained?\",\n","            \"What example is mentioned in the content?\",\n","            \"What is the key conclusion of the video?\"\n","        ]\n","\n","        if qs:\n","            bleu_vals = [\n","                sentence_bleu([g.split()], q.split(), smoothing_function=smooth)\n","                for g in gold_qs for q in qs\n","            ]\n","            qa_bleu.append(np.mean(bleu_vals))\n","        else:\n","            qa_bleu.append(0.0)\n","\n","        toks = [t for q in qs for t in q.split()]\n","        qa_div.append(len(set(toks)) / len(toks) if toks else 0.0)\n","\n","        ref_tokens = set(tokenize(ref_summary))\n","        ans_count = sum(\n","            1 for q in qs\n","            if len(set(tokenize(q)) & ref_tokens) / max(1, len(tokenize(q))) >= 0.3\n","        )\n","        qa_ans.append(ans_count / len(qs) if qs else 0.0)\n","\n","        # -------------------- Key Concepts --------------------\n","        kc_text = str(row.get(\"key_concepts\", \"\") or \"\")\n","        pred_concepts = [c.strip().lower() for c in kc_text.split(\",\") if c.strip()]\n","\n","        ref_concepts = tokenize(ref_summary)\n","        ref_top = ref_concepts[:25]\n","\n","        tp_kc = len([p for p in pred_concepts[:10] if any(p in r or r in p for r in ref_top)])\n","\n","        p_val = tp_kc / 10\n","        r_val = tp_kc / len(ref_top) if ref_top else 0\n","        f1_val = (2*p_val*r_val/(p_val+r_val)) if (p_val+r_val) else 0\n","\n","        kc_p.append(p_val)\n","        kc_r.append(r_val)\n","        kc_f.append(f1_val)\n","\n","    # Compute macro/weighted F1\n","    all_true = np.array(all_true)\n","    all_pred = np.array(all_pred)\n","\n","    macro_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"macro\", zero_division=0)[2]\n","    weighted_f1 = precision_recall_fscore_support(all_true, all_pred, average=\"weighted\", zero_division=0)[2]\n","\n","    return {\n","        \"Summarisation\": {\n","            \"ROUGE-L F1\": float(np.mean(sum_r)),\n","            \"BLEU\": float(np.mean(sum_b)),\n","            \"BERTScore F1\": float(np.mean(sum_bert))\n","        },\n","        \"Topic Classification\": {\n","            \"Overlap Accuracy\": float(np.mean(overlap_acc_list)),\n","            \"Jaccard Index\": float(np.mean(jaccard_list)),\n","            \"Micro F1\": float(np.mean(micro_f1_list)),\n","            \"Macro F1\": float(macro_f1),\n","            \"Weighted F1\": float(weighted_f1)\n","        },\n","        \"Q&A Generation\": {\n","            \"BLEU\": float(np.mean(qa_bleu)),\n","            \"Diversity\": float(np.mean(qa_div)),\n","            \"Answerability\": float(np.mean(qa_ans))\n","        },\n","        \"Key Concept Extraction\": {\n","            \"Precision@10\": float(np.mean(kc_p)),\n","            \"Recall@10\": float(np.mean(kc_r)),\n","            \"F1@10\": float(np.mean(kc_f))\n","        }\n","    }\n","\n","\n","#####################################################################\n","# 7. RUN EVALUATION\n","#####################################################################\n","df_ref = pd.read_excel(INPUT_FILE)\n","df_out = pd.read_excel(OUTPUT_FILE)\n","\n","eval_summary = evaluate(df_out, df_ref)\n","\n","print(\"\\n==================== FINAL EVALUATION METRICS ====================\")\n","for task, vals in eval_summary.items():\n","    print(f\"\\n{task}:\")\n","    for metric, value in vals.items():\n","        print(f\"  - {metric}: {value:.4f}\")\n","\n","with open(FINAL_EVAL_JSON, \"w\") as f:\n","    json.dump(eval_summary, f, indent=2)\n","\n","print(\"\\nSaved corrected evaluation JSON to:\", FINAL_EVAL_JSON)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":695,"referenced_widgets":["ccce3e56390042bcad2151d453f195e7","2297fcb9a93a4d708fdb6408d47ab762","18f2c61341f3436991409b6d373aab18","9228b99eafd34ebb8ca7cde9116aeadc","ecaa2f752f8c4418b782d71e0d6070ec","96ed93e1eddc45b18f184a1b1edeffdb","b2c4dbf0015147e2b725dc625db98532","cbc3736d61a5447b88d64bb611bb2823","4381fd49987c4b35a1b33b7af241527d","9d21de52000c49e2af8a94cabaf1daab","ad10470bd8e74c51958d1084eede6516","3dc6b52d41b14a97821e8e655303febe","6e3c6752f08d42bcb3b78c2d52b42bd8","9959d33b9d224ee1821fbe62094af5cb","835d051169dd4d8e9f750239a73469a8","ea15d67520104c5ea0402bebea415c1c","2d4a772bfc3648578eb34589104c02c1","cc92916b320246dd92f3ac34d5369019","480fdbbc51334f61aa5c270471128d48","c91866b550ed495aab462222bfaa1c12","576f8b11680e49c999e14321ff860082","35062c0465794ad7957b78e9f2abb90d","33e1a386f92e44daae40a720beab9996","2385f49497174a20b1b4d59d7bcabeb7","e10cf33b7c5f455296ade032a87a80bd","4ff29752e848492b8988b13380238adf","9fe4e9c169a04259a46ce8238c415cfd","c6dde1a699864d7eb76ee37202803f90","b8eadf912e794dad8c47534f0d71fc9a","479b607a3df945d1bd65a7562520abb3","eed8510ca0924d00b7e58d17bed297e4","e36acc9979a24f6b908a6b1a5f5aed66","ea0a6641a088415b810ea48fc610f68c","b02f7a94f748463f9f6117be71b727a5","2a5614002710483fa2a6375251c23202","879ac44acd8a4980b2bf806f656982f1","fe6860de73354fa3a9a8e136dbc90e59","092b3730fbd7470ebe5d8acbda428aad","c714c9b865ad452baf6c6287d4f11498","06c3de536d204dfba3145973ec36f2fa","4017180746874b9fb99319f842d32d11","134a11ec686c4291a9aef7e63c50a48f","9818ce7424dd4742a8c0a93b51d99a3c","1500de5f528643eab3c57d07db63720b","734fb28f406c4b9d9d8acfb748400c43","6fa223292d92431993d8c07da9dbcaee","a68b18899db445e297e6eb6f5af8055c","802f51647daf4da6a4a2ab82441df731","c66912e61dec4b099b139f32c01b8992","7696b65d056b401980f7d0485c34d034","056a933926f145338cc17fdbd78d8e51","9b642940d9e94eb9a08340682b4cc368","8ef989cf672640cdaa256f08a91d7e87","8d04f729b90e44a9ba564355874952e0","6499f041cd1b458ea63ffdfba05fe6e1","2cb43ed848fa41fd9bc070edebd44265","1aba318813764469ac4fb83dbc78860f","bef1429818b8458196478a57d1f0ddb3","f6c81bf961f54015b83f7f98b23fefb2","19b8bfd84b7543c7b2a5e79904b9f984","52c5751c30c34bb5b387af03883ead9e","e2be8869f2904bc8a662203d0753aed5","f9ed84c751e64f6bb3d695844dfde89b","d78b4b63ae624f86800bca9d0e785967","c1c6c71c72db487cbaf3933d39139d80","4ef4ad1ebdb7442da83c79b387f4f17d"]},"id":"jnJtmvo7Iewv","executionInfo":{"status":"ok","timestamp":1763749695587,"user_tz":-330,"elapsed":163635,"user":{"displayName":"Sarah Smruthi","userId":"05354743683624481075"}},"outputId":"16a3c9b3-7f57-4910-9107-2f4ec33dd57b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded input: /content/drive/MyDrive/Final Thesis Code/Input/clean_input_30.xlsx\n","Loaded model output: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/llama-3.1-8b-instant_role_full_output.xlsx\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccce3e56390042bcad2151d453f195e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3dc6b52d41b14a97821e8e655303febe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e1a386f92e44daae40a720beab9996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b02f7a94f748463f9f6117be71b727a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"734fb28f406c4b9d9d8acfb748400c43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb43ed848fa41fd9bc070edebd44265"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","==================== FINAL EVALUATION METRICS ====================\n","\n","Summarisation:\n","  - ROUGE-L F1: 0.2966\n","  - BLEU: 0.0633\n","  - BERTScore F1: 0.8875\n","\n","Topic Classification:\n","  - Overlap Accuracy: 0.8000\n","  - Jaccard Index: 0.2748\n","  - Micro F1: 0.3882\n","  - Macro F1: 0.3446\n","  - Weighted F1: 0.3565\n","\n","Q&A Generation:\n","  - BLEU: 0.0347\n","  - Diversity: 0.6535\n","  - Answerability: 0.7200\n","\n","Key Concept Extraction:\n","  - Precision@10: 0.5200\n","  - Recall@10: 0.2080\n","  - F1@10: 0.2971\n","\n","Saved corrected evaluation JSON to: /content/drive/MyDrive/Final Thesis Code/Output/Role Prompting/llama-3.1-8b-instant/evaluation_final.json\n"]}]}]}